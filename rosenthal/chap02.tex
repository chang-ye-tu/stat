\chapter{Random Variables and Distributions}
\label{ch:2}

\section*{Chapter Outline}
\begin{itemize}
\item Section 1 \quad Random Variables
\item Section 2 \quad Distributions of Random Variables
\item Section 3 \quad Discrete Distributions
\item Section 4 \quad Continuous Distributions
\item Section 5 \quad Cumulative Distribution Functions
\item Section 6 \quad One-Dimensional Change of Variable
\item Section 7 \quad Joint Distributions
\item Section 8 \quad Conditioning and Independence
\item Section 9 \quad Multidimensional Change of Variable
\item Section 10 \quad Simulating Probability Distributions
\item Section 11 \quad Further Proofs (Advanced)
\end{itemize}

In Chapter~\ref{ch:1}, we discussed the probability model as the central object of study in the theory of probability. This required defining a probability measure $\prb$ on a class of subsets of the sample space $S$. It turns out that there are simpler ways of presenting a particular probability assignment than this---ways that are much more convenient to work with than $\prb$. This chapter is concerned with the definitions of random variables, distribution functions, probability functions, density functions, and the development of the concepts necessary for carrying out calculations for a probability model using these entities. This chapter also discusses the concept of the conditional distribution of one random variable, given the values of others. Conditional distributions of random variables provide the framework for discussing what it means to say that variables are related, which is important in many applications of probability and statistics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Random Variables}
\label{sec:2.1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The previous chapter explained how to construct probability models, including a sample space $S$ and a probability measure $\prb$. Once we have a probability model, we may define random variables for that probability model.

Intuitively, a random variable assigns a numerical value to each possible outcome in the sample space. For example, if the sample space is $\{\text{rain}, \text{snow}, \text{clear}\}$, then we might define a random variable $X$ such that $X = 3$ if it rains, $X = 6$ if it snows, and $X = 2.7$ if it is clear.

More formally, we have the following definition.

\begin{definition}
\label{def:2.1.1}
A \emph{random variable} is a function from the sample space $S$ to the set $\mathbf{R}^1$ of all real numbers.
\end{definition}

Figure~\ref{fig:2.1.1} provides a graphical representation of a random variable $X$ taking a response value $s \in S$ into a real number $X(s) \in \mathbf{R}^1$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-1-1.pdf}
\caption{A random variable $X$ as a function on the sample space $S$ and taking values in $\mathbf{R}^1$.}
\label{fig:2.1.1}
\end{figure}

\begin{example}[A Very Simple Random Variable]
\label{ex:2.1.1}
The random variable described above could be written formally as $X : \{\text{rain}, \text{snow}, \text{clear}\} \to \mathbf{R}^1$ by $X(\text{rain}) = 3$, $X(\text{snow}) = 6$, and $X(\text{clear}) = 2.7$. We will return to this example below.
\end{example}

We now present several further examples. The point is, we can define random variables any way we like, as long as they are functions from the sample space to $\mathbf{R}^1$.

\begin{example}
\label{ex:2.1.2}
For the case $S = \{\text{rain}, \text{snow}, \text{clear}\}$, we might define a second random variable $Y$ by saying that $Y = 0$ if it rains, $Y = 1/2$ if it snows, and $Y = 7/8$ if it is clear. That is, $Y(\text{rain}) = 0$, $Y(\text{snow}) = 1/2$, and $Y(\text{clear}) = 7/8$.
\end{example}

\begin{example}
\label{ex:2.1.3}
If the sample space corresponds to flipping three different coins, then we could let $X$ be the total number of heads showing, let $Y$ be the total number of tails showing, let $Z = 0$ if there is exactly one head, and otherwise $Z = 17$, etc.
\end{example}

\begin{example}
\label{ex:2.1.4}
If the sample space corresponds to rolling two fair dice, then we could let $X$ be the square of the number showing on the first die, let $Y$ be the square of the number showing on the second die, let $Z$ be the sum of the two numbers showing, let $W$ be the square of the sum of the two numbers showing, let $R$ be the sum of the squares of the two numbers showing, etc.
\end{example}

\begin{example}[Constants as Random Variables]
\label{ex:2.1.5}
As a special case, every constant value $c$ is also a random variable, by saying that $c(s) = c$ for all $s \in S$. Thus, $5$ is a random variable, as is $-3$ or $21.6$.
\end{example}

\begin{example}[Indicator Functions]
\label{ex:2.1.6}
One special kind of random variable is worth mentioning. If $A$ is any event, then we can define the \emph{indicator function} of $A$, written $\indc_A$, to be the random variable
\[
\indc_A(s) = \begin{cases} 1 & s \in A \\ 0 & s \notin A \end{cases}
\]
which is equal to $1$ on $A$, and is equal to $0$ on $A^C$.
\end{example}

Given random variables $X$ and $Y$, we can perform the usual arithmetic operations on them. Thus, for example, $Z = X^2$ is another random variable, defined by $Z(s) = X^2(s) = [X(s)]^2 = X(s) \cdot X(s)$. Similarly, if $W = XY^3$, then $W(s) = X(s) \cdot Y(s) \cdot Y(s) \cdot Y(s)$, etc. Also, if $Z = X + Y$, then $Z(s) = X(s) + Y(s)$, etc.

\begin{example}
\label{ex:2.1.7}
Consider rolling a fair six-sided die, so that $S = \{1, 2, 3, 4, 5, 6\}$. Let $X$ be the number showing, so that $X(s) = s$ for $s \in S$. Let $Y$ be three more than the number showing, so that $Y(s) = s + 3$. Let $Z = X^2 + Y$. Then $Z(s) = [X(s)]^2 + Y(s) = s^2 + s + 3$. So $Z(1) = 5$, $Z(2) = 9$, etc.
\end{example}

We write $X \leqslant Y$ to mean that $X(s) \leqslant Y(s)$ for all $s \in S$. Similarly, we write $X \geqslant Y$ to mean that $X(s) \geqslant Y(s)$ for all $s \in S$, and $X = Y$ to mean that $X(s) = Y(s)$ for all $s \in S$. For example, we write $X \leqslant c$ to mean that $X(s) \leqslant c$ for all $s \in S$.

\begin{example}
\label{ex:2.1.8}
Again consider rolling a fair six-sided die, with $S = \{1, 2, 3, 4, 5, 6\}$. For $s \in S$, let $X(s) = s$, and let $Y = X + \indc_{\{6\}}$. This means that
\[
Y(s) = X(s) + \indc_{\{6\}}(s) = \begin{cases} s & s \leqslant 5 \\ 7 & s = 6. \end{cases}
\]
Hence, $Y(s) = X(s)$ for $1 \leqslant s \leqslant 5$. But it is not true that $Y = X$, because $Y(6) \neq X(6)$. On the other hand, it is true that $Y \geqslant X$.
\end{example}

\begin{example}
\label{ex:2.1.9}
For the random variable of Example~\ref{ex:2.1.1} above, it is not true that $X \leqslant 0$, nor is it true that $X \geqslant 0$. However, it is true that $X \geqslant 2.7$ and that $X \leqslant 6$. It is also true that $X \leqslant 10$ and $X \leqslant 100$.
\end{example}

If $S$ is infinite, then a random variable $X$ can take on infinitely many different values.

\begin{example}
\label{ex:2.1.10}
If $S = \{1, 2, 3, \ldots\}$, with $\prb(\{s\}) = 2^{-s}$ for all $s \in S$, and if $X$ is defined by $X(s) = s^2$, then we always have $X \geqslant 1$. But there is no largest value of $X(s)$ because the value $X(s)$ increases without bound as $s \to \infty$. We shall call such a random variable an \emph{unbounded random variable}.
\end{example}

Finally, suppose $X$ is a random variable. We know that different states $s$ occur with different probabilities. It follows that $X(s)$ also takes different values with different probabilities. These probabilities are called the \emph{distribution} of $X$; we consider them next.

\subsection*{Summary of Section~\ref{sec:2.1}}

\begin{itemize}
\item A random variable is a function from the state space to the set of real numbers.
\item The function could be constant, or correspond to counting some random quantity that arises, or any other sort of function.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.1.1}
Let $S = \{1, 2, 3, \ldots\}$, and let $X(s) = s^2$ and $Y(s) = 1/s$ for $s \in S$. For each of the following quantities, determine (with explanation) whether or not it exists. If it does exist, then give its value.
\begin{enumerate}[(a)]
\item $\min_{s \in S} X(s)$
\item $\max_{s \in S} X(s)$
\item $\min_{s \in S} Y(s)$
\item $\max_{s \in S} Y(s)$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\min_{s \in S} X(s) = X(1) = 1$ since $X(s) > 1$ for all other $s \in S$.
    \item $\max_{s \in S} X(s)$ does not exist since $\lim_{s \to \infty} X(s) = \infty$ but $X(s) \neq \infty$ for all $s \in S$.
    \item $\min_{s \in S} Y(s)$ does not exist since $\lim_{s \to \infty} Y(s) = 0$ but $Y(s) \neq 0$ for all $s \in S$.
    \item $\max_{s \in S} Y(s) = Y(1) = 1$ since $Y(s) < 1$ for all other $s \in S$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.1.2}
Let $S = \{\text{high}, \text{middle}, \text{low}\}$. Define random variables $X$, $Y$, and $Z$ by $X(\text{high}) = 12$, $X(\text{middle}) = 2$, $X(\text{low}) = 3$, $Y(\text{high}) = 0$, $Y(\text{middle}) = 0$, $Y(\text{low}) = 1$, $Z(\text{high}) = 6$, $Z(\text{middle}) = 0$, $Z(\text{low}) = -4$. Determine whether each of the following relations is true or false.
\begin{enumerate}[(a)]
\item $X \geqslant Y$
\item $X \leqslant Y$
\item $Y \geqslant Z$
\item $Y \leqslant Z$
\item $XY \geqslant Z$
\item $XY \leqslant Z$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item No, since $X(\text{low}) > Y(\text{low})$.
    \item No, since $X(\text{low}) > Y(\text{low})$.
    \item No, since $Y(\text{middle}) = Z(\text{middle})$.
    \item Yes, since $Y(s) \leqslant Z(s)$ for all $s \in S$.
    \item No, since $X(\text{middle}) Y(\text{middle}) = Z(\text{middle})$.
    \item Yes, since $X(\text{middle}) Y(\text{middle}) \leqslant Z(\text{middle})$ for all $s \in S$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.1.3}
Let $S = \{1, 2, 3, 4, 5\}$.
\begin{enumerate}[(a)]
\item Define two different (i.e., nonequal) nonconstant random variables, $X$ and $Y$, on $S$.
\item For the random variables $X$ and $Y$ that you have chosen, let $Z = (X + Y)^2$. Compute $Z(s)$ for all $s \in S$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item For example, let $X(s) = s$ and $Y(s) = s^2$ for all $s \in S$.
    \item For the above example, $Z(1) = X(1) + Y(1)^2 = 1 + 1^2 = 2$, $Z(2) = X(2) + Y(2)^2 = 2 + 4^2 = 18$, $Z(3) = X(3) + Y(3)^2 = 3 + 9^2 = 84$, $Z(4) = X(4) + Y(4)^2 = 4 + 16^2 = 260$, and $Z(5) = X(5) + Y(5)^2 = 5 + 25^2 = 630$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.1.4}
Consider rolling a fair six-sided die, so that $S = \{1, 2, 3, 4, 5, 6\}$. Let $X(s) = s$, and $Y(s) = s^3 - 2$. Let $Z = XY$. Compute $Z(s)$ for all $s \in S$.
\end{exercise}

\begin{solution}
Here $Z(1) = X(1) Y(1) = (1)(1^3 + 2) = 3$, $Z(2) = X(2) Y(2) = (2)(2^3 + 2) = 20$, $Z(3) = X(3) Y(3) = (3)(3^3 + 2) = 87$, $Z(4) = X(4) Y(4) = (4)(4^4 + 2) = 1032$, $Z(5) = X(5) Y(5) = (5)(5^5 + 2) = 15{,}635$, and $Z(6) = X(6) Y(6) = (6)(6^6 + 2) = 279{,}948$.
\end{solution}

\begin{exercise}
\label{exer:2.1.5}
Let $A$ and $B$ be events, and let $X = \indc_A + \indc_B$. Is $X$ an indicator function? If yes, then of what event?
\end{exercise}

\begin{solution}
Yes, $X$ is an indicator function of the event $A \cap B$, i.e., $X = \indc_{A \cap B}$.
\end{solution}

\begin{exercise}
\label{exer:2.1.6}
Let $S = \{1, 2, 3, 4\}$, $X = \indc_{\{1,2\}}$, $Y = \indc_{\{2,3\}}$, and $Z = \indc_{\{3,4\}}$. Let $W = X + Y + Z$.
\begin{enumerate}[(a)]
\item Compute $W(1)$.
\item Compute $W(2)$.
\item Compute $W(4)$.
\item Determine whether or not $W \geqslant Z$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item By the definition, $W(1) = X(1) + Y(1) + Z(1) = \indc_{\{1,2\}}(1) + \indc_{\{2,3\}}(1) + \indc_{\{3,4\}}(1) = 1 + 0 + 0 = 1$.
    \item By the definition, $W(2) = X(2) + Y(2) + Z(2) = \indc_{\{1,2\}}(2) + \indc_{\{2,3\}}(2) + \indc_{\{3,4\}}(2) = 1 + 1 + 0 = 2$.
    \item By the definition, $W(4) = X(4) + Y(4) + Z(4) = \indc_{\{1,2\}}(4) + \indc_{\{2,3\}}(4) + \indc_{\{3,4\}}(4) = 0 + 0 + 1 = 1$.
    \item Note that $\indc_A \geqslant 0$. Thus, $\indc_A(s) \geqslant 0$ for all $s \in S$. Then, $W(s) = X(s) + Y(s) + Z(s) = \indc_{\{1,2\}}(s) + \indc_{\{2,3\}}(s) + Z(s) \geqslant Z(s)$ for all $s \in S$. Therefore, $W \geqslant Z$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.1.7}
Let $S = \{1, 2, 3\}$, $X = \indc_{\{1\}}$, $Y = \indc_{\{2,3\}}$, and $Z = \indc_{\{1,2\}}$. Let $W = X + Y + Z$.
\begin{enumerate}[(a)]
\item Compute $W(1)$.
\item Compute $W(2)$.
\item Compute $W(3)$.
\item Determine whether or not $W \geqslant Z$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item By definition, $W(1) = X(1) - Y(1) + Z(1) = \indc_{\{1,2\}}(1) - \indc_{\{2,3\}}(1) + \indc_{\{3,4\}}(1) = 1 - 0 + 0 = 1$.
    \item By definition, $W(2) = X(2) - Y(2) + Z(2) = \indc_{\{1,2\}}(2) - \indc_{\{2,3\}}(2) + \indc_{\{3,4\}}(2) = 1 - 1 + 0 = 0$.
    \item By definition, $W(3) = X(3) - Y(3) + Z(3) = \indc_{\{1,2\}}(3) - \indc_{\{2,3\}}(3) + \indc_{\{3,4\}}(3) = 0 - 1 + 1 = 0$.
    \item In (c), $W(3) = 0$ but $Z(3) = 1$. Hence $W \geqslant Z$ is not true.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.1.8}
Let $S = \{1, 2, 3, 4, 5\}$, $X = \indc_{\{1,2,3\}}$, $Y = \indc_{\{2,3\}}$, and $Z = \indc_{\{3,4,5\}}$. Let $W = X + Y + Z$.
\begin{enumerate}[(a)]
\item Compute $W(1)$.
\item Compute $W(2)$.
\item Compute $W(5)$.
\item Determine whether or not $W \geqslant Z$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item By definition $W(1) = X(1) - Y(1) + Z(1) = 1 - 1 + 0 = 0$.
    \item By definition $W(2) = X(2) - Y(2) + Z(2) = 1 - 1 + 0 = 0$.
    \item By definition $W(5) = X(5) - Y(5) + Z(5) = 0 - 0 + 1 = 1$.
    \item Suppose that $A \subset B \subset S$. We will show that $\indc_A - \indc_B = \indc_{A-B}$. For all $s \in A^c$, $\indc_A(s) = \indc_B(s) = \indc_{A-B}(s) = 0$. Hence $\indc_A(s) - \indc_B(s) = 0 = \indc_{A-B}(s)$. For all $s \in B$, $\indc_A(s) = \indc_B(s) = 1$ and $\indc_{A-B}(s) = 0$. Thus, $\indc_A(s) - \indc_B(s) = 0 = \indc_{A-B}(s)$. Finally, for $s \in A - B$, $\indc_A(s) = \indc_{A-B}(s) = 1$ and $\indc_B(s) = 0$. Hence, $\indc_A(s) - \indc_B(s) = 1 = \indc_{A-B}(s)$. Since $\{1, 2\} \subset \{1, 2, 3\}$, we have $X - Y = \indc_{\{1,2,3\}} - \indc_{\{1,2\}} = \indc_{\{3\}} \geqslant 0$. Therefore $W(s) = X(s) - Y(s) + Z(s) \geqslant Z(s)$ for all $s \in S$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.1.9}
Let $S = \{1, 2, 3, 4\}$, $X = \indc_{\{1,2\}}$, and $Y(s) = s^2 + X(s)$.
\begin{enumerate}[(a)]
\item Compute $Y(1)$.
\item Compute $Y(2)$.
\item Compute $Y(4)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item By definition, $Y(1) = 1^2 X(1) = 1$.
    \item By definition, $Y(2) = 2^2 X(2) = 4$.
    \item By definition, $Y(4) = 4^2 X(4) = 0$.
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.1.10}
Let $X$ be a random variable.
\begin{enumerate}[(a)]
\item Is it necessarily true that $X \geqslant 0$?
\item Is it necessarily true that there is some real number $c$ such that $X - c \geqslant 0$?
\item Suppose the sample space $S$ is finite. Then is it necessarily true that there is some real number $c$ such that $X - c \geqslant 0$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item No, we could have $X(s) < 0$ for some $s \in S$.
    \item No, if $S$ is infinite, then it could be that for all $c$ there is some $s \in S$ with $X(s) < c$, so that $X(s) + c < 0$.
    \item Yes, if $S$ is finite, then we can take $c = -\min_{s \in S} X(s) < \infty$ and then $X(s) + c \geqslant 0$ for all $s \in S$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.1.11}
Suppose the sample space $S$ is finite. Is it possible to define an unbounded random variable on $S$? Why or why not?
\end{exercise}

\begin{solution}
No, if $S$ is finite, then $\max_{s \in S} |X(s)|$ must be finite, so $X$ must be bounded.
\end{solution}

\begin{exercise}
\label{exer:2.1.12}
Suppose $X$ is a random variable that takes only the values $0$ or $1$. Must $X$ be an indicator function? Explain.
\end{exercise}

\begin{solution}
Yes, then $X = \indc_A$, where $A = \{s \in S : X(s) = 1\}$.
\end{solution}

\begin{exercise}
\label{exer:2.1.13}
Suppose the sample space $S$ is finite, of size $m$. How many different indicator functions can be defined on $S$?
\end{exercise}

\begin{solution}
If $|S| = m$, then the number of subsets of $S$ is $2^m$ (since each $s \in S$ can be either included or not). Since subsets are in one-to-one correspondence with indicator functions, this means there are $2^m$ indicator functions as well.
\end{solution}

\begin{exercise}
\label{exer:2.1.14}
Suppose $X$ is a random variable. Let $Y = |X|$. Must $Y$ be a random variable? Explain.
\end{exercise}

\begin{solution}
No, if $X(s) < 0$ for some $s \in S$, then $Y(s) = \sqrt{X(s)}$ is undefined (or, at least, not a real number), so $Y$ is not a random variable.
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:2.1.15}
Mathematical probability theory was introduced to the English-speaking world largely by two American mathematicians, William Feller and Joe Doob, writing in the early 1950s. According to Professor Doob, the two of them had an argument about whether random variables should be called ``random variables'' or ``chance variables.'' They decided by flipping a coin---and ``random variables'' won. (Source: \emph{Statistical Science} 12 (1997), No.\ 4, page 307.) Which name do you think would have been a better choice?
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distributions of Random Variables}
\label{sec:2.2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Because random variables are defined to be functions of the outcome $s$, and because the outcome $s$ is assumed to be random (i.e., to take on different values with different probabilities), it follows that the value of a random variable will itself be random (as the name implies).

Specifically, if $X$ is a random variable, then what is the probability that $X$ will equal some particular value $x$? Well, $X = x$ precisely when the outcome $s$ is chosen such that $X(s) = x$.

\begin{example}
\label{ex:2.2.1}
Let us again consider the random variable of Example~\ref{ex:2.1.1}, where $S = \{\text{rain}, \text{snow}, \text{clear}\}$, and $X$ is defined by $X(\text{rain}) = 3$, $X(\text{snow}) = 6$, and $X(\text{clear}) = 2.7$. Suppose further that the probability measure $\prb$ is such that $\prb(\{\text{rain}\}) = 0.4$, $\prb(\{\text{snow}\}) = 0.15$, and $\prb(\{\text{clear}\}) = 0.45$. Then clearly, $X = 3$ only when it rains, $X = 6$ only when it snows, and $X = 2.7$ only when it is clear. Thus, $\prb(X = 3) = \prb(\{\text{rain}\}) = 0.4$, $\prb(X = 6) = \prb(\{\text{snow}\}) = 0.15$, and $\prb(X = 2.7) = \prb(\{\text{clear}\}) = 0.45$. Also, $\prb(X = 17) = 0$, and in fact $\prb(X = x) = \prb(\emptyset) = 0$ for all $x \notin \{3, 6, 2.7\}$. We can also compute that
\[
\prb(X \in \{3, 6\}) = \prb(X = 3) + \prb(X = 6) = 0.4 + 0.15 = 0.55,
\]
while
\[
\prb(X \leqslant 5) = \prb(X = 3) + \prb(X = 2.7) = 0.4 + 0.45 = 0.85,
\]
etc.
\end{example}

We see from this example that, if $B$ is any subset of the real numbers, then $\prb(X \in B) = \prb(\{s \in S : X(s) \in B\})$. Furthermore, to understand $X$ well requires knowing the probabilities $\prb(X \in B)$ for different subsets $B$. That is the motivation for the following definition.

\begin{definition}
\label{def:2.2.1}
If $X$ is a random variable, then the \emph{distribution} of $X$ is the collection of probabilities $\prb(X \in B)$ for all subsets $B$ of the real numbers.
\end{definition}

Strictly speaking, it is required that $B$ be a \emph{Borel subset}, which is a technical restriction from measure theory that need not concern us here. Any subset that we could ever write down is a Borel subset.

In Figure~\ref{fig:2.2.1}, we provide a graphical representation of how we compute the distribution of a random variable $X$. For a set $B$, we must find the elements in $s \in S$ such that $X(s) \in B$. These elements are given by the set $\{s \in S : X(s) \in B\}$. Then we evaluate the probability $\prb(\{s \in S : X(s) \in B\})$. We must do this for every subset $B \subseteq \mathbf{R}^1$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-2-1.pdf}
\caption{If $B = (a, b) \subseteq \mathbf{R}^1$, then $\{s \in S : X(s) \in B\}$ is the set of elements such that $a < X(s) < b$.}
\label{fig:2.2.1}
\end{figure}

\begin{example}[A Very Simple Distribution]
\label{ex:2.2.2}
Consider once again the above random variable, where $S = \{\text{rain}, \text{snow}, \text{clear}\}$ and where $X$ is defined by $X(\text{rain}) = 3$, $X(\text{snow}) = 6$, and $X(\text{clear}) = 2.7$, and $\prb(\{\text{rain}\}) = 0.4$, $\prb(\{\text{snow}\}) = 0.15$, and $\prb(\{\text{clear}\}) = 0.45$. What is the distribution of $X$? Well, if $B$ is any subset of the real numbers, then $\prb(X \in B)$ should count $0.4$ if $3 \in B$, plus $0.15$ if $6 \in B$, plus $0.45$ if $2.7 \in B$. We can formally write all this information at once by saying that
\[
\prb(X \in B) = 0.4 \, \indc_B(3) + 0.15 \, \indc_B(6) + 0.45 \, \indc_B(2.7),
\]
where again $\indc_B(x) = 1$ if $x \in B$, and $\indc_B(x) = 0$ if $x \notin B$.
\end{example}

\begin{example}[An Almost-As-Simple Distribution]
\label{ex:2.2.3}
Consider once again the above setting, with $S = \{\text{rain}, \text{snow}, \text{clear}\}$, and $\prb(\{\text{rain}\}) = 0.4$, $\prb(\{\text{snow}\}) = 0.15$, and $\prb(\{\text{clear}\}) = 0.45$. Consider a random variable $Y$ defined by $Y(\text{rain}) = 5$, $Y(\text{snow}) = 7$, and $Y(\text{clear}) = 5$.

What is the distribution of $Y$? Clearly, $Y = 7$ only when it snows, so that $\prb(Y = 7) = \prb(\{\text{snow}\}) = 0.15$. However, here $Y = 5$ if it rains or if it is clear. Hence, $\prb(Y = 5) = \prb(\{\text{rain}, \text{clear}\}) = 0.4 + 0.45 = 0.85$. Therefore, if $B$ is any subset of the real numbers, then
\[
\prb(Y \in B) = 0.15 \, \indc_B(7) + 0.85 \, \indc_B(5).
\]
\end{example}

While the above examples show that it is possible to keep track of $\prb(X \in B)$ for all subsets $B$ of the real numbers, they also indicate that it is rather cumbersome to do so. Fortunately, there are simpler functions available to help us keep track of probability distributions, including cumulative distribution functions, probability functions, and density functions. We discuss these next.

\subsection*{Summary of Section~\ref{sec:2.2}}

\begin{itemize}
\item The distribution of a random variable $X$ is the collection of probabilities $\prb(X \in B)$ of $X$ belonging to various sets.
\item The probability $\prb(X \in B)$ is determined by calculating the probability of the set of response values $s$ such that $X(s) \in B$, i.e., $\prb(X \in B) = \prb(\{s \in S : X(s) \in B\})$.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.2.1}
Consider flipping two independent fair coins. Let $X$ be the number of heads that appear. Compute $\prb(X = x)$ for all real numbers $x$.
\end{exercise}

\begin{solution}
Clearly, $X$ must equal 0, 1, or 2. Also, $X = 0$ if and only if the coins are both tails, which has probability $(1/2)^2 = 1/4$. Similarly, $X = 2$ if and only if the coins are both heads, which also has probability $(1/2)^2 = 1/4$. Hence, $\prb(X = 0) = \prb(X = 2) = 1/4$ and $\prb(X = 1) = 1 - \prb(X = 0) - \prb(X = 2) = 1/2$, with $\prb(X = x) = 0$ for $x \neq 0, 1, 2$.
\end{solution}

\begin{exercise}
\label{exer:2.2.2}
Suppose we flip three fair coins, and let $X$ be the number of heads showing.
\begin{enumerate}[(a)]
\item Compute $\prb(X = x)$ for every real number $x$.
\item Write a formula for $\prb(X \in B)$, for any subset $B$ of the real numbers.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Clearly, $\prb(X = x) = 0$ for $x \neq 0, 1, 2, 3$. For $x \in \{0, 1, 2, 3\}$, there are $\binom{3}{x}$ ways we could end up with $x$ heads, and each has probability $(1/2)^3 = 1/8$. Hence, $\prb(X = x) = \binom{3}{x}/8$ for $x = 0, 1, 2, 3$, so that $\prb(X = 0) = \prb(X = 3) = 1/8$, and $\prb(X = 1) = \prb(X = 2) = 3/8$.
    \item Here $\prb(X \in B) = (1/8)\indc_B(0) + (3/8)\indc_B(1) + (3/8)\indc_B(2) + (1/8)\indc_B(3)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.2.3}
Suppose we roll two fair six-sided dice, and let $Y$ be the sum of the two numbers showing.
\begin{enumerate}[(a)]
\item Compute $\prb(Y = y)$ for every real number $y$.
\item Write a formula for $\prb(Y \in B)$, for any subset $B$ of the real numbers.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Here $\prb(Y = y) = 0$ for $y \neq 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12$. For $y \in \{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\}$, $\prb(Y = y)$ equals the number of ways the two dice can add up to $y$, divided by 36. Thus, $\prb(Y = 2) = 1/36$, $\prb(Y = 3) = 2/36$, $\prb(Y = 4) = 3/36$, $\prb(Y = 5) = 4/36$, $\prb(Y = 6) = 5/36$, $\prb(Y = 7) = 6/36$, $\prb(Y = 8) = 5/36$, $\prb(Y = 9) = 4/36$, $\prb(Y = 10) = 3/36$, $\prb(Y = 11) = 2/36$, and $\prb(Y = 12) = 1/36$.
    \item Here
    \begin{align*}
        \prb(Y \in B) &= (1/36)\indc_B(2) + (2/36)\indc_B(3) + (3/36)\indc_B(4) + (4/36)\indc_B(5) \\
        &\quad + (5/36)\indc_B(6) + (6/36)\indc_B(7) + (5/36)\indc_B(8) + (4/36)\indc_B(9) \\
        &\quad + (3/36)\indc_B(10) + (2/36)\indc_B(11) + (1/36)\indc_B(12).
    \end{align*}
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.2.4}
Suppose we roll one fair six-sided die, and let $Z$ be the number showing. Let $W = Z^3 - 4$, and let $V = \sqrt{Z}$.
\begin{enumerate}[(a)]
\item Compute $\prb(W = \omega)$ for every real number $\omega$.
\item Compute $\prb(V = \nu)$ for every real number $\nu$.
\item Compute $\prb(ZW = x)$ for every real number $x$.
\item Compute $\prb(V + W = y)$ for every real number $y$.
\item Compute $\prb(V - W = r)$ for every real number $r$.
\end{enumerate}
\end{exercise}

\begin{solution}
Here $\prb(Z = z) = 1/6$ for $z = 1, 2, 3, 4, 5, 6$. Hence:
\begin{enumerate}[(a)]
    \item $\prb(W = w) = 1/6$ for $w = 5, 12, 31, 68, 129, 220$, with $\prb(W = w) = 0$ otherwise.
    \item $\prb(V = v) = 1/6$ for $v = 1, \sqrt{2}, \sqrt{3}, 2, \sqrt{5}, \sqrt{6}$, with $\prb(V = v) = 0$ otherwise.
    \item $\prb(ZW = x) = 1/6$ for $x = 5, 24, 93, 272, 645, 1320$, with $\prb(ZW = x) = 0$ otherwise.
    \item $\prb(VW = y) = 1/6$ for $y = 5, 12\sqrt{2}, 31\sqrt{3}, 136, 129\sqrt{5}, 220\sqrt{6}$, with $\prb(VW = y) = 0$ otherwise.
    \item $\prb(V + W = r) = 1/6$ for $r = 6, 12 + \sqrt{2}, 31 + \sqrt{3}, 72, 129 + \sqrt{5}, 220 + \sqrt{6}$, with $\prb(V + W = r) = 0$ otherwise.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.2.5}
Suppose that a bowl contains 100 chips: 30 are labelled 1, 20 are labelled 2, and 50 are labelled 3. The chips are thoroughly mixed, a chip is drawn, and the number $X$ on the chip is noted.
\begin{enumerate}[(a)]
\item Compute $\prb(X = x)$ for every real number $x$.
\item Suppose the first chip is replaced, a second chip is drawn, and the number $Y$ on the chip noted. Compute $\prb(Y = y)$ for every real number $y$.
\item Compute $\prb(W = \omega)$ for every real number $\omega$ when $W = X + Y$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\prb(X = 1) = 0.3$, $\prb(X = 2) = 0.2$, $\prb(X = 3) = 0.5$, and $\prb(X = x) = 0$ for all $x \notin \{1, 2, 3\}$.
    \item $\prb(Y = 1) = 0.3$, $\prb(Y = 2) = 0.2$, $\prb(Y = 3) = 0.5$, and $\prb(Y = y) = 0$ for all $y \notin \{1, 2, 3\}$.
    \item $\prb(W = 2) = (0.3)^2 = 0.09$, $\prb(W = 3) = 2(0.3)(0.2) = 0.12$, $\prb(W = 4) = (0.2)^2 + 2(0.3)(0.5) = 0.34$, $\prb(W = 5) = 2(0.2)(0.5) = 0.2$, $\prb(W = 6) = (0.5)^2 = 0.25$ and $\prb(W = w) = 0$ for all other choices of $w$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.2.6}
Suppose a standard deck of 52 playing cards is thoroughly shuffled and a single card is drawn. Suppose an ace has value 1, a jack has value 11, a queen has value 12, and a king has value 13.
\begin{enumerate}[(a)]
\item Compute $\prb(X = x)$ for every real number $x$ when $X$ is the value of the card drawn.
\item Suppose that $Y = 1, 2, 3$, or $4$ when a diamond, heart, club, or spade is drawn. Compute $\prb(Y = y)$ for every real number $y$.
\item Compute $\prb(W = \omega)$ for every real number $\omega$ when $W = X + Y$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\prb(X = x) = 4/52 = 1/13$ for $x \in \{1, 2, \ldots, 13\}$ and $\prb(X = x) = 0$ otherwise.
    \item $\prb(Y = y) = 13/52 = 1/4$ for $y \in \{1, 2, 3, 4\}$ and $\prb(Y = y) = 0$ otherwise.
    \item $\prb(W = 2) = \prb(X = 1, Y = 1) = 1/52$, $\prb(W = 3) = \prb(X = 1, Y = 2) + \prb(X = 2, Y = 1) = 2/52$, $\prb(W = 4) = 3/52$, $\prb(W = 5) = 4/52$, $\prb(W = 6) = 4/52$, $\prb(W = 7) = 4/52$, $\prb(W = 8) = 4/52$, $\prb(W = 9) = 4/52$, $\prb(W = 10) = 4/52$, $\prb(W = 11) = 4/52$, $\prb(W = 12) = 4/52$, $\prb(W = 13) = 4/52$, and $\prb(W = 14) = 4/52$, $\prb(W = 15) = 3/52$, $\prb(W = 16) = 2/52$, $\prb(W = 17) = 1/52$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.2.7}
Suppose a university is composed of 55\% female students and 45\% male students. A student is selected to complete a questionnaire. There are 25 questions on the questionnaire administered to a male student and 30 questions on the questionnaire administered to a female student. If $X$ denotes the number of questions answered by a randomly selected student, then compute $\prb(X = x)$ for every real number $x$.
\end{exercise}

\begin{solution}
$\prb(X = 25) = 0.45$, $\prb(X = 30) = 0.55$, and $\prb(X = x) = 0$ otherwise.
\end{solution}

\begin{exercise}
\label{exer:2.2.8}
Suppose that a bowl contains 10 chips, each uniquely numbered 0 through 9. The chips are thoroughly mixed, one is drawn and the number on it, $X_1$, is noted. This chip is then replaced in the bowl. A second chip is drawn and the number on it, $X_2$, is noted. Compute $\prb(W = \omega)$ for every real number $\omega$ when $W = X_1 + 10X_2$.
\end{exercise}

\begin{solution}
Note that each number $w \in \{0, 1, \ldots, 99\}$ can occur and
\[
    \prb(W = w) = \prb(X_2 = \lfloor w/10 \rfloor, X_1 = w - 10\lfloor w/10 \rfloor) = (1/10)^2 = 1/100.
\]
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.2.9}
Suppose that a bowl contains 10 chips each uniquely numbered 0 through 9. The chips are thoroughly mixed, one is drawn and the number on it, $X_1$, is noted. This chip is not replaced in the bowl. A second chip is drawn and the number on it, $X_2$, is noted. Compute $\prb(W = \omega)$ for every real number $\omega$ when $W = X_1 + 10X_2$.
\end{exercise}

\begin{solution}
Note that each number $w \in \{0, 1, \ldots, 99\} \cap \{0, 11, 22, \ldots, 99\}$ can occur and so
\[
    \prb(W = w) = \prb(X_2 = \lfloor w/10 \rfloor, X_1 = w - 10\lfloor w/10 \rfloor) = (1/10)(1/9) = 1/90.
\]
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:2.2.10}
Suppose Alice flips three fair coins, and let $X$ be the number of heads showing. Suppose Barbara flips five fair coins, and let $Y$ be the number of heads showing. Let $Z = X + Y$. Compute $\prb(Z = z)$ for every real number $z$.
\end{exercise}

\begin{solution}
Clearly, $\prb(Z = z) = 0$ unless $z \in \{-5, -4, \ldots, 2, 3\}$, in which case
\begin{align*}
    \prb(Z = z) &= \sum_{\substack{0 \leqslant x \leqslant 3, 0 \leqslant y \leqslant 5 \\ x - y = z}} \prb(X = x, Y = y) \\
    &= \sum_{\substack{0 \leqslant x \leqslant 3, 0 \leqslant y \leqslant 5 \\ x - y = z}} \binom{3}{x}(1/2)^3 \binom{5}{y}(1/2)^5 \\
    &= (1/2)^8 \sum_{\substack{0 \leqslant x \leqslant 3, 0 \leqslant y \leqslant 5 \\ x - y = z}} \binom{3}{x}\binom{5}{y}.
\end{align*}
Hence,
\begin{align*}
    \prb(Z = 3) &= (1/2)^8 \binom{3}{3}\binom{5}{0} = 1/2^8 \\
    \prb(Z = 2) &= (1/2)^8 \left[\binom{3}{3}\binom{5}{1} + \binom{3}{2}\binom{5}{0}\right] = 8/2^8 \\
    \prb(Z = 1) &= (1/2)^8 \left[\binom{3}{3}\binom{5}{2} + \binom{3}{2}\binom{5}{1} + \binom{3}{1}\binom{5}{0}\right] = 28/2^8 \\
    \prb(Z = 0) &= (1/2)^8 \left[\binom{3}{3}\binom{5}{3} + \binom{3}{2}\binom{5}{2} + \binom{3}{1}\binom{5}{1} + \binom{3}{0}\binom{5}{0}\right] = 56/2^8 \\
    \prb(Z = -1) &= (1/2)^8 \left[\binom{3}{3}\binom{5}{4} + \binom{3}{2}\binom{5}{3} + \binom{3}{1}\binom{5}{2} + \binom{3}{0}\binom{5}{1}\right] = 70/2^8 \\
    \prb(Z = -2) &= (1/2)^8 \left[\binom{3}{3}\binom{5}{5} + \binom{3}{2}\binom{5}{4} + \binom{3}{1}\binom{5}{3} + \binom{3}{0}\binom{5}{2}\right] = 56/2^8 \\
    \prb(Z = -3) &= (1/2)^8 \left[\binom{3}{2}\binom{5}{5} + \binom{3}{1}\binom{5}{4} + \binom{3}{0}\binom{5}{3}\right] = 28/2^8 \\
    \prb(Z = -4) &= (1/2)^8 \left[\binom{3}{1}\binom{5}{5} + \binom{3}{0}\binom{5}{4}\right] = 8/2^8 \\
    \prb(Z = -5) &= (1/2)^8 \binom{3}{0}\binom{5}{5} = 1/2^8.
\end{align*}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete Distributions}
\label{sec:2.3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For many random variables $X$, we have $\prb(X = x) > 0$ for certain $x$ values. This means there is positive probability that the variable will be equal to certain particular values.

If
\[
\sum_{x \in \mathbf{R}^1} \prb(X = x) = 1,
\]
then all of the probability associated with the random variable $X$ can be found from the probability that $X$ will be equal to certain particular values. This prompts the following definition.

\begin{definition}
\label{def:2.3.1}
A random variable $X$ is \emph{discrete} if
\begin{equation}
\label{eq:2.3.1}
\sum_{x \in \mathbf{R}^1} \prb(X = x) = 1.
\end{equation}
\end{definition}

At first glance one might expect \eqref{eq:2.3.1} to be true for any random variable. However, \eqref{eq:2.3.1} does not hold for the uniform distribution on $[0, 1]$ or for other continuous distributions, as we shall see in the next section.

Random variables satisfying \eqref{eq:2.3.1} are simple in some sense because we can understand them completely just by understanding their probabilities of being equal to particular values $x$. Indeed, by simply listing out all the possible values $x$ such that $\prb(X = x) > 0$, we obtain a second, equivalent definition, as follows.

\begin{definition}
\label{def:2.3.2}
A random variable $X$ is \emph{discrete} if there is a finite or countable sequence $x_1, x_2, \ldots$ of distinct real numbers, and a corresponding sequence $p_1, p_2, \ldots$ of nonnegative real numbers, such that $\prb(X = x_i) = p_i$ for all $i$, and $\sum_i p_i = 1$.
\end{definition}

This second definition also suggests how to keep track of discrete distributions. It prompts the following definition.

\begin{definition}
\label{def:2.3.3}
For a discrete random variable $X$, its \emph{probability function} is the function $p_X : \mathbf{R}^1 \to [0, 1]$ defined by
\[
p_X(x) = \prb(X = x).
\]
Hence, if $x_1, x_2, \ldots$ are the distinct values such that $\prb(X = x_i) = p_i$ for all $i$ with $\sum_i p_i = 1$, then
\[
p_X(x) = \begin{cases} p_i & x = x_i \text{ for some } i \\ 0 & \text{otherwise.} \end{cases}
\]
\end{definition}

Clearly, all the information about the distribution of $X$ is contained in its probability function, but only if we know that $X$ is a discrete random variable.

Finally, we note that Theorem~1.5.1 immediately implies the following.

\begin{theorem}[Law of total probability, discrete random variable version]
\label{thm:2.3.1}
Let $X$ be a discrete random variable, and let $A$ be some event. Then
\[
\prb(A) = \sum_{x \in \mathbf{R}^1} \prb(X = x) \, \prb(A \mid X = x).
\]
\end{theorem}

\subsection{Important Discrete Distributions}
\label{ssec:2.3.1}

Certain particular discrete distributions are so important that we list them here.

\begin{example}[Degenerate Distributions]
\label{ex:2.3.1}
Let $c$ be some fixed real number. Then, as already discussed, $c$ is also a random variable (in fact, $c$ is a constant random variable). In this case, clearly $c$ is discrete, with probability function $p_c$ satisfying that $p_c(c) = 1$, and $p_c(x) = 0$ for $x \neq c$. Because $c$ is always equal to a particular value (namely, $c$) with probability $1$, the distribution of $c$ is sometimes called a \emph{point mass} or \emph{point distribution} or \emph{degenerate distribution}.
\end{example}

\begin{example}[The Bernoulli Distribution]
\label{ex:2.3.2}
Consider flipping a coin that has probability $\theta$ of coming up heads and probability $1 - \theta$ of coming up tails, where $0 < \theta < 1$. Let $X = 1$ if the coin is heads, while $X = 0$ if the coin is tails. Then $p_X(1) = \prb(X = 1) = \theta$, while $p_X(0) = \prb(X = 0) = 1 - \theta$. The random variable $X$ is said to have the \emph{Bernoulli$(\theta)$ distribution}; we write this as $X \sim \text{Bernoulli}(\theta)$.

Bernoulli distributions arise anytime we have a response variable that takes only two possible values, and we label one of these outcomes as $1$ and the other as $0$. For example, $1$ could correspond to success and $0$ to failure of some quality test applied to an item produced in a manufacturing process. In this case, $\theta$ is the proportion of manufactured items that will pass the test. Alternatively, we could be randomly selecting an individual from a population and recording a $1$ when the individual is female and a $0$ if the individual is a male. In this case, $\theta$ is the proportion of females in the population.
\end{example}

\begin{example}[The Binomial Distribution]
\label{ex:2.3.3}
Consider flipping $n$ coins, each of which has (independent) probability $\theta$ of coming up heads, and probability $1 - \theta$ of coming up tails. (Again, $0 < \theta < 1$.) Let $X$ be the total number of heads showing. By (1.4.2), we see that for $x = 0, 1, 2, \ldots, n$,
\[
p_X(x) = \prb(X = x) = \binom{n}{x} \theta^x (1-\theta)^{n-x} = \frac{n!}{x!(n-x)!} \theta^x (1-\theta)^{n-x}.
\]
The random variable $X$ is said to have the \emph{Binomial$(n, \theta)$ distribution}; we write this as $X \sim \text{Binomial}(n, \theta)$. The Bernoulli$(\theta)$ distribution corresponds to the special case of the Binomial$(n, \theta)$ distribution when $n = 1$, namely, $\text{Bernoulli}(\theta) = \text{Binomial}(1, \theta)$.

Figure~\ref{fig:2.3.1} contains the plots of several Binomial$(20, \theta)$ probability functions.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-3-1.pdf}
\caption{Plot of the Binomial$(20, 1/2)$ ($\bullet$) and the Binomial$(20, 1/5)$ ($\circ$) probability functions.}
\label{fig:2.3.1}
\end{figure}

The binomial distribution is applicable to any situation involving $n$ independent performances of a random system; for each performance, we are recording whether a particular event has occurred, called a \emph{success}, or has not occurred, called a \emph{failure}. If we denote the event in question by $A$ and put $\prb(A) = \theta$, we have that the number of successes in the $n$ performances is distributed Binomial$(n, \theta)$. For example, we could be testing light bulbs produced by a manufacturer, and $\theta$ is the probability that a bulb works when we test it. Then the number of bulbs that work in a batch of $n$ is distributed Binomial$(n, \theta)$. If a baseball player has probability $\theta$ of getting a hit when at bat, then the number of hits obtained in $n$ at-bats is distributed Binomial$(n, \theta)$.

There is another way of expressing the binomial distribution that is sometimes useful. For example, if $X_1, X_2, \ldots, X_n$ are chosen independently and each has the Bernoulli$(\theta)$ distribution, and $Y = X_1 + \cdots + X_n$, then $Y$ will have the Binomial$(n, \theta)$ distribution (see Example~\ref{ex:3.4.10} for the details).
\end{example}

\begin{example}[The Geometric Distribution]
\label{ex:2.3.4}
Consider repeatedly flipping a coin that has probability $\theta$ of coming up heads and probability $1 - \theta$ of coming up tails, where again $0 < \theta < 1$. Let $X$ be the number of tails that appear before the first head. Then for $k \geqslant 0$, $X = k$ if and only if the coin shows exactly $k$ tails followed by a head. The probability of this is equal to $(1 - \theta)^k \theta$. (In particular, the probability of getting an infinite number of tails before the first head is equal to $1 - 1 = 0$, so $X$ is never equal to infinity.) Hence, $p_X(k) = (1 - \theta)^k \theta$, for $k = 0, 1, 2, 3, \ldots$. The random variable $X$ is said to have the \emph{Geometric$(\theta)$ distribution}; we write this as $X \sim \text{Geometric}(\theta)$. Figure~\ref{fig:2.3.2} contains the plots of several Geometric$(\theta)$ probability functions.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-3-2.pdf}
\caption{Plot of the Geometric$(1/2)$ ($\bullet$) and the Geometric$(1/5)$ ($\circ$) probability functions at the values $0, 1, \ldots, 15$.}
\label{fig:2.3.2}
\end{figure}

The geometric distribution applies whenever we are counting the number of failures until the first success for independent performances of a random system where the occurrence of some event is considered a success. For example, the number of light bulbs tested that work until the first bulb that does not (a working bulb is considered a ``failure'' for the test) and the number of at-bats without a hit until the first hit for the baseball player both follow the geometric distribution.

We note that some books instead define the geometric distribution to be the number of coin flips up to and including the first head, which is simply equal to one plus the random variable defined here.
\end{example}

\begin{example}[The Negative-Binomial Distribution]
\label{ex:2.3.5}
Generalizing the previous example, consider again repeatedly flipping a coin that has probability $\theta$ of coming up heads and probability $1 - \theta$ of coming up tails. Let $r$ be a positive integer, and let $Y$ be the number of tails that appear before the $r$th head. Then for $k \geqslant 0$, $Y = k$ if and only if the coin shows exactly $r - 1$ heads (and $k$ tails) on the first $r - 1 + k$ flips, and then shows a head on the $(r + k)$-th flip. The probability of this is equal to
\[
p_Y(k) = \binom{r - 1 + k}{r - 1} \theta^{r-1} (1-\theta)^k \cdot \theta = \binom{r - 1 + k}{k} \theta^r (1-\theta)^k,
\]
for $k = 0, 1, 2, 3, \ldots$. The random variable $Y$ is said to have the \emph{Negative-Binomial$(r, \theta)$ distribution}; we write this as $Y \sim \text{Negative-Binomial}(r, \theta)$. Of course, the special case $r = 1$ corresponds to the Geometric$(\theta)$ distribution. So in terms of our notation, we have that $\text{Negative-Binomial}(1, \theta) = \text{Geometric}(\theta)$. Figure~\ref{fig:2.3.3} contains the plots of several Negative-Binomial$(r, \theta)$ probability functions.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-3-3.pdf}
\caption{Plot of the Negative-Binomial$(2, 1/2)$ ($\bullet$) probability function and the Negative-Binomial$(10, 1/2)$ ($\circ$) probability function at the values $0, 1, \ldots, 20$.}
\label{fig:2.3.3}
\end{figure}

The Negative-Binomial$(r, \theta)$ distribution applies whenever we are counting the number of failures until the $r$th success for independent performances of a random system where the occurrence of some event is considered a success. For example, the number of light bulbs tested that work until the third bulb that does not and the number of at-bats without a hit until the fifth hit for the baseball player both follow the negative-binomial distribution.
\end{example}

\begin{example}[The Poisson Distribution]
\label{ex:2.3.6}
We say that a random variable $Y$ has the \emph{Poisson$(\lambda)$ distribution}, and write $Y \sim \text{Poisson}(\lambda)$, if
\[
p_Y(y) = \prb(Y = y) = \frac{\lambda^y}{y!} e^{-\lambda}
\]
for $y = 0, 1, 2, 3, \ldots$. We note that since (from calculus) $\sum_{y=0}^{\infty} \lambda^y / y! = e^{\lambda}$, it is indeed true (as it must be) that $\sum_{y=0}^{\infty} \prb(Y = y) = 1$. Figure~\ref{fig:2.3.4} contains the plots of several Poisson$(\lambda)$ probability functions.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-3-4.pdf}
\caption{Plot of the Poisson$(2)$ ($\bullet$) and the Poisson$(10)$ ($\circ$) probability functions at the values $0, 1, \ldots, 20$.}
\label{fig:2.3.4}
\end{figure}

We motivate the Poisson distribution as follows. Suppose $X \sim \text{Binomial}(n, \theta)$, i.e., $X$ has the Binomial$(n, \theta)$ distribution as in Example~\ref{ex:2.3.3}. Then for $0 \leqslant x \leqslant n$,
\[
\prb(X = x) = \binom{n}{x} \theta^x (1-\theta)^{n-x}.
\]
If we set $\theta = \lambda/n$ for some $\lambda > 0$, then this becomes
\begin{equation}
\label{eq:2.3.2}
\prb(X = x) = \binom{n}{x} \left(\frac{\lambda}{n}\right)^x \left(1 - \frac{\lambda}{n}\right)^{n-x} = \frac{n(n-1) \cdots (n-x+1)}{x!} \cdot \frac{\lambda^x}{n^x} \cdot \left(1 - \frac{\lambda}{n}\right)^{n-x}.
\end{equation}

Let us now consider what happens if we let $n \to \infty$ in \eqref{eq:2.3.2}, while keeping $x$ fixed at some nonnegative integer. In that case,
\[
\frac{n(n-1)(n-2) \cdots (n-x+1)}{n^x} = 1 \cdot \left(1 - \frac{1}{n}\right) \cdot \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{x-1}{n}\right)
\]
converges to $1$, while (since from calculus $(1 - c/n)^n \to e^{-c}$ for any $c$)
\[
\left(1 - \frac{\lambda}{n}\right)^{n-x} = \left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-x} \to e^{-\lambda} \cdot 1 = e^{-\lambda}.
\]
Substituting these limits into \eqref{eq:2.3.2}, we see that
\[
\lim_{n \to \infty} \prb(X = x) = \frac{\lambda^x}{x!} e^{-\lambda}
\]
for $x = 0, 1, 2, 3, \ldots$.

Intuitively, we can phrase this result as follows. \emph{If we flip a very large number of coins $n$, and each coin has a very small probability $\lambda/n$ of coming up heads, then the probability that the total number of heads will be $x$ is approximately given by $\lambda^x e^{-\lambda} / x!$.} Figure~\ref{fig:2.3.5} displays the accuracy of this estimate when we are approximating the Binomial$(100, 1/10)$ distribution by the Poisson$(\lambda)$ distribution where $\lambda = n\theta = 100 \cdot 1/10 = 10$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-3-5.pdf}
\caption{Plot of the Binomial$(100, 1/10)$ ($\bullet$) and the Poisson$(10)$ ($\circ$) probability functions at the values $0, 1, \ldots, 20$.}
\label{fig:2.3.5}
\end{figure}

The Poisson distribution is a good model for counting random occurrences of an event when there are many possible occurrences, but each occurrence has very small probability. Examples include the number of house fires in a city on a given day, the number of radioactive events recorded by a Geiger counter, the number of phone calls arriving at a switchboard, the number of hits on a popular World Wide Web page on a given day, etc.
\end{example}

\begin{example}[The Hypergeometric Distribution]
\label{ex:2.3.7}
Suppose that an urn contains $M$ white balls and $N - M$ black balls. Suppose we draw $n \leqslant N$ balls from the urn in such a fashion that each subset of $n$ balls has the same probability of being drawn. Because there are $\binom{N}{n}$ such subsets, this probability is $1/\binom{N}{n}$.

One way of accomplishing this is to thoroughly mix the balls in the urn and then draw a first ball. Accordingly, each ball has probability $1/N$ of being drawn. Then, without replacing the first ball, we thoroughly mix the balls in the urn and draw a second ball. So each ball in the urn has probability $1/(N-1)$ of being drawn. We then have that any two balls, say the $i$th and $j$th balls, have probability
\begin{align*}
\prb(\text{ball } i \text{ and } j \text{ are drawn}) &= \prb(\text{ball } i \text{ is drawn first}) \, \prb(\text{ball } j \text{ is drawn second} \mid \text{ball } i \text{ is drawn first}) \\
&\quad + \prb(\text{ball } j \text{ is drawn first}) \, \prb(\text{ball } i \text{ is drawn second} \mid \text{ball } j \text{ is drawn first}) \\
&= \frac{1}{N} \cdot \frac{1}{N-1} + \frac{1}{N} \cdot \frac{1}{N-1} = \frac{1}{\binom{N}{2}}
\end{align*}
of being drawn in the first two draws. Continuing in this fashion for $n$ draws, we obtain that the probability of any particular set of $n$ balls being drawn is $1/\binom{N}{n}$. This type of sampling is called \emph{sampling without replacement}.

Given that we take a sample of $n$, let $X$ denote the number of white balls obtained. Note that we must have $X \geqslant 0$ and $X \geqslant n - (N - M)$ because at most $N - M$ of the balls could be black. Hence, $X \geqslant \max(0, n - M - N)$. Furthermore, $X \leqslant n$ and $X \leqslant M$ because there are only $M$ white balls. Hence, $X \leqslant \min(n, M)$.

So suppose $\max(0, n - M - N) \leqslant x \leqslant \min(n, M)$. What is the probability that $x$ white balls are obtained? In other words, what is $\prb(X = x)$? To evaluate this, we know that we need to count the number of subsets of $n$ balls that contain $x$ white balls. Using the combinatorial principles of Section~\ref{ssec:1.4.1}, we see that this number is given by $\binom{M}{x} \binom{N-M}{n-x}$. Therefore,
\[
\prb(X = x) = \frac{\binom{M}{x} \binom{N-M}{n-x}}{\binom{N}{n}}
\]
for $\max(0, n - M - N) \leqslant x \leqslant \min(n, M)$. The random variable $X$ is said to have the \emph{Hypergeometric$(N, M, n)$ distribution}. In Figure~\ref{fig:2.3.6}, we have plotted some hypergeometric probability functions. The Hypergeometric$(20, 10, 10)$ probability function is $0$ for $x > 10$, while the Hypergeometric$(20, 10, 5)$ probability function is $0$ for $x > 5$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-3-6.pdf}
\caption{Plot of Hypergeometric$(20, 10, 10)$ ($\bullet$) and Hypergeometric$(20, 10, 5)$ ($\circ$) probability functions.}
\label{fig:2.3.6}
\end{figure}

Obviously, the hypergeometric distribution will apply to any context wherein we are sampling without replacement from a finite set of $N$ elements and where each element of the set either has a characteristic or does not. For example, if we randomly select people to participate in an opinion poll so that each set of $n$ individuals in a population of $N$ has the same probability of being selected, then the number of people who respond yes to a particular question is distributed Hypergeometric$(N, M, n)$ where $M$ is the number of people in the entire population who would respond yes. We will see the relevance of this to statistics in Section~\ref{ssec:5.4.2}.

Suppose in Example~\ref{ex:2.3.7} we had instead replaced the drawn ball before drawing the next ball. This is called \emph{sampling with replacement}. It is then clear, from Example~\ref{ex:2.3.3}, that the number of white balls observed in $n$ draws is distributed Binomial$(n, M/N)$.
\end{example}

\subsection*{Summary of Section~\ref{sec:2.3}}

\begin{itemize}
\item A random variable $X$ is discrete if $\sum_x \prb(X = x) = 1$, i.e., if all its probability comes from being equal to particular values.
\item A discrete random variable $X$ takes on only a finite, or countable, number of distinct values.
\item Important discrete distributions include the degenerate, Bernoulli, binomial, geometric, negative-binomial, Poisson, and hypergeometric distributions.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.3.1}
Consider rolling two fair six-sided dice. Let $Y$ be the sum of the numbers showing. What is the probability function of $Y$?
\end{exercise}

\begin{solution}
Here $p_Y(2) = 1/36$, $p_Y(3) = 2/36$, $p_Y(4) = 3/36$, $p_Y(5) = 4/36$, $p_Y(6) = 5/36$, $p_Y(7) = 6/36$, $p_Y(8) = 5/36$, $p_Y(9) = 4/36$, $p_Y(10) = 3/36$, $p_Y(11) = 2/36$, and $p_Y(12) = 1/36$, with $p_Y(y) = 0$ otherwise.
\end{solution}

\begin{exercise}
\label{exer:2.3.2}
Consider flipping a fair coin. Let $Z = 1$ if the coin is heads, and $Z = -3$ if the coin is tails. Let $W = Z^2 + Z$.
\begin{enumerate}[(a)]
\item What is the probability function of $Z$?
\item What is the probability function of $W$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $p_Z(1) = p_Z(3) = 1/2$, with $p_Z(z) = 0$ otherwise.
    \item $p_W(2) = p_W(12) = 1/2$, with $p_W(w) = 0$ otherwise.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.3.3}
Consider flipping two fair coins. Let $X = 1$ if the first coin is heads, and $X = 0$ if the first coin is tails. Let $Y = 1$ if the second coin is heads, and $Y = 5$ if the second coin is tails. Let $Z = XY$. What is the probability function of $Z$?
\end{exercise}

\begin{solution}
Here $p_Z(1) = p_Z(5) = 1/4$, with $p_Z(0) = 1/4 + 1/4 = 1/2$ and $p_Z(z) = 0$ otherwise.
\end{solution}

\begin{exercise}
\label{exer:2.3.4}
Consider flipping two fair coins. Let $X = 1$ if the first coin is heads, and $X = 0$ if the first coin is tails. Let $Y = 1$ if the two coins show the same thing (i.e., both heads or both tails), with $Y = 0$ otherwise. Let $Z = X + Y$, and $W = XY$.
\begin{enumerate}[(a)]
\item What is the probability function of $Z$?
\item What is the probability function of $W$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $p_Z(0) = p_Z(2) = 1/4$, with $p_Z(1) = 1/4 + 1/4 = 1/2$ and $p_Z(z) = 0$ otherwise.
    \item $p_W(1) = 1/4$, with $p_W(0) = 1/4 + 1/4 + 1/4 = 3/4$ and $p_W(w) = 0$ otherwise.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.3.5}
Consider rolling two fair six-sided dice. Let $W$ be the product of the numbers showing. What is the probability function of $W$?
\end{exercise}

\begin{solution}
Here $p_W(1) = 1/36$, $p_W(2) = 2/36$, $p_W(3) = 2/36$, $p_W(4) = 2/36 + 1/36 = 3/36$, $p_W(5) = 2/36$, $p_W(6) = 2/36 + 2/36 = 4/36$, $p_W(8) = 2/36$, $p_W(9) = 1/36$, $p_W(10) = 2/36$, $p_W(12) = 2/36 + 2/36 = 4/36$, $p_W(15) = 2/36$, $p_W(16) = 1/36$, $p_W(18) = 2/36$, $p_W(20) = 2/36$, $p_W(24) = 2/36$, $p_W(25) = 1/36$, $p_W(30) = 2/36$, and $p_W(36) = 1/36$, with $p_W(w) = 0$ otherwise.
\end{solution}

\begin{exercise}
\label{exer:2.3.6}
Let $Z \sim \text{Geometric}(\theta)$. Compute $\prb(5 \leqslant Z \leqslant 9)$.
\end{exercise}

\begin{solution}
$\prb(5 \leqslant Z \leqslant 9) = \sum_{z=5}^{9}(1-p)^z p = p\frac{(1-p)^5 - (1-p)^{10}}{1-(1-p)} = (1-p)^5 - (1-p)^{10}$.
\end{solution}

\begin{exercise}
\label{exer:2.3.7}
Let $X \sim \text{Binomial}(12, \theta)$. For what value of $\theta$ is $\prb(X = 11)$ maximized?
\end{exercise}

\begin{solution}
Here $\prb(X = 11) = \binom{12}{11}p^{11}(1-p)^1 = 12p^{11}(1-p)$. This has derivative $12 \cdot 11p^{10}(1-p) - 12p^{11}$, which equals 0 if either $p = 0$, or $11(1-p) = p$ whence $p = 11/12$. We see that $p = 11/12$ maximizes $\prb(X = 11)$.
\end{solution}

\begin{exercise}
\label{exer:2.3.8}
Let $W \sim \text{Poisson}(\lambda)$. For what value of $\lambda$ is $\prb(W = 11)$ maximized?
\end{exercise}

\begin{solution}
Here $\prb(W = 11) = e^{-\lambda}\lambda^{11}/11!$. This is maximized when $e^{-\lambda}\lambda^{11}$ is maximized. This has derivative $-e^{-\lambda}\lambda^{11} + e^{-\lambda}11\lambda^{10}$, which equals 0 if either $\lambda = 0$, or $\lambda = 11$. We see that $\lambda = 11$ maximizes $\prb(W = 11)$.
\end{solution}

\begin{exercise}
\label{exer:2.3.9}
Let $Z \sim \text{Negative-Binomial}(3, 1/4)$. Compute $\prb(Z \leqslant 2)$.
\end{exercise}

\begin{solution}
\begin{align*}
    \prb(Z \leqslant 2) &= \prb(Z = 0) + \prb(Z = 1) + \prb(Z = 2) \\
    &= \binom{2}{0}(1/4)^3(1-1/4)^0 + \binom{3}{1}(1/4)^3(1-1/4)^1 + \binom{4}{2}(1/4)^3(1-1/4)^2 \\
    &= 1/4^3 + 9/4^4 + 54/4^5 = 53/512
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:2.3.10}
Let $X \sim \text{Geometric}(1/5)$. Compute $\prb(X^2 \leqslant 15)$.
\end{exercise}

\begin{solution}
\[
    \prb(X^2 \leqslant 15) = \prb(X \leqslant \sqrt{15}) = \prb(X \leqslant 3) = \sum_{k=0}^{3}\prb(X = k) = \sum_{k=0}^{3}(1-1/5)^k(1/5) = \frac{1-(4/5)^4}{1-(4/5)}(1/5) = 369/625
\]
\end{solution}

\begin{exercise}
\label{exer:2.3.11}
Let $Y \sim \text{Binomial}(10, \theta)$. Compute $\prb(Y = 10)$.
\end{exercise}

\begin{solution}
$\prb(Y = 10) = \binom{10}{10}p^{10}(1-p)^{10-10} = p^{10}$.
\end{solution}

\begin{exercise}
\label{exer:2.3.12}
Let $X \sim \text{Poisson}(\lambda)$. Let $Y = X + 7$. What is the probability function of $Y$?
\end{exercise}

\begin{solution}
$p_Y(y) = \prb(Y = y) = \prb(X - 7 = y) = \prb(X = y + 7) = e^{-\lambda}\lambda^{y+7}/(y+7)!$ for $y = -7, -6, -5, \ldots$, with $p_Y(y) = 0$ otherwise.
\end{solution}

\begin{exercise}
\label{exer:2.3.13}
Let $X \sim \text{Hypergeometric}(20, 7, 8)$. What is the probability that $X \leqslant 3$? What is the probability that $X \geqslant 8$?
\end{exercise}

\begin{solution}
$p_X(3) = \binom{7}{3}\binom{13}{5}/\binom{20}{8} = 0.35759$ and $\prb(X = 8) = 0$ since there are only seven elements in the population with the label in question.
\end{solution}

\begin{exercise}
\label{exer:2.3.14}
Suppose that a symmetrical die is rolled 20 independent times, and each time we record whether or not the event $\{2, 3, 5, 6\}$ has occurred.
\begin{enumerate}[(a)]
\item What is the distribution of the number of times this event occurs in 20 rolls?
\item Calculate the probability that the event occurs five times.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\text{Binomial}(20, 2/3)$.
    \item $\prb(X = 5) = \binom{20}{5}(2/3)^5(1/3)^{15} = 1.4229 \times 10^{-4}$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.3.15}
Suppose that a basketball player sinks a basket from a certain position on the court with probability $0.35$.
\begin{enumerate}[(a)]
\item What is the probability that the player sinks three baskets in 10 independent throws?
\item What is the probability that the player scores their first basket on their tenth attempt?
\item What is the probability that the player scores their second basket on their tenth attempt?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\binom{10}{3}(0.35)^3(0.65)^7 = 0.25222$.
    \item $(0.35)(0.65)^9 = 7.2492 \times 10^{-3}$.
    \item $\binom{9}{1}(0.35)^2(0.65)^8 = 3.5131 \times 10^{-2}$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.3.16}
An urn contains 4 black balls and 5 white balls. After a thorough mixing, a ball is drawn from the urn, its color is noted, and the ball is returned to the urn.
\begin{enumerate}[(a)]
\item What is the probability that 5 black balls are observed in 15 such draws?
\item What is the probability that 15 draws are required until the first black ball is observed?
\item What is the probability that 15 draws are made with 5 black balls observed and the fifth black ball is observed on the 15th draw?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\binom{15}{5}(4/9)^5(5/9)^{10} = 0.14585$.
    \item $(4/9)(5/9)^{14} = 1.1858 \times 10^{-4}$.
    \item $\binom{15}{4}(4/9)^5(5/9)^{10} = 6.6297 \times 10^{-2}$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.3.17}
An urn contains 4 black balls and 5 white balls. After a thorough mixing, a ball is drawn from the urn, its color is noted, and the ball is set aside. The remaining balls are then mixed and a second ball is drawn.
\begin{enumerate}[(a)]
\item What is the probability distribution of the number of black balls observed?
\item What is the probability distribution of the number of white balls observed?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\text{Hypergeometric}(9, 4, 2)$.
    \item $\text{Hypergeometric}(9, 5, 2)$.
\end{enumerate}
\end{solution}

\begin{exercise}[Poisson processes and queues]
\label{exer:2.3.18}
Consider a situation involving a server, e.g., a cashier at a fast-food restaurant, an automatic bank teller machine, a telephone exchange, etc. Units typically arrive for service in a random fashion and form a queue when the server is busy. It is often the case that the number of arrivals at the server, for some specific unit of time $t$, can be modeled by a Poisson$(\lambda t)$ distribution and is such that the number of arrivals in nonoverlapping periods are independent. In Chapter~\ref{ch:3}, we will show that $\lambda t$ is the average number of arrivals during a time period of length $t$ and so $\lambda$ is the rate of arrivals per unit of time.

Suppose telephone calls arrive at a help line at the rate of two per minute. A Poisson process provides a good model.
\begin{enumerate}[(a)]
\item What is the probability that five calls arrive in the next 2 minutes?
\item What is the probability that five calls arrive in the next 2 minutes and then five more calls arrive in the following 2 minutes?
\item What is the probability that no calls will arrive during a 10-minute period?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\prb(X = 5) = \bigl((2 \cdot 2)^5/5!\bigr)\exp\{-2 \cdot 2\} = 0.15629$.
    \item $\prb(X = 5, Y = 5) = \prb(X = 5)\prb(Y = 5) = (0.15629)^2 = 2.4427 \times 10^{-2}$.
    \item $\prb(X = 0) = (2 \cdot 10)^0 \exp\{-2 \cdot 10\} = 2.0612 \times 10^{-9}$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.3.19}
Suppose an urn contains 1000 balls---one of these is black, and the other 999 are white. Suppose that 100 balls are randomly drawn from the urn with replacement. Use the appropriate Poisson distribution to approximate the probability that five black balls are observed.
\end{exercise}

\begin{solution}
The number of black balls observed is distributed $\text{Binomial}(10, 1/1000)$. Then
\[
    \prb(X = 5) \approx \frac{(100/1000)^5}{5!}\exp\{-100/1000\} = 7.5403 \times 10^{-8}.
\]
\end{solution}

\begin{exercise}
\label{exer:2.3.20}
Suppose that there is a loop in a computer program and that the test to exit the loop depends on the value of a random variable $X$. The program exits the loop whenever $X \in A$ and this occurs with probability $1/3$. If the loop is executed at least once, what is the probability that the loop is executed five times before exiting?
\end{exercise}

\begin{solution}
This is the probability that the test fails 4 times and passes on the fifth test, so this probability is $(1/3)(2/3)^4 = 6.5844 \times 10^{-2}$.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:2.3.21}
Tabulate and plot the Hypergeometric$(20, 8, 10)$ probability function.
\end{exercise}

\begin{solution}
The tabulation is given by
\begin{center}
\begin{tabular}{cc}
$x$ & $p$ \\
\hline
0 & 0.000357 \\
1 & 0.009526 \\
2 & 0.075018 \\
3 & 0.240057 \\
4 & 0.350083 \\
5 & 0.240057 \\
6 & 0.075018 \\
7 & 0.009526 \\
8 & 0.000357
\end{tabular}
\end{center}
and the plot is as below.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig_2_3_21.pdf}
    \caption{Binomial probability function for Exercise 2.3.21}
    %\label{fig:binomial-2-3-21}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:2.3.22}
Tabulate and plot the Binomial$(30, 0.3)$ probability function. Tabulate and plot the Binomial$(30, 0.7)$ probability function. Explain why the Binomial$(30, 0.3)$ probability function at $x$ agrees with the Binomial$(30, 0.7)$ probability function at $n - x$.
\end{exercise}

\begin{solution}
The $\text{Binomial}(30, 0.3)$ probability function is plotted below.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig_2_3_22a.pdf}
    \caption{Binomial(30, 0.3) probability function}
    %\label{fig:binomial-30-03}
\end{figure}

The $\text{Binomial}(30, 0.7)$ probability function is plotted below.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig_2_3_22b.pdf}
    \caption{Binomial(30, 0.7) probability function}
    %\label{fig:binomial-30-07}
\end{figure}

We have that
\[
    \binom{n}{x}p^x(1-p)^{n-x} = \binom{n}{n-x}(1-p)^{n-x}(1-(1-p))^x.
\]
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.3.23}
Let $X$ be a discrete random variable with probability function $p_X(x) = 2^{-x}$ for $x = 1, 2, 3, \ldots$, with $p_X(x) = 0$ otherwise.
\begin{enumerate}[(a)]
\item Let $Y = X^2$. What is the probability function $p_Y$ of $Y$?
\item Let $Z = X - 1$. What is the distribution of $Z$? (Identify the distribution by name and specify all parameter values.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $p_Y(y) = 2^{-\sqrt{y}}$ for $y = 1, 4, 9, 16, 25, \ldots$ (i.e., for $y$ a positive perfect square), with $p_Y(y) = 0$ otherwise.
    \item $p_Z(z) = 2^{-z-1}$ for $z = 0, 1, 2, \ldots$, with $p_Z(z) = 0$ otherwise. Hence, $Z \sim \text{Geometric}(1/2)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.3.24}
Let $X \sim \text{Binomial}(n_1, \theta)$ and $Y \sim \text{Binomial}(n_2, \theta)$, with $X$ and $Y$ chosen independently. Let $Z = X + Y$. What will be the distribution of $Z$? (Explain your reasoning.) (Hint: See the end of Example~\ref{ex:2.3.3}.)
\end{exercise}

\begin{solution}
Here $Z \sim \text{Binomial}(n_1 + n_2, p)$. This is because $X$ corresponds to the number of heads on the first $n_1$ coins (where each coin has probability $p$ of being heads), and $Y$ corresponds to the number of heads on the next $n_2$ coins, so $Z$ corresponds to the number of heads on the first $n_1 + n_2$ coins.
\end{solution}

\begin{exercise}
\label{exer:2.3.25}
Let $X \sim \text{Geometric}(\theta)$ and $Y \sim \text{Geometric}(\theta)$, with $X$ and $Y$ chosen independently. Let $Z = X + Y$. What will be the distribution of $Z$? Generalize this to $r$ coins. (Explain your reasoning.)
\end{exercise}

\begin{solution}
$Z \sim \text{Negative Binomial}(2, \theta)$ since $X + Y$ is equal to the number of tails until the second head is observed in independent tosses with heads occurring with probability $\theta$. With $r$ coins the sum will be distributed $\text{Negative Binomial}(r, \theta)$.
\end{solution}

\begin{exercise}
\label{exer:2.3.26}
Let $X \sim \text{Geometric}(\theta_1)$ and $Y \sim \text{Geometric}(\theta_2)$, with $X$ and $Y$ chosen independently. Compute $\prb(X \leqslant Y)$. Explain what this probability is in terms of coin tossing.
\end{exercise}

\begin{solution}
\begin{align*}
    \prb(X \leqslant Y) &= \sum_{y=0}^{\infty} \prb(X \leqslant y)\theta(1-\theta)^y = \sum_{y=0}^{\infty}\left(\sum_{x=0}^{y}\theta_1(1-\theta_1)^x\right)\theta_2(1-\theta_2)^y \\
    &= \sum_{y=0}^{\infty}\bigl(1-(1-\theta_1)^{y+1}\bigr)\theta_2(1-\theta_2)^y = 1 - \theta_2(1-\theta_1)\sum_{y=0}^{\infty}(1-\theta_1)^y(1-\theta_2)^y \\
    &= 1 - \frac{\theta_2(1-\theta_1)}{1-(1-\theta_1)(1-\theta_2)}
\end{align*}
This is the probability that, in tossing two coins that have probability $\theta_1$ and $\theta_2$ of yielding a head respectively, the first head occurs on the first coin.
\end{solution}

\begin{exercise}
\label{exer:2.3.27}
Suppose that $X_n \sim \text{Geometric}(\lambda/n)$. Compute $\lim_{n \to \infty} \prb(X_n \geqslant n)$.
\end{exercise}

\begin{solution}
$\lim_{n \to \infty}\prb(X \leqslant n) = \lim_{n \to \infty}1 - (1-\lambda/n)^{n+1} = 1 - e^{-\lambda}$.
\end{solution}

\begin{exercise}
\label{exer:2.3.28}
Let $X \sim \text{Negative-Binomial}(r, \theta)$ and $Y \sim \text{Negative-Binomial}(s, \theta)$, with $X$ and $Y$ chosen independently. Let $Z = X + Y$. What will be the distribution of $Z$? (Explain your reasoning.)
\end{exercise}

\begin{solution}
$Z \sim \text{Negative Binomial}(r+s, \theta)$ since $X + Y$ is equal to the number of tails until the $(r+s)$th head is observed in independent tosses with heads occurring with probability $\theta$.
\end{solution}

\begin{exercise}[Generalized hypergeometric distribution]
\label{exer:2.3.29}
Suppose that a set contains $N$ objects, $M_1$ of which are labelled $1$, $M_2$ of which are labelled $2$, and the remainder of which are labelled $3$. Suppose we select a sample of $n \leqslant N$ objects from the set using sampling without replacement, as described in Example~\ref{ex:2.3.7}. Determine the probability that we obtain the counts $f_1, f_2, f_3$, where $f_i$ is the number of objects labelled $i$ in the sample.
\end{exercise}

\begin{solution}
The probability is $\binom{M_1}{f_1}\binom{M_2}{f_2}\binom{N-M_1-M_2}{f_3}/\binom{N}{n}$, provided
\begin{align*}
    \max\{0, n-(N-M_1)\} &\leqslant f_1 \leqslant \min\{M_1, n\}, \\
    \max\{0, n-(N-M_2)\} &\leqslant f_2 \leqslant \min\{M_2, n\}, \\
    \max\{0, n-(M_1+M_2)\} &\leqslant f_3 \leqslant \min\{N-M_1-M_2, n\},
\end{align*}
and $f_1 + f_2 + f_3 = n$.
\end{solution}

\begin{exercise}
\label{exer:2.3.30}
Suppose that units arrive at a server according to a Poisson process at rate $\lambda$ (see Exercise~\ref{exer:2.3.18}). Let $T$ be the amount of time until the first call. Calculate $\prb(T > t)$.
\end{exercise}

\begin{solution}
$\prb(T > t) = \prb(\text{no units arrive in }(0, t]) = \bigl((\lambda t)^0/0!\bigr)\exp\{-\lambda t\}$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Continuous Distributions}
\label{sec:2.4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the previous section, we considered discrete random variables $X$ for which $\prb(X = x) > 0$ for certain values of $x$. However, for some random variables $X$, such as one having the uniform distribution, we have $\prb(X = x) = 0$ for all $x$. This prompts the following definition.

\begin{definition}
\label{def:2.4.1}
A random variable $X$ is \emph{continuous} if
\begin{equation}
\label{eq:2.4.1}
\prb(X = x) = 0
\end{equation}
for all $x \in \mathbf{R}^1$.
\end{definition}

\begin{example}[{The Uniform$[0, 1]$ Distribution}]
\label{ex:2.4.1}
Consider a random variable whose distribution is the uniform distribution on $[0, 1]$, as presented in (1.2.2). That is,
\begin{equation}
\label{eq:2.4.2}
\prb(a \leqslant X \leqslant b) = b - a
\end{equation}
whenever $0 \leqslant a \leqslant b \leqslant 1$, with $\prb(X < 0) = \prb(X > 1) = 0$. The random variable $X$ is said to have the \emph{Uniform$[0, 1]$ distribution}; we write this as $X \sim \text{Uniform}[0, 1]$. For example,
\[
\prb\left(\frac{1}{2} \leqslant X \leqslant \frac{3}{4}\right) = \frac{3}{4} - \frac{1}{2} = \frac{1}{4}.
\]
Also,
\[
\prb\left(X \geqslant \frac{2}{3}\right) = \prb\left(\frac{2}{3} \leqslant X \leqslant 1\right) + \prb(X > 1) = 1 - \frac{2}{3} + 0 = \frac{1}{3}.
\]
In fact, for any $x \in [0, 1]$,
\[
\prb(X \leqslant x) = \prb(X < 0) + \prb(0 \leqslant X \leqslant x) = 0 + (x - 0) = x.
\]

Note that setting $a = b = x$ in \eqref{eq:2.4.2}, we see in particular that $\prb(X = x) = x - x = 0$ for every $x \in \mathbf{R}^1$. Thus, the uniform distribution is an example of a continuous distribution. In fact, it is one of the most important examples!
\end{example}

The Uniform$[0, 1]$ distribution is fairly easy to work with. However, in general, continuous distributions are very difficult to work with. Because $\prb(X = x) = 0$ for all $x$, we cannot simply add up probabilities as we can for discrete random variables. Thus, how can we keep track of all the probabilities?

A possible solution is suggested by rewriting \eqref{eq:2.4.2}, as follows. For $x \in \mathbf{R}^1$, let
\begin{equation}
\label{eq:2.4.3}
f(x) = \begin{cases} 1 & 0 \leqslant x \leqslant 1 \\ 0 & \text{otherwise.} \end{cases}
\end{equation}
Then \eqref{eq:2.4.2} can be rewritten as
\begin{equation}
\label{eq:2.4.4}
\prb(a \leqslant X \leqslant b) = \int_a^b f(x) \, \mathrm{d}x
\end{equation}
whenever $a \leqslant b$.

One might wonder about the wisdom of converting the simple equation \eqref{eq:2.4.2} into the complicated integral equation \eqref{eq:2.4.4}. However, the advantage of \eqref{eq:2.4.4} is that, by modifying the function $f$, we can obtain many other continuous distributions besides the uniform distribution. To explore this, we make the following definitions.

\begin{definition}
\label{def:2.4.2}
Let $f : \mathbf{R}^1 \to \mathbf{R}^1$ be a function. Then $f$ is a \emph{density function} if $f(x) \geqslant 0$ for all $x \in \mathbf{R}^1$, and $\int_{-\infty}^{\infty} f(x) \, \mathrm{d}x = 1$.
\end{definition}

\begin{definition}
\label{def:2.4.3}
A random variable $X$ is \emph{absolutely continuous} if there is a density function $f$, such that
\begin{equation}
\label{eq:2.4.5}
\prb(a \leqslant X \leqslant b) = \int_a^b f(x) \, \mathrm{d}x
\end{equation}
whenever $a \leqslant b$, as in \eqref{eq:2.4.4}.
\end{definition}

In particular, if $b = a + \epsilon$ with $\epsilon$ a small positive number, and if $f$ is continuous at $a$, then we see that
\[
\prb(a \leqslant X \leqslant a + \epsilon) = \int_a^{a+\epsilon} f(x) \, \mathrm{d}x \approx f(a) \, \epsilon.
\]
Thus, a density function evaluated at $a$ may be thought of as measuring the probability of a random variable being in a small interval about $a$.

To better understand absolutely continuous random variables, we note the following theorem.

\begin{theorem}
\label{thm:2.4.1}
Let $X$ be an absolutely continuous random variable. Then $X$ is a continuous random variable, i.e., $\prb(X = a) = 0$ for all $a \in \mathbf{R}^1$.
\end{theorem}

\begin{proof}
Let $a$ be any real number. Then $\prb(X = a) = \prb(a \leqslant X \leqslant a)$. On the other hand, setting $a = b$ in \eqref{eq:2.4.5}, we see that $\prb(a \leqslant X \leqslant a) = \int_a^a f(x) \, \mathrm{d}x = 0$. Hence, $\prb(X = a) = 0$ for all $a$, as required.
\end{proof}

It turns out that the converse to Theorem~\ref{thm:2.4.1} is false. That is, not all continuous distributions are absolutely continuous.\footnote{For examples of this, see more advanced probability books, e.g., page 143 of \emph{A First Look at Rigorous Probability Theory}, Second Edition, by J.\ S.\ Rosenthal (World Scientific Publishing, Singapore, 2006).} However, most of the continuous distributions that arise in statistics are absolutely continuous. Furthermore, absolutely continuous distributions are much easier to work with than are other kinds of continuous distributions. Hence, we restrict our discussion to absolutely continuous distributions here. In fact, statisticians sometimes say that $X$ is continuous as shorthand for saying that $X$ is absolutely continuous.

\subsection{Important Absolutely Continuous Distributions}
\label{ssec:2.4.1}

Certain absolutely continuous distributions are so important that we list them here.

\begin{example}[{The Uniform$[0, 1]$ Distribution}]
\label{ex:2.4.2}
Clearly, the uniform distribution is absolutely continuous, with the density function given by \eqref{eq:2.4.3}. We will see, in Section~\ref{sec:2.10}, that the Uniform$[0, 1]$ distribution has an important relationship with every absolutely continuous distribution.
\end{example}

\begin{example}[{The Uniform$[L, R]$ Distribution}]
\label{ex:2.4.3}
Let $L$ and $R$ be any two real numbers with $L < R$. Consider a random variable $X$ such that
\begin{equation}
\label{eq:2.4.6}
\prb(a \leqslant X \leqslant b) = \frac{b - a}{R - L}
\end{equation}
whenever $L \leqslant a \leqslant b \leqslant R$, with $\prb(X < L) = \prb(X > R) = 0$. The random variable $X$ is said to have the \emph{Uniform$[L, R]$ distribution}; we write this as $X \sim \text{Uniform}[L, R]$. (If $L = 0$ and $R = 1$, then this definition coincides with the previous definition of the Uniform$[0, 1]$ distribution.) Note that $X \sim \text{Uniform}[L, R]$ has the same probability of being in any two subintervals of $[L, R]$ that have the same length.

Note that the Uniform$[L, R]$ distribution is also absolutely continuous, with density given by
\[
f(x) = \begin{cases} \dfrac{1}{R - L} & L \leqslant x \leqslant R \\ 0 & \text{otherwise.} \end{cases}
\]
In Figure~\ref{fig:2.4.1} we have plotted a Uniform$[2, 4]$ density.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-4-1.pdf}
\caption{A Uniform$[2, 4]$ density function.}
\label{fig:2.4.1}
\end{figure}
\end{example}

\begin{example}[The Exponential$(1)$ Distribution]
\label{ex:2.4.4}
Define a function $f : \mathbf{R}^1 \to \mathbf{R}^1$ by
\[
f(x) = \begin{cases} e^{-x} & x \geqslant 0 \\ 0 & x < 0. \end{cases}
\]
Then clearly, $f(x) \geqslant 0$ for all $x$. Also,
\[
\int_{-\infty}^{\infty} f(x) \, \mathrm{d}x = \int_0^{\infty} e^{-x} \, \mathrm{d}x = \left[-e^{-x}\right]_0^{\infty} = 0 - (-1) = 1.
\]
Hence, $f$ is a density function. See Figure~\ref{fig:2.4.2} for a plot of this density.

Consider now a random variable $X$ having this density function $f$. If $0 \leqslant a \leqslant b < \infty$, then
\[
\prb(a \leqslant X \leqslant b) = \int_a^b f(x) \, \mathrm{d}x = \int_a^b e^{-x} \, \mathrm{d}x = \left[-e^{-x}\right]_a^b = e^{-a} - e^{-b}.
\]
The random variable $X$ is said to have the \emph{Exponential$(1)$ distribution}, which we write as $X \sim \text{Exponential}(1)$. The exponential distribution has many important properties, which we will explore in the coming sections.
\end{example}

\begin{example}[The Exponential$(\lambda)$ Distribution]
\label{ex:2.4.5}
Let $\lambda > 0$ be a fixed constant. Define a function $f : \mathbf{R}^1 \to \mathbf{R}^1$ by
\[
f(x) = \begin{cases} \lambda e^{-\lambda x} & x \geqslant 0 \\ 0 & x < 0. \end{cases}
\]
Then clearly, $f(x) \geqslant 0$ for all $x$. Also,
\[
\int_{-\infty}^{\infty} f(x) \, \mathrm{d}x = \int_0^{\infty} \lambda e^{-\lambda x} \, \mathrm{d}x = \left[-e^{-\lambda x}\right]_0^{\infty} = 0 - (-1) = 1.
\]
Hence, $f$ is again a density function. (If $\lambda = 1$, then this corresponds to the Exponential$(1)$ density.)

If $X$ is a random variable having this density function $f$, then
\[
\prb(a \leqslant X \leqslant b) = \int_a^b \lambda e^{-\lambda x} \, \mathrm{d}x = \left[-e^{-\lambda x}\right]_a^b = e^{-\lambda a} - e^{-\lambda b}
\]
for $0 \leqslant a \leqslant b < \infty$. The random variable $X$ is said to have the \emph{Exponential$(\lambda)$ distribution}; we write this as $X \sim \text{Exponential}(\lambda)$. Note that some books and software packages instead replace $\lambda$ by $1/\lambda$ in the definition of the Exponential$(\lambda)$ distribution---always check this when using another book or when using software.

An exponential distribution can often be used to model lifelengths. For example, a certain type of light bulb produced by a manufacturer might follow an Exponential$(\lambda)$ distribution for an appropriate choice of $\lambda$. By this we mean that the lifelength $X$ of a randomly selected light bulb from those produced by this manufacturer has probability
\[
\prb(X > x) = \int_x^{\infty} \lambda e^{-\lambda z} \, \mathrm{d}z = e^{-\lambda x}
\]
of lasting longer than $x$ in whatever units of time are being used. We will see in Chapter~\ref{ch:3} that, in a specific application, the value $1/\lambda$ will correspond to the average lifelength of the light bulbs.

As another application of this distribution, consider a situation involving a server, e.g., a cashier at a fast-food restaurant, an automatic bank teller machine, a telephone exchange, etc. Units arrive for service in a random fashion and form a queue when the server is busy. It is often the case that the number of arrivals at the server, for some specific unit of time $t$, can be modeled by a Poisson$(\lambda t)$ distribution. Now let $T_1$ be the time until the first arrival. Then we have
\[
\prb(T_1 > t) = \prb(\text{no arrivals in } (0, t]) = \frac{(\lambda t)^0}{0!} e^{-\lambda t} = e^{-\lambda t}
\]
and $T_1$ has density given by
\[
f(t) = -\frac{\mathrm{d}}{\mathrm{d}t} \int_t^{\infty} f(z) \, \mathrm{d}z = -\frac{\mathrm{d}}{\mathrm{d}t} \prb(T_1 > t) = \lambda e^{-\lambda t}.
\]
So $T_1 \sim \text{Exponential}(\lambda)$.
\end{example}

\begin{example}[The Gamma Distribution]
\label{ex:2.4.6}
The \emph{gamma function} is defined by
\[
\Gamma(\alpha) = \int_0^{\infty} t^{\alpha - 1} e^{-t} \, \mathrm{d}t, \qquad \alpha > 0.
\]
It turns out (see Problem~\ref{exer:2.4.15}) that
\begin{equation}
\label{eq:2.4.7}
\Gamma(\alpha + 1) = \alpha \, \Gamma(\alpha)
\end{equation}
and that if $n$ is a positive integer, then $\Gamma(n) = (n-1)!$, while $\Gamma(1/2) = \sqrt{\pi}$.

We can use the gamma function to define the density of the Gamma distribution, as follows. Let $\alpha > 0$ and $\lambda > 0$, and define a function $f$ by
\begin{equation}
\label{eq:2.4.8}
f(x) = \frac{\lambda^{\alpha} x^{\alpha - 1} e^{-\lambda x}}{\Gamma(\alpha)}
\end{equation}
when $x > 0$, with $f(x) = 0$ for $x \leqslant 0$. Then clearly $f \geqslant 0$. Furthermore, it is not hard to verify (see Problem~\ref{exer:2.4.17}) that $\int_0^{\infty} f(x) \, \mathrm{d}x = 1$. Hence, $f$ is a density function.

A random variable $X$ having density function $f$ given by \eqref{eq:2.4.8} is said to have the \emph{Gamma$(\alpha, \lambda)$ distribution}; we write this as $X \sim \text{Gamma}(\alpha, \lambda)$. Note that some books and software packages instead replace $\lambda$ by $1/\lambda$ in the definition of the Gamma$(\alpha, \lambda)$ distribution---always check this when using another book or when using software.

The case $\alpha = 1$ corresponds (because $\Gamma(1) = 0! = 1$) to the Exponential$(\lambda)$ distribution: $\text{Gamma}(1, \lambda) = \text{Exponential}(\lambda)$. In Figure~\ref{fig:2.4.2}, we have plotted several Gamma$(\alpha, \lambda)$ density functions.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-4-2.pdf}
\caption{Graph of an Exponential$(1)$ (solid line), a Gamma$(2, 1)$ (dashed line), and a Gamma$(3, 1)$ (dotted line) density.}
\label{fig:2.4.2}
\end{figure}

A gamma distribution can also be used to model lifelengths. As Figure~\ref{fig:2.4.2} shows, the gamma family gives a much greater variety of shapes to choose from than from the exponential family.
\end{example}

We now define a function $\phi : \mathbf{R}^1 \to \mathbf{R}^1$ by
\begin{equation}
\label{eq:2.4.9}
\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}.
\end{equation}
This function is the famous ``bell-shaped curve'' because its graph is in the shape of a bell, as shown in Figure~\ref{fig:2.4.3}.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-4-3.pdf}
\caption{Plot of the function $\phi$ in \eqref{eq:2.4.9}.}
\label{fig:2.4.3}
\end{figure}

We have the following result for $\phi$.

\begin{theorem}
\label{thm:2.4.2}
The function $\phi$ given by \eqref{eq:2.4.9} is a density function.
\end{theorem}

\begin{proof}
  See Section~\ref{sec:2.11} for the proof of this result.
\end{proof}

This leads to the following important distributions.

\begin{example}[The $N(0, 1)$ Distribution]
\label{ex:2.4.7}
Let $X$ be a random variable having the density function $\phi$ given by \eqref{eq:2.4.9}. This means that for $-\infty < a < b < \infty$,
\[
\prb(a \leqslant X \leqslant b) = \int_a^b \phi(x) \, \mathrm{d}x = \int_a^b \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \, \mathrm{d}x.
\]
The random variable $X$ is said to have the \emph{$N(0, 1)$ distribution} (or the \emph{standard normal distribution}); we write this as $X \sim N(0, 1)$.
\end{example}

\begin{example}[The $N(\mu, \sigma^2)$ Distribution]
\label{ex:2.4.8}
Let $\mu \in \mathbf{R}^1$, and let $\sigma > 0$. Let $f$ be the function defined by
\[
f(x) = \frac{1}{\sigma} \phi\left(\frac{x - \mu}{\sigma}\right) = \frac{1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/(2\sigma^2)}.
\]
(If $\mu = 0$ and $\sigma = 1$, then this corresponds with the previous example.) Clearly, $f \geqslant 0$. Also, letting $y = (x - \mu)/\sigma$, we have
\[
\int_{-\infty}^{\infty} f(x) \, \mathrm{d}x = \int_{-\infty}^{\infty} \frac{1}{\sigma} \phi\left(\frac{x - \mu}{\sigma}\right) \, \mathrm{d}x = \int_{-\infty}^{\infty} \phi(y) \, \mathrm{d}y = 1.
\]
Hence, $f$ is a density function.

Let $X$ be a random variable having this density function $f$. The random variable $X$ is said to have the \emph{$N(\mu, \sigma^2)$ distribution}; we write this as $X \sim N(\mu, \sigma^2)$. In Figure~\ref{fig:2.4.4}, we have plotted the $N(0, 1)$ and the $N(1, 1)$ densities. Note that changes in $\mu$ simply shift the density without changing its shape. In Figure~\ref{fig:2.4.5}, we have plotted the $N(0, 1)$ and the $N(0, 4)$ densities. Note that both densities are centered on $0$, but the $N(0, 4)$ density is much more spread out. The value of $\sigma^2$ controls the amount of spread.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-4-4.pdf}
\caption{Graph of the $N(1, 1)$ density (solid line) and the $N(0, 1)$ density (dashed line).}
\label{fig:2.4.4}
\end{figure}

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-4-5.pdf}
\caption{Graph of an $N(0, 4)$ density (solid line) and an $N(0, 1)$ density (dashed line).}
\label{fig:2.4.5}
\end{figure}

The $N(\mu, \sigma^2)$ distribution, for some choice of $\mu$ and $\sigma^2$, arises quite often in applications. Part of the reason for this is an important result known as the \emph{central limit theorem}, which we will discuss in Section~\ref{sec:4.4}. In particular, this result leads to using a normal distribution to approximate other distributions, just as we used the Poisson distribution to approximate the binomial distribution in Example~\ref{ex:2.3.6}.

In a large human population, it is not uncommon for various body measurements to be normally distributed (at least to a reasonable degree of approximation). For example, let us suppose that heights (measured in feet) of students at a particular university are distributed $N(\mu, \sigma^2)$ for some choice of $\mu$ and $\sigma^2$. Then the probability that a randomly selected student has height between $a$ and $b$ feet, with $a < b$, is given by
\[
\int_a^b \frac{1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/(2\sigma^2)} \, \mathrm{d}x.
\]
In Section~\ref{sec:2.5}, we will discuss how to evaluate such an integral. Later in this text, we will discuss how to select an appropriate value for $\mu$ and $\sigma^2$ and to assess whether or not any normal distribution is appropriate to model the distribution of a variable defined on a particular population.
\end{example}

Given an absolutely continuous random variable $X$, we will write its density as $f_X$, or as $f$ if no confusion arises. Absolutely continuous random variables will be used extensively in later chapters of this book.

\begin{remark}
\label{rem:2.4.1}
Finally, we note that density functions are not unique. Indeed, if $f$ is a density function and we change its value at a finite number of points, then the value of $\int_a^b f(x) \, \mathrm{d}x$ will remain unchanged. Hence, the changed function will also qualify as a density corresponding to the same distribution. On the other hand, often a particular ``best'' choice of density function is clear. For example, if the density function can be chosen to be continuous, or even piecewise continuous, then this is preferred over some other version of the density function.

To take a specific example, for the Uniform$[0, 1]$ distribution, we could replace the density $f$ of \eqref{eq:2.4.3} by
\[
g(x) = \begin{cases} 1 & 0 < x < 1 \\ 0 & \text{otherwise,} \end{cases}
\]
or even by
\[
h(x) = \begin{cases} 1 & 0 \leqslant x < 3/4 \\ 17 & x = 3/4 \\ 1 & 3/4 < x \leqslant 1 \\ 0 & \text{otherwise.} \end{cases}
\]
Either of these new densities would again define the Uniform$[0, 1]$ distribution, because we would have $\int_a^b f(x) \, \mathrm{d}x = \int_a^b g(x) \, \mathrm{d}x = \int_a^b h(x) \, \mathrm{d}x$ for any $a \leqslant b$.

On the other hand, the densities $f$ and $g$ are both piecewise continuous and are therefore natural choices for the density function, whereas $h$ is an unnecessarily complicated choice. Hence, when dealing with density functions, we shall always assume that they are as continuous as possible, such as $f$ and $g$, rather than having removable discontinuities such as $h$. This will be particularly important when discussing likelihood methods in Chapter~\ref{ch:6}.
\end{remark}

\subsection*{Summary of Section~\ref{sec:2.4}}

\begin{itemize}
\item A random variable $X$ is continuous if $\prb(X = x) = 0$ for all $x$, i.e., if none of its probability comes from being equal to particular values.
\item $X$ is absolutely continuous if there exists a density function $f_X$ with $\prb(a \leqslant X \leqslant b) = \int_a^b f_X(x) \, \mathrm{d}x$ for all $a \leqslant b$.
\item Important absolutely continuous distributions include the uniform, exponential, gamma, and normal.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.4.1}
Let $U \sim \text{Uniform}[0, 1]$. Compute each of the following.
\begin{enumerate}[(a)]
\item $\prb(U < 0)$
\item $\prb(U < 1/2)$
\item $\prb(U < 1/3)$
\item $\prb(U > 2/3)$
\item $\prb(U \geqslant 2/3)$
\item $\prb(U < 1)$
\item $\prb(U < 17)$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\prb(U \leqslant 0) = 0$.
    \item $\prb(U = 1/2) = 0$.
    \item $\prb(U < -1/3) = 0$.
    \item $\prb(U \leqslant 2/3) = 2/3$.
    \item[(f)] $\prb(U < 1) = 1$.
    \item[(g)] $\prb(U \leqslant 17) = 1$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.4.2}
Let $W \sim \text{Uniform}[1, 4]$. Compute each of the following.
\begin{enumerate}[(a)]
\item $\prb(W < 5)$
\item $\prb(W < 2)$
\item $\prb(W^2 < 9)$ (Hint: If $W^2 < 9$, what must $W$ be?)
\item $\prb(W^2 < 2)$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\prb(W \geqslant 5) = 0$.
    \item $\prb(W \geqslant 2) = 2/3$.
    \item $\prb(W^2 \leqslant 9) = \prb(W \leqslant 3) = 2/3$.
    \item $\prb(W^2 \leqslant 2) = \prb(W \leqslant \sqrt{2}) = (\sqrt{2}-1)/3$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.4.3}
Let $Z \sim \text{Exponential}(4)$. Compute each of the following.
\begin{enumerate}[(a)]
\item $\prb(Z < 5)$
\item $\prb(Z > 5)$
\item $\prb(Z^2 < 9)$
\item $\prb(Z^4 - 17 < 9)$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\prb(Z \geqslant 5) = e^{-20}$.
    \item $\prb(Z \geqslant -5) = 1$.
    \item $\prb(Z^2 \geqslant 9) = \prb(Z \geqslant 3) = e^{-12}$.
    \item $\prb(Z^2 - 17 \geqslant 9) = \prb(Z^2 \geqslant 25) = \prb(Z \geqslant 5) = e^{-25}$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.4.4}
Establish for which constants $c$ the following functions are densities.
\begin{enumerate}[(a)]
\item $f(x) = cx$ on $[0, 1]$ and $0$ otherwise.
\item $f(x) = cx^n$ on $[0, 1]$ and $0$ otherwise, for $n$ a nonnegative integer.
\item $f(x) = cx^{1/2}$ on $[0, 2]$ and $0$ otherwise.
\item $f(x) = c \sin(x)$ on $[0, 2\pi]$ and $0$ otherwise.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $1 = \int_0^1 cx\,\mathrm{d}x = c/2$ so $c = 2$.
    \item $1 = \int_0^1 cx^n\,\mathrm{d}x = c/(n+1)$ so $c = n+1$.
    \item $1 = \int_0^2 cx^{1/2}\,\mathrm{d}x = c(2/3)x^{3/2}\big|_0^2 = c(2/3)2^{3/2}$ so $c = 3/\sqrt{2^4} = 3/(2\sqrt{2})$.
    \item $1 = \int_0^{\pi/2} c\sin x\,\mathrm{d}x = -c\cos x\big|_0^{\pi/2} = c$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.4.5}
Is the function defined by $f(x) = (x + 3)/4$ for $-1 \leqslant x \leqslant 2$ and $0$ otherwise, a density? Why or why not?
\end{exercise}

\begin{solution}
This is not a density because it takes negative values.
\end{solution}

\begin{exercise}
\label{exer:2.4.6}
Let $X \sim \text{Exponential}(3)$. Compute each of the following.
\begin{enumerate}[(a)]
\item $\prb(0 < X < 1)$
\item $\prb(0 < X < 3)$
\item $\prb(0 < X < 5)$
\item $\prb(2 < X < 5)$
\item $\prb(2 < X < 10)$
\item $\prb(X > 2)$
\end{enumerate}
\end{exercise}

\begin{solution}
Let $F(x) = \prb(0 < X < x)$ for $x \in [0, \infty)$. Then, $F(x) = \int_0^x 3e^{-3y}\,\mathrm{d}y = -e^{-3y}\big|_{y=0}^{y=x} = 1 - e^{-3x}$.
\begin{enumerate}[(a)]
    \item $\prb(0 < X < 1) = F(1) = 1 - e^{-3} = 0.95021$.
    \item $\prb(0 < X < 3) = F(3) = 1 - e^{-9} = 0.99988$.
    \item $\prb(0 < X < 5) = F(5) = 1 - e^{-15} = 0.9999997$.
    \item $\prb(2 < X < 5) = \prb(0 < X < 5) - \prb(0 < X \leqslant 2) = F(5) - F(2) = 1 - e^{-15} - (1 - e^{-6}) = e^{-6}(1 - e^{-9}) = 0.00247845$.
    \item[(e)] $\prb(2 < X < 10) = \prb(0 < X < 10) - \prb(0 < X \leqslant 2) = F(10) - F(2) = 1 - e^{-30} - (1 - e^{-6}) = e^{-6}(1 - e^{-24}) = 0.00247875$.
    \item[(f)] $\prb(X > 2) = 1 - \prb(0 < X \leqslant 2) = 1 - F(2) = e^{-6} = 0.00247852$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.4.7}
Let $M > 0$, and suppose $f(x) = cx^2$ for $0 \leqslant x \leqslant M$, otherwise $f(x) = 0$. For what value of $c$ (depending on $M$) is $f$ a density?
\end{exercise}

\begin{solution}
To be a density function, $f$ must satisfy $f \geqslant 0$ and $\int_0^M f(x)\,\mathrm{d}x = 1$. The first condition is equivalent to $c \geqslant 0$. The second condition is
\[
    1 = \int_0^M f(x)\,\mathrm{d}x = c\int_0^M x^2\,\mathrm{d}x = c\frac{x^3}{3}\bigg|_{x=0}^{x=M} = cM^3/3.
\]
Hence, the constant is $c = 3/M^3$.
\end{solution}

\begin{exercise}
\label{exer:2.4.8}
Suppose $X$ has density $f$ and that $f(x) = 2$ for $0.3 \leqslant x \leqslant 0.4$. Prove that $\prb(0.3 \leqslant X \leqslant 0.4) = 0.2$.
\end{exercise}

\begin{solution}
The probability $\prb(0.3 < X < 0.4)$ is
\[
    \prb(0.3 < X < 0.4) = \int_{0.3}^{0.4} f(x)\,\mathrm{d}x \geqslant \int_{0.3}^{0.4} 2\,\mathrm{d}x = 2x\big|_{x=0.3}^{x=0.4} = 2(0.4 - 0.3) = 0.2.
\]
\end{solution}

\begin{exercise}
\label{exer:2.4.9}
Suppose $X$ has density $f$ and $Y$ has density $g$. Suppose $f(x) = g(x)$ for $1 \leqslant x \leqslant 2$. Prove that $\prb(1 \leqslant X \leqslant 2) = \prb(1 \leqslant Y \leqslant 2)$.
\end{exercise}

\begin{solution}
By the definition of a density,
\[
    \prb(1 < X < 2) = \int_1^2 f(x)\,\mathrm{d}x \geqslant \int_1^2 g(x)\,\mathrm{d}x = \prb(1 < Y < 2).
\]
Suppose $\prb(1 < X < 2) = \prb(1 < Y < 2)$. Then,
\[
    0 = \prb(1 < X < 2) - \prb(1 < Y < 2) = \int_1^2 (f(x) - g(x))\,\mathrm{d}x
\]
implies $f(x) = g(x)$ for almost everywhere on $(1, 2)$. It contradicts to the assumption $f(x) > g(x)$. Hence, we must have $\prb(1 < X < 2) > \prb(1 < Y < 2)$.
\end{solution}

\begin{exercise}
\label{exer:2.4.10}
Suppose $X$ has density $f$ and $Y$ has density $g$. Is it possible that $f(x) > g(x)$ for all $x$? Explain.
\end{exercise}

\begin{solution}
Suppose $X$ takes values on $(1, 2)$ and $f(x) > g(x)$ for all $x \in (1, 2)$. Then, $\prb(1 < X < 2) = \prb(1 < Y < 2) = 1$ but, from Exercise \ref{exer:2.4.9}, $\prb(1 < X < 2) > \prb(1 < Y < 2) = 1$ and this is a contradiction. Hence, $f(x) > g(x)$ for all $x$ is impossible.
\end{solution}

\begin{exercise}
\label{exer:2.4.11}
Suppose $X$ has density $f$ and $f(x) \leqslant f(y)$ whenever $0 \leqslant x \leqslant 1 \leqslant y \leqslant 2$. Does it follow that $\prb(0 \leqslant X \leqslant 1) \leqslant \prb(1 \leqslant X \leqslant 2)$? Explain.
\end{exercise}

\begin{solution}
Let $c = \sup_{y \in (1,2)} f(y)$. Then, $f(x) \geqslant c \geqslant f(y)$ for all $0 < x < 1 < y < 2$. Hence, $\prb(0 < X < 1) = \int_0^1 f(x)\,\mathrm{d}x \geqslant \int_0^1 c\,\mathrm{d}x = c \geqslant \int_1^2 f(y)\,\mathrm{d}y = \prb(1 < X < 2)$.

Note that $f(x) - f(x+1) > 0$ for all $0 < x < 1$. If $\prb(0 < X < 1) = \prb(1 < X < 2)$, then $\int_0^1 f(x) - f(x+1)\,\mathrm{d}x = \prb(0 < X < 1) - \prb(1 < X < 2) = 0$. Hence, we get $f(x) = f(x+1)$ almost everywhere on $(0, 1)$ and this is a contradiction. Thus, $\prb(0 < X < 1) > \prb(1 < X < 2)$ holds.
\end{solution}

\begin{exercise}
\label{exer:2.4.12}
Suppose $X$ has density $f$ and $f(x) \leqslant f(y)$ whenever $0 \leqslant x \leqslant 1 \leqslant y \leqslant 3$. Does it follow that $\prb(0 \leqslant X \leqslant 1) \leqslant \prb(1 \leqslant X \leqslant 3)$? Explain.
\end{exercise}

\begin{solution}
Let $f$ be a density function given by $f(x) = 2/5$ if $x \in (0, 1)$, $f(x) = 3/10$ if $x \in (1, 3)$ and 0 otherwise. Then, $f$ satisfies $f(x) = 2/5 > 3/10 = f(y)$ whenever $0 < x < 1 < y < 3$. However, $\prb(0 < X < 1) = \int_0^1 2/5\,\mathrm{d}x = 2/5 < 3/5 = \int_1^3 3/10\,\mathrm{d}y = \prb(1 < X < 3)$. Therefore, $\prb(0 < X < 1) > \prb(1 < X < 3)$ doesn't hold.
\end{solution}

\begin{exercise}
\label{exer:2.4.13}
Suppose $X \sim N(0, 1)$ and $Y \sim N(1, 1)$. Prove that $\prb(X > 3) < \prb(Y > 3)$.
\end{exercise}

\begin{solution}
The density of $N(\mu, \sigma^2)$ is $(2\pi\sigma^2)^{-1/2}\exp(-(x-\mu)^2/(2\sigma^2))$.
\begin{align*}
    \prb(Y < 3) &= \int_{-\infty}^{3}(2\pi)^{-1/2}\exp(-(y-1)^2/2)\,\mathrm{d}y \\
    &= \int_{-\infty}^{2}(2\pi)^{-1/2}\exp(-u^2/2)\,\mathrm{d}u = \prb(X < 2).
\end{align*}
Hence, $\prb(Y < 3) = \prb(X < 2) < \prb(X < 3)$ by the monotonicity.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.4.14}
Let $Y \sim \text{Exponential}(\lambda)$ for some $\lambda > 0$. Let $y, h > 0$. Prove that $\prb(Y > h + y \mid Y > h) = \prb(Y > y)$. That is, conditional on knowing that $Y > h$, the random variable $Y - h$ has the same distribution as $Y$ did originally. This is called the \emph{memoryless property} of the exponential distributions; it says that they immediately ``forget'' their past behavior.
\end{exercise}

\begin{solution}
$\prb(Y - h \geqslant y \mid Y \geqslant h) = \prb(Y \geqslant y + h)/\prb(Y \geqslant y) = e^{-\lambda(y+h)}/e^{-\lambda h} = e^{-\lambda y} = \prb(Y \geqslant y)$.
\end{solution}

\begin{exercise}
\label{exer:2.4.15}
Consider the gamma function $\Gamma(\alpha) = \int_0^{\infty} t^{\alpha - 1} e^{-t} \, \mathrm{d}t$, for $\alpha > 0$.
\begin{enumerate}[(a)]
\item Prove that $\Gamma(\alpha + 1) = \alpha \, \Gamma(\alpha)$. (Hint: Use integration by parts.)
\item Prove that $\Gamma(1) = 1$.
\item Use parts (a) and (b) to show that $\Gamma(n) = (n-1)!$ if $n$ is a positive integer.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Using integration by parts, $\Gamma(\alpha+1) = \int_0^{\infty} t^{\alpha}e^{-t}\,dt = 0 - \int_0^{\infty}(\alpha t^{\alpha-1})(-e^{-t})\,dt = \alpha\int_0^{\infty}t^{\alpha-1}e^{-t}\,dt = \alpha\Gamma(\alpha)$.
    \item $\Gamma(1) = \int_0^{\infty} t^0 e^{-t}\,dt = (-e^{-t})\big|_{t=0}^{t=\infty} = 1$.
    \item We use induction on $n$. By part (b), the statement is true for $n = 1$. By part (a), if the statement is true for $n$, then it is also true for $n+1$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.4.16}
Use the fact that $\Gamma(1/2) = \sqrt{\pi}$ to give an alternate proof that $\int_{-\infty}^{\infty} \phi(x) \, \mathrm{d}x = 1$ (as in Theorem~\ref{thm:2.4.2}). (Hint: Make the substitution $t = x^2/2$.)
\end{exercise}

\begin{solution}
Using the substitution $t = x^2/2$,
\[
    \int_{-\infty}^{\infty}\phi(x)\,\mathrm{d}x = 2\int_0^{\infty}\phi(x)\,\mathrm{d}x = 2\int_0^{\infty}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,\mathrm{d}x = \frac{2}{\sqrt{2\pi}}\int_0^{\infty}e^{-t}\frac{1}{2}(2t)^{-1/2}\,dt = \frac{2}{\sqrt{2\pi}}\cdot\frac{1}{2}\cdot 2^{-1/2}\Gamma(1/2) = 1
\]
since $\Gamma(1/2) = \sqrt{\pi}$.
\end{solution}

\begin{exercise}
\label{exer:2.4.17}
Let $f$ be the density of the Gamma$(\alpha, \lambda)$ distribution, as in \eqref{eq:2.4.8}. Prove that $\int_0^{\infty} f(x) \, \mathrm{d}x = 1$. (Hint: Let $t = \lambda x$.)
\end{exercise}

\begin{solution}
Using the substitution $t = \lambda x$, $\int_0^{\infty} f(x)\,\mathrm{d}x = \int_0^{\infty}\frac{\lambda^{\alpha}x^{\alpha-1}}{\Gamma(\alpha)}e^{-\lambda x}\,\mathrm{d}x = \frac{\lambda^{\alpha}}{\Gamma(\alpha)}\int_0^{\infty}x^{\alpha-1}e^{-\lambda x}\,\mathrm{d}x = \frac{\lambda^{\alpha}}{\Gamma(\alpha)}\int_0^{\infty}(t/\lambda)^{\alpha-1}e^{-t}(1/\lambda)\,dt = \frac{1}{\Gamma(\alpha)}\int_0^{\infty}t^{\alpha-1}e^{-t}\,dt = \frac{1}{\Gamma(\alpha)}\Gamma(\alpha) = 1$.
\end{solution}

\begin{exercise}[Logistic distribution]
\label{exer:2.4.18}
Consider the function given by $f(x) = e^{-x}/(1 + e^{-x})^2$ for $x \in \mathbf{R}^1$. Prove that $f$ is a density function.
\end{exercise}

\begin{solution}
We have that $f(x) \geqslant 0$ for every $x$ and putting $u = e^{-x}$, $\mathrm{d}u = -e^{-x}\,\mathrm{d}x$ we have $\int_{-\infty}^{\infty}e^{-x}(1+e^{-x})^{-2}\,\mathrm{d}x = \int_0^{\infty}(1+u)^{-2}\,\mathrm{d}u = -(1+u)^{-1}\big|_0^{\infty} = 1$.
\end{solution}

\begin{exercise}[Weibull distribution]
\label{exer:2.4.19}
Consider, for $\alpha > 0$ fixed, the function given by $f(x) = \alpha x^{\alpha - 1} e^{-x^{\alpha}}$ for $0 \leqslant x < \infty$ and $0$ otherwise. Prove that $f$ is a density function.
\end{exercise}

\begin{solution}
We have that $f(x) \geqslant 0$ for every $x$ and putting $u = x^{\alpha}$, $\mathrm{d}u = \alpha x^{\alpha-1}\,\mathrm{d}x$ we have $\int_0^{\infty}\alpha x^{\alpha-1}e^{-x^{\alpha}}\,\mathrm{d}x = \int_0^{\infty}e^{-u}\,\mathrm{d}u = -e^{-u}\big|_0^{\infty} = 1$.
\end{solution}

\begin{exercise}[Pareto distribution]
\label{exer:2.4.20}
Consider, for $\alpha > 0$ fixed, the function given by $f(x) = \alpha(1 + x)^{-(\alpha + 1)}$ for $0 \leqslant x < \infty$ and $0$ otherwise. Prove that $f$ is a density function.
\end{exercise}

\begin{solution}
We have that $f(x) \geqslant 0$ for every $x$, and we have $\int_0^{\infty}\alpha(1+x)^{-\alpha-1}\,\mathrm{d}x = -(1+x)^{-\alpha}\big|_0^{\infty} = 1$.
\end{solution}

\begin{exercise}[Cauchy distribution]
\label{exer:2.4.21}
Consider the function given by
\[
f(x) = \frac{1}{\pi} \cdot \frac{1}{1 + x^2}
\]
for $x \in \mathbf{R}^1$. Prove that $f$ is a density function. (Hint: Recall the derivative of $\arctan(x)$.)
\end{exercise}

\begin{solution}
We have that $f(x) \geqslant 0$ for every $x$, and we have $\int_{-\infty}^{\infty}\frac{1}{\pi(1+x^2)}\,\mathrm{d}x = \frac{1}{\pi}\arctan x\big|_{-\infty}^{\infty} = \frac{1}{\pi}\left(\frac{\pi}{2} - \left(-\frac{\pi}{2}\right)\right) = 1$.
\end{solution}

\begin{exercise}[Laplace distribution]
\label{exer:2.4.22}
Consider the function given by $f(x) = e^{-|x|}/2$ for $x \in \mathbf{R}^1$ and $0$ otherwise. Prove that $f$ is a density function.
\end{exercise}

\begin{solution}
We have that $f(x) \geqslant 0$ for every $x$, and we have
\[
    \int_{-\infty}^{\infty}\frac{1}{2}e^{-|x|}\,\mathrm{d}x = \int_0^{\infty}\frac{1}{2}e^{-x}\,\mathrm{d}x + \int_{-\infty}^{0}\frac{1}{2}e^{x}\,\mathrm{d}x = \frac{1}{2} + \frac{1}{2}\bigl(e^x\big|_{-\infty}^{0}\bigr) = \frac{1}{2} + \frac{1}{2} = 1.
\]
\end{solution}

\begin{exercise}[Extreme value distribution]
\label{exer:2.4.23}
Consider the function given by $f(x) = e^{-x} \exp(-e^{-x})$ for $x \in \mathbf{R}^1$ and $0$ otherwise. Prove that $f$ is a density function.
\end{exercise}

\begin{solution}
We have that $f(x) \geqslant 0$ for every $x$, and we have $\int_{-\infty}^{\infty}e^{-x}\exp\{-e^{-x}\}\,\mathrm{d}x = \exp\{-e^{-x}\}\big|_{-\infty}^{\infty} = 1 - 0 = 1$.
\end{solution}

\begin{exercise}[Beta$(a, b)$ distribution]
\label{exer:2.4.24}
The \emph{beta function} is the function $B : (0, \infty)^2 \to \mathbf{R}^1$ given by
\[
B(a, b) = \int_0^1 x^{a-1} (1-x)^{b-1} \, \mathrm{d}x.
\]
It can be proved (see Challenge~\ref{exer:2.4.25}) that
\begin{equation}
\label{eq:2.4.10}
B(a, b) = \frac{\Gamma(a) \, \Gamma(b)}{\Gamma(a + b)}.
\end{equation}
\begin{enumerate}[(a)]
\item Prove that the function $f$ given by $f(x) = B^{-1}(a, b) \, x^{a-1} (1-x)^{b-1}$ for $0 < x < 1$ and $0$ otherwise, is a density function.
\item Determine and plot the density when $a = 1$, $b = 1$. Can you name this distribution?
\item Determine and plot the density when $a = 2$, $b = 1$.
\item Determine and plot the density when $a = 1$, $b = 2$.
\item Determine and plot the density when $a = 2$, $b = 2$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item We have that $f(x) \geqslant 0$ for every $x$, and $\int_0^1 B^{-1}(a,b)x^{a-1}(1-x)^{b-1}\,\mathrm{d}x = B^{-1}(a,b)B(a,b) = 1$.
    \item $f(x) = 1$ for $0 < x < 1$ and this is the $\text{Uniform}(0,1)$ distribution.
    \item $f(x) = B^{-1}(2,1)x = (\Gamma(2)\Gamma(1)/\Gamma(3))^{-1}x = 2x$ for $0 < x < 1$.
    \item $f(x) = B^{-1}(1,2)(1-x) = (\Gamma(1)\Gamma(2)/\Gamma(3))^{-1}(1-x) = 2(1-x)$ for $0 < x < 1$.
    \item $f(x) = B^{-1}(2,2)x(1-x) = (\Gamma(2)\Gamma(2)/\Gamma(4))^{-1}x(1-x) = 6x(1-x)$ for $0 < x < 1$.
\end{enumerate}
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:2.4.25}
Prove \eqref{eq:2.4.10}. (Hint: Use $\Gamma(a) \, \Gamma(b) = \int_0^{\infty} \int_0^{\infty} x^{a-1} y^{b-1} e^{-(x+y)} \, \mathrm{d}x \, \mathrm{d}y$ and make the change of variable $u = x + y$, $\nu = x/u$.)
\end{exercise}

\begin{solution}
The transformation $u = x + y$, $v = x/u$ has inverse $x = uv$, $y = u(1-v)$ and therefore Jacobian
\[
    \left|\det\begin{pmatrix} v & u \\ 1-v & -u \end{pmatrix}\right| = |uv + u(1-v)| = 1
\]
so we have that
\begin{align*}
    \Gamma(a)\Gamma(b) &= \int_0^{\infty}\int_0^{\infty}x^{a-1}y^{b-1}e^{-x-y}\,\mathrm{d}x\,\mathrm{d}y = \int_0^1\int_0^{\infty}(uv)^{a-1}u^{b-1}(1-v)^{b-1}e^{-u}\,\mathrm{d}u\,\mathrm{d}v \\
    &= \int_0^1 v^{a-1}(1-v)^{b-1}\left(\int_0^{\infty}u^{a+b-1}e^{-u}\,\mathrm{d}u\right)\mathrm{d}v \\
    &= \Gamma(a+b)\int_0^1 v^{a-1}(1-v)^{b-1}\,\mathrm{d}v.
\end{align*}
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:2.4.26}
Suppose $X \sim N(0, 1)$ and $Y \sim N(0, 4)$. Which do you think is larger, $\prb(|X| > 2)$ or $\prb(|Y| > 2)$? Why? (Hint: Look at Figure~\ref{fig:2.4.5}.)
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cumulative Distribution Functions}
\label{sec:2.5}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

If $X$ is a random variable, then its distribution consists of the values of $\prb(X \in B)$ for all subsets $B$ of the real numbers. However, there are certain special subsets $B$ that are convenient to work with. Specifically, if $B = (-\infty, x]$ for some real number $x$, then $\prb(X \in B) = \prb(X \leqslant x)$. It turns out (see Theorem~\ref{thm:2.5.1}) that it is sufficient to keep track of $\prb(X \leqslant x)$ for all real numbers $x$.

This motivates the following definition.

\begin{definition}
\label{def:2.5.1}
Given a random variable $X$, its \emph{cumulative distribution function} (or \emph{distribution function}, or \emph{cdf} for short) is the function $F_X : \mathbf{R}^1 \to [0, 1]$, defined by $F_X(x) = \prb(X \leqslant x)$. (Where there is no confusion, we sometimes write $F(x)$ for $F_X(x)$.)
\end{definition}

The reason for calling $F_X$ the ``distribution function'' is that the full distribution of $X$ can be determined directly from $F_X$. We demonstrate this for some events of particular importance.

First, suppose that $B = (a, b]$ is a left-open interval. Using (1.3.3),
\[
\prb(X \in B) = \prb(a < X \leqslant b) = \prb(X \leqslant b) - \prb(X \leqslant a) = F_X(b) - F_X(a).
\]

Now, suppose that $B = [a, b]$ is a closed interval. Using the continuity of probability (see Theorem~1.6.1), we have
\[
\prb(X \in B) = \prb(a \leqslant X \leqslant b) = \lim_{n \to \infty} \prb(a - 1/n < X \leqslant b) = \lim_{n \to \infty} [F_X(b) - F_X(a - 1/n)] = F_X(b) - \lim_{n \to \infty} F_X(a - 1/n).
\]

We sometimes write $\lim_{n \to \infty} F_X(a - 1/n)$ as $F_X(a^-)$, so that $\prb(X \in [a, b]) = F_X(b) - F_X(a^-)$. In the special case where $a = b$, we have
\begin{equation}
\label{eq:2.5.1}
\prb(X = a) = F_X(a) - F_X(a^-).
\end{equation}

Similarly, if $B = (a, b)$ is an open interval, then
\[
\prb(X \in B) = \prb(a < X < b) = \lim_{n \to \infty} [F_X(b - 1/n) - F_X(a)] = F_X(b^-) - F_X(a).
\]

If $B = [a, b)$ is a right-open interval, then
\[
\prb(X \in B) = \prb(a \leqslant X < b) = \lim_{n \to \infty} [F_X(b - 1/n)] - \lim_{n \to \infty} [F_X(a - 1/n)] = F_X(b^-) - F_X(a^-).
\]

We conclude that we can determine $\prb(X \in B)$ from $F_X$ whenever $B$ is any kind of interval.

Now, if $B$ is instead a union of intervals, then we can use additivity to again compute $\prb(X \in B)$ from $F_X$. For example, if
\[
B = (a_1, b_1] \cup (a_2, b_2] \cup \cdots \cup (a_k, b_k]
\]
with $a_1 < b_1 < a_2 < b_2 < \cdots < a_k < b_k$, then by additivity,
\[
\prb(X \in B) = \prb(X \in (a_1, b_1]) + \cdots + \prb(X \in (a_k, b_k]) = [F_X(b_1) - F_X(a_1)] + \cdots + [F_X(b_k) - F_X(a_k)].
\]

Hence, we can still compute $\prb(X \in B)$ solely from the values of $F_X(x)$.

\begin{theorem}
\label{thm:2.5.1}
Let $X$ be any random variable, with cumulative distribution function $F_X$. Let $B$ be any subset of the real numbers. Then $\prb(X \in B)$ can be determined solely from the values of $F_X(x)$.
\end{theorem}

\begin{proof}[Proof (Outline)]
It turns out that all relevant subsets $B$ can be obtained by applying limiting operations to unions of intervals. Hence, because $F_X$ determines $\prb(X \in B)$ when $B$ is a union of intervals, it follows that $F_X$ determines $\prb(X \in B)$ for all relevant subsets $B$.
\end{proof}

\subsection{Properties of Distribution Functions}
\label{ssec:2.5.1}

In light of Theorem~\ref{thm:2.5.1}, we see that cumulative distribution functions $F_X$ are very useful. Thus, we note a few of their basic properties here.

\begin{theorem}
\label{thm:2.5.2}
Let $F_X$ be the cumulative distribution function of a random variable $X$. Then
\begin{enumerate}[(a)]
\item $0 \leqslant F_X(x) \leqslant 1$ for all $x$,
\item $F_X(x) \leqslant F_X(y)$ whenever $x \leqslant y$ (i.e., $F_X$ is increasing),
\item $\lim_{x \to \infty} F_X(x) = 1$,
\item $\lim_{x \to -\infty} F_X(x) = 0$.
\end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}[(a)]
    \item Because $F_X(x) = \prb(X \leqslant x)$ is a probability, it is between $0$ and $1$.
    \item Let $A = \{X \leqslant x\}$ and $B = \{X \leqslant y\}$. Then if $x \leqslant y$, then $A \subseteq B$, so that $\prb(A) \leqslant \prb(B)$. But $\prb(A) = F_X(x)$ and $\prb(B) = F_X(y)$, so the result follows.
    \item Let $A_n = \{X \leqslant n\}$. Because $X$ must take on some value and hence $X \leqslant n$ for sufficiently large $n$, we see that $A_n$ increases to $S$, i.e., $A_n \nearrow S$ (see Section~\ref{sec:1.6}). Hence, by continuity of $\prb$ (see Theorem~1.6.1), $\lim_{n \to \infty} \prb(A_n) = \prb(S) = 1$. But $\prb(A_n) = \prb(X \leqslant n) = F_X(n)$, so the result follows.
    \item Let $B_n = \{X \leqslant -n\}$. Because $X > -n$ for sufficiently large $n$, $B_n$ decreases to the empty set, i.e., $B_n \searrow \emptyset$. Hence, again by continuity of $\prb$, $\lim_{n \to \infty} \prb(B_n) = \prb(\emptyset) = 0$. But $\prb(B_n) = \prb(X \leqslant -n) = F_X(-n)$, so the result follows.
  \end{enumerate}
\end{proof}

If $F_X$ is a cumulative distribution function, then $F_X$ is also right continuous; see Problem~\ref{exer:2.5.17}. It turns out that if a function $F : \mathbf{R}^1 \to \mathbf{R}^1$ satisfies properties (a) through (d) and is right continuous, then there is a unique probability measure $\prb$ on $\mathbf{R}^1$ such that $F$ is the cdf of $\prb$. We will not prove this result here.\footnote{For example, see page 67 of \emph{A First Look at Rigorous Probability Theory}, Second Edition, by J.\ S.\ Rosenthal (World Scientific Publishing, Singapore, 2006).}

\subsection{Cdfs of Discrete Distributions}
\label{ssec:2.5.2}

We can compute the cumulative distribution function (cdf) $F_X$ of a discrete random variable from its probability function $p_X$, as follows.

\begin{theorem}
\label{thm:2.5.3}
Let $X$ be a discrete random variable with probability function $p_X$. Then its cumulative distribution function $F_X$ satisfies $F_X(x) = \sum_{y \leqslant x} p_X(y)$.
\end{theorem}

\begin{proof}
Let $x_1, x_2, \ldots$ be the possible values of $X$. Then $F_X(x) = \prb(X \leqslant x) = \sum_{x_i \leqslant x} \prb(X = x_i) = \sum_{y \leqslant x} \prb(X = y) = \sum_{y \leqslant x} p_X(y)$, as claimed.
\end{proof}

Hence, if $X$ is a discrete random variable, then by Theorem~\ref{thm:2.5.3}, $F_X$ is piecewise constant, with a jump of size $p_X(x_i)$ at each value $x_i$. A plot of such a distribution looks like that depicted in Figure~\ref{fig:2.5.1}.

We consider an example of a distribution function of a discrete random variable.

\begin{example}
\label{ex:2.5.1}
Consider rolling one fair six-sided die, so that $S = \{1, 2, 3, 4, 5, 6\}$, with $\prb(\{s\}) = 1/6$ for each $s \in S$. Let $X$ be the number showing on the die divided by $6$, so that $X(s) = s/6$ for $s \in S$. What is $F_X(x)$? Since $X(s) \leqslant x$ if and only if $s \leqslant 6x$, we have that
\[
F_X(x) = \prb(X \leqslant x) = \sum_{s \in S : s \leqslant 6x} \prb(\{s\}) = \sum_{s \in S : s \leqslant 6x} \frac{1}{6} = \frac{1}{6} \, |\{s \in S : s \leqslant 6x\}|.
\]
That is, to compute $F_X(x)$, we count how many elements $s \in S$ satisfy $s \leqslant 6x$ and multiply that number by $1/6$. Therefore,
\[
F_X(x) = \begin{cases}
0 & x < 1/6 \\
1/6 & 1/6 \leqslant x < 2/6 \\
2/6 & 2/6 \leqslant x < 3/6 \\
3/6 & 3/6 \leqslant x < 4/6 \\
4/6 & 4/6 \leqslant x < 5/6 \\
5/6 & 5/6 \leqslant x < 1 \\
6/6 & 1 \leqslant x.
\end{cases}
\]
In Figure~\ref{fig:2.5.1}, we present a graph of the function $F_X$ and note that this is a step function. Note (see Exercise~\ref{exer:2.5.1}) that the properties of Theorem~\ref{thm:2.5.2} are indeed satisfied by the function $F_X$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-5-1.pdf}
\caption{Graph of the cdf $F_X$ in Example~\ref{ex:2.5.1}.}
\label{fig:2.5.1}
\end{figure}
\end{example}

\subsection{Cdfs of Absolutely Continuous Distributions}
\label{ssec:2.5.3}

Once we know the density $f_X$ of $X$, then it is easy to compute the cumulative distribution function of $X$, as follows.

\begin{theorem}
\label{thm:2.5.4}
Let $X$ be an absolutely continuous random variable, with density function $f_X$. Then the cumulative distribution function $F_X$ of $X$ satisfies
\[
F_X(x) = \int_{-\infty}^{x} f_X(t) \, \mathrm{d}t
\]
for $x \in \mathbf{R}^1$.
\end{theorem}

\begin{proof}
This follows from \eqref{eq:2.4.5}, by setting $b = x$ and letting $a \to -\infty$.
\end{proof}

From the fundamental theorem of calculus, we see that it is also possible to compute a density $f_X$ once we know the cumulative distribution function $F_X$.

\begin{corollary}
\label{cor:2.5.1}
Let $X$ be an absolutely continuous random variable, with cumulative distribution function $F_X$. Let
\[
f_X(x) = \frac{\mathrm{d}}{\mathrm{d}x} F_X(x) = F_X'(x).
\]
Then $f_X$ is a density function for $X$.
\end{corollary}

We note that $F_X$ might not be differentiable everywhere, so that the function $f_X$ of the corollary might not be defined at certain isolated points. The density function may take any value at such points.

Consider again the $N(0, 1)$ distribution, with density $\phi$ given by \eqref{eq:2.4.9}. According to Theorem~\ref{thm:2.5.4}, the cumulative distribution function $F$ of this distribution is given by
\[
F(x) = \int_{-\infty}^{x} \phi(t) \, \mathrm{d}t = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \, \mathrm{d}t.
\]
It turns out that it is provably impossible to evaluate this integral exactly, except for certain specific values of $x$ (e.g., $x = -\infty$, $x = 0$, or $x = \infty$). Nevertheless, the cumulative distribution function of the $N(0, 1)$ distribution is so important that it is assigned a special symbol. Furthermore, this is tabulated in Table~D.2 of Appendix~D for certain values of $x$.

\begin{definition}
\label{def:2.5.2}
The symbol $\Phi$ stands for the cumulative distribution function of a standard normal distribution, defined by
\begin{equation}
\label{eq:2.5.2}
\Phi(x) = \int_{-\infty}^{x} \phi(t) \, \mathrm{d}t = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \, \mathrm{d}t
\end{equation}
for $x \in \mathbf{R}^1$.
\end{definition}

\begin{example}[Normal Probability Calculations]
\label{ex:2.5.2}
Suppose that $X \sim N(0, 1)$ and we want to calculate
\[
\prb(-0.63 \leqslant X \leqslant 2.0) = \prb(X \leqslant 2.0) - \prb(X < -0.63).
\]
Then $\prb(X \leqslant 2) = \Phi(2)$, while $\prb(X < -0.63) = \Phi(-0.63)$. Unfortunately, $\Phi(2)$ and $\Phi(-0.63)$ cannot be computed exactly, but they can be approximated using a computer to numerically calculate the integral \eqref{eq:2.5.2}. Virtually all statistical software packages will provide such approximations, but many tabulations such as Table~D.2, are also available. Using this table, we obtain $\Phi(2) = 0.9772$ while $\Phi(-0.63) = 0.2643$. This implies that
\[
\prb(-0.63 \leqslant X \leqslant 2.0) = \Phi(2.0) - \Phi(-0.63) = 0.9772 - 0.2643 = 0.7129.
\]

Now suppose that $X \sim N(\mu, \sigma^2)$ and we want to calculate $\prb(a \leqslant X \leqslant b)$. Letting $f$ denote the density of $X$ and following Example~\ref{ex:2.4.8}, we have
\[
\prb(a \leqslant X \leqslant b) = \int_a^b f(x) \, \mathrm{d}x = \int_a^b \frac{1}{\sigma} \phi\left(\frac{x - \mu}{\sigma}\right) \, \mathrm{d}x.
\]
Then, again following Example~\ref{ex:2.4.8}, we make the substitution $y = (x - \mu)/\sigma$ in the above integral to obtain
\[
\prb(a \leqslant X \leqslant b) = \int_{(a-\mu)/\sigma}^{(b-\mu)/\sigma} \phi\left(\frac{x - \mu}{\sigma}\right) \, \mathrm{d}x = \Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right).
\]
Therefore, general normal probabilities can be computed using the function $\Phi$.

Suppose now that $a = -0.63$, $b = 2.0$, $\mu = 1.3$, and $\sigma^2 = 4$. We obtain
\begin{align*}
\prb(-0.63 \leqslant X \leqslant 2.0) &= \Phi\left(\frac{2.0 - 1.3}{2}\right) - \Phi\left(\frac{-0.63 - 1.3}{2}\right) \\
&= \Phi(0.35) - \Phi(-0.965) = 0.6368 - 0.16725 = 0.46955
\end{align*}
because, using Table~D.2, $\Phi(0.35) = 0.6368$. We approximate $\Phi(-0.965)$ by the linear interpolation between the values $\Phi(-0.96) = 0.1685$, $\Phi(-0.97) = 0.1660$ given by
\[
\Phi(-0.965) = \Phi(-0.96) + \frac{\Phi(-0.97) - \Phi(-0.96)}{-0.97 - (-0.96)} \cdot (-0.965 - (-0.96)) = 0.1685 + \frac{0.1660 - 0.1685}{-0.97 + 0.96} \cdot (-0.965 + 0.96) = 0.16725.
\]
\end{example}

\begin{example}
\label{ex:2.5.3}
Let $X$ be a random variable with cumulative distribution function given by
\[
F_X(x) = \begin{cases}
0 & x < 2 \\
(x - 2)^4 / 16 & 2 \leqslant x \leqslant 4 \\
1 & 4 < x.
\end{cases}
\]
In Figure~\ref{fig:2.5.2}, we present a graph of $F_X$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-5-2.pdf}
\caption{Graph of the cdf $F_X$ in Example~\ref{ex:2.5.3}.}
\label{fig:2.5.2}
\end{figure}

Suppose for this random variable $X$ we want to compute $\prb(X \leqslant 3)$, $\prb(X < 3)$, $\prb(X > 2.5)$, and $\prb(1.2 \leqslant X \leqslant 3.4)$. We can compute all these probabilities directly from $F_X$. We have that
\begin{align*}
\prb(X \leqslant 3) &= F_X(3) = (3 - 2)^4 / 16 = 1/16, \\
\prb(X < 3) &= F_X(3^-) = \lim_{n \to \infty} (3 - 1/n - 2)^4 / 16 = 1/16, \\
\prb(X > 2.5) &= 1 - \prb(X \leqslant 2.5) = 1 - F_X(2.5) \\
&= 1 - (2.5 - 2)^4 / 16 = 1 - 0.0625/16 = 0.996, \\
\prb(1.2 \leqslant X \leqslant 3.4) &= F_X(3.4) - F_X(1.2^-) = (3.4 - 2)^4 / 16 - 0 = 0.2401.
\end{align*}
\end{example}

\subsection{Mixture Distributions}
\label{ssec:2.5.4}

Suppose now that $F_1, F_2, \ldots, F_k$ are cumulative distribution functions, corresponding to various distributions. Also let $p_1, p_2, \ldots, p_k$ be positive real numbers with $\sum_{i=1}^{k} p_i = 1$ (so these values form a probability distribution). Then we can define a new function $G$ by
\begin{equation}
\label{eq:2.5.3}
G(x) = p_1 F_1(x) + p_2 F_2(x) + \cdots + p_k F_k(x).
\end{equation}

It is easily verified (see Exercise~\ref{exer:2.5.6}) that the function $G$ given by \eqref{eq:2.5.3} will satisfy properties (a) through (d) of Theorem~\ref{thm:2.5.2} and is right continuous. Hence, $G$ is also a cdf.

The distribution whose cdf is given by \eqref{eq:2.5.3} is called a \emph{mixture distribution} because it mixes the various distributions with cdfs $F_1, \ldots, F_k$ according to the probability distribution given by the $p_1, p_2, \ldots, p_k$.

To see how a mixture distribution arises in applications, consider a two-stage system, as discussed in Section~\ref{ssec:1.5.1}. Let $Z$ be a random variable describing the outcome of the first stage and such that $\prb(Z = i) = p_i$ for $i = 1, 2, \ldots, k$. Suppose that for the second stage, we observe a random variable $Y$ where the distribution of $Y$ depends on the outcome of the first stage, so that $Y$ has cdf $F_i$ when $Z = i$. In effect, $F_i$ is the conditional distribution of $Y$ given that $Z = i$ (see Section~\ref{sec:2.8}). Then, by the law of total probability (see Theorem~1.5.1), the distribution function of $Y$ is given by
\[
\prb(Y \leqslant y) = \sum_{i=1}^{k} \prb(Y \leqslant y \mid Z = i) \, \prb(Z = i) = \sum_{i=1}^{k} p_i F_i(y) = G(y).
\]
Therefore, the distribution function of $Y$ is given by a mixture of the $F_i$.

Consider the following example of this.

\begin{example}
\label{ex:2.5.4}
Suppose we have two bowls containing chips. Bowl \#1 contains one chip labelled $0$, two chips labelled $3$, and one chip labelled $5$. Bowl \#2 contains one chip labelled $2$, one chip labelled $4$, and one chip labelled $5$. Now let $X_i$ be the random variable corresponding to randomly drawing a chip from bowl \#$i$. Therefore, $\prb(X_1 = 0) = 1/4$, $\prb(X_1 = 3) = 1/2$, and $\prb(X_1 = 5) = 1/4$, while $\prb(X_2 = 2) = \prb(X_2 = 4) = \prb(X_2 = 5) = 1/3$. Then $X_1$ has distribution function given by
\[
F_1(x) = \begin{cases}
0 & x < 0 \\
1/4 & 0 \leqslant x < 3 \\
3/4 & 3 \leqslant x < 5 \\
1 & x \geqslant 5
\end{cases}
\]
and $X_2$ has distribution function given by
\[
F_2(x) = \begin{cases}
0 & x < 2 \\
1/3 & 2 \leqslant x < 4 \\
2/3 & 4 \leqslant x < 5 \\
1 & x \geqslant 5.
\end{cases}
\]

Now suppose that we choose a bowl by randomly selecting a card from a deck of five cards where one card is labelled $1$ and four cards are labelled $2$. Let $Z$ denote the value on the card obtained, so that $\prb(Z = 1) = 1/5$ and $\prb(Z = 2) = 4/5$. Then, having obtained the value $Z = i$, we observe $Y$ by randomly drawing a chip from bowl \#$i$. We see immediately that the cdf of $Y$ is given by
\[
G(x) = (1/5) F_1(x) + (4/5) F_2(x),
\]
and this is a mixture of the cdfs $F_1$ and $F_2$.
\end{example}

As the following examples illustrate, it is also possible to have infinite mixtures of distributions.

\begin{example}[Location and Scale Mixtures]
\label{ex:2.5.5}
Suppose $F$ is some cumulative distribution function. Then for any real number $y$, the function $F_y$ defined by $F_y(x) = F(x - y)$ is also a cumulative distribution function. In fact, $F_y$ is just a ``shifted'' version of $F$. An example of this is depicted in Figure~\ref{fig:2.5.3}.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-5-3.pdf}
\caption{Plot of the distribution functions $F$ (solid line) and $F_2$ (dashed line) in Example~\ref{ex:2.5.5}, where $F(x) = e^{-x}/(e^{-x} + 1)$ for $x \in \mathbf{R}^1$.}
\label{fig:2.5.3}
\end{figure}

If $p_i \geqslant 0$ with $\sum_i p_i = 1$ (so the $p_i$ form a probability distribution), and $y_1, y_2, \ldots$ are real numbers, then we can define a \emph{discrete location mixture} by
\[
H(x) = \sum_i p_i F_{y_i}(x) = \sum_i p_i F(x - y_i).
\]
Indeed, the shift $F_y(x) = F(x - y)$ itself corresponds to a special case of a discrete location mixture, with $p_1 = 1$ and $y_1 = y$.

Furthermore, if $g$ is some nonnegative function with $\int_{-\infty}^{\infty} g(t) \, \mathrm{d}t = 1$ (so $g$ is a density function), then we can define
\[
H(x) = \int_{-\infty}^{\infty} F_y(x) \, g(y) \, \mathrm{d}y = \int_{-\infty}^{\infty} F(x - y) \, g(y) \, \mathrm{d}y.
\]
Then it is not hard to see that $H$ is also a cumulative distribution function---one that is called a \emph{continuous location mixture} of $F$. The idea is that $H$ corresponds to a mixture of different shifted distributions $F_y$, with the density $g$ giving the distribution of the mixing coefficient $y$.

We can also define a \emph{discrete scale mixture} by
\[
K(x) = \sum_i p_i F(x/y_i)
\]
whenever $y_i > 0$, $p_i \geqslant 0$, and $\sum_i p_i = 1$. Similarly, if $\int_0^{\infty} g(t) \, \mathrm{d}t = 1$, then we can write
\[
K(x) = \int_0^{\infty} F(x/y) \, g(y) \, \mathrm{d}y.
\]
Then $K$ is also a cumulative distribution function, called a \emph{continuous scale mixture} of $F$.
\end{example}

You might wonder at this point whether a mixture distribution is discrete or continuous. The answer depends on the distributions being mixed and the mixing distribution. For example, discrete location mixtures of discrete distributions are discrete and discrete location mixtures of continuous distributions are continuous.

There is nothing restricting us, however, to mixing only discrete distributions or only continuous distributions. Other kinds of distribution are considered in the following section.

\subsection{Distributions Neither Discrete Nor Continuous (Advanced)}
\label{ssec:2.5.5}

There are some distributions that are neither discrete nor continuous, as the following example shows.

\begin{example}
\label{ex:2.5.6}
Suppose that $X_1 \sim \text{Poisson}(3)$ is discrete with cdf $F_1$, while $X_2 \sim N(0, 1)$ is continuous with cdf $F_2$, and $Y$ has the mixture distribution given by $F_Y(y) = (1/5) F_1(y) + (4/5) F_2(y)$. Using \eqref{eq:2.5.1}, we have
\begin{align*}
\prb(Y = y) &= F_Y(y) - F_Y(y^-) \\
&= (1/5) F_1(y) + (4/5) F_2(y) - (1/5) F_1(y^-) - (4/5) F_2(y^-) \\
&= (1/5) [F_1(y) - F_1(y^-)] + (4/5) [F_2(y) - F_2(y^-)] \\
&= \frac{1}{5} \prb(X_1 = y) + \frac{4}{5} \prb(X_2 = y).
\end{align*}
Therefore,
\[
\prb(Y = y) = \begin{cases}
\dfrac{1}{5} \cdot \dfrac{3^y}{y!} e^{-3} & y \text{ a nonnegative integer} \\[6pt]
0 & \text{otherwise.}
\end{cases}
\]

Because $\prb(Y = y) > 0$ for nonnegative integers $y$, the random variable $Y$ is not continuous. On the other hand, we have
\[
\sum_y \prb(Y = y) = \sum_{y=0}^{\infty} \frac{1}{5} \cdot \frac{3^y}{y!} e^{-3} = \frac{1}{5} \neq 1.
\]
Hence, $Y$ is not discrete either.

In fact, $Y$ is neither discrete nor continuous. Rather, $Y$ is a mixture of a discrete and a continuous distribution.
\end{example}

For the most part in this book, we shall treat discrete and continuous distributions separately. However, it is important to keep in mind that actual distributions may be neither discrete nor continuous but rather a mixture of the two.\footnote{In fact, there exist probability distributions that cannot be expressed even as a mixture of a discrete and a continuous distribution, but these need not concern us here.} In most applications, however, the distributions we deal with are either continuous or discrete.

Recall that a continuous distribution need not be absolutely continuous, i.e., have a density. Hence, a distribution that is a mixture of a discrete and a continuous distribution might not be a mixture of a discrete and an absolutely continuous distribution.

\subsection*{Summary of Section~\ref{sec:2.5}}

\begin{itemize}
\item The cumulative distribution function (cdf) of $X$ is $F_X(x) = \prb(X \leqslant x)$.
\item All probabilities associated with $X$ can be determined from $F_X$.
\item As $x$ increases from $-\infty$ to $\infty$, $F_X(x)$ increases from $0$ to $1$.
\item If $X$ is discrete, then $F_X(x) = \sum_{y \leqslant x} \prb(X = y)$.
\item If $X$ is absolutely continuous, then $F_X(x) = \int_{-\infty}^{x} f_X(t) \, \mathrm{d}t$, and $f_X(x) = F_X'(x)$.
\item We write $\Phi(x)$ for the cdf of the standard normal distribution evaluated at $x$.
\item A mixture distribution has a cdf that is a linear combination of other cdfs. Two special cases are location and scale mixtures.
\item Some mixture distributions are neither discrete nor continuous.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.5.1}
Verify explicitly that properties (a) through (d) of Theorem~\ref{thm:2.5.2} are indeed satisfied by the function $F_X$ in Example~\ref{ex:2.5.1}.
\end{exercise}

\begin{solution}
Properties (a) and (b) follow by inspection. Properties (c) and (d) follow since $F_X(x) = 0$ for $x < 1$, and $F_X(x) = 1$ for $x > 6$.
\end{solution}

\begin{exercise}
\label{exer:2.5.2}
Consider rolling one fair six-sided die, so that $S = \{1, 2, 3, 4, 5, 6\}$, and $\prb(\{s\}) = 1/6$ for all $s \in S$. Let $X$ be the number showing on the die, so that $X(s) = s$ for $s \in S$. Let $Y = X^2$. Compute the cumulative distribution function $F_Y(y) = \prb(Y \leqslant y)$, for all $y \in \mathbf{R}^1$. Verify explicitly that properties (a) through (d) of Theorem~\ref{thm:2.5.2} are satisfied by this function $F_Y$.
\end{exercise}

\begin{solution}
\[
    F_X(x) = \begin{cases}
        0 & x < 1 \\
        1/6 & 1 \leqslant x < 4 \\
        2/6 & 4 \leqslant x < 9 \\
        3/6 & 9 \leqslant x < 16 \\
        4/6 & 16 \leqslant x < 25 \\
        5/6 & 25 \leqslant x < 36 \\
        1 & 36 \leqslant x
    \end{cases}
\]
Properties (a) and (b) follow by inspection. Properties (c) and (d) follow since $F_Y(y) = 0$ for $y < 1$, and $F_Y(y) = 1$ for $y > 36$.
\end{solution}

\begin{exercise}
\label{exer:2.5.3}
For each of the following functions $F$, determine whether or not $F$ is a valid cumulative distribution function, i.e., whether or not $F$ satisfies properties (a) through (d) of Theorem~\ref{thm:2.5.2}.
\begin{enumerate}[(a)]
\item $F(x) = x$ for all $x \in \mathbf{R}^1$.
\item $F(x) = \begin{cases} 0 & x < 0 \\ x & 0 \leqslant x \leqslant 1 \\ 1 & x > 1. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < 0 \\ x^2 & 0 \leqslant x \leqslant 1 \\ 1 & x > 1. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < 0 \\ x^2 & 0 \leqslant x \leqslant 3 \\ 1 & x > 3. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < 0 \\ x^2/9 & 0 \leqslant x \leqslant 3 \\ 1 & x > 3. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < -1 \\ x^2/9 & -1 \leqslant x \leqslant 3 \\ 1 & x > 3. \end{cases}$
\item $F(x) = \begin{cases} 0 & x \leqslant -1 \\ x^2/9 & -1 < x < 3 \\ 1 & x \geqslant 3. \end{cases}$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item No, since $F(x) > 1$ for $x > 1$.
    \item Yes.
    \item Yes.
    \item No, since, e.g., $F(2) = 4 > 1$.
    \item Yes.
    \item Yes. (This distribution has mass $1/9$ at the point 1.)
    \item No, since $F(-1) > F(0)$ so $F$ is not non-decreasing.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.4}
Let $X \sim N(0, 1)$. Compute each of the following in terms of the function $\Phi$ of Definition~\ref{def:2.5.2} and use Table~D.2 (or software) to evaluate these probabilities numerically.
\begin{enumerate}[(a)]
\item $\prb(X \leqslant 5)$
\item $\prb(-2 \leqslant X \leqslant 7)$
\item $\prb(X \geqslant 3)$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\prb(X \leqslant -5) = \Phi(-5) = 2.87 \times 10^{-7}$.
    \item $\prb(-2 \leqslant X \leqslant 7) = \Phi(7) - \Phi(-2) = 0.977$.
    \item $\prb(X \geqslant 3) = 1 - \prb(X < 3) = 1 - \Phi(3) = 0.00135$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.5}
Let $Y \sim N(8, 4)$. Compute each of the following, in terms of the function $\Phi$ of Definition~\ref{def:2.5.2} and use Table~D.2 (or software) to evaluate these probabilities numerically.
\begin{enumerate}[(a)]
\item $\prb(Y \leqslant 5)$
\item $\prb(2 \leqslant Y \leqslant 7)$
\item $\prb(Y \geqslant 3)$
\end{enumerate}
\end{exercise}

\begin{solution}
Here $(Y + 8)/2 \sim \text{Normal}(0, 1)$. Hence:
\begin{enumerate}[(a)]
    \item $\prb(Y \leqslant -5) = \prb((Y+8)/2 \leqslant (-5+8)/2) = \Phi((5+8)/2) = \Phi(3/2) = 0.933$.
    \item $\prb(-2 \leqslant Y \leqslant 7) = \prb((-2+8)/2 \leqslant (Y+8)/2 \leqslant (7+8)/2) = \Phi((7+8)/2) - \Phi((-2+8)/2) = \Phi(15/2) - \Phi(3) = 0.00135$.
    \item $\prb(Y \geqslant 3) = \prb((Y+8)/2 \geqslant (3+8)/2) = 1 - \Phi((3+8)/2) = 1 - \Phi(11/2) = 1.90 \times 10^{-8}$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.6}
Verify that the function $G$ given by \eqref{eq:2.5.3} satisfies properties (a) through (d) of Theorem~\ref{thm:2.5.2}.
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The fact $p_i \geqslant 0$ and $F_i(x) \geqslant 0$ for all $i = 1, \ldots, k$ implies $G(x) = \sum_{i=1}^{k}p_i F_i(x) \geqslant \sum_{i=1}^{k}p_i \cdot 0 = 0$. Similarly, $p_1 + \cdots + p_k = 1$, $p_i \geqslant 0$ and $F_i(x) \leqslant 1$ for all $i = 1, \ldots, k$ implies $G(x) = \sum_{i=1}^{k}p_i F_i(x) \leqslant \sum_{i=1}^{k}p_i \cdot 1 = 1$.
    \item Suppose $y > x$. Then, $F_i(y) \geqslant F_i(x)$ for all $i = 1, \ldots, k$.
    \[
        G(y) = \sum_{i=1}^{k}p_i F_i(y) \geqslant \sum_{i=1}^{k}p_i F_i(x) = G(x).
    \]
    \item For all $i = 1, \ldots, k$, $\lim_{x \to \infty}F_i(x) = 1$. Hence,
    \[
        \lim_{x \to \infty}G(x) = \lim_{x \to \infty}\sum_{i=1}^{k}p_i F_i(x) = \sum_{i=1}^{k}p_i\lim_{x \to \infty}F_i(x) = \sum_{i=1}^{k}p_i = 1.
    \]
    \item For $i = 1, \ldots, k$, $\lim_{x \to -\infty}F_i(x) = 0$. Thus,
    \[
        \lim_{x \to -\infty}G(x) = \lim_{x \to -\infty}\sum_{i=1}^{k}p_i F_i(x) = \sum_{i=1}^{k}p_i\lim_{x \to -\infty}F_i(x) = \sum_{i=1}^{k}p_i \cdot 0 = 0.
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.7}
Suppose $F_X(x) = x^2$ for $0 \leqslant x \leqslant 1$. Compute each of the following.
\begin{enumerate}[(a)]
\item $\prb(X \leqslant 1/3)$
\item $\prb(1/4 \leqslant X \leqslant 1/2)$
\item $\prb(2/5 \leqslant X \leqslant 4/5)$
\item $\prb(X < 0)$
\item $\prb(X < 1)$
\item $\prb(X \leqslant 1)$
\item $\prb(X > 3)$
\item $\prb(X < 3/7)$
\end{enumerate}
\end{exercise}

\begin{solution}
By definition $F_X(x) = \prb(X \leqslant x)$. Since $F_X(x)$ is a continuous function, $\prb(X = x) = F(x) - \lim_{y \nearrow x}F(y) = x^2 - \lim_{y \nearrow x}y^2 = x^2 - x^2 = 0$. Hence, $\prb(X < x) = \prb(X \leqslant x) - \prb(X = x) = F(x) - 0 = F(x)$.
\begin{enumerate}[(a)]
    \item $\prb(X < 1/3) = \prb(X \leqslant 1/3) = F(1/3) = (1/3)^2 = 1/9$.
    \item $\prb(1/4 < X < 1/2) = \prb(X < 1/2) - \prb(X \leqslant 1/4) = F(1/2) - F(1/4) = (1/2)^2 - (1/4)^2 = 3/16$.
    \item $\prb(2/5 < X < 4/5) = \prb(X < 4/5) - \prb(X \leqslant 2/5) = F(4/5) - F(2/5) = (4/5)^2 - (2/5)^2 = 12/25$.
    \item $\prb(X < 0) = F(0) = 0$.
    \item $\prb(X < 1) = F(1) = 1^2 = 1$.
    \item Since $0 \leqslant \prb(X < -1) \leqslant \prb(X \leqslant 0) = F(0) = 0$, we have $\prb(X < -1) = 0$.
    \item Since $1 \geqslant \prb(X < 3) \geqslant \prb(X \leqslant 1) = F(1) = 1^2 = 1$, we have $\prb(X < 3) = 1$.
    \item $\prb(X = 3/7) = \prb(X \leqslant 3/7) - \prb(X < 3/7) = F(3/7) - F(3/7) = 0$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.8}
Suppose $F_Y(y) = y^3$ for $0 \leqslant y \leqslant 1/2$, and $F_Y(y) = 1$ for $1/2 < y$. Compute each of the following.
\begin{enumerate}[(a)]
\item $\prb(1/3 \leqslant Y \leqslant 3/4)$
\item $\prb(Y < 1/3)$
\item $\prb(Y < 1/2)$
\end{enumerate}
\end{exercise}

\begin{solution}
The function $F_Y$ is continuous on $(0, 1/2)$ and $(1/2, 1)$. Hence, $\prb(Y < y) = \lim_{x \nearrow y}\prb(Y \leqslant x) = \lim_{x \nearrow y}F_Y(x) = F_Y(y) = \prb(Y \leqslant y)$ for all $y \in (0, 1/2) \cup (1/2, 1)$.
\begin{enumerate}[(a)]
    \item $\prb(1/3 < Y < 3/4) = \prb(Y < 3/4) - \prb(Y \leqslant 1/3) = F_Y(3/4) - F_Y(1/3) = 1 - (3/4)^3 - (1/2)^3 = 29/64$.
    \item $\prb(Y = 1/3) = \prb(Y \leqslant 1/3) - \prb(Y < 1/3) = F_Y(1/3) - F_Y(1/3) = 0$.
    \item $\prb(Y = 1/2) = \prb(Y \leqslant 1/2) - \prb(Y < 1/2) = F_Y(1/2) - \lim_{x \nearrow 1/2}F_Y(x) = 1 - (1/2)^3 - \lim_{x \nearrow 1/2}x^3 = 1 - (1/2)^3 - (1/2)^3 = 3/4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.9}
Let $F(x) = x^2$ for $0 \leqslant x \leqslant 2$, with $F(x) = 0$ for $x < 0$ and $F(x) = 4$ for $x > 2$.
\begin{enumerate}[(a)]
\item Sketch a graph of $F$.
\item Is $F$ a valid cumulative distribution function? Why or why not?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig_2_5_9.pdf}
        \caption{Plot of $F(x) = x^2$ for Exercise 2.5.9}
        %\label{fig:cdf-2-5-9}
    \end{figure}
  \item The given $F$ doesn't satisfy (a) and (c) in Theorem \ref{thm:2.5.2} because $F(2) = 2^2 = 4 > 1$ and $\lim_{x \to \infty}F(x) = 4 > 1$. Thus $F$ can't be a cumulative distribution function.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.10}
Let $F(x) = 0$ for $x < 0$, with $F(x) = e^{-x}$ for $x \geqslant 0$.
\begin{enumerate}[(a)]
\item Sketch a graph of $F$.
\item Is $F$ a valid cumulative distribution function? Why or why not?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig_2_5_10.pdf}
        \caption{Plot of $F(x) = e^{-x}$ for Exercise 2.5.10}
        %\label{fig:cdf-2-5-10}
    \end{figure}
  \item The given $F$ doesn't satisfy (b) and (c) in Theorem \ref{thm:2.5.2} because $F(0) = 1 > e^{-1} = F(1)$ even though $0 < 1$, $\lim_{x \to \infty}F(x) = \lim_{x \to \infty}e^{-x} = 0$. Hence, $F$ is not a cumulative distribution function.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.11}
Let $F(x) = 0$ for $x < 0$, with $F(x) = 1 - e^{-x}$ for $x \geqslant 0$.
\begin{enumerate}[(a)]
\item Sketch a graph of $F$.
\item Is $F$ a valid cumulative distribution function? Why or why not?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig_2_5_11.pdf}
        \caption{Plot of $F(x) = 1 - e^{-x}$ for Exercise 2.5.11}
        %\label{fig:cdf-2-5-11}
    \end{figure}
    \item Since $0 < e^{-x} \leqslant 1$ for $x \geqslant 0$, $0 \leqslant F(x) \leqslant 1$ for all $x$. On $[0, \infty)$, $F'(x) > 0$. Hence, $F$ is increasing on $[0, \infty)$. $\lim_{x \to \infty}F(x) = \lim_{x \to \infty}(1 - e^{-x}) = 1$ and $\lim_{x \to -\infty}F(x) = 0$. Hence, $F$ is a cumulative distribution function.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.12}
Let $X \sim \text{Exponential}(3)$. Compute the function $F_X$.
\end{exercise}

\begin{solution}
The density of $X$ is $f_X(x) = 3e^{-3x}\indc(x \geqslant 0)$. Since $X$ is defined on $[0, \infty)$, $F_X(x) = \prb(X \leqslant x) = 0$ for all $x < 0$. For $x \geqslant 0$,
\[
    F_X(x) = \int_{-\infty}^{x}f_X(y)\,\mathrm{d}y = \int_0^x 3e^{-3y}\,\mathrm{d}y = -e^{-3y}\big|_{y=0}^{y=x} = -e^{-3x} + 1 = 1 - e^{-3x}.
\]
\end{solution}

\begin{exercise}
\label{exer:2.5.13}
Let $F(x) = 0$ for $x < 0$, with $F(x) = 1/3$ for $0 \leqslant x < 2.5$, and $F(x) = 3/4$ for $2.5 \leqslant x < 4.5$, and $F(x) = 1$ for $x \geqslant 4.5$.
\begin{enumerate}[(a)]
\item Sketch a graph of $F$.
\item Prove that $F$ is a valid cumulative distribution function.
\item If $X$ has cumulative distribution function equal to $F$, then compute $\prb(X = 4.5)$ and $\prb(-1 \leqslant X \leqslant 1/2)$ and $\prb(X = 2.5)$ and $\prb(X < 4.5)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig_2_5_13.pdf}
        \caption{Plot of step function $F$ for Exercise 2.5.13}
        %\label{fig:cdf-2-5-13}
    \end{figure}
    \item The function $F$ is non-decreasing in (a) and the range of $F$ is $[0, 1]$. Finally, $\lim_{x \to -\infty}F(x) = 0$ and $\lim_{x \to \infty}F(x) = 1$. Hence, $F$ is a valid cumulative distribution function.
    \item $\prb(X > 4/5) = 1 - \prb(X \leqslant 4/5) = 1 - F(4/5) = 1 - 1 = 0$. $\prb(-1 < X < 1/2) = \prb(X < 1/2) - \prb(X \leqslant -1) = \lim_{x \nearrow 1/2}F(x) - F(-1) = 3/4 - 0 = 3/4$. $\prb(X = 2/5) = \prb(X \leqslant 2/5) - \prb(X < 2/5) = F(2/5) - \lim_{x \nearrow 2/5}F(x) = 3/4 - 1/3 = 5/12$. $\prb(X = 4/5) = \prb(X \leqslant 4/5) - \prb(X < 4/5) = F(4/5) - \lim_{x \nearrow 4/5}F(x) = 1 - 3/4 = 1/4$. Besides, it is not hard to show that $\prb(X = 0) = 1/3$. Hence, $\prb(X \in \{0, 2/5, 4/5\}) = 1/3 + 5/12 + 1/4 = 1$, that is,
    \[
        \prb(X = x) = \begin{cases}
            1/3 & \text{if } x = 0, \\
            5/12 & \text{if } x = 2/5, \\
            1/4 & \text{if } x = 4/5, \\
            0 & \text{otherwise}.
        \end{cases}
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.14}
Let $G(x) = 0$ for $x < 0$, with $G(x) = 1 - e^{-x^2}$ for $x \geqslant 0$.
\begin{enumerate}[(a)]
\item Prove that $G$ is a valid cumulative distribution function.
\item If $Y$ has cumulative distribution function equal to $G$, then compute $\prb(Y \leqslant 4)$ and $\prb(1 \leqslant Y \leqslant 2)$ and $\prb(Y = 0)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item For all $x \geqslant 0$, $0 < e^{-x} \leqslant 1$. Hence, $0 \leqslant G(x) < 1$ for all $x \geqslant 0$. Since $G'(x)$, for $x > 0$, is non-negative, $G$ is non-decreasing, i.e., $G(x) \geqslant G(y)$ whenever $x \geqslant y$. Finally, $\lim_{x \to -\infty}G(x) = 0$ and $\lim_{x \to \infty}G(x) = \lim_{x \to \infty}1 - e^{-x} = 1 - \lim_{x \to \infty}e^{-x} = 1 - 0 = 1$. Hence, $G$ is a valid cumulative distribution function.
    \item Since $G$ is a continuous function, $\prb(Y < y) = \lim_{x \nearrow y}G(x) = G(y)$. $\prb(Y > 4) = 1 - \prb(Y \leqslant 4) = 1 - G(4) = 1 - e^{-4} = 0.98168$. $\prb(-1 < Y < 2) = \prb(Y < 2) - \prb(Y \leqslant -1) = G(2) - G(-1) = 1 - e^{-2} - 0 = 1 - e^{-2} = 0.86466$. $\prb(Y = 0) = \prb(Y \leqslant 0) - \prb(Y < 0) = G(0) - G(0) = 0$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.5.15}
Let $F$ and $G$ be as in the previous two exercises. Let $H(x) = (1/3) F(x) + (2/3) G(x)$. Suppose $Z$ has cumulative distribution function equal to $H$. Compute each of the following.
\begin{enumerate}[(a)]
\item $\prb(Z = 4.5)$
\item $\prb(-1 \leqslant Z \leqslant 1/2)$
\item $\prb(Z = 2.5)$
\item $\prb(Z < 4.5)$
\item $\prb(Z = 0)$
\item $\prb(Z \leqslant 1/2)$
\end{enumerate}
\end{exercise}

\begin{solution}
Since $G$ is continuous, $\lim_{y \nearrow z}G(y) = G(z)$. $\prb(Z = z) = \prb(Z \leqslant z) - \prb(Z < z) = H(z) - \lim_{x \nearrow z}H(x) = (1/3)F(z) + (2/3)G(z) - (1/3)\lim_{x \nearrow z}F(x) - (2/3)\lim_{x \nearrow z}G(x) = (1/3)(F(z) - \lim_{x \nearrow z}F(x)) + (2/3)(G(z) - G(z)) = (1/3)\prb(X = z)$. We already showed that $\prb(X = z) > 0$ only if $z \in \{0, 2/5, 4/5\}$ in Exercise \ref{exer:2.5.13}.
\begin{enumerate}[(a)]
    \item $\prb(Z > 4/5) = 1 - \prb(Z \leqslant 4/5) = 1 - H(4/5) = 1 - (F(4/5)/3 + 2G(4/5)/3) = 1 - (1/3 + 2(1 - e^{-4/5})/3) = 2e^{-4/5}/3 = 0.29955$.
    \item $\prb(-1 < Z < 1/2) = \prb(Z < 1/2) - \prb(Z \leqslant -1) = \lim_{z \nearrow 1/2}H(z) - H(-1) = (1/3)\lim_{z \nearrow 1/2}F(z) + (2/3)\lim_{z \nearrow 1/2}G(z) - ((1/3) \cdot 0 + (2/3) \cdot 0) = (1/3)(3/4) + (2/3)(1 - e^{-1/2}) = 11/12 - 2e^{-1/2}/3 = 0.51231$.
    \item $\prb(Z = 2/5) = \prb(Z \leqslant 2/5) - \prb(Z < 2/5) = H(2/5) - \lim_{z \nearrow 2/5}H(z) = (1/3)F(2/5) + (2/3)G(2/5) - (1/3)\lim_{z \nearrow 2/5}F(z) - (2/3)\lim_{z \nearrow 2/5}G(z) = (1/3)(3/4) + (2/3)(1 - e^{-2/5}) - (1/3)(1/3) - (2/3)(1 - e^{-2/5}) = 5/36 = 0.13889$.
    \item $\prb(Z = 4/5) = \prb(Z \leqslant 4/5) - \prb(Z < 4/5) = H(4/5) - \lim_{z \nearrow 4/5}H(z) = (1/3)F(4/5) + (2/3)G(4/5) - (1/3)\lim_{z \nearrow 4/5}F(z) - (2/3)\lim_{z \nearrow 4/5}G(z) = (1/3) \cdot 1 + (2/3)(1 - e^{-4/5}) - (1/3)(3/4) - (2/3)(1 - e^{-4/5}) = 1/12 = 0.08333$.
    \item $\prb(Z = 0) = \prb(Z \leqslant 0) - \prb(Z < 0) = H(0) - \lim_{z \nearrow 0}H(z) = (1/3)F(0) + (2/3)G(0) - (1/3)\lim_{z \nearrow 0}F(z) - (2/3)\lim_{z \nearrow 0}G(z) = (1/3)(1/3) + (2/3) \cdot 0 - (1/3) \cdot 0 - (2/3) \cdot 0 = 1/9 = 0.11111$.
    \item $\prb(Z = 1/2) = \prb(Z \leqslant 1/2) - \prb(Z < 1/2) = H(1/2) - \lim_{z \nearrow 1/2}H(z) = (1/3)(3/4) + (2/3)(1 - e^{-1/2}) - (1/3)\lim_{z \nearrow 1/2}F(z) - (2/3)\lim_{z \nearrow 1/2}G(z) = (1/3)(3/4) + (2/3)(1 - e^{-1/2}) = 11/12 - 2e^{-1/2}/3 = 0.51231$.
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.5.16}
Let $F$ be a cumulative distribution function. Compute (with explanation) the value of $\lim_{n \to \infty} [F(2n) - F(-n)]$.
\end{exercise}

\begin{solution}
Since $F$ is non-decreasing, $\lim_{n \to \infty}|F(2n) - F(n)| = \lim_{n \to \infty}[F(2n) - F(n)] = \lim_{n \to \infty}F(2n) - \lim_{n \to \infty}F(n) = 1 - 1 = 0$. (Hence, $\lim_{n \to \infty}\prb(n < X \leqslant 2n) = 0$ for any $X$.)
\end{solution}

\begin{exercise}
\label{exer:2.5.17}
Let $F$ be a cumulative distribution function. For $x \in \mathbf{R}^1$, we could define $F(x^+)$ by $F(x^+) = \lim_{n \to \infty} F(x + 1/n)$. Prove that $F$ is right continuous, meaning that for each $x \in \mathbf{R}^1$, we have $F(x^+) = F(x)$. (Hint: You will need to use continuity of $\prb$ (Theorem~1.6.1).)
\end{exercise}

\begin{solution}
Let $X$ have cdf $F$, let $A$ be the event $\{X \leqslant x\}$, and let $A_n$ be the event $\{X \leqslant x + \frac{1}{n}\}$. Then $A_{n+1} \subseteq A_n$ and $\bigcap_n A_n = A$. Hence, $\{A_n\} \searrow A$, so by continuity of probabilities, $\lim_{n \to \infty}\prb(A_n) = \prb(A)$, i.e., $\lim_{n \to \infty}\prb(X \leqslant x + \frac{1}{n}) = \prb(X \leqslant x)$, i.e., $\lim_{n \to \infty}F(x + \frac{1}{n}) = F(x)$.
\end{solution}

\begin{exercise}
\label{exer:2.5.18}
Let $X$ be a random variable, with cumulative distribution function $F_X$. Prove that $\prb(X = a) = 0$ if and only if the function $F_X$ is continuous at $a$. (Hint: Use \eqref{eq:2.5.1} and the previous problem.)
\end{exercise}

\begin{solution}
Since $F$ is non-decreasing, then $F$ is continuous at $a$ if and only if $F(a+) = F(a-)$. But the previous exercise shows $F(a+) = F(a)$. Hence, $F$ is continuous at $a$ if and only if $F(a) = F(a-)$, i.e., $F(a) - F(a-) = 0$. The result follows since $\prb(X = a) = F(a) - F(a-)$.
\end{solution}

\begin{exercise}
\label{exer:2.5.19}
Let $\Phi$ be as in Definition~\ref{def:2.5.2}. Derive a formula for $\Phi(-x)$ in terms of $\Phi(x)$. (Hint: Let $s = -t$ in \eqref{eq:2.5.2}, and do not forget Theorem~\ref{thm:2.5.2}.)
\end{exercise}

\begin{solution}
Note that $\phi(-x) = \phi(x)$. Hence, using the substitution $s = -t$, we have $\Phi(-x) = \int_{-\infty}^{-x}\phi(t)\,dt = -\int_{\infty}^{x}\phi(s)(-ds) = \int_x^{\infty}\phi(s)\,ds = \Phi(\infty) - \Phi(x) = 1 - \Phi(x)$.
\end{solution}

\begin{exercise}
\label{exer:2.5.20}
Determine the distribution function for the logistic distribution of Problem~\ref{exer:2.4.18}.
\end{exercise}

\begin{solution}
$F(x) = \int_{-\infty}^{x}e^{-z}(1 + e^{-z})^{-2}\,dz = (1 + e^{-x})^{-1}$.
\end{solution}

\begin{exercise}
\label{exer:2.5.21}
Determine the distribution function for the Weibull distribution of Problem~\ref{exer:2.4.19}.
\end{exercise}

\begin{solution}
$F(x) = \int_0^x \alpha z^{\alpha-1}\exp\{-z^{\alpha}\}\,dz = 1 - \exp\{-x^{\alpha}\}$.
\end{solution}

\begin{exercise}
\label{exer:2.5.22}
Determine the distribution function for the Pareto distribution of Problem~\ref{exer:2.4.20}.
\end{exercise}

\begin{solution}
$F(x) = \alpha\int_0^x (1 + z)^{-\alpha-1}\,dz = 1 - (1 + x)^{-\alpha}$.
\end{solution}

\begin{exercise}
\label{exer:2.5.23}
Determine the distribution function for the Cauchy distribution of Problem~\ref{exer:2.4.21}.
\end{exercise}

\begin{solution}
$F(x) = \pi^{-1}\int_{-\infty}^{x}(1 + z^2)^{-1}\,dz = (\arctan(x) + \pi/2)/\pi$.
\end{solution}

\begin{exercise}
\label{exer:2.5.24}
Determine the distribution function for the Laplace distribution of Problem~\ref{exer:2.4.22}.
\end{exercise}

\begin{solution}
\[
    F(x) = \begin{cases}
        \frac{1}{2}\int_{-\infty}^{x}e^z\,dz = \frac{1}{2}e^x & x \leqslant 0 \\[6pt]
        \frac{1}{2} + \frac{1}{2}\int_0^x e^{-z}\,dz = \frac{1}{2} + \frac{1}{2}(1 - e^{-x}) & x > 0
    \end{cases}
\]
\end{solution}

\begin{exercise}
\label{exer:2.5.25}
Determine the distribution function for the extreme value distribution of Problem~\ref{exer:2.4.23}.
\end{exercise}

\begin{solution}
$F(x) = \int_{-\infty}^{x}e^{-z}\exp\{-e^{-z}\}\,dz = \exp\{-e^{-z}\}\big|_{-\infty}^{x} = \exp\{-e^{-x}\}$.
\end{solution}

\begin{exercise}
\label{exer:2.5.26}
Determine the distribution function for the beta distributions of Problem~\ref{exer:2.4.24} for parts (b) through (e).
\end{exercise}

\begin{solution}
\begin{enumerate}[(b)]
    \item $F(x) = \int_0^x dz = x$ for $0 < x < 1$.
    \item $F(x) = \int_0^x 2z\,dz = x^2$ for $0 < x < 1$.
    \item $F(x) = \int_0^x 2(1-z)\,dz = 1 - (1-x)^2$ for $0 < x < 1$.
    \item $F(x) = \int_0^x 6z(1-z)\,dz = \int_0^x 6(z - z^2)\,dz = 3x^2 - 2x^3$ for $0 < x < 1$.
\end{enumerate}
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:2.5.27}
Does it surprise you that all information about the distribution of a random variable $X$ can be stored by a single function $F_X$? Why or why not? What other examples can you think of where lots of different information is stored by a single function?
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{One-Dimensional Change of Variable}
\label{sec:2.6}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $X$ be a random variable with a known distribution. Suppose that $Y = h(X)$, where $h : \mathbf{R}^1 \to \mathbf{R}^1$ is some function. (Recall that this really means that $Y(s) = h(X(s))$, for all $s \in S$.) Then what is the distribution of $Y$?

\subsection{The Discrete Case}
\label{ssec:2.6.1}

If $X$ is a discrete random variable, this is quite straightforward. To compute the probability that $Y = y$, we need to compute the probability of the set consisting of all the $x$ values satisfying $h(x) = y$, namely, compute $\prb(X \in \{x : h(x) = y\})$. This is depicted graphically in Figure~\ref{fig:2.6.1}.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-6-1.pdf}
\caption{An example where the set of $x$ values that satisfy $h(x) = y$ consists of three points $x_1$, $x_2$, and $x_3$.}
\label{fig:2.6.1}
\end{figure}

We now establish the basic result.

\begin{theorem}
\label{thm:2.6.1}
Let $X$ be a discrete random variable, with probability function $p_X$. Let $Y = h(X)$, where $h : \mathbf{R}^1 \to \mathbf{R}^1$ is some function. Then $Y$ is also discrete, and its probability function $p_Y$ satisfies $p_Y(y) = \sum_{x \in h^{-1}(y)} p_X(x)$, where $h^{-1}(y)$ is the set of all real numbers $x$ with $h(x) = y$.
\end{theorem}

\begin{proof}
We compute that $p_Y(y) = \prb(h(X) = y) = \sum_{x \in h^{-1}(y)} \prb(X = x) = \sum_{x \in h^{-1}(y)} p_X(x)$, as claimed.
\end{proof}

\begin{example}
\label{ex:2.6.1}
Let $X$ be the number of heads when flipping three fair coins. Let $Y = 1$ if $X \geqslant 1$, with $Y = 0$ if $X = 0$. Then $Y = h(X)$ where $h(0) = 0$ and $h(1) = h(2) = h(3) = 1$. Hence, $h^{-1}(0) = \{0\}$, so $\prb(Y = 0) = \prb(X = 0) = 1/8$. On the other hand, $h^{-1}(1) = \{1, 2, 3\}$, so $\prb(Y = 1) = \prb(X = 1) + \prb(X = 2) + \prb(X = 3) = 3/8 + 3/8 + 1/8 = 7/8$.
\end{example}

\begin{example}
\label{ex:2.6.2}
Let $X$ be the number showing on a fair six-sided die, so that $\prb(X = x) = 1/6$ for $x = 1, 2, 3, 4, 5$, and $6$. Let $Y = X^2 - 3X + 2$. Then $Y = h(X)$ where $h(x) = x^2 - 3x + 2$. Note that $h(x) = 0$ if and only if $x = 1$ or $x = 2$. Hence, $h^{-1}(0) = \{1, 2\}$, and
\[
\prb(Y = 0) = p_X(1) + p_X(2) = \frac{1}{6} + \frac{1}{6} = \frac{1}{3}.
\]
\end{example}

\subsection{The Continuous Case}
\label{ssec:2.6.2}

If $X$ is continuous and $Y = h(X)$, then the situation is more complicated. Indeed, $Y$ might not be continuous at all, as the following example shows.

\begin{example}
\label{ex:2.6.3}
Let $X$ have the uniform distribution on $[0, 1]$, i.e., $X \sim \text{Uniform}[0, 1]$ as in Example~\ref{ex:2.4.2}. Let $Y = h(X)$, where
\[
h(x) = \begin{cases} 7 & x \leqslant 3/4 \\ 5 & x > 3/4. \end{cases}
\]
Here, $Y = 7$ if and only if $X \leqslant 3/4$ (which happens with probability $3/4$), whereas $Y = 5$ if and only if $X > 3/4$ (which happens with probability $1/4$). Hence, $Y$ is discrete, with probability function $p_Y$ satisfying $p_Y(7) = 3/4$, $p_Y(5) = 1/4$, and $p_Y(y) = 0$ when $y \notin \{5, 7\}$.
\end{example}

On the other hand, if $X$ is absolutely continuous, and the function $h$ is strictly increasing, then the situation is considerably simpler, as the following theorem shows.

\begin{theorem}
\label{thm:2.6.2}
Let $X$ be an absolutely continuous random variable, with density function $f_X$. Let $Y = h(X)$, where $h : \mathbf{R}^1 \to \mathbf{R}^1$ is a function that is differentiable and strictly increasing. Then $Y$ is also absolutely continuous, and its density function $f_Y$ is given by
\begin{equation}
\label{eq:2.6.1}
f_Y(y) = f_X(h^{-1}(y)) \, |h'(h^{-1}(y))|
\end{equation}
where $h'$ is the derivative of $h$, and where $h^{-1}(y)$ is the unique number $x$ such that $h(x) = y$.
\end{theorem}

\begin{proof}
  See Section~\ref{sec:2.11} for the proof of this result.
\end{proof}

\begin{example}
\label{ex:2.6.4}
Let $X \sim \text{Uniform}[0, 1]$, and let $Y = 3X$. What is the distribution of $Y$?

Here, $X$ has density $f_X$ given by $f_X(x) = 1$ if $0 \leqslant x \leqslant 1$, and $f_X(x) = 0$ otherwise. Also, $Y = h(X)$, where $h$ is defined by $h(x) = 3x$. Note that $h$ is strictly increasing because if $x < y$, then $3x < 3y$, i.e., $h(x) < h(y)$. Hence, we may apply Theorem~\ref{thm:2.6.2}.

We note first that $h'(x) = 3$ and that $h^{-1}(y) = y/3$. Then, according to Theorem~\ref{thm:2.6.2}, $Y$ is absolutely continuous with density
\[
f_Y(y) = f_X(h^{-1}(y)) \, |h'(h^{-1}(y))| = \frac{1}{3} f_X(y/3) = \begin{cases} 1/3 & 0 \leqslant y/3 \leqslant 1 \\ 0 & \text{otherwise} \end{cases} = \begin{cases} 1/3 & 0 \leqslant y \leqslant 3 \\ 0 & \text{otherwise.} \end{cases}
\]
By comparison with Example~\ref{ex:2.4.3}, we see that $Y \sim \text{Uniform}[0, 3]$, i.e., that $Y$ has the Uniform$[L, R]$ distribution with $L = 0$ and $R = 3$.
\end{example}

\begin{example}
\label{ex:2.6.5}
Let $X \sim N(0, 1)$, and let $Y = 2X + 5$. What is the distribution of $Y$?

Here, $X$ has density $f_X$ given by
\[
f_X(x) = \phi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}.
\]
Also, $Y = h(X)$, where $h$ is defined by $h(x) = 2x + 5$. Note that again, $h$ is strictly increasing because if $x < y$, then $2x + 5 < 2y + 5$, i.e., $h(x) < h(y)$. Hence, we may again apply Theorem~\ref{thm:2.6.2}.

We note first that $h'(x) = 2$ and that $h^{-1}(y) = (y - 5)/2$. Then, according to Theorem~\ref{thm:2.6.2}, $Y$ is absolutely continuous with density
\[
f_Y(y) = f_X(h^{-1}(y)) \, |h'(h^{-1}(y))| = \frac{f_X((y-5)/2)}{2} = \frac{1}{2\sqrt{2\pi}} e^{-(y-5)^2/8}.
\]
By comparison with Example~\ref{ex:2.4.8}, we see that $Y \sim N(5, 4)$, i.e., that $Y$ has the $N(\mu, \sigma^2)$ distribution with $\mu = 5$ and $\sigma^2 = 4$.
\end{example}

If instead the function $h$ is strictly decreasing, then a similar result holds.

\begin{theorem}
\label{thm:2.6.3}
Let $X$ be an absolutely continuous random variable, with density function $f_X$. Let $Y = h(X)$, where $h : \mathbf{R}^1 \to \mathbf{R}^1$ is a function that is differentiable and strictly decreasing. Then $Y$ is also absolutely continuous, and its density function $f_Y$ may again be defined by \eqref{eq:2.6.1}.
\end{theorem}

\begin{proof}
  See Section~\ref{sec:2.11} for the proof of this result.
\end{proof}

\begin{example}
\label{ex:2.6.6}
Let $X \sim \text{Uniform}[0, 1]$, and let $Y = -\ln(1 - X)$. What is the distribution of $Y$?

Here, $X$ has density $f_X$ given by $f_X(x) = 1$ for $0 \leqslant x \leqslant 1$, and $f_X(x) = 0$ otherwise. Also, $Y = h(X)$, where $h$ is defined by $h(x) = -\ln(1 - x)$. Note that here, $h$ is strictly decreasing because if $x < y$, then $1 - x > 1 - y$, so $\ln(1 - x) > \ln(1 - y)$, i.e., $h(x) < h(y)$. Hence, we may apply Theorem~\ref{thm:2.6.3}.

We note first that $h'(x) = 1/(x)$ and that $h^{-1}(y) = e^{-y}$. Then, by Theorem~\ref{thm:2.6.3}, $Y$ is absolutely continuous with density
\[
f_Y(y) = f_X(h^{-1}(y)) \, |h'(h^{-1}(y))| = e^{-y} f_X(e^{-y}) = \begin{cases} e^{-y} & 0 \leqslant e^{-y} \leqslant 1 \\ 0 & \text{otherwise} \end{cases} = \begin{cases} e^{-y} & y \geqslant 0 \\ 0 & \text{otherwise.} \end{cases}
\]
By comparison with Example~\ref{ex:2.4.4}, we see that $Y \sim \text{Exponential}(1)$, i.e., that $Y$ has the Exponential$(1)$ distribution.
\end{example}

Finally, we note the following.

\begin{theorem}
\label{thm:2.6.4}
Theorem~\ref{thm:2.6.2} (and \ref{thm:2.6.3}) remains true assuming only that $h$ is strictly increasing (or decreasing) at places for which $f_X(x) > 0$. If $f_X(x) = 0$ for an interval of $x$ values, then it does not matter how the function $h$ behaves in that interval (or even if it is well defined there).
\end{theorem}

\begin{example}
\label{ex:2.6.7}
If $X \sim \text{Exponential}(\lambda)$, then $f_X(x) = 0$ for $x < 0$. Therefore, it is required that $h$ be strictly increasing (or decreasing) only for $x > 0$. Thus, functions such as $h(x) = \sqrt{x}$, $h(x) = x^8$, and $h(x) = \ln(x)$ could still be used with Theorem~\ref{thm:2.6.2}, while functions such as $h(x) = -\sqrt{x}$, $h(x) = -x^8$, and $h(x) = -\ln(x)$ could still be used with Theorem~\ref{thm:2.6.3}, even though such functions may not necessarily be strictly increasing (or decreasing) and well defined on the entire real line.
\end{example}

\subsection*{Summary of Section~\ref{sec:2.6}}

\begin{itemize}
\item If $X$ is discrete, and $Y = h(X)$, then $\prb(Y = y) = \sum_{x: h(x) = y} \prb(X = x)$.
\item If $X$ is absolutely continuous, and $Y = h(X)$ with $h$ strictly increasing or strictly decreasing, then the density of $Y$ is given by $f_Y(y) = f_X(h^{-1}(y)) \, |h'(h^{-1}(y))|$.
\item This allows us to compute the distribution of a function of a random variable.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.6.1}
Let $X \sim \text{Uniform}[L, R]$. Let $Y = cX + d$, where $c > 0$. Prove that $Y \sim \text{Uniform}[cL + d, cR + d]$. (This generalizes Example~\ref{ex:2.6.4}.)
\end{exercise}

\begin{solution}
Let $h(x) = cx + d$. Then $Y = h(X)$ and $h$ is strictly increasing, so $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = f_X((y-d)/c)/c$, which equals $1/(R-L)c$ for $L \leqslant (y-d)/c \leqslant R$, i.e., $cL + d \leqslant y \leqslant cR + d$, otherwise equals 0. Hence, $Y \sim \text{Uniform}[cL + d, cR + d]$.
\end{solution}

\begin{exercise}
\label{exer:2.6.2}
Let $X \sim \text{Uniform}[L, R]$. Let $Y = cX + d$, where $c < 0$. Prove that $Y \sim \text{Uniform}[cR + d, cL + d]$. (In particular, if $L = 0$ and $R = 1$ and $c = -1$ and $d = 1$, then $X \sim \text{Uniform}[0, 1]$ and also $Y = 1 - X \sim \text{Uniform}[0, 1]$.)
\end{exercise}

\begin{solution}
Let $h(x) = cx + d$. Then $Y = h(X)$ and $h$ is strictly decreasing, so $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = f_X((y-d)/c)/|c|$, which equals $1/(R-L)|c| = 1/(cL - cR)$ for $L \leqslant (y-d)/c \leqslant R$, i.e., $cR + d \leqslant y \leqslant cL + d$, otherwise equals 0. Hence, $Y \sim \text{Uniform}[cR + d, cL + d]$.
\end{solution}

\begin{exercise}
\label{exer:2.6.3}
Let $X \sim N(\mu, \sigma^2)$. Let $Y = cX + d$, where $c > 0$. Prove that $Y \sim N(c\mu + d, c^2\sigma^2)$. (This generalizes Example~\ref{ex:2.6.5}.)
\end{exercise}

\begin{solution}
Let $h(x) = cx + d$. Then $Y = h(X)$ and $h$ is strictly increasing, so $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = e^{-[((y-d)/c)-\mu]^2/2\sigma^2}/\sigma\sqrt{2\pi}c = e^{-[y-d-c\mu]^2/2c^2\sigma^2}/c\sigma\sqrt{2\pi}$. Hence, $Y \sim \text{Normal}(c\mu + d, c^2\sigma^2)$.
\end{solution}

\begin{exercise}
\label{exer:2.6.4}
Let $X \sim \text{Exponential}(\lambda)$. Let $Y = cX$, where $c > 0$. Prove that $Y \sim \text{Exponential}(\lambda/c)$.
\end{exercise}

\begin{solution}
Let $h(x) = cx$. Then $Y = h(X)$ and $h$ is strictly increasing, so $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = f_X(y/c)/c$, which equals $\lambda e^{-\lambda y/c}/c = (\lambda/c)e^{-(\lambda/c)y}$ for $y > 0$, otherwise equals 0. Hence, $Y \sim \text{Exponential}(\lambda/c)$.
\end{solution}

\begin{exercise}
\label{exer:2.6.5}
Let $X \sim \text{Exponential}(\lambda)$. Let $Y = X^3$. Compute the density $f_Y$ of $Y$.
\end{exercise}

\begin{solution}
Let $h(x) = x^3$. Then $Y = h(X)$ and $h$ is strictly increasing, and $h^{-1}(y) = y^{1/3}$. Hence, $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = f_X(y^{1/3})/3(y^{1/3})^2$, which equals $\lambda e^{-\lambda y^{1/3}}/3y^{2/3} = (\lambda/3)y^{-2/3}e^{-\lambda y^{1/3}}$ for $y > 0$, otherwise equals 0.
\end{solution}

\begin{exercise}
\label{exer:2.6.6}
Let $X \sim \text{Exponential}(\lambda)$. Let $Y = X^{1/4}$. Compute the density $f_Y$ of $Y$. (Hint: Use Theorem~\ref{thm:2.6.4}.)
\end{exercise}

\begin{solution}
Let $h(x) = x^{1/4}$. Then $Y = h(X)$, and $h$ is strictly increasing over the region $\{x > 0\}$, where $f_X(x) > 0$. Also, $h^{-1}(y) = y^4$ on this region. Hence, for $y > 0$, $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = f_X(y^4)/(1/4)(y^4)^{-3/4} = \lambda e^{-\lambda y^4}/(1/4)y^{-3} = 4\lambda y^3 e^{-\lambda y^4}$, with $f_Y(y) = 0$ for $y \leqslant 0$.
\end{solution}

\begin{exercise}
\label{exer:2.6.7}
Let $X \sim \text{Uniform}[0, 3]$. Let $Y = X^2$. Compute the density function $f_Y$ of $Y$.
\end{exercise}

\begin{solution}
Let $h(x) = x^2$. Then $Y = h(X)$, and $h$ is strictly increasing over the region $\{0 < x < 3\}$, where $f_X(x) > 0$. Also, $h^{-1}(y) = y^{1/2}$ on this region. Hence, $f_Y(y) = 0$ unless $y > 0$ and $0 < y^{1/2} < 3$, i.e., $0 < y < 9$, in which case $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = f_X(y^{1/2})/2y^{1/2} = (1/3)/(2y^{1/2}) = 1/6y^{1/2}$ for $0 < y < 9$.
\end{solution}

\begin{exercise}
\label{exer:2.6.8}
Let $X$ have a density such that $f_X(x) = f_X(-x)$, i.e., it is symmetric about $\mu$. Let $Y = 2\mu - X$. Show that the density of $Y$ is given by $f_X$. Use this to determine the distribution of $Y$ when $X \sim N(\mu, \sigma^2)$.
\end{exercise}

\begin{solution}
The transformation is $y = h(x) = 2\mu - x$ and so $h^{-1}(y) = 2\mu - y$ and $h'(x) = -1$ and so the density of $Y$ is given by $f_Y(y) = f_X(h^{-1}(y)) = f_X(2\mu - y) = f_X(\mu + (\mu - y)) = f_X(\mu - (\mu - y)) = f_X(y)$ so $X$ and $Y$ have the same distribution. Since the $N(\mu, \sigma^2)$ density is symmetric about $\mu$, this proves that $Y \sim N(\mu, \sigma^2)$.
\end{solution}

\begin{exercise}
\label{exer:2.6.9}
Let $X$ have density function $f_X(x) = x^3/4$ for $0 \leqslant x \leqslant 2$, otherwise $f_X(x) = 0$.
\begin{enumerate}[(a)]
\item Let $Y = X^2$. Compute the density function $f_Y(y)$ for $Y$.
\item Let $Z = \sqrt{X}$. Compute the density function $f_Z(z)$ for $Z$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The inverse function of $Y$ is $Y^{-1}(y) = y^{1/2}$ and the derivative of $Y$ is $Y'(x) = 2x$. Hence, $f_Y(y) = f_X(Y^{-1}(y))/|Y'(Y^{-1}(y))| = f_X(y^{1/2})/2y^{1/2} = y/8$.
    \item Since $Z^{-1}(z) = z$ and $Z'(z) = 1$, $f_Z(z) = f_X(z)/1 = z^3/4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.6.10}
Let $X \sim \text{Uniform}[0, 2\pi]$. Let $Y = \sin(X)$. Compute the density function $f_Y(y)$ for $Y$.
\end{exercise}

\begin{solution}
The density function of $X$ is $f_X(x) = 2/\pi$ if $0 \leqslant x \leqslant \pi/2$ and $f_X(x) = 0$ otherwise. The inverse image of $y$ is $Y^{-1}(y) = \arcsin(y)$. The derivative of $Y$ is $Y'(x) = \cos(x)$. $f_Y(y) = f_X(\arcsin(y))/|Y'(\arcsin(y))| = 2/(\pi\sqrt{1-y^2})$ for $y \in [0, 1]$.
\end{solution}

\begin{exercise}
\label{exer:2.6.11}
Let $X$ have density function $f_X(x) = (1/2) \sin(x)$ for $0 \leqslant x \leqslant \pi$, otherwise $f_X(x) = 0$. Let $Y = X^2$. Compute the density function $f_Y(y)$ for $Y$.
\end{exercise}

\begin{solution}
Since $f_X$ is defined on $0 < x < \pi$, the inverse of $Y$ is $Y^{-1}(y) = \sqrt{y}$. The derivative of $Y$ is $Y'(x) = 2x$. From Theorem \ref{thm:2.6.2},
\[
    f_Y(y) = f_X(\sqrt{y})/|2\sqrt{y}| = y^{-1/2}\sin(y^{1/2})/4
\]
for $y > 1$ and $f_Y(y) = 0$ otherwise.
\end{solution}

\begin{exercise}
\label{exer:2.6.12}
Let $X$ have density function $f_X(x) = 1/x^2$ for $x \geqslant 1$, otherwise $f_X(x) = 0$. Let $Y = X^{1/3}$. Compute the density function $f_Y(y)$ for $Y$.
\end{exercise}

\begin{solution}
Since $Y(x) = x^{1/3}$ is increasing, $Y$ is also 1--1. The inverse is $Y^{-1}(y) = y^3$ and the derivative is $Y'(x) = x^{-2/3}/3$. By applying Theorem \ref{thm:2.6.2}, we get
\[
    f_Y(y) = f_X(y^3)/|(y^3)^{-2/3}/3| = y^{-6}/y^{-2} = y^{-4}.
\]
\end{solution}

\begin{exercise}
\label{exer:2.6.13}
Let $X \sim \text{Normal}(0, 1)$. Let $Y = X^3$. Compute the density function $f_Y(y)$ for $Y$.
\end{exercise}

\begin{solution}
Note $f_X(x) = (2\pi)^{-1/2}\exp(-x^2/2)$. The transformation $x \mapsto x^3$ is monotone increasing. The inverse of $Y$ is $Y^{-1}(y) = y^{1/3}$ and the derivative is $Y'(x) = 3x^2$. By Theorem \ref{thm:2.6.3}, we have
\[
    f_Y(y) = f_X(y^{1/3})/|3(y^{1/3})^2| = (2\pi)^{-1/2}(3|y|^{2/3})^{-1}\exp(-|y|^{2/3}/2).
\]
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.6.14}
Let $X \sim \text{Uniform}[2, 7]$, $Y = X^3$, and $Z = \sqrt{Y}$. Compute the density $f_Z$ of $Z$, in two ways.
\begin{enumerate}[(a)]
\item Apply Theorem~\ref{thm:2.6.2} to first obtain the density of $Y$, then apply Theorem~\ref{thm:2.6.2} again to obtain the density of $Z$.
\item Observe that $Z = \sqrt{Y} = \sqrt{X^3} = X^{3/2}$, and apply Theorem~\ref{thm:2.6.2} just once.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First, let $h(x) = x^3$. Then $Y = h(X)$ and $h$ is strictly increasing and $h^{-1}(y) = y^{1/3}$. Hence, $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = f_X(y^{1/3})/3(y^{1/3})^2$, which equals $(1/5)/3y^{2/3} = y^{-2/3}/15$ for $2 < y^{1/3} < 7$, i.e., $8 < y < 343$, otherwise equals 0. Second, let $h(y) = y^{1/2}$. Then $Z = h(Y)$ and $h$ is strictly increasing over the region $\{8 < y < 343\}$, where $f_Y(y) > 0$. Also, $h^{-1}(z) = z^2$ on this region. Hence, for $\sqrt{8} < z < \sqrt{343}$, $f_Z(z) = f_Y(h^{-1}(z))/|h'(h^{-1}(z))| = f_Y(z^2)/(1/2)(z^2)^{-1/2} = [(z^2)^{-2/3}/15]/(1/2)(z^2)^{-1/2} = 2z^{-1/3}/15$, with $f_Z(z) = 0$ otherwise.
    \item Let $h(x) = x^{3/2}$. Then $Z = h(X)$ and $h$ is strictly increasing over the region $\{2 < x < 7\}$, where $f_X(x) > 0$. Hence, $f_Z(z) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = f_X(z^{2/3})/(3/2)(z^{2/3})^{1/2}$, which equals $(1/5)/(3/2)z^{1/3} = 2z^{-1/3}/15$ for $2 < z^{2/3} < 7$, i.e., $2^{3/2} < z < 7^{3/2}$, otherwise equals 0.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.6.15}
Let $X \sim \text{Uniform}[L, R]$, and let $Y = h(X)$ where $h(x) = (x - c)^6$. According to Theorem~\ref{thm:2.6.4}, under what conditions on $L$, $R$, and $c$ can we apply Theorem~\ref{thm:2.6.2} or Theorem~\ref{thm:2.6.3} to this choice of $X$ and $Y$?
\end{exercise}

\begin{solution}
Here $h$ is strictly decreasing on $x \leqslant c$, and is strictly increasing on $x \geqslant c$. Hence, we can apply Theorem \ref{thm:2.6.2} if $c \leqslant L < R$ and Theorem \ref{thm:2.6.3} if $L < R \leqslant c$.
\end{solution}

\begin{exercise}
\label{exer:2.6.16}
Let $X \sim N(\mu, \sigma^2)$. Let $Y = cX + d$, where $c < 0$. Prove that again $Y \sim N(c\mu + d, c^2\sigma^2)$, just like in Exercise~\ref{exer:2.6.3}.
\end{exercise}

\begin{solution}
Let $h(x) = cx + d$. Then $Y = h(X)$ and $h$ is strictly decreasing, so $f_Y(y) = f_X(h^{-1}(y))/|h'(h^{-1}(y))| = \frac{1}{\sigma\sqrt{2\pi}}e^{-[((y-d)/c)-\mu]^2/2\sigma^2}/|c| = \frac{1}{|c|\sigma\sqrt{2\pi}}e^{-[y-d-c\mu]^2/2c^2\sigma^2}$. Hence, $Y \sim \text{Normal}(c\mu + d, c^2\sigma^2)$.
\end{solution}

\begin{exercise}[Log-normal distribution]
\label{exer:2.6.17}
Suppose that $X \sim N(\mu_0, \sigma^2)$. Prove that $Y = e^X$ has density
\[
f(y) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(\ln y - \mu)^2}{2\sigma^2}\right) \cdot \frac{1}{y}
\]
for $y > 0$ and where $\sigma > 0$ is unknown. We say that $Y \sim \text{Log-normal}(\mu, \sigma^2)$.
\end{exercise}

\begin{solution}
The transformation $y = h(x) = e^x$ has $h'(x) = e^x$ and $h^{-1}(y) = \ln y$. Therefore, $Y$ has density
\[
    \frac{1}{\sqrt{2\pi}\tau}\exp\left(-\frac{(\ln y)^2}{2\tau^2}\right)e^{-\ln y} = \frac{1}{\sqrt{2\pi}\tau}\exp\left(-\frac{(\ln y)^2}{2\tau^2}\right)\frac{1}{y}
\]
for $y > 0$.
\end{solution}

\begin{exercise}
\label{exer:2.6.18}
Suppose that $X \sim \text{Weibull}(\alpha)$ (see Problem~\ref{exer:2.4.19}). Determine the distribution of $Y = X^{\alpha}$.
\end{exercise}

\begin{solution}
The transformation $y = h(x) = x^{\beta}$ has $h'(x) = \beta x^{\beta-1}$ and $h^{-1}(y) = y^{1/\beta}$. Therefore, $Y$ has density $(\alpha/\beta)(y^{1/\beta})^{\alpha-1}e^{-(y^{1/\beta})^{\alpha}}/(y^{1/\beta})^{\beta-1} = (\alpha/\beta)y^{(\alpha/\beta)-1}e^{-y^{\alpha/\beta}}$ for $y > 0$, so $Y \sim \text{Weibull}(\alpha/\beta)$.
\end{solution}

\begin{exercise}
\label{exer:2.6.19}
Suppose that $X \sim \text{Pareto}(\alpha)$ (see Problem~\ref{exer:2.4.20}). Determine the distribution of $Y = (1 + X)^{-1}$.
\end{exercise}

\begin{solution}
The transformation $y = h(x) = (1 + x)^{\beta} - 1$ has $h'(x) = \beta(1 + x)^{\beta-1}$ and $h^{-1}(y) = (1 + y)^{1/\beta} - 1$. Therefore, $Y$ has density
\[
    \frac{\alpha}{\beta}\bigl(1 + (1 + y)^{1/\beta} - 1\bigr)^{-\alpha-1}/\bigl(1 + (1 + y)^{1/\beta} - 1\bigr)^{\beta-1} = \frac{\alpha}{\beta}(1 + y)^{-(\alpha/\beta)-1}
\]
for $y > 0$, so $Y \sim \text{Pareto}(\alpha/\beta)$.
\end{solution}

\begin{exercise}
\label{exer:2.6.20}
Suppose that $X$ has the extreme value distribution (see Problem~\ref{exer:2.4.23}). Determine the distribution of $Y = e^{-X}$.
\end{exercise}

\begin{solution}
The transformation $y = h(x) = e^{-x}$ has $h'(x) = -e^{-x}$ and $h^{-1}(y) = -\ln y$. Therefore $Y$ has density $e^{\ln y}\exp\{-e^{\ln y}\}/e^{\ln y} = e^{-y}$ for $y > 0$ and so $Y \sim \text{Exponential}(1)$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:2.6.21}
Theorems~\ref{thm:2.6.2} and \ref{thm:2.6.3} require that $h$ be an increasing or decreasing function, at least at places where the density of $X$ is positive (see Theorem~\ref{thm:2.6.4}). Suppose now that $X \sim N(0, 1)$ and $Y = h(X)$ where $h(x) = x^2$. Then $f_X(x) > 0$ for all $x$, while $h$ is increasing only for $x > 0$ and decreasing only for $x < 0$. Hence, Theorems~\ref{thm:2.6.2} and \ref{thm:2.6.3} do not directly apply. Compute $f_Y(y)$ anyway. (Hint: $\prb(a \leqslant Y \leqslant b) = \prb(a \leqslant Y \leqslant b \mid X \geqslant 0) + \prb(a \leqslant Y \leqslant b \mid X < 0)$.)
\end{exercise}

\begin{solution}
We have that, for $y > 0$,
\begin{align*}
    f_Y(y) &= \frac{d}{\mathrm{d}y}F_Y(y) = \frac{d}{\mathrm{d}y}\prb(Y \leqslant y) = \frac{d}{\mathrm{d}y}\prb(Y \leqslant y) = \frac{d}{\mathrm{d}y}\prb(-\sqrt{y} \leqslant X \leqslant \sqrt{y}) \\
    &= \frac{d}{\mathrm{d}y}(\Phi(\sqrt{y}) - \Phi(-\sqrt{y})) = \frac{\phi(\sqrt{y})}{2\sqrt{y}} + \frac{\phi(-\sqrt{y})}{2\sqrt{y}} = \frac{\phi(\sqrt{y})}{2\sqrt{y}} + \frac{\phi(\sqrt{y})}{2\sqrt{y}} = \frac{\phi(\sqrt{y})}{\sqrt{y}} = \frac{\exp\{-y/2\}}{\sqrt{2\pi}\sqrt{y}}
\end{align*}
for $y > 0$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Joint Distributions}
\label{sec:2.7}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Suppose $X$ and $Y$ are two random variables. Even if we know the distributions of $X$ and $Y$ exactly, this still does not tell us anything about the relationship between $X$ and $Y$.

\begin{example}
\label{ex:2.7.1}
Let $X \sim \text{Bernoulli}(1/2)$, so that $\prb(X = 0) = \prb(X = 1) = 1/2$. Let $Y_1 = X$, and let $Y_2 = 1 - X$. Then we clearly have $Y_1 \sim \text{Bernoulli}(1/2)$ and $Y_2 \sim \text{Bernoulli}(1/2)$ as well.

On the other hand, the relationship between $X$ and $Y_1$ is very different from the relationship between $X$ and $Y_2$. For example, if we know that $X = 1$, then we also must have $Y_1 = 1$, but $Y_2 = 0$. Hence, merely knowing that $X$, $Y_1$, and $Y_2$ all have the distribution Bernoulli$(1/2)$ does not give us complete information about the relationships among these random variables.
\end{example}

A formal definition of joint distribution is as follows.

\begin{definition}
\label{def:2.7.1}
If $X$ and $Y$ are random variables, then the \emph{joint distribution} of $X$ and $Y$ is the collection of probabilities $\prb((X, Y) \in B)$, for all subsets $B \subseteq \mathbf{R}^2$ of pairs of real numbers.
\end{definition}

Joint distributions, like other distributions, are so complicated that we use various functions to describe them, including joint cumulative distribution functions, joint probability functions, and joint density functions, as we now discuss.

\subsection{Joint Cumulative Distribution Functions}
\label{ssec:2.7.1}

\begin{definition}
\label{def:2.7.2}
Let $X$ and $Y$ be random variables. Then their \emph{joint cumulative distribution function} is the function $F_{X,Y} : \mathbf{R}^2 \to [0, 1]$ defined by
\[
F_{X,Y}(x, y) = \prb(X \leqslant x, Y \leqslant y).
\]
(Recall that the comma means ``and'' here, so that $F_{X,Y}(x, y)$ is the probability that $X \leqslant x$ and $Y \leqslant y$.)
\end{definition}

\begin{example}[Example~\ref{ex:2.7.1} continued]
\label{ex:2.7.2}
Again, let $X \sim \text{Bernoulli}(1/2)$, $Y_1 = X$, and $Y_2 = 1 - X$. Then we compute that
\[
F_{X,Y_1}(x, y) = \prb(X \leqslant x, Y_1 \leqslant y) = \begin{cases}
0 & \min(x, y) < 0 \\
1/2 & 0 \leqslant \min(x, y) < 1 \\
1 & \min(x, y) \geqslant 1.
\end{cases}
\]
On the other hand,
\[
F_{X,Y_2}(x, y) = \prb(X \leqslant x, Y_2 \leqslant y) = \begin{cases}
0 & \min(x, y) < 0 \text{ or } \max(x, y) < 1 \\
1/2 & 0 \leqslant \min(x, y) < 1 \leqslant \max(x, y) \\
1 & \min(x, y) \geqslant 1.
\end{cases}
\]

We thus see that $F_{X,Y_1}$ is quite a different function from $F_{X,Y_2}$. This reflects the fact that, even though $Y_1$ and $Y_2$ each have the same distribution, their relationship with $X$ is quite different. On the other hand, the functions $F_{X,Y_1}$ and $F_{X,Y_2}$ are rather cumbersome and awkward to work with.
\end{example}

We see from this example that joint cumulative distribution functions (or joint cdfs) do indeed keep track of the relationship between $X$ and $Y$. Indeed, joint cdfs tell us everything about the joint probabilities of $X$ and $Y$, as the following theorem (an analog of Theorem~\ref{thm:2.5.1}) shows.

\begin{theorem}
\label{thm:2.7.1}
Let $X$ and $Y$ be any random variables, with joint cumulative distribution function $F_{X,Y}$. Let $B$ be a subset of $\mathbf{R}^2$. Then $\prb((X, Y) \in B)$ can be determined solely from the values of $F_{X,Y}(x, y)$.
\end{theorem}

We shall not give a proof of Theorem~\ref{thm:2.7.1}, although it is similar to the proof of Theorem~\ref{thm:2.5.1}. However, the following theorem indicates why Theorem~\ref{thm:2.7.1} is true, and it also provides a useful computational fact.

\begin{theorem}
\label{thm:2.7.2}
Let $X$ and $Y$ be any random variables, with joint cumulative distribution function $F_{X,Y}$. Suppose $a < b$ and $c < d$. Then
\[
\prb(a < X \leqslant b, c < Y \leqslant d) = F_{X,Y}(b, d) - F_{X,Y}(a, d) - F_{X,Y}(b, c) + F_{X,Y}(a, c).
\]
\end{theorem}

\begin{proof}
According to (1.3.3),
\[
\prb(a < X \leqslant b, c < Y \leqslant d) = \prb(X \leqslant b, Y \leqslant d) - \prb(\{X \leqslant b, Y \leqslant d\} \text{ and either } \{X \leqslant a\} \text{ or } \{Y \leqslant c\}).
\]
But by the principle of inclusion--exclusion (1.3.4),
\[
\prb(\{X \leqslant b, Y \leqslant c\} \cup \{X \leqslant a, Y \leqslant d\}) = \prb(X \leqslant b, Y \leqslant c) + \prb(X \leqslant a, Y \leqslant d) - \prb(X \leqslant a, Y \leqslant c).
\]
Combining these two equations, we see that
\[
\prb(a < X \leqslant b, c < Y \leqslant d) = \prb(X \leqslant b, Y \leqslant d) - \prb(X \leqslant a, Y \leqslant d) - \prb(X \leqslant b, Y \leqslant c) + \prb(X \leqslant a, Y \leqslant c),
\]
and from this we obtain
\[
\prb(a < X \leqslant b, c < Y \leqslant d) = F_{X,Y}(b, d) - F_{X,Y}(a, d) - F_{X,Y}(b, c) + F_{X,Y}(a, c),
\]
as claimed.
\end{proof}

Joint cdfs are not easy to work with. Thus, in this section we shall also consider other functions, which are more convenient for pairs of discrete or absolutely continuous random variables.

\subsection{Marginal Distributions}
\label{ssec:2.7.2}

We have seen how a joint cumulative distribution function $F_{X,Y}$ tells us about the relationship between $X$ and $Y$. However, the function $F_{X,Y}$ also tells us everything about each of $X$ and $Y$ separately, as the following theorem shows.

\begin{theorem}
\label{thm:2.7.3}
Let $X$ and $Y$ be two random variables, with joint cumulative distribution function $F_{X,Y}$. Then the cumulative distribution function $F_X$ of $X$ satisfies
\[
F_X(x) = \lim_{y \to \infty} F_{X,Y}(x, y)
\]
for all $x \in \mathbf{R}^1$. Similarly, the cumulative distribution function $F_Y$ of $Y$ satisfies
\[
F_Y(y) = \lim_{x \to \infty} F_{X,Y}(x, y)
\]
for all $y \in \mathbf{R}^1$.
\end{theorem}

\begin{proof}
Note that we always have $Y < \infty$. Hence, using continuity of $\prb$, we have
\[
F_X(x) = \prb(X \leqslant x) = \prb(X \leqslant x, Y < \infty) = \lim_{y \to \infty} \prb(X \leqslant x, Y \leqslant y) = \lim_{y \to \infty} F_{X,Y}(x, y),
\]
as claimed. Similarly,
\[
F_Y(y) = \prb(Y \leqslant y) = \prb(X < \infty, Y \leqslant y) = \lim_{x \to \infty} \prb(X \leqslant x, Y \leqslant y) = \lim_{x \to \infty} F_{X,Y}(x, y),
\]
completing the proof.
\end{proof}

In the context of Theorem~\ref{thm:2.7.3}, $F_X$ is called the \emph{marginal cumulative distribution function} of $X$, and the distribution of $X$ is called the \emph{marginal distribution} of $X$. (Similarly, $F_Y$ is called the marginal cumulative distribution function of $Y$, and the distribution of $Y$ is called the marginal distribution of $Y$.) Intuitively, if we think of $F_{X,Y}$ as being a function of a pair $(x, y)$, then $F_X$ and $F_Y$ are functions of $x$ and $y$, respectively, which could be written into the ``margins'' of a graph of $F_{X,Y}$.

\begin{example}
\label{ex:2.7.3}
In Figure~\ref{fig:2.7.1}, we have plotted the joint distribution function
\[
F_{X,Y}(x, y) = \begin{cases}
0 & x < 0 \text{ or } y < 0 \\
xy^2 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
x & 0 \leqslant x \leqslant 1, \, y > 1 \\
y^2 & x > 1, \, 0 \leqslant y \leqslant 1 \\
1 & x > 1 \text{ and } y > 1.
\end{cases}
\]

It is easy to see that
\[
F_X(x) = F_{X,Y}(x, 1) = x
\]
for $0 \leqslant x \leqslant 1$, and that
\[
F_Y(y) = F_{X,Y}(1, y) = y^2
\]
for $0 \leqslant y \leqslant 1$. The graphs of these functions are given by the outermost edges of the surface depicted in Figure~\ref{fig:2.7.1}.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-7-1.pdf}
\caption{Graph of the joint distribution function $F_{X,Y}(x, y) = xy^2$ for $0 \leqslant x \leqslant 1$ and $0 \leqslant y \leqslant 1$ in Example~\ref{ex:2.7.3}.}
\label{fig:2.7.1}
\end{figure}
\end{example}

Theorem~\ref{thm:2.7.3} thus tells us that the joint cdf $F_{X,Y}$ is very useful indeed. Not only does it tell us about the relationship of $X$ to $Y$, but it also contains all the information about the marginal distributions of $X$ and of $Y$.

We will see in the next subsections that joint probability functions, and joint density functions, similarly contain information about both the relationship of $X$ and $Y$ and the marginal distributions of $X$ and $Y$.

\subsection{Joint Probability Functions}
\label{ssec:2.7.3}

Suppose $X$ and $Y$ are both discrete random variables. Then we can define a joint probability function for $X$ and $Y$, as follows.

\begin{definition}
\label{def:2.7.3}
Let $X$ and $Y$ be discrete random variables. Then their \emph{joint probability function}, $p_{X,Y}$, is a function from $\mathbf{R}^2$ to $\mathbf{R}^1$, defined by
\[
p_{X,Y}(x, y) = \prb(X = x, Y = y).
\]
\end{definition}

Consider the following example.

\begin{example}[Examples~\ref{ex:2.7.1} and \ref{ex:2.7.2} continued]
\label{ex:2.7.4}
Again, let $X \sim \text{Bernoulli}(1/2)$, $Y_1 = X$, and $Y_2 = 1 - X$. Then we see that
\[
p_{X,Y_1}(x, y) = \prb(X = x, Y_1 = y) = \begin{cases}
1/2 & x = y = 1 \\
1/2 & x = y = 0 \\
0 & \text{otherwise.}
\end{cases}
\]
On the other hand,
\[
p_{X,Y_2}(x, y) = \prb(X = x, Y_2 = y) = \begin{cases}
1/2 & x = 1, \, y = 0 \\
1/2 & x = 0, \, y = 1 \\
0 & \text{otherwise.}
\end{cases}
\]

We thus see that $p_{X,Y_1}$ and $p_{X,Y_2}$ are two simple functions that are easy to work with and that clearly describe the relationships between $X$ and $Y_1$ and between $X$ and $Y_2$. Hence, for pairs of discrete random variables, joint probability functions are usually the best way to describe their relationships.
\end{example}

Once we know the joint probability function $p_{X,Y}$, the marginal probability functions of $X$ and $Y$ are easily obtained.

\begin{theorem}
\label{thm:2.7.4}
Let $X$ and $Y$ be two discrete random variables, with joint probability function $p_{X,Y}$. Then the probability function $p_X$ of $X$ can be computed as
\[
p_X(x) = \sum_y p_{X,Y}(x, y).
\]
Similarly, the probability function $p_Y$ of $Y$ can be computed as
\[
p_Y(y) = \sum_x p_{X,Y}(x, y).
\]
\end{theorem}

\begin{proof}
Using additivity of $\prb$, we have that
\[
p_X(x) = \prb(X = x) = \sum_y \prb(X = x, Y = y) = \sum_y p_{X,Y}(x, y),
\]
as claimed. Similarly,
\[
p_Y(y) = \prb(Y = y) = \sum_x \prb(X = x, Y = y) = \sum_x p_{X,Y}(x, y).
\]
\end{proof}

\begin{example}
\label{ex:2.7.5}
Suppose the joint probability function of $X$ and $Y$ is given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/7 & x = 5, \, y = 0 \\
1/7 & x = 5, \, y = 3 \\
1/7 & x = 5, \, y = 4 \\
3/7 & x = 8, \, y = 0 \\
1/7 & x = 8, \, y = 4 \\
0 & \text{otherwise.}
\end{cases}
\]
Then
\[
p_X(5) = \sum_y p_{X,Y}(5, y) = p_{X,Y}(5, 0) + p_{X,Y}(5, 3) + p_{X,Y}(5, 4) = \frac{1}{7} + \frac{1}{7} + \frac{1}{7} = \frac{3}{7},
\]
while
\[
p_X(8) = \sum_y p_{X,Y}(8, y) = p_{X,Y}(8, 0) + p_{X,Y}(8, 4) = \frac{3}{7} + \frac{1}{7} = \frac{4}{7}.
\]
Similarly,
\[
p_Y(4) = \sum_x p_{X,Y}(x, 4) = p_{X,Y}(5, 4) + p_{X,Y}(8, 4) = \frac{1}{7} + \frac{1}{7} = \frac{2}{7},
\]
etc.

Note that in such a simple context it is possible to tabulate the joint probability function in a table, as illustrated below for $p_{X,Y}$, $p_X$, and $p_Y$ of this example.

\begin{center}
\begin{tabular}{c|ccc|c}
& $Y = 0$ & $Y = 3$ & $Y = 4$ & \\
\hline
$X = 5$ & $1/7$ & $1/7$ & $1/7$ & $3/7$ \\
$X = 8$ & $3/7$ & $0$ & $1/7$ & $4/7$ \\
\hline
& $4/7$ & $1/7$ & $2/7$ &
\end{tabular}
\end{center}

Summing the rows and columns and placing the totals in the margins gives the marginal distributions of $X$ and $Y$.
\end{example}

\subsection{Joint Density Functions}
\label{ssec:2.7.4}

If $X$ and $Y$ are continuous random variables, then clearly $p_{X,Y}(x, y) = 0$ for all $x$ and $y$. Hence, joint probability functions are not useful in this case. On the other hand, we shall see here that if $X$ and $Y$ are jointly absolutely continuous, then their relationship may be usefully described by a joint density function.

\begin{definition}
\label{def:2.7.4}
Let $f : \mathbf{R}^2 \to \mathbf{R}^1$ be a function. Then $f$ is a \emph{joint density function} if $f(x, y) \geqslant 0$ for all $x$ and $y$, and $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) \, \mathrm{d}x \, \mathrm{d}y = 1$.
\end{definition}

\begin{definition}
\label{def:2.7.5}
Let $X$ and $Y$ be random variables. Then $X$ and $Y$ are \emph{jointly absolutely continuous} if there is a joint density function $f$, such that
\[
\prb(a \leqslant X \leqslant b, c \leqslant Y \leqslant d) = \int_c^d \int_a^b f(x, y) \, \mathrm{d}x \, \mathrm{d}y
\]
for all $a \leqslant b$, $c \leqslant d$.
\end{definition}

Consider the following example.

\begin{example}
\label{ex:2.7.6}
Let $X$ and $Y$ be jointly absolutely continuous, with joint density function $f$ given by
\[
f(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]

We first verify that $f$ is indeed a density function. Clearly, $f(x, y) \geqslant 0$ for all $x$ and $y$. Also,
\[
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \int_0^1 (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \left(\frac{4}{3} y + 2y^5\right) \, \mathrm{d}y = \frac{4}{3} \cdot \frac{1}{2} + 2 \cdot \frac{1}{6} = \frac{2}{3} + \frac{1}{3} = 1.
\]
Hence, $f$ is a joint density function. In Figure~\ref{fig:2.7.2}, we have plotted the function $f$, which gives a surface over the unit square.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-7-2.pdf}
\caption{A plot of the density $f$ in Example~\ref{ex:2.7.6}.}
\label{fig:2.7.2}
\end{figure}

We next compute $\prb(0.5 \leqslant X \leqslant 0.7, 0.2 \leqslant Y \leqslant 0.9)$. Indeed, we have
\begin{align*}
\prb(0.5 \leqslant X \leqslant 0.7, 0.2 \leqslant Y \leqslant 0.9) &= \int_{0.2}^{0.9} \int_{0.5}^{0.7} (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{0.2}^{0.9} \left[\frac{4}{3}(0.7^3 - 0.5^3) y + 2y^5 (0.7 - 0.5)\right] \, \mathrm{d}y \\
&= \frac{4}{3}(0.7^3 - 0.5^3) \cdot \frac{1}{2}(0.9^2 - 0.2^2) + \frac{2}{6}(0.9^6 - 0.2^6)(0.7 - 0.5) \\
&= \frac{2}{3}(0.7^3 - 0.5^3)(0.9^2 - 0.2^2) + \frac{1}{3}(0.9^6 - 0.2^6)(0.7 - 0.5) \\
&\approx 0.147.
\end{align*}
Other probabilities can be computed similarly.
\end{example}

Once we know a joint density $f_{X,Y}$, then computing the marginal densities of $X$ and $Y$ is very easy, as the following theorem shows.

\begin{theorem}
\label{thm:2.7.5}
Let $X$ and $Y$ be jointly absolutely continuous random variables, with joint density function $f_{X,Y}$. Then the (marginal) density $f_X$ of $X$ satisfies
\[
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y
\]
for all $x \in \mathbf{R}^1$. Similarly, the (marginal) density $f_Y$ of $Y$ satisfies
\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}x
\]
for all $y \in \mathbf{R}^1$.
\end{theorem}

\begin{proof}
We need to show that, for $a \leqslant b$, $\prb(a \leqslant X \leqslant b) = \int_a^b f_X(x) \, \mathrm{d}x = \int_a^b \left[\int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y\right] \, \mathrm{d}x$. Now, we always have $Y < \infty$. Hence, using continuity of $\prb$, we have that $\prb(a \leqslant X \leqslant b) = \prb(a \leqslant X \leqslant b, -\infty < Y < \infty)$, and
\begin{align*}
\prb(a \leqslant X \leqslant b, -\infty < Y < \infty) &= \lim_{c \to -\infty, \, d \to \infty} \prb(a \leqslant X \leqslant b, c \leqslant Y \leqslant d) \\
&= \lim_{c \to -\infty, \, d \to \infty} \int_c^d \int_a^b f(x, y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \lim_{c \to -\infty, \, d \to \infty} \int_a^b \int_c^d f(x, y) \, \mathrm{d}y \, \mathrm{d}x \\
&= \int_a^b \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y \, \mathrm{d}x,
\end{align*}
as claimed. The result for $f_Y$ follows similarly.
\end{proof}

\begin{example}[Example~\ref{ex:2.7.6} continued]
\label{ex:2.7.7}
Let $X$ and $Y$ again have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]
Then by Theorem~\ref{thm:2.7.5}, for $0 \leqslant x \leqslant 1$,
\[
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y = \int_0^1 (4x^2 y + 2y^5) \, \mathrm{d}y = 2x^2 + 1/3,
\]
while for $x < 0$ or $x > 1$,
\[
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y = \int_{-\infty}^{\infty} 0 \, \mathrm{d}y = 0.
\]
Similarly, for $0 \leqslant y \leqslant 1$,
\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}x = \int_0^1 (4x^2 y + 2y^5) \, \mathrm{d}x = \frac{4}{3} y + 2y^5,
\]
while for $y < 0$ or $y > 1$, $f_Y(y) = 0$.
\end{example}

\begin{example}
\label{ex:2.7.8}
Suppose $X$ and $Y$ are jointly absolutely continuous, with joint density
\[
f_{X,Y}(x, y) = \begin{cases}
120x^3 y & x \geqslant 0, \, y \geqslant 0, \, x + y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]
Then the region where $f_{X,Y}(x, y) > 0$ is a triangle, as depicted in Figure~\ref{fig:2.7.3}.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-7-3.pdf}
\caption{Region of the plane where the density $f_{X,Y}$ in Example~\ref{ex:2.7.8} is positive.}
\label{fig:2.7.3}
\end{figure}

We check that
\begin{align*}
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y &= \int_0^1 \int_0^{1-x} 120x^3 y \, \mathrm{d}y \, \mathrm{d}x = \int_0^1 120x^3 \cdot \frac{(1-x)^2}{2} \, \mathrm{d}x \\
&= \int_0^1 60(x^3 - 2x^4 + x^5) \, \mathrm{d}x = 60 \left(\frac{1}{4} - \frac{2}{5} + \frac{1}{6}\right) \\
&= 60 \cdot \frac{15 - 24 + 10}{60} = 1,
\end{align*}
so that $f_{X,Y}$ is indeed a joint density function. We then compute that, for example,
\[
f_X(x) = \int_0^{1-x} 120x^3 y \, \mathrm{d}y = 120x^3 \cdot \frac{(1-x)^2}{2} = 60(x^3 - 2x^4 + x^5)
\]
for $0 \leqslant x \leqslant 1$ (with $f_X(x) = 0$ for $x < 0$ or $x > 1$).
\end{example}

\begin{example}[Bivariate Normal$(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$ Distribution]
\label{ex:2.7.9}
Let $\mu_1, \mu_2, \sigma_1, \sigma_2$, and $\rho$ be real numbers, with $\sigma_1, \sigma_2 > 0$ and $-1 < \rho < 1$. Let $X$ and $Y$ have joint density given by
\[
f_{X,Y}(x, y) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \exp\left(-\frac{1}{2(1 - \rho^2)} \left[\left(\frac{x - \mu_1}{\sigma_1}\right)^2 - 2\rho \left(\frac{x - \mu_1}{\sigma_1}\right)\left(\frac{y - \mu_2}{\sigma_2}\right) + \left(\frac{y - \mu_2}{\sigma_2}\right)^2\right]\right)
\]
for $x \in \mathbf{R}^1$, $y \in \mathbf{R}^1$. We say that $X$ and $Y$ have the \emph{Bivariate Normal$(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$ distribution}.

It can be shown (see Problem~\ref{exer:2.7.13}) that $X \sim N(\mu_1, \sigma_1^2)$ and $Y \sim N(\mu_2, \sigma_2^2)$. Hence, $X$ and $Y$ are each normally distributed. The parameter $\rho$ measures the degree of the relationship that exists between $X$ and $Y$ (see Problem~3.3.17) and is called the \emph{correlation}. In particular, $X$ and $Y$ are independent (see Section~\ref{ssec:2.8.3}), and so unrelated, if and only if $\rho = 0$ (see Problem~2.8.21).

Figure~\ref{fig:2.7.4} is a plot of the \emph{standard bivariate normal} density, given by setting $\mu_1 = 0$, $\mu_2 = 0$, $\sigma_1 = 1$, $\sigma_2 = 1$, and $\rho = 0$. This is a bell-shaped surface in $\mathbf{R}^3$ with its peak at the point $(0, 0)$ in the $xy$-plane. The graph of the general Bivariate Normal$(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$ distribution is also a bell-shaped surface, but the peak is at the point $(\mu_1, \mu_2)$ in the $xy$-plane and the shape of the bell is controlled by $\sigma_1$, $\sigma_2$, and $\rho$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-7-4.pdf}
\caption{A plot of the standard bivariate normal density function.}
\label{fig:2.7.4}
\end{figure}

It can be shown (see Problem~2.9.16) that, when $Z_1, Z_2$ are independent random variables, both distributed $N(0, 1)$, and we put
\begin{equation}
\label{eq:2.7.1}
X = \mu_1 + \sigma_1 Z_1, \qquad Y = \mu_2 + \sigma_2 \left[\rho Z_1 + \sqrt{1 - \rho^2} \, Z_2\right],
\end{equation}
then $(X, Y) \sim \text{Bivariate Normal}(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$. This relationship can be quite useful in establishing various properties of this distribution. We can also write an analogous version $Y = \mu_2 + \sigma_2 Z_1$, $X = \mu_1 + \sigma_1[\rho Z_1 + \sqrt{1 - \rho^2} \, Z_2]$ and obtain the same distributional result.

The bivariate normal distribution is one of the most commonly used bivariate distributions in applications. For example, if we randomly select an individual from a population and measure his weight $X$ and height $Y$, then a bivariate normal distribution will often provide a reasonable description of the joint distribution of these variables.
\end{example}

Joint densities can also be used to compute probabilities of more general regions, as the following result shows. (We omit the proof. The special case $B = [a, b] \times [c, d]$ corresponds directly to the definition of $f_{X,Y}$.)

\begin{theorem}
\label{thm:2.7.6}
Let $X$ and $Y$ be jointly absolutely continuous random variables, with joint density $f_{X,Y}$, and let $B \subseteq \mathbf{R}^2$ be any region. Then
\[
\prb((X, Y) \in B) = \iint_B f(x, y) \, \mathrm{d}x \, \mathrm{d}y.
\]
\end{theorem}

The previous discussion has centered around having just two random variables, $X$ and $Y$. More generally, we may consider $n$ random variables $X_1, \ldots, X_n$. If the random variables are all discrete, then we can further define a joint probability function $p_{X_1, \ldots, X_n} : \mathbf{R}^n \to [0, 1]$ by $p_{X_1, \ldots, X_n}(x_1, \ldots, x_n) = \prb(X_1 = x_1, \ldots, X_n = x_n)$. If the random variables are jointly absolutely continuous, then we can define a joint density function $f_{X_1, \ldots, X_n} : \mathbf{R}^n \to [0, 1]$ so that
\[
\prb(a_1 \leqslant X_1 \leqslant b_1, \ldots, a_n \leqslant X_n \leqslant b_n) = \int_{a_n}^{b_n} \cdots \int_{a_1}^{b_1} f_{X_1, \ldots, X_n}(x_1, \ldots, x_n) \, \mathrm{d}x_1 \cdots \mathrm{d}x_n
\]
whenever $a_i \leqslant b_i$ for all $i$.

\subsection*{Summary of Section~\ref{sec:2.7}}

\begin{itemize}
\item It is often important to keep track of the joint probabilities of two random variables, $X$ and $Y$.
\item Their joint cumulative distribution function is given by $F_{X,Y}(x, y) = \prb(X \leqslant x, Y \leqslant y)$.
\item If $X$ and $Y$ are discrete, then their joint probability function is given by $p_{X,Y}(x, y) = \prb(X = x, Y = y)$.
\item If $X$ and $Y$ are absolutely continuous, then their joint density function $f_{X,Y}(x, y)$ is such that $\prb(a \leqslant X \leqslant b, c \leqslant Y \leqslant d) = \int_c^d \int_a^b f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y$.
\item The marginal density of $X$ and $Y$ can be computed from any of $F_{X,Y}$, or $p_{X,Y}$, or $f_{X,Y}$.
\item An important example of a joint distribution is the bivariate normal distribution.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.7.1}
Let $X \sim \text{Bernoulli}(1/3)$, and let $Y = 4X - 2$. Compute the joint cdf $F_{X,Y}$.
\end{exercise}

\begin{solution}
\[
    F_{X,Y}(x, y) = \begin{cases}
        0 & \min[x, (y + 2)/4] < 0 \\
        1/3 & 0 \leqslant \min[x, (y + 2)/4] < 1 \\
        1 & \min[x, (y + 2)/4] \geqslant 1
    \end{cases}
\]
\end{solution}

\begin{exercise}
\label{exer:2.7.2}
Let $X \sim \text{Bernoulli}(1/4)$, and let $Y = 7X$. Compute the joint cdf $F_{X,Y}$.
\end{exercise}

\begin{solution}
\[
    F_{X,Y}(x, y) = \begin{cases}
        0 & x < 0 \text{ and } y < 0 \\
        0 & x < 1 \text{ and } y < -7 \\
        1/4 & 0 \leqslant x < 1 \text{ and } y \geqslant 0 \\
        3/4 & x \geqslant 1 \text{ and } -7 \leqslant y < 0 \\
        1 & x \geqslant 1 \text{ and } y \geqslant 0
    \end{cases}
\]
\end{solution}

\begin{exercise}
\label{exer:2.7.3}
Suppose
\[
p_{X,Y}(x, y) = \begin{cases}
1/5 & x = -2, \, y = -3 \\
1/5 & x = -3, \, y = 2 \\
1/5 & x = 3, \, y = -2 \\
1/5 & x = 2, \, y = 3 \\
1/5 & x = 17, \, y = 19 \\
0 & \text{otherwise.}
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $p_X$.
\item Compute $p_Y$.
\item Compute $\prb(Y > X)$.
\item Compute $\prb(Y < X)$.
\item Compute $\prb(XY > 0)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $p_X(2) = p_X(3) = p_X(-3) = p_X(-2) = p_X(17) = 1/5$, with $p_X(x) = 0$ otherwise.
    \item $p_Y(3) = p_Y(2) = p_Y(-2) = p_Y(-3) = p_Y(19) = 1/5$, with $p_Y(y) = 0$ otherwise.
    \item $\prb(Y > X) = p_{X,Y}(2, 3) + p_{X,Y}(-3, -2) + p_{X,Y}(17, 19) = 3/5$.
    \item $\prb(Y = X) = 0$ since this never occurs.
    \item $\prb(XY < 0) = 0$ since this never occurs.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.7.4}
For each of the following joint density functions $f_{X,Y}$, find the value of $C$ and compute $f_X(x)$, $f_Y(y)$, and $\prb(X \leqslant 0.8, Y \leqslant 0.6)$.
\begin{enumerate}[(a)]
\item $f_{X,Y}(x, y) = \begin{cases} 2x^2 y + Cy^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\ 0 & \text{otherwise.} \end{cases}$
\item $f_{X,Y}(x, y) = \begin{cases} C(xy + x^5 y^5) & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\ 0 & \text{otherwise.} \end{cases}$
\item $f_{X,Y}(x, y) = \begin{cases} C(xy + x^5 y^5) & 0 \leqslant x \leqslant 4, \, 0 \leqslant y \leqslant 10 \\ 0 & \text{otherwise.} \end{cases}$
\item $f_{X,Y}(x, y) = \begin{cases} Cx^5 y^5 & 0 \leqslant x \leqslant 4, \, 0 \leqslant y \leqslant 10 \\ 0 & \text{otherwise.} \end{cases}$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $C = 4$, and $\prb(X \leqslant 0.8, Y \leqslant 0.6) = 0.0863$.
    \item $C = 18/5$ and $\prb(X \leqslant 0.8, Y \leqslant 0.6) = 0.209$.
    \item $C = 9/1024003600$ and $\prb(X \leqslant 0.8, Y \leqslant 0.6) = 5.09 \times 10^{-10}$.
    \item $C = 9/1024000000$ and $\prb(X \leqslant 0.8, Y \leqslant 0.6) = 2.99 \times 10^{-12}$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.7.5}
Prove that $F_{X,Y}(x, y) \leqslant \min(F_X(x), F_Y(y))$.
\end{exercise}

\begin{solution}
Since $\{X \leqslant x, Y \leqslant y\} \subseteq \{X \leqslant x\}$ and $\{X \leqslant x, Y \leqslant y\} \subseteq \{Y \leqslant y\}$, then $\prb(X \leqslant x, Y \leqslant y) \leqslant \prb(X \leqslant x)$ and $\prb(X \leqslant x, Y \leqslant y) \leqslant \prb(Y \leqslant y)$, i.e., $F_{X,Y}(x, y) \leqslant F_X(x)$ and $F_{X,Y}(x, y) \leqslant F_Y(y)$, so $F_{X,Y}(x, y) \leqslant \min(F_X(x), F_Y(y))$.
\end{solution}

\begin{exercise}
\label{exer:2.7.6}
Suppose $\prb(X = x, Y = y) = 1/8$ for $x \in \{3, 5\}$ and $y \in \{1, 2, 4, 7\}$, otherwise $\prb(X = x, Y = y) = 0$. Compute each of the following.
\begin{enumerate}[(a)]
\item $F_{X,Y}(x, y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\item $p_{X,Y}(x, y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\item $p_X(x)$ for all $x \in \mathbf{R}^1$.
\item $p_Y(y)$ for all $x \in \mathbf{R}^1$.
\item The marginal cdf $F_X(x)$ for all $x \in \mathbf{R}^1$.
\item The marginal cdf $F_Y(y)$ for all $y \in \mathbf{R}^1$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The joint cdf is defined by $F_{X,Y}(x, y) = \prb(X \leqslant x, Y \leqslant y)$. Since both variables are discrete, the value of $F_{X,Y}$ is constant on some rectangles. For example, for $x < 3$ and $y \in \mathbb{R}$,
    \[
        F_{X,Y}(x, y) = \prb(X \leqslant x, Y \leqslant y) \leqslant \prb(X \leqslant x) \leqslant \prb(X < 3) = 0.
    \]
    The rectangles having the same $F_{X,Y}$ value are $(-\infty, 3)$, $[3, 5)$, and $[5, \infty)$ for $X$ and $(-\infty, 1)$, $[1, 2)$, $[2, 4)$, $[4, 7)$, and $[7, \infty)$. Hence, the joint cdf is summarized in the following table.
    \begin{center}
    \begin{tabular}{c|ccccc}
        $F_{X,Y}(x, y)$ & $y < 1$ & $1 \leqslant y < 2$ & $2 \leqslant y < 4$ & $4 \leqslant y < 7$ & $y \geqslant 7$ \\
        \hline
        $x < 3$ & 0 & 0 & 0 & 0 & 0 \\
        $3 \leqslant x < 5$ & 0 & $1/8$ & $1/4$ & $3/8$ & $1/2$ \\
        $x \geqslant 5$ & 0 & $1/4$ & $1/2$ & $3/4$ & 1
    \end{tabular}
    \end{center}
    \item Recall $p_{X,Y}(x, y) = \prb(X = x, Y = y)$. Hence, $p_{X,Y}(x, y) = 1/8$ if $x = 3, 5$ and $y = 1, 2, 4, 7$, otherwise $p_{X,Y}(x, y) = 0$.
    \item Since $p_{X,Y}(x, y) > 0$ holds only for $x = 3$ or $x = 5$ among $x \in \mathbb{R}$, we have $p_X(x) > 0$ only for $x = 3$ or $x = 5$. By definition, $p_X(3) = \sum_{y \in \mathbb{R}}\prb(X = 3, Y = y) = \prb(X = 3, Y = 1) + \prb(X = 3, Y = 2) + \prb(X = 3, Y = 4) + \prb(X = 3, Y = 7) = 1/2$. Similarly, $p_X(5) = 1/2$. In sum, $p_X(x) = 1/2$ if $x = 3$ or $x = 5$, otherwise $p_X(x) = 0$.
    \item Similar to (c), $p_Y(y) > 0$ only for $y = 1, 2, 4$, and $7$. Note $p_Y(1) = \prb(X = 3, Y = 1) + \prb(X = 5, Y = 1) = 1/4$. Similar computations give $p_Y(y) = 1/4$ for $y = 1, 2, 4$, and $7$, otherwise $p_Y(y) = 0$.
    \item By definition, $F_X(x) = \prb(X \leqslant x) = \sum_{z \leqslant x}p_X(z)$. Since $p_X(x) > 0$ only for $x = 3$ and $x = 5$, $F_X(x) = 0$ for $x < 3$, $F_X(x) = p_X(3) = 1/2$ for $3 \leqslant x < 5$, and $F_X(x) = p_X(3) + p_X(5) = 1$ for $x \geqslant 5$.
    \item Similar to (e), the range of $y$ is separated into $(-\infty, 1)$, $[1, 2)$, $[2, 4)$, $[4, 7)$ and $[7, \infty)$. Hence, we have $F_Y(y) = 0$ for $y < 1$, $F_Y(y) = 1/4$ for $1 \leqslant y < 2$, $F_Y(y) = 1/2$ for $2 \leqslant y < 4$, $F_Y(y) = 3/4$ for $4 \leqslant y < 7$, and $F_Y(y) = 1$ for $y \geqslant 7$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.7.7}
Let $X$ and $Y$ have joint density $f_{X,Y}(x, y) = c \sin(xy)$ for $0 \leqslant x \leqslant 1$ and $0 \leqslant y \leqslant 2\pi$, otherwise $f_{X,Y}(x, y) = 0$, for appropriate constant $c > 0$ (which cannot be computed explicitly). In terms of $c$, compute each of the following.
\begin{enumerate}[(a)]
\item The marginal density $f_X(x)$ for all $x \in \mathbf{R}^1$.
\item The marginal density $f_Y(y)$ for all $y \in \mathbf{R}^1$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item By integrating $y$ out, the marginal density $f_X(x)$ is given by
    \[
        f_X(x) = \int_{\mathbb{R}}f_{X,Y}(x, y)\,\mathrm{d}y = c\int_0^2 \sin(xy)\,\mathrm{d}y = c\bigl[-\cos(xy)/x\bigr]_{y=0}^{y=2} = c(1 - \cos(2x))/x
    \]
    for $0 < x < 1$ and otherwise $f_X(x) = 0$.
    \item Now integrating $x$ out is required.
    \[
        f_Y(y) = c\int_0^1 \sin(xy)\,\mathrm{d}x = c\bigl[-\cos(xy)/y\bigr]_{x=0}^{x=1} = c(1 - \cos(y))/y
    \]
    for $0 < y < 2$ and otherwise $f_Y(y) = 0$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.7.8}
Let $X$ and $Y$ have joint density $f_{X,Y}(x, y) = (x^2 + y)/36$ for $-2 \leqslant x \leqslant 1$ and $0 \leqslant y \leqslant 4$, otherwise $f_{X,Y}(x, y) = 0$. Compute each of the following.
\begin{enumerate}[(a)]
\item The marginal density $f_X(x)$ for all $x \in \mathbf{R}^1$.
\item The marginal density $f_Y(y)$ for all $y \in \mathbf{R}^1$.
\item $\prb(Y > 1)$.
\item The joint cdf $F_{X,Y}(x, y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The marginal density $f_X(x)$ is given by
    \[
        f_X(x) = \int_{\mathbb{R}}f_{X,Y}(x, y)\,\mathrm{d}y = \int_0^4 \frac{x^2 + y}{36}\,\mathrm{d}y = \frac{x^2 y + y^2/2}{36}\bigg|_{y=0}^{y=4} = \frac{4x^2 + 8}{36} = \frac{x^2 + 2}{9}
    \]
    for $-2 < x < 1$, otherwise $f_X(x) = 0$.
    \item The marginal density $f_Y(y)$ is given by
    \[
        f_Y(y) = \int_{\mathbb{R}}f_{X,Y}(x, y)\,\mathrm{d}x = \int_{-2}^{1}\frac{x^2 + y}{36}\,\mathrm{d}x = \frac{x^3/3 + xy}{36}\bigg|_{x=-2}^{x=1} = \frac{3 + 3y}{36} = \frac{1 + y}{12}
    \]
    for $0 < y < 4$, otherwise $f_Y(y) = 0$.
    \item By integrating $f_Y(y)$, we get
    \[
        \prb(Y < 1) = \int_0^1 \frac{1 + y}{12}\,\mathrm{d}y = \frac{y + y^2/2}{12}\bigg|_{y=0}^{y=1} = \frac{1}{8}.
    \]
    \item By the definition of cdf, we get $F_{X,Y}(x, y) = \prb(X \leqslant x, Y \leqslant y) = 0$ if $x \leqslant -2$ or $y \leqslant 0$. If $x \geqslant 1$ and $y \geqslant 4$, then $F_{X,Y}(x, y) = 1$. If $-2 < x < 1$ and $0 < y < 4$, then
    \begin{align*}
        F_{X,Y}(x, y) &= \int_{-2}^{x}\int_0^y \frac{u^2 + v}{36}\,\mathrm{d}v\,\mathrm{d}u = \int_{-2}^{x}\frac{u^2 v + v^2/2}{36}\bigg|_{v=0}^{v=y}\,\mathrm{d}u = \int_{-2}^{x}\frac{2u^2 y + y^2}{72}\,\mathrm{d}u \\
        &= \frac{2yu^3/3 + y^2 u}{72}\bigg|_{u=-2}^{u=x} = \frac{2y(x^3 + 8) + 3y^2(x + 2)}{216}.
    \end{align*}
    If $-2 < x < 1$ and $y \geqslant 4$, then
    \begin{align*}
        F_{X,Y}(x, y) &= \int_{-2}^{x}\int_0^y f_{X,Y}(u, v)\,\mathrm{d}v\,\mathrm{d}u = \int_{-2}^{x}f_X(u)\,\mathrm{d}u = \int_{-2}^{x}\frac{u^2 + 2}{4}\,\mathrm{d}u \\
        &= \frac{u^3/3 + 2u}{4}\bigg|_{u=-2}^{u=x} = \frac{(x + 2)(x^2 - 2x + 10)}{12}.
    \end{align*}
    Finally, if $x \geqslant 1$ and $0 < y < 4$, then
    \begin{align*}
        F_{X,Y}(x, y) &= \int_0^y \int_{-2}^x f_{X,Y}(u, v)\,\mathrm{d}u\,\mathrm{d}v = \int_0^y f_Y(v)\,\mathrm{d}v = \int_0^y \frac{1 + v}{12}\,\mathrm{d}v \\
        &= \frac{v + v^2/2}{12}\bigg|_{v=0}^{v=y} = \frac{2y + y^2}{24}.
    \end{align*}
    In sum, the joint cdf is
    \[
        F_{X,Y}(x, y) = \begin{cases}
            0 & \text{if } x \leqslant -2 \text{ or } y \leqslant 0, \\
            1 & \text{if } x \geqslant 1, y \geqslant 4, \\
            (x + 2)(3y^2 + 2y(x^2 - 2x + 4))/216 & \text{if } -2 < x < 1, 0 < y < 4, \\
            (x + 2)(x^2 - 2x + 10)/12 & \text{if } -2 < x < 1, y \geqslant 4, \\
            y(2 + y)/24 & \text{if } x \geqslant 1, 0 < y < 4.
        \end{cases}
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.7.9}
Let $X$ and $Y$ have joint density $f_{X,Y}(x, y) = (x^2 + y)/4$ for $0 \leqslant x \leqslant y \leqslant 2$, otherwise $f_{X,Y}(x, y) = 0$. Compute each of the following.
\begin{enumerate}[(a)]
\item The marginal density $f_X(x)$ for all $x \in \mathbf{R}^1$.
\item The marginal density $f_Y(y)$ for all $y \in \mathbf{R}^1$.
\item $\prb(Y > 1)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item It is not hard to see that $f_X(x) = 0$ if $x \notin (0, 2)$. For $x \in (0, 2)$,
    \[
        f_X(x) = \int_{\mathbb{R}}f_{X,Y}(x, y)\,\mathrm{d}y = \int_x^2 \frac{x^2 + y}{4}\,\mathrm{d}y = \frac{x^2 y + y^2/2}{4}\bigg|_{y=x}^{y=2} = \frac{4 + 3x^2 - 2x^3}{8}.
    \]
    \item From the range of $f_{X,Y}$, $f_Y(y) = 0$ if $y \notin (0, 2)$. For $y \in (0, 2)$,
    \[
        f_Y(y) = \int_{\mathbb{R}}f_{X,Y}(x, y)\,\mathrm{d}x = \int_0^y \frac{x^2 + y}{4}\,\mathrm{d}x = \frac{x^3/3 + xy}{4}\bigg|_{x=0}^{x=y} = \frac{y^3 + 3y^2}{12}.
    \]
    \item By integrating $f_Y(y)$, we get
    \[
        \prb(Y < 1) = \int_{-\infty}^{1}f_Y(y)\,\mathrm{d}y = \int_0^1 \frac{y^3 + 3y^2}{12}\,\mathrm{d}y = \frac{y^4/4 + y^3}{12}\bigg|_{y=0}^{y=1} = \frac{5}{48}.
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.7.10}
Let $X$ and $Y$ have the Bivariate-Normal$(3, 5, 2, 4, 1/2)$ distribution.
\begin{enumerate}[(a)]
\item Specify the marginal distribution of $X$.
\item Specify the marginal distribution of $Y$.
\item Are $X$ and $Y$ independent? Why or why not?
\end{enumerate}
\end{exercise}

\begin{solution}
Note that $f_{X,Y}(x, y) = (2\pi\sigma_1\sigma_2)^{-1}(1 - \rho^2)^{-1/2}\exp\bigl[-\frac{1}{2(1-\rho^2)}\bigl((x - \mu_1)^2/\sigma_1^2 + (y - \mu_2)^2/\sigma_2^2 - 2\rho(x-\mu_1)(y-\mu_2)/(\sigma_1\sigma_2)\bigr)\bigr]$.
\begin{enumerate}[(a)]
    \item Let $z_1 = (x - \mu_1)/\sigma_1$ and $z_2 = (y - \mu_2)/\sigma_2$. Since $z_1^2 + z_2^2 - 2\rho z_1 z_2 = (1 - \rho)^2 z_1^2 + (z_2 - \rho z_1)^2$,
    \begin{align*}
        f_X(x) &= \int_{-\infty}^{\infty}\frac{\exp(-(x - \mu_1)^2/(2\sigma_1^2))}{(2\pi\sigma_1^2)^{1/2}} \cdot \frac{\exp\bigl(-\frac{(y-\mu_2-\rho(x-\mu_1)\sigma_2/\sigma_1)^2}{2\sigma_2^2(1-\rho^2)}\bigr)}{(2\pi\sigma_2^2(1 - \rho^2))^{1/2}}\,\mathrm{d}y \\
        &= \frac{\exp(-(x - \mu_1)^2/(2\sigma_1^2))}{(2\pi\sigma_1^2)^{1/2}} \cdot \int_{-\infty}^{\infty}\frac{\exp(-u^2/2)}{(2\pi)^{1/2}}\,\mathrm{d}u \\
        &= \frac{\exp(-(x - \mu_1)^2/(2\sigma_1^2))}{(2\pi\sigma_1^2)^{1/2}}.
    \end{align*}
    Hence, $X \sim N(\mu_1, \sigma_1^2)$. In the question, $\mu_1 = 3$, $\sigma_1 = 2$. Thus, $X \sim N(3, 4)$.
    \item By changing $X$ and $Y$, we have $Y \sim N(\mu_2, \sigma_2^2)$. Since $\mu_2 = 5$, $\sigma_2 = 4$, $Y \sim N(5, 16)$.
    \item We know that $X$ and $Y$ are independent if and only if $f_X(x)f_Y(y) = f_{X,Y}(x, y)$.
    \[
        f_X(x)f_Y(y) = \frac{1}{2\pi\sigma_1\sigma_2}\exp\left(-\frac{1}{2}\left(\frac{x - \mu_1}{\sigma_1}\right)^2 - \frac{1}{2}\left(\frac{y - \mu_2}{\sigma_2}\right)^2\right).
    \]
    Hence, $f_X(x)f_Y(y) = f_{X,Y}(x, y)$ if and only if $\rho = 0$. Thus, $X$ and $Y$ are independent if and only if $\rho = 0$. In the question $\rho = 1/2$ is given. Therefore, $X$ and $Y$ are not independent.
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.7.11}
Let $X \sim \text{Exponential}(\lambda)$, and let $Y = X^3$. Compute the joint cdf, $F_{X,Y}(x, y)$.
\end{exercise}

\begin{solution}
$F_{X,Y}(x, y) = \prb(X \leqslant x, Y \leqslant y) = \prb(X \leqslant x, X^3 \leqslant y) = \prb(X \leqslant x, X \leqslant y^{1/3}) = \prb(X \leqslant \min(x, y^{1/3}))$, which equals $1 - e^{-\lambda\min(x, y^{1/3})}$ for $x, y > 0$, otherwise equals 0.
\end{solution}

\begin{exercise}
\label{exer:2.7.12}
Let $F_{X,Y}$ be a joint cdf. Prove that for all $y \in \mathbf{R}^1$, $\lim_{x \to -\infty} F_{X,Y}(x, y) = 0$.
\end{exercise}

\begin{solution}
We know $F_{X,Y}(x, y) \leqslant F_X(x)$ and that $\lim_{x \to -\infty}F_X(x) = 0$. Hence, $\lim_{x \to -\infty}F_{X,Y}(x, y) \leqslant \lim_{x \to -\infty}F_X(x) = 0$.
\end{solution}

\begin{exercise}
\label{exer:2.7.13}
Let $X$ and $Y$ have the Bivariate Normal$(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$ distribution, as in Example~\ref{ex:2.7.9}. Prove that $X \sim N(\mu_1, \sigma_1^2)$, by proving that
\[
\int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y = \frac{1}{\sigma_1 \sqrt{2\pi}} \exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right).
\]
\end{exercise}

\begin{solution}
Let $z = (x - \mu_1)/\sigma_1$ and $w = [(y - \mu_2)/\sigma_2] - [\rho(x - \mu_1)/\sigma_1]$. Then $f_{X,Y}(x, y) = (2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2})^{-1}\exp\{-[2(1 - \rho^2)]^{-1}[(1 - \rho^2)z^2 + w^2]\}$. Also, $\mathrm{d}y = \sigma_2\,\mathrm{d}w$. Hence,
\begin{align*}
    \int_{-\infty}^{\infty}f_{X,Y}(x, y)\,\mathrm{d}y &= (2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2})^{-1}\int_{-\infty}^{\infty}\exp\{-[2(1 - \rho^2)]^{-1}[(1 - \rho^2)z^2 + w^2]\}\sigma_2\,\mathrm{d}w \\
    &= (2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2})^{-1}\exp\{-[2(1 - \rho^2)]^{-1}(1 - \rho^2)z^2\}[\sqrt{2\pi(1 - \rho^2)}\sigma_2] \\
    &= \frac{1}{\sigma_1\sqrt{2\pi}}e^{-z^2/2} = \frac{1}{\sigma_1\sqrt{2\pi}}e^{-(x-\mu_1)^2/2\sigma_1}.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:2.7.14}
Suppose that the joint density $f_{X,Y}$ is given by $f_{X,Y}(x, y) = Cye^{-xy}$ for $0 \leqslant x \leqslant 1$, $0 \leqslant y \leqslant 1$ and is $0$ otherwise.
\begin{enumerate}[(a)]
\item Determine $C$ so that $f_{X,Y}$ is a density.
\item Compute $\prb(1/2 \leqslant X \leqslant 1, 1/2 \leqslant Y \leqslant 1)$.
\item Compute the marginal densities of $X$ and $Y$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\int_0^1\int_0^1 Cye^{-xy}\,\mathrm{d}x\,\mathrm{d}y = \int_0^1 -Ce^{-xy}\big|_0^1\,\mathrm{d}y = C\int_0^1(1 - e^{-y})\,\mathrm{d}y = C(1 + e^{-y}\big|_0^1) = Ce^{-1}$ and so $C = e$.
    \item $e\int_{1/2}^{1}\int_{1/2}^{1}ye^{-xy}\,\mathrm{d}x\,\mathrm{d}y = e\int_{1/2}^{1}e^{-xy}\big|_{1/2}^{1}\,\mathrm{d}y = e\int_{1/2}^{1}(e^{-y/2} - e^{-y})\,\mathrm{d}y = e(-2e^{-y/2}\big|_{1/2}^{1} + e^{-y}\big|_{1/2}^{1}) = e(2e^{-1/4} - 2e^{-1/2} + e^{-1} - e^{-1/2}) = 0.28784$.
    \item Using integration by parts with $u = y$, $\mathrm{d}u = 1$, $\mathrm{d}v = e^{-xy+1}$, and $v = -e^{-xy+1}/x$, we have that $f_X(x) = \int_0^1 ye^{-xy+1}\,\mathrm{d}y = -\frac{y}{x}e^{-xy+1}\big|_0^1 + \frac{1}{x}\int_0^1 e^{-xy+1}\,\mathrm{d}y = -\frac{e^{-x+1}}{x} - \frac{1}{x^2}e^{-xy+1}\big|_0^1 = e\bigl(\frac{1}{x^2} - \frac{e^{-x}}{x} - \frac{e^{-x}}{x^2}\bigr)$ for $0 < x < 1$. Also, we have that $f_Y(y) = \int_0^1 ye^{-xy+1}\,\mathrm{d}x = -e^{-xy+1}\big|_0^1 = e(1 - e^{-y})$ for $0 < y < 1$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.7.15}
Suppose that the joint density $f_{X,Y}$ is given by $f_{X,Y}(x, y) = Cye^{-xy}$ for $0 \leqslant x \leqslant y \leqslant 1$ and is $0$ otherwise.
\begin{enumerate}[(a)]
\item Determine $C$ so that $f_{X,Y}$ is a density.
\item Compute $\prb(1/2 \leqslant X \leqslant 1, 1/2 \leqslant Y \leqslant 1)$.
\item Compute the marginal densities of $X$ and $Y$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\int_0^1\int_0^y Cye^{-xy}\,\mathrm{d}x\,\mathrm{d}y = \int_0^1 -Ce^{-xy}\big|_0^y\,\mathrm{d}y = C\int_0^1(1 - e^{-y^2})\,\mathrm{d}y = C(1 + \sqrt{\pi}(\Phi(0) - \Phi(\sqrt{2})))$ and so $C = (1 + \sqrt{\pi}(\Phi(0) - \Phi(\sqrt{2})))^{-1}$.
    \item 
    \begin{align*}
        C\int_{1/2}^{1}\int_{1/2}^{y}ye^{-xy}\,\mathrm{d}x\,\mathrm{d}y &= C\int_{1/2}^{1}e^{-xy}\big|_{1/2}^{y}\,\mathrm{d}y = C\int_{1/2}^{1}(e^{-y/2} - e^{-y^2})\,\mathrm{d}y \\
        &= C\left(-2e^{-y/2}\big|_{1/2}^{1} + \sqrt{\pi}\bigl(\Phi(\sqrt{2}/2) - \Phi(\sqrt{2})\bigr)\right) \\
        &= C\bigl(2e^{-1/4} - 2e^{-1/2} + \sqrt{\pi}(\Phi(\sqrt{2}/2) - \Phi(\sqrt{2}))\bigr).
    \end{align*}
    \item Using integration by parts with $u = y$, $\mathrm{d}u = 1$, $\mathrm{d}v = e^{-xy+1}$, $v = -e^{-xy+1}/x$ we have that
    \begin{align*}
        f_X(x) &= \int_x^1 ye^{-xy+1}\,\mathrm{d}y = -\frac{y}{x}e^{-xy+1}\bigg|_x^1 + \frac{1}{x}\int_x^1 e^{-xy+1}\,\mathrm{d}y \\
        &= \frac{e^{-x^2+1} - e^{-x+1}}{x} - \frac{1}{x^2}e^{-xy+1}\bigg|_x^1 = e\left(\frac{e^{-x^2} - e^{-x}}{x} + \frac{e^{-x^2}}{x^2} - \frac{e^{-x}}{x^2}\right)
    \end{align*}
    for $0 < x < 1$. Also, we have that $f_Y(y) = \int_0^y ye^{-xy+1}\,\mathrm{d}x = -e^{-xy+1}\big|_0^y = e(1 - e^{-y^2})$ for $0 < y < 1$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.7.16}
Suppose that the joint density $f_{X,Y}$ is given by $f_{X,Y}(x, y) = Ce^{-(x+y)}$ for $0 \leqslant x \leqslant y < \infty$ and is $0$ otherwise.
\begin{enumerate}[(a)]
\item Determine $C$ so that $f_{X,Y}$ is a density.
\item Compute the marginal densities of $X$ and $Y$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\int_0^{\infty}\int_0^y Ce^{-(x+y)}\,\mathrm{d}x\,\mathrm{d}y = C\int_0^{\infty}-e^{-(x+y)}\big|_0^y\,\mathrm{d}x\,\mathrm{d}y = C\int_0^{\infty}e^{-y}(1 - e^{-y})\,\mathrm{d}y = C(1 - \int_0^{\infty}e^{-2y}\,\mathrm{d}y) = C(1 - 1/2) = C/2$, so $C = 2$.
    \item We have that $f_X(x) = 2\int_x^{\infty}e^{-(x+y)}\,\mathrm{d}y = 2e^{-x}\int_x^{\infty}e^{-y}\,\mathrm{d}y = 2e^{-2x}$ so $X \sim \text{Exponential}(2)$ and $f_Y(y) = 2\int_0^y e^{-(x+y)}\,\mathrm{d}x = 2e^{-y}\int_0^y e^{-x}\,\mathrm{d}x = 2e^{-y}(1 - e^{-y})$ for $y > 0$.
\end{enumerate}
\end{solution}

\begin{exercise}[Dirichlet$(\alpha_1, \alpha_2, \alpha_3)$ distribution]
\label{exer:2.7.17}
Let $X_1, X_2$ have the joint density
\[
f_{X_1, X_2}(x_1, x_2) = \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1) \, \Gamma(\alpha_2) \, \Gamma(\alpha_3)} x_1^{\alpha_1 - 1} x_2^{\alpha_2 - 1} (1 - x_1 - x_2)^{\alpha_3 - 1}
\]
for $x_1 > 0$, $x_2 > 0$, and $0 < x_1 + x_2 < 1$. A Dirichlet distribution is often applicable when $X_1$, $X_2$, and $1 - X_1 - X_2$ correspond to random proportions.
\begin{enumerate}[(a)]
\item Prove that $f_{X_1, X_2}$ is a density. (Hint: Sketch the region where $f_{X_1, X_2}$ is nonnegative, integrate out $x_1$ first by making the transformation $u = x_1/(1 - x_2)$ in this integral, and use \eqref{eq:2.4.10} from Problem~\ref{exer:2.4.24}.)
\item Prove that $X_1 \sim \text{Beta}(\alpha_1, \alpha_2 + \alpha_3)$ and $X_2 \sim \text{Beta}(\alpha_2, \alpha_1 + \alpha_3)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item We need to calculate $\int_0^1\int_0^{1-x_2}x_1^{\alpha_1-1}x_2^{\alpha_2-1}(1 - x_1 - x_2)^{\alpha_3-1}\,\mathrm{d}x_1\,\mathrm{d}x_2$
    \begin{align*}
        &= \int_0^1 x_2^{\alpha_2-1}(1 - x_2)^{\alpha_1+\alpha_3-2}\left(\int_0^{1-x_2}\left(\frac{x_1}{1-x_2}\right)^{\alpha_1-1}\left(1 - \frac{x_1}{1-x_2}\right)^{\alpha_3-1}\,\mathrm{d}x_1\right)\mathrm{d}x_2
    \end{align*}
    and, making the transformation $u = x_1/(1 - x_2)$, $\mathrm{d}u = (1 - x_2)^{-1}\,\mathrm{d}x_1$, we have that this integral equals
    \begin{align*}
        &\int_0^1 x_2^{\alpha_2-1}(1 - x_2)^{\alpha_1+\alpha_3-1}\left(\int_0^1 u^{\alpha_1-1}(1 - u)^{\alpha_3-1}\,\mathrm{d}u\right)\mathrm{d}x_2 \\
        &= \frac{\Gamma(\alpha_1)\Gamma(\alpha_3)}{\Gamma(\alpha_1 + \alpha_3)}\int_0^1 x_2^{\alpha_2-1}(1 - x_2)^{\alpha_1+\alpha_3-1}\,\mathrm{d}x_2 \\
        &= \frac{\Gamma(\alpha_1)\Gamma(\alpha_3)}{\Gamma(\alpha_1 + \alpha_3)}\cdot\frac{\Gamma(\alpha_2)\Gamma(\alpha_1 + \alpha_3)}{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)} = \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}
    \end{align*}
    by two applications of (2.4.10). This establishes that $f_{X_1,X_2}$ is a density.
    \item We have that
    \begin{align*}
        f_{X_1}(x_1) &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}\int_0^{1-x_1}x_1^{\alpha_1-1}x_2^{\alpha_2-1}(1 - x_1 - x_2)^{\alpha_3-1}\,\mathrm{d}x_2 \\
        &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}x_1^{\alpha_1-1}(1 - x_1)^{\alpha_2+\alpha_3-2} \\
        &\quad \times \int_0^{1-x_1}\left(\frac{x_2}{1 - x_1}\right)^{\alpha_2-1}\left(1 - \frac{x_2}{1 - x_1}\right)^{\alpha_3-1}\,\mathrm{d}x_2 \\
        &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}x_1^{\alpha_1-1}(1 - x_1)^{\alpha_2+\alpha_3-1}\int_0^{1-x_1}u^{\alpha_2-1}(1 - u)^{\alpha_3-1}\,\mathrm{d}u \\
        &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}x_1^{\alpha_1-1}(1 - x_1)^{\alpha_2+\alpha_3-1}\cdot\frac{\Gamma(\alpha_2)\Gamma(\alpha_3)}{\Gamma(\alpha_2 + \alpha_3)} \\
        &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2 + \alpha_3)}x_1^{\alpha_1-1}(1 - x_1)^{\alpha_2+\alpha_3-1},
    \end{align*}
    so $X_1 \sim \text{Beta}(\alpha_1, \alpha_2 + \alpha_3)$. Similarly, $X_2 \sim \text{Beta}(\alpha_2, \alpha_1 + \alpha_3)$.
\end{enumerate}
\end{solution}

\begin{exercise}[Dirichlet$(\alpha_1, \ldots, \alpha_{k+1})$ distribution]
\label{exer:2.7.18}
Let $X_1, \ldots, X_k$ have the joint density
\[
f_{X_1, \ldots, X_k}(x_1, \ldots, x_k) = \frac{\Gamma(\alpha_1 + \cdots + \alpha_{k+1})}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_{k+1})} x_1^{\alpha_1 - 1} \cdots x_k^{\alpha_k - 1} (1 - x_1 - \cdots - x_k)^{\alpha_{k+1} - 1}
\]
for $x_i > 0$ $(i = 1, \ldots, k)$ and $0 < x_1 + \cdots + x_k < 1$. Prove that $f_{X_1, \ldots, X_k}$ is a density. (Hint: Problem~\ref{exer:2.7.17}.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    &\int_0^1 \cdots \int_0^{1-x_3-\cdots-x_k}\int_0^{1-x_2-\cdots-x_k}x_1^{\alpha_1-1}x_2^{\alpha_2-1}\cdots x_k^{\alpha_k-1}(1 - x_1 - x_2 - \cdots - x_k)^{\alpha_{k+1}-1}\,\mathrm{d}x_1\,\mathrm{d}x_2\cdots \mathrm{d}x_k \\
    &= \int_0^1 \cdots \int_0^{1-x_3-\cdots-x_k}x_2^{\alpha_2-1}\cdots x_k^{\alpha_k-1}(1 - x_2 - \cdots - x_k)^{\alpha_1+\alpha_{k+1}-2} \\
    &\quad \times \left(\int_0^{1-x_2-\cdots-x_k}\left(\frac{x_1}{1 - x_2 - \cdots - x_k}\right)^{\alpha_1-1}\left(1 - \frac{x_1}{1 - x_2 - \cdots - x_k}\right)^{\alpha_{k+1}-1}\,\mathrm{d}x_1\right)\mathrm{d}x_2\cdots \mathrm{d}x_k \\
    &= \int_0^1 \cdots \int_0^{1-x_3-\cdots-x_k}x_2^{\alpha_2-1}\cdots x_k^{\alpha_k-1}(1 - x_2 - \cdots - x_k)^{\alpha_1+\alpha_{k+1}-1} \\
    &\quad \times \left(\int_0^1 u^{\alpha_1-1}(1 - u)^{\alpha_{k+1}-1}\,\mathrm{d}u\right)\mathrm{d}x_2\cdots \mathrm{d}x_k
\end{align*}
and this in turn equals
\begin{align*}
    &\frac{\Gamma(\alpha_1)\Gamma(\alpha_{k+1})}{\Gamma(\alpha_1 + \alpha_{k+1})}\int_0^1 \cdots \int_0^{1-x_3-\cdots-x_k}x_2^{\alpha_2-1}\cdots x_k^{\alpha_k-1}(1 - x_2 - \cdots - x_k)^{\alpha_1+\alpha_{k+1}-1}\,\mathrm{d}x_2\cdots \mathrm{d}x_k \\
    &= \cdots = \frac{\Gamma(\alpha_1)\Gamma(\alpha_{k+1})}{\Gamma(\alpha_1 + \alpha_{k+1})}\cdot\frac{\Gamma(\alpha_2)\Gamma(\alpha_1 + \alpha_{k+1})}{\Gamma(\alpha_1 + \alpha_2 + \alpha_{k+1})}\cdots\frac{\Gamma(\alpha_k)\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1 + \cdots + \alpha_{k+1})} \\
    &= \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)\cdots\Gamma(\alpha_{k+1})}{\Gamma(\alpha_1 + \cdots + \alpha_{k+1})}
\end{align*}
and this establishes that $f_{X_1,\ldots,X_k}$ is a density.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:2.7.19}
Find an example of two random variables $X$ and $Y$ and a function $h : \mathbf{R}^1 \to \mathbf{R}^1$, such that $F_X(x) > 0$ and $F_Y(x) > 0$ for all $x \in \mathbf{R}^1$, but $\lim_{x \to \infty} F_{X,Y}(x, h(x)) \neq 0$.
\end{exercise}

\begin{solution}
For example, take $X$ and $Y$ to be i.i.d.\ $\sim \text{Normal}(0, 1)$, with $h(x) = -x$. Then $F_{X,Y}(x, h(x)) = \prb(X \leqslant x, Y \leqslant -x) \leqslant \prb(Y \leqslant -x) = \Phi(-x) \to 0$ as $x \to \infty$.
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:2.7.20}
What are examples of pairs of real-life random quantities that have interesting relationships? (List as many as you can, and describe each relationship as well as you can.)
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conditioning and Independence}
\label{sec:2.8}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $X$ and $Y$ be two random variables. Suppose we know that $X = 5$. What does that tell us about $Y$? Depending on the relationship between $X$ and $Y$, that may tell us everything about $Y$ (e.g., if $Y = X$), or nothing about $Y$. Usually, the answer will be between these two extremes, and the knowledge that $X = 5$ will change the probabilities for $Y$ somewhat.

\subsection{Conditioning on Discrete Random Variables}
\label{ssec:2.8.1}

Suppose $X$ is a discrete random variable, with $\prb(X = 5) > 0$. Let $a < b$, and suppose we are interested in the conditional probability $\prb(a \leqslant Y \leqslant b \mid X = 5)$. Well, we already know how to compute such conditional probabilities. Indeed, by (1.5.1),
\[
\prb(a \leqslant Y \leqslant b \mid X = 5) = \frac{\prb(a \leqslant Y \leqslant b, X = 5)}{\prb(X = 5)}
\]
provided that $\prb(X = 5) > 0$. This prompts the following definition.

\begin{definition}
\label{def:2.8.1}
Let $X$ and $Y$ be random variables, and suppose that $\prb(X = x) > 0$. The \emph{conditional distribution} of $Y$, given that $X = x$, is the probability distribution assigning probability
\[
\frac{\prb(Y \in B, X = x)}{\prb(X = x)}
\]
to each event $\{Y \in B\}$. In particular, it assigns probability
\[
\frac{\prb(a \leqslant Y \leqslant b, X = x)}{\prb(X = x)}
\]
to the event that $a \leqslant Y \leqslant b$.
\end{definition}

\begin{example}
\label{ex:2.8.1}
Suppose as in Example~\ref{ex:2.7.5} that $X$ and $Y$ have joint probability function
\[
p_{X,Y}(x, y) = \begin{cases}
1/7 & x = 5, \, y = 0 \\
1/7 & x = 5, \, y = 3 \\
1/7 & x = 5, \, y = 4 \\
3/7 & x = 8, \, y = 0 \\
1/7 & x = 8, \, y = 4 \\
0 & \text{otherwise.}
\end{cases}
\]
We compute $\prb(Y = 4 \mid X = 8)$ as
\[
\prb(Y = 4 \mid X = 8) = \frac{\prb(Y = 4, X = 8)}{\prb(X = 8)} = \frac{1/7}{3/7 + 1/7} = \frac{1/7}{4/7} = \frac{1}{4}.
\]
On the other hand,
\[
\prb(Y = 4 \mid X = 5) = \frac{\prb(Y = 4, X = 5)}{\prb(X = 5)} = \frac{1/7}{1/7 + 1/7 + 1/7} = \frac{1/7}{3/7} = \frac{1}{3}.
\]
Thus, depending on the value of $X$, we obtain different probabilities for $Y$.
\end{example}

Generalizing from the above example, we see that if $X$ and $Y$ are discrete, then
\[
\prb(Y = y \mid X = x) = \frac{\prb(Y = y, X = x)}{\prb(X = x)} = \frac{p_{X,Y}(x, y)}{p_X(x)} = \frac{p_{X,Y}(x, y)}{\sum_z p_{X,Y}(x, z)}.
\]

This prompts the following definition.

\begin{definition}
\label{def:2.8.2}
Suppose $X$ and $Y$ are two discrete random variables. Then the \emph{conditional probability function} of $Y$, given $X$, is the function $p_{Y|X}$ defined by
\[
p_{Y|X}(y \mid x) = \frac{p_{X,Y}(x, y)}{\sum_z p_{X,Y}(x, z)} = \frac{p_{X,Y}(x, y)}{p_X(x)},
\]
defined for all $y \in \mathbf{R}^1$ and all $x$ with $p_X(x) > 0$.
\end{definition}

\subsection{Conditioning on Continuous Random Variables}
\label{ssec:2.8.2}

If $X$ is continuous, then we will have $\prb(X = x) = 0$. In this case, Definitions~\ref{def:2.8.1} and \ref{def:2.8.2} cannot be used because we cannot divide by $0$. So how can we condition on $X = x$ in this case?

One approach is suggested by instead conditioning on $\{x - \epsilon \leqslant X \leqslant x + \epsilon\}$, where $\epsilon > 0$ is a very small number. Even if $X$ is continuous, we might still have $\prb(x - \epsilon \leqslant X \leqslant x + \epsilon) > 0$. On the other hand, if $\epsilon$ is very small and $x - \epsilon \leqslant X \leqslant x + \epsilon$, then $X$ must be very close to $x$.

Indeed, suppose that $X$ and $Y$ are jointly absolutely continuous, with joint density function $f_{X,Y}$. Then
\[
\prb(a \leqslant Y \leqslant b \mid x - \epsilon \leqslant X \leqslant x + \epsilon) = \frac{\prb(a \leqslant Y \leqslant b, x - \epsilon \leqslant X \leqslant x + \epsilon)}{\prb(x - \epsilon \leqslant X \leqslant x + \epsilon)} = \frac{\int_a^b \int_{x-\epsilon}^{x+\epsilon} f_{X,Y}(t, y) \, \mathrm{d}t \, \mathrm{d}y}{\int_{-\infty}^{\infty} \int_{x-\epsilon}^{x+\epsilon} f_{X,Y}(t, y) \, \mathrm{d}t \, \mathrm{d}y}.
\]

In Figure~\ref{fig:2.8.1}, we have plotted the region $\{(x, y) : a \leqslant y \leqslant b, x - \epsilon \leqslant x \leqslant x + \epsilon\}$ for $(X, Y)$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-8-1.pdf}
\caption{The shaded region is the set $\{(x, y) : a \leqslant y \leqslant b, x - \epsilon \leqslant x \leqslant x + \epsilon\}$.}
\label{fig:2.8.1}
\end{figure}

Now, if $\epsilon$ is very small, then in the above integrals we will always have $t$ very close to $x$. If $f_{X,Y}$ is a continuous function, then this implies that $f_{X,Y}(t, y)$ will be very close to $f_{X,Y}(x, y)$. We conclude that, if $\epsilon$ is very small, then
\[
\prb(a \leqslant Y \leqslant b \mid x - \epsilon \leqslant X \leqslant x + \epsilon) \approx \frac{\int_a^b \int_{x-\epsilon}^{x+\epsilon} f_{X,Y}(x, y) \, \mathrm{d}t \, \mathrm{d}y}{\int_{-\infty}^{\infty} \int_{x-\epsilon}^{x+\epsilon} f_{X,Y}(x, y) \, \mathrm{d}t \, \mathrm{d}y} = \frac{\int_a^b 2\epsilon \, f_{X,Y}(x, y) \, \mathrm{d}y}{2\epsilon \int_{-\infty}^{\infty} f_{X,Y}(x, z) \, \mathrm{d}z} = \int_a^b \frac{f_{X,Y}(x, y)}{\int_{-\infty}^{\infty} f_{X,Y}(x, z) \, \mathrm{d}z} \, \mathrm{d}y.
\]

This suggests that the quantity
\[
\frac{f_{X,Y}(x, y)}{\int_{-\infty}^{\infty} f_{X,Y}(x, z) \, \mathrm{d}z} = \frac{f_{X,Y}(x, y)}{f_X(x)}
\]
plays the role of a density, for the conditional distribution of $Y$ given that $X = x$. This prompts the following definitions.

\begin{definition}
\label{def:2.8.3}
Let $X$ and $Y$ be jointly absolutely continuous, with joint density function $f_{X,Y}$. The \emph{conditional density} of $Y$, given $X = x$, is the function $f_{Y|X}(y \mid x)$, defined by
\[
f_{Y|X}(y \mid x) = \frac{f_{X,Y}(x, y)}{f_X(x)},
\]
valid for all $y \in \mathbf{R}^1$, and for all $x$ such that $f_X(x) > 0$.
\end{definition}

\begin{definition}
\label{def:2.8.4}
Let $X$ and $Y$ be jointly absolutely continuous, with joint density function $f_{X,Y}$. The \emph{conditional distribution} of $Y$, given $X = x$, is defined by saying that
\[
\prb(a \leqslant Y \leqslant b \mid X = x) = \int_a^b f_{Y|X}(y \mid x) \, \mathrm{d}y
\]
when $a \leqslant b$, with $f_{Y|X}$ as in Definition~\ref{def:2.8.3}, valid for all $x$ such that $f_X(x) > 0$.
\end{definition}

\begin{example}
\label{ex:2.8.2}
Let $X$ and $Y$ have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise,}
\end{cases}
\]
as in Examples~\ref{ex:2.7.6} and \ref{ex:2.7.7}.

We know from Example~\ref{ex:2.7.7} that
\[
f_X(x) = \begin{cases}
2x^2 + 1/3 & 0 \leqslant x \leqslant 1 \\
0 & \text{otherwise,}
\end{cases}
\]
while
\[
f_Y(y) = \begin{cases}
\frac{4}{3} y + 2y^5 & 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]

Let us now compute $\prb(0.2 \leqslant Y \leqslant 0.3 \mid X = 0.8)$. Using Definitions~\ref{def:2.8.4} and \ref{def:2.8.3}, we have
\begin{align*}
\prb(0.2 \leqslant Y \leqslant 0.3 \mid X = 0.8) &= \int_{0.2}^{0.3} f_{Y|X}(y \mid 0.8) \, \mathrm{d}y = \int_{0.2}^{0.3} \frac{f_{X,Y}(0.8, y)}{f_X(0.8)} \, \mathrm{d}y \\
&= \frac{\int_{0.2}^{0.3} (4(0.8)^2 y + 2y^5) \, \mathrm{d}y}{2(0.8)^2 + \frac{1}{3}} \\
&= \frac{\frac{4}{2}(0.8)^2(0.3^2 - 0.2^2) + \frac{2}{6}(0.3^6 - 0.2^6)}{2(0.8)^2 + \frac{1}{3}} \approx 0.0398.
\end{align*}

By contrast, if we compute the unconditioned (i.e., usual) probability that $0.2 \leqslant Y \leqslant 0.3$, we see that
\[
\prb(0.2 \leqslant Y \leqslant 0.3) = \int_{0.2}^{0.3} f_Y(y) \, \mathrm{d}y = \int_{0.2}^{0.3} \left(\frac{4}{3} y + 2y^5\right) \, \mathrm{d}y = \frac{4}{3} \cdot \frac{1}{2}(0.3^2 - 0.2^2) + \frac{2}{6}(0.3^6 - 0.2^6) \approx 0.0336.
\]
We thus see that conditioning on $X = 0.8$ increases the probability that $0.2 \leqslant Y \leqslant 0.3$, from about $0.0336$ to about $0.0398$.
\end{example}

By analogy with Theorem~1.3.1, we have the following.

\begin{theorem}[Law of total probability, absolutely continuous random variable version]
\label{thm:2.8.1}
Let $X$ and $Y$ be jointly absolutely continuous random variables, and let $a \leqslant b$ and $c \leqslant d$. Then
\[
\prb(a \leqslant X \leqslant b, c \leqslant Y \leqslant d) = \int_c^d \int_a^b f_X(x) \, f_{Y|X}(y \mid x) \, \mathrm{d}x \, \mathrm{d}y.
\]
More generally, if $B \subseteq \mathbf{R}^2$ is any region, then
\[
\prb((X, Y) \in B) = \iint_B f_X(x) \, f_{Y|X}(y \mid x) \, \mathrm{d}x \, \mathrm{d}y.
\]
\end{theorem}

\begin{proof}
By Definition~\ref{def:2.8.3},
\[
f_X(x) \, f_{Y|X}(y \mid x) = f_{X,Y}(x, y).
\]
Hence, the result follows immediately from Definition~\ref{def:2.7.4} and Theorem~\ref{thm:2.7.6}.
\end{proof}

\subsection{Independence of Random Variables}
\label{ssec:2.8.3}

Recall from Definition~1.5.2 that two events $A$ and $B$ are independent if $\prb(A \cap B) = \prb(A) \, \prb(B)$. We wish to have a corresponding definition of independence for random variables $X$ and $Y$. Intuitively, independence of $X$ and $Y$ means that $X$ and $Y$ have no influence on each other, i.e., that the values of $X$ make no change to the probabilities for $Y$ (and vice versa).

The idea of the formal definition is that $X$ and $Y$ give rise to events, of the form ``$a \leqslant X \leqslant b$'' or ``$Y \in B$'', and we want all such events involving $X$ to be independent of all such events involving $Y$. Specifically, our definition is the following.

\begin{definition}
\label{def:2.8.5}
Let $X$ and $Y$ be two random variables. Then $X$ and $Y$ are \emph{independent} if, for all subsets $B_1$ and $B_2$ of the real numbers,
\[
\prb(X \in B_1, Y \in B_2) = \prb(X \in B_1) \, \prb(Y \in B_2).
\]
That is, the events ``$X \in B_1$'' and ``$Y \in B_2$'' are independent events.
\end{definition}

Intuitively, $X$ and $Y$ are independent if they have no influence on each other, as we shall see.

Now, Definition~\ref{def:2.8.5} is very difficult to work with. Fortunately, there is a much simpler characterization of independence.

\begin{theorem}
\label{thm:2.8.2}
Let $X$ and $Y$ be two random variables. Then $X$ and $Y$ are independent if and only if
\begin{equation}
\label{eq:2.8.1}
\prb(a \leqslant X \leqslant b, c \leqslant Y \leqslant d) = \prb(a \leqslant X \leqslant b) \, \prb(c \leqslant Y \leqslant d)
\end{equation}
whenever $a \leqslant b$ and $c \leqslant d$.
\end{theorem}

That is, $X$ and $Y$ are independent if and only if the events ``$a \leqslant X \leqslant b$'' and ``$c \leqslant Y \leqslant d$'' are independent events whenever $a \leqslant b$ and $c \leqslant d$.

We shall not prove Theorem~\ref{thm:2.8.2} here, although it is similar in spirit to the proof of Theorem~\ref{thm:2.5.1}. However, we shall sometimes use \eqref{eq:2.8.1} to check for the independence of $X$ and $Y$.

Still, even \eqref{eq:2.8.1} is not so easy to check directly. For discrete and for absolutely continuous distributions, easier conditions are available, as follows.

\begin{theorem}
\label{thm:2.8.3}
Let $X$ and $Y$ be two random variables.
\begin{enumerate}[(a)]
\item If $X$ and $Y$ are discrete, then $X$ and $Y$ are independent if and only if their joint probability function $p_{X,Y}$ satisfies
\[
p_{X,Y}(x, y) = p_X(x) \, p_Y(y)
\]
for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\item If $X$ and $Y$ are jointly absolutely continuous, then $X$ and $Y$ are independent if and only if their joint density function $f_{X,Y}$ can be chosen to satisfy
\[
f_{X,Y}(x, y) = f_X(x) \, f_Y(y)
\]
for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}[(a)]
    \item If $X$ and $Y$ are independent, then setting $a = b = x$ and $c = d = y$ in \eqref{eq:2.8.1}, we see that $\prb(X = x, Y = y) = \prb(X = x) \, \prb(Y = y)$. Hence, $p_{X,Y}(x, y) = p_X(x) \, p_Y(y)$.

      Conversely, if $p_{X,Y}(x, y) = p_X(x) \, p_Y(y)$ for all $x$ and $y$, then
\begin{align*}
\prb(a \leqslant X \leqslant b, c \leqslant Y \leqslant d) &= \sum_{a \leqslant x \leqslant b} \sum_{c \leqslant y \leqslant d} p_{X,Y}(x, y) = \sum_{a \leqslant x \leqslant b} \sum_{c \leqslant y \leqslant d} p_X(x) \, p_Y(y) \\
&= \left(\sum_{a \leqslant x \leqslant b} p_X(x)\right) \left(\sum_{c \leqslant y \leqslant d} p_Y(y)\right) = \prb(a \leqslant X \leqslant b) \, \prb(c \leqslant Y \leqslant d).
\end{align*}
This completes the proof of (a).

  \item If $f_{X,Y}(x, y) = f_X(x) \, f_Y(y)$ for all $x$ and $y$, then
\begin{align*}
\prb(a \leqslant X \leqslant b, c \leqslant Y \leqslant d) &= \int_a^b \int_c^d f_{X,Y}(x, y) \, \mathrm{d}y \, \mathrm{d}x = \int_a^b \int_c^d f_X(x) \, f_Y(y) \, \mathrm{d}y \, \mathrm{d}x \\
&= \left(\int_a^b f_X(x) \, \mathrm{d}x\right) \left(\int_c^d f_Y(y) \, \mathrm{d}y\right) = \prb(a \leqslant X \leqslant b) \, \prb(c \leqslant Y \leqslant d).
\end{align*}
This completes the proof of the ``if'' part of (b). The proof of the ``only if'' part of (b) is more technical, and we do not include it here.
\end{enumerate}
\end{proof}

\begin{example}
\label{ex:2.8.3}
Let $X$ and $Y$ have, as in Example~\ref{ex:2.7.6}, joint density
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise}
\end{cases}
\]
and so, as derived in Example~\ref{ex:2.7.7}, marginal densities
\[
f_X(x) = \begin{cases}
2x^2 + 1/3 & 0 \leqslant x \leqslant 1 \\
0 & \text{otherwise}
\end{cases}
\]
and
\[
f_Y(y) = \begin{cases}
\frac{4}{3} y + 2y^5 & 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]
Then we compute that
\[
f_X(x) \, f_Y(y) = \begin{cases}
\left(2x^2 + \frac{1}{3}\right)\left(\frac{4}{3} y + 2y^5\right) & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]
We therefore see that $f_X(x) \, f_Y(y) \neq f_{X,Y}(x, y)$. Hence, $X$ and $Y$ are \emph{not} independent.
\end{example}

\begin{example}
\label{ex:2.8.4}
Let $X$ and $Y$ have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
\frac{1}{8080}(12xy^2 + 6x + 4y^2 + 2) & 0 \leqslant x \leqslant 6, \, 3 \leqslant y \leqslant 5 \\
0 & \text{otherwise.}
\end{cases}
\]
We compute the marginal densities as
\[
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y = \frac{1}{60}\left(\frac{1}{20} + x\right) \quad \text{for } 0 \leqslant x \leqslant 6, \text{ otherwise } 0,
\]
and
\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}x = \frac{3}{202}\left(\frac{3}{101} + y^2\right) \quad \text{for } 3 \leqslant y \leqslant 5, \text{ otherwise } 0.
\]
Then we compute that
\[
f_X(x) \, f_Y(y) = \begin{cases}
\frac{1}{60}\left(\frac{1}{20} + x\right) \cdot \frac{3}{202}\left(\frac{3}{101} + y^2\right) & 0 \leqslant x \leqslant 6, \, 3 \leqslant y \leqslant 5 \\
0 & \text{otherwise.}
\end{cases}
\]
Multiplying this out, we see that $f_X(x) \, f_Y(y) = f_{X,Y}(x, y)$. Hence, $X$ and $Y$ are independent in this case.
\end{example}

Combining Theorem~\ref{thm:2.8.3} with Definitions~\ref{def:2.8.2} and \ref{def:2.8.3}, we immediately obtain the following result about independence. It says that independence of random variables is the same as saying that conditioning on one has no effect on the other, which corresponds to an intuitive notion of independence.

\begin{theorem}
\label{thm:2.8.4}
Let $X$ and $Y$ be two random variables.
\begin{enumerate}[(a)]
\item If $X$ and $Y$ are discrete, then $X$ and $Y$ are independent if and only if $p_{Y|X}(y \mid x) = p_Y(y)$ for every $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\item If $X$ and $Y$ are jointly absolutely continuous, then $X$ and $Y$ are independent if and only if $f_{Y|X}(y \mid x) = f_Y(y)$ for every $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\end{enumerate}
\end{theorem}

While Definition~\ref{def:2.8.5} is quite difficult to work with, it does provide the easiest way to prove one very important property of independence, as follows.

\begin{theorem}
\label{thm:2.8.5}
Let $X$ and $Y$ be independent random variables. Let $f, g : \mathbf{R}^1 \to \mathbf{R}^1$ be any two functions. Then the random variables $f(X)$ and $g(Y)$ are also independent.
\end{theorem}

\begin{proof}
Using Definition~\ref{def:2.8.5}, we compute that
\begin{align*}
\prb(f(X) \in B_1, g(Y) \in B_2) &= \prb(X \in f^{-1}(B_1), Y \in g^{-1}(B_2)) \\
&= \prb(X \in f^{-1}(B_1)) \, \prb(Y \in g^{-1}(B_2)) \\
&= \prb(f(X) \in B_1) \, \prb(g(Y) \in B_2).
\end{align*}
(Here $f^{-1}(B_1) = \{x \in \mathbf{R}^1 : f(x) \in B_1\}$ and $g^{-1}(B_2) = \{y \in \mathbf{R}^1 : g(y) \in B_2\}$.)

Because this is true for any $B_1$ and $B_2$, we see that $f(X)$ and $g(Y)$ are independent.
\end{proof}

Suppose now that we have $n$ random variables $X_1, \ldots, X_n$. The random variables are \emph{independent} if and only if the collection of events $\{a_i \leqslant X_i \leqslant b_i\}$ are independent, whenever $a_i \leqslant b_i$ for all $i = 1, 2, \ldots, n$. Generalizing Theorem~\ref{thm:2.8.3}, we have the following result.

\begin{theorem}
\label{thm:2.8.6}
Let $X_1, \ldots, X_n$ be a collection of random variables.
\begin{enumerate}[(a)]
\item If $X_1, \ldots, X_n$ are discrete, then $X_1, \ldots, X_n$ are independent if and only if their joint probability function $p_{X_1, \ldots, X_n}$ satisfies
\[
p_{X_1, \ldots, X_n}(x_1, \ldots, x_n) = p_{X_1}(x_1) \cdots p_{X_n}(x_n)
\]
for all $(x_1, \ldots, x_n) \in \mathbf{R}^1 \times \cdots \times \mathbf{R}^1$.
\item If $X_1, \ldots, X_n$ are jointly absolutely continuous, then $X_1, \ldots, X_n$ are independent if and only if their joint density function $f_{X_1, \ldots, X_n}$ can be chosen to satisfy
\[
f_{X_1, \ldots, X_n}(x, y) = f_{X_1}(x_1) \cdots f_{X_n}(x_n)
\]
for all $(x_1, \ldots, x_n) \in \mathbf{R}^1 \times \cdots \times \mathbf{R}^1$.
\end{enumerate}
\end{theorem}

A particularly common case in statistics is the following.

\begin{definition}
\label{def:2.8.6}
A collection $X_1, \ldots, X_n$ of random variables is \emph{independent and identically distributed} (or \emph{i.i.d.}) if the collection is independent and if, furthermore, each of the $n$ variables has the same distribution. The i.i.d.\ sequence $X_1, \ldots, X_n$ is also referred to as a \emph{sample} from the common distribution.
\end{definition}

In particular, if a collection $X_1, \ldots, X_n$ of random variables is i.i.d.\ and discrete, then each of the probability functions $p_{X_i}$ is the same, so that $p_{X_1}(x) = p_{X_2}(x) = \cdots = p_{X_n}(x) = p(x)$ for all $x \in \mathbf{R}^1$. Furthermore, from Theorem~\ref{thm:2.8.6}(a), it follows that
\[
p_{X_1, \ldots, X_n}(x_1, \ldots, x_n) = p_{X_1}(x_1) \, p_{X_2}(x_2) \cdots p_{X_n}(x_n) = p(x_1) \, p(x_2) \cdots p(x_n)
\]
for all $(x_1, \ldots, x_n) \in \mathbf{R}^1 \times \cdots \times \mathbf{R}^1$.

Similarly, if a collection $X_1, \ldots, X_n$ of random variables is i.i.d.\ and jointly absolutely continuous, then each of the density functions $f_{X_i}$ is the same, so that $f_{X_1}(x) = f_{X_2}(x) = \cdots = f_{X_n}(x) = f(x)$ for all $x \in \mathbf{R}^1$. Furthermore, from Theorem~\ref{thm:2.8.6}(b), it follows that
\[
f_{X_1, \ldots, X_n}(x_1, \ldots, x_n) = f_{X_1}(x_1) \, f_{X_2}(x_2) \cdots f_{X_n}(x_n) = f(x_1) \, f(x_2) \cdots f(x_n)
\]
for all $(x_1, \ldots, x_n) \in \mathbf{R}^1 \times \cdots \times \mathbf{R}^1$.

We now consider an important family of discrete distributions that arise via sampling.

\begin{example}[Multinomial Distributions]
\label{ex:2.8.5}
Suppose we have a response $s$ that can take three possible values---for convenience, labelled $1$, $2$, and $3$---with the probability distribution
\[
\prb(s = 1) = \theta_1, \quad \prb(s = 2) = \theta_2, \quad \prb(s = 3) = \theta_3,
\]
so that each $\theta_i \geqslant 0$ and $\theta_1 + \theta_2 + \theta_3 = 1$. As a simple example, consider a bowl of chips of which a proportion $\theta_i$ of the chips are labelled $i$ (for $i = 1, 2, 3$). If we randomly draw a chip from the bowl and observe its label $s$, then $\prb(s = i) = \theta_i$. Alternatively, consider a population of students at a university of which a proportion $\theta_1$ live on campus (denoted by $s = 1$), a proportion $\theta_2$ live off-campus with their parents (denoted by $s = 2$), and a proportion $\theta_3$ live off-campus independently (denoted by $s = 3$). If we randomly draw a student from this population and determine $s$ for that student, then $\prb(s = i) = \theta_i$.

We can also write
\[
\prb(s = i) = \theta_1^{\indc_{\{1\}}(i)} \theta_2^{\indc_{\{2\}}(i)} \theta_3^{\indc_{\{3\}}(i)}
\]
for $i = 1, 2, 3$, where $\indc_{\{j\}}$ is the indicator function for $\{j\}$. Therefore, if $s_1, \ldots, s_n$ is a sample from the distribution on $\{1, 2, 3\}$ given by the $\theta_i$, Theorem~\ref{thm:2.8.6}(a) implies that the joint probability function for the sample equals
\begin{equation}
\label{eq:2.8.2}
\prb(s_1 = k_1, \ldots, s_n = k_n) = \prod_{j=1}^{n} \theta_1^{\indc_{\{1\}}(k_j)} \theta_2^{\indc_{\{2\}}(k_j)} \theta_3^{\indc_{\{3\}}(k_j)} = \theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3},
\end{equation}
where $x_i = \sum_{j=1}^{n} \indc_{\{i\}}(k_j)$ is equal to the number of $i$'s in $(k_1, \ldots, k_n)$.

Now, based on the sample $s_1, \ldots, s_n$, define the random variables
\[
X_i = \sum_{j=1}^{n} \indc_{\{i\}}(s_j)
\]
for $i = 1, 2$, and $3$. Clearly, $X_i$ is the number of $i$'s observed in the sample and we always have $X_i \in \{0, 1, \ldots, n\}$ and $X_1 + X_2 + X_3 = n$. We refer to the $X_i$ as the \emph{counts} formed from the sample.

For $x_1, x_2, x_3$ satisfying $x_i \in \{0, 1, \ldots, n\}$ and $x_1 + x_2 + x_3 = n$, \eqref{eq:2.8.2} implies that the joint probability function for $(X_1, X_2, X_3)$ is given by
\[
p_{(X_1, X_2, X_3)}(x_1, x_2, x_3) = \prb(X_1 = x_1, X_2 = x_2, X_3 = x_3) = C(x_1, x_2, x_3) \, \theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3},
\]
where $C(x_1, x_2, x_3)$ equals the number of samples $(s_1, \ldots, s_n)$ with $x_1$ of its elements equal to $1$, $x_2$ of its elements equal to $2$, and $x_3$ of its elements equal to $3$. To calculate $C(x_1, x_2, x_3)$, we note that there are $\binom{n}{x_1}$ choices for the places of the $1$'s in the sample sequence, $\binom{n - x_1}{x_2}$ choices for the places of the $2$'s in the sequence, and finally $\binom{n - x_1 - x_2}{x_3} = 1$ choices for the places of the $3$'s in the sequence (recall the multinomial coefficient defined in (1.4.4)). Therefore, the probability function for the counts $(X_1, X_2, X_3)$ is equal to
\[
p_{(X_1, X_2, X_3)}(x_1, x_2, x_3) = \binom{n}{x_1} \binom{n - x_1}{x_2} \binom{n - x_1 - x_2}{x_3} \theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3} = \binom{n}{x_1, x_2, x_3} \theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3}.
\]
We say that
\[
(X_1, X_2, X_3) \sim \text{Multinomial}(n, \theta_1, \theta_2, \theta_3).
\]

Notice that the Multinomial$(n, \theta_1, \theta_2, \theta_3)$ generalizes the Binomial$(n, \theta)$ distribution, as we are now counting the number of response values in three possible categories rather than two. Also, it is immediate that
\[
X_i \sim \text{Binomial}(n, \theta_i)
\]
because $X_i$ equals the number of occurrences of $i$ in the $n$ independent response values, and $i$ occurs for an individual response with probability equal to $\theta_i$ (also see Problem~\ref{exer:2.8.18}).

As a simple example, suppose that we have an urn containing $10$ red balls, $20$ white balls, and $30$ black balls. If we randomly draw $10$ balls from the urn with replacement, what is the probability that we will obtain $3$ red, $4$ white, and $3$ black balls? Because we are drawing with replacement, the draws are i.i.d., so the counts are distributed Multinomial$(10, 10/60, 20/60, 30/60)$. The required probability equals
\[
\binom{10}{3, 4, 3} \left(\frac{10}{60}\right)^3 \left(\frac{20}{60}\right)^4 \left(\frac{30}{60}\right)^3 \approx 3.0007 \times 10^{-2}.
\]
Note that if we had drawn without replacement, then the draws would not be i.i.d., the counts would thus not follow a multinomial distribution but rather a generalization of the hypergeometric distribution, as discussed in Problem~2.3.29.

Now suppose we have a response $s$ that takes $k$ possible values---for convenience, labelled $1, 2, \ldots, k$---with the probability distribution given by $\prb(s = i) = \theta_i$. For a sample $s_1, \ldots, s_n$, define the counts $X_i = \sum_{j=1}^{n} \indc_{\{i\}}(s_j)$ for $i = 1, \ldots, k$. Then, arguing as above and recalling the development of (1.4.4), we have
\[
p_{(X_1, \ldots, X_k)}(x_1, \ldots, x_k) = \binom{n}{x_1, \ldots, x_k} \theta_1^{x_1} \cdots \theta_k^{x_k}
\]
whenever each $x_i \in \{0, \ldots, n\}$ and $x_1 + \cdots + x_k = n$. In this case, we write
\[
(X_1, \ldots, X_k) \sim \text{Multinomial}(n, \theta_1, \ldots, \theta_k).
\]
\end{example}

\subsection{Order Statistics}
\label{ssec:2.8.4}

Suppose now that $X_1, \ldots, X_n$ is a sample. In many applications of statistics, we will have $n$ data values where the assumption that these arise as an i.i.d.\ sequence makes sense. It is often of interest, then, to order these from smallest to largest to obtain the \emph{order statistics}
\[
X_{(1)} \leqslant \cdots \leqslant X_{(n)}.
\]
Here, $X_{(i)}$ is equal to the $i$th smallest value in the sample $X_1, \ldots, X_n$. So, for example, if $n = 5$ and
\[
X_1 = 2.3, \quad X_2 = 4.5, \quad X_3 = 1.2, \quad X_4 = 2.2, \quad X_5 = 4.3,
\]
then
\[
X_{(1)} = 1.2, \quad X_{(2)} = 2.2, \quad X_{(3)} = 2.3, \quad X_{(4)} = 4.3, \quad X_{(5)} = 4.5.
\]

Of considerable interest in many situations are the distributions of the order statistics. Consider the following examples.

\begin{example}[Distribution of the Sample Maximum]
\label{ex:2.8.6}
Suppose $X_1, X_2, \ldots, X_n$ are i.i.d.\ so that $F_{X_1}(x) = F_{X_2}(x) = \cdots = F_{X_n}(x)$. Then the largest-order statistic $X_{(n)} = \max(X_1, X_2, \ldots, X_n)$ is the maximum of these $n$ random variables.

Now $X_{(n)}$ is another random variable. What is its cumulative distribution function? We see that $X_{(n)} \leqslant x$ if and only if $X_i \leqslant x$ for all $i$. Hence,
\begin{align*}
F_{X_{(n)}}(x) = \prb(X_{(n)} \leqslant x) &= \prb(X_1 \leqslant x, X_2 \leqslant x, \ldots, X_n \leqslant x) \\
&= \prb(X_1 \leqslant x) \, \prb(X_2 \leqslant x) \cdots \prb(X_n \leqslant x) \\
&= F_{X_1}(x) \, F_{X_2}(x) \cdots F_{X_n}(x) = [F_{X_1}(x)]^n.
\end{align*}
If $F_{X_1}$ corresponds to an absolutely continuous distribution, then we can differentiate this expression to obtain the density of $X_{(n)}$.
\end{example}

\begin{example}
\label{ex:2.8.7}
As a special case of Example~\ref{ex:2.8.6}, suppose that $X_1, X_2, \ldots, X_n$ are identically and independently distributed Uniform$[0, 1]$. From the above, for $0 \leqslant x \leqslant 1$, we have $F_{X_{(n)}}(x) = [F_{X_1}(x)]^n = x^n$. It then follows from Corollary~\ref{cor:2.5.1} that the density $f_{X_{(n)}}$ of $X_{(n)}$ equals $f_{X_{(n)}}(x) = F_{X_{(n)}}'(x) = nx^{n-1}$ for $0 \leqslant x \leqslant 1$, with (of course) $f_{X_{(n)}}(x) = 0$ for $x < 0$ and $x > 1$. Note that, from Problem~\ref{exer:2.4.24}, we can write $X_{(n)} \sim \text{Beta}(n, 1)$.
\end{example}

\begin{example}[Distribution of the Sample Minimum]
\label{ex:2.8.8}
Following Example~\ref{ex:2.8.6}, we can also obtain the distribution function of the sample minimum, or smallest-order statistic, $X_{(1)} = \min(X_1, X_2, \ldots, X_n)$. We have
\begin{align*}
F_{X_{(1)}}(x) = \prb(X_{(1)} \leqslant x) &= 1 - \prb(X_{(1)} > x) \\
&= 1 - \prb(X_1 > x, X_2 > x, \ldots, X_n > x) \\
&= 1 - \prb(X_1 > x) \, \prb(X_2 > x) \cdots \prb(X_n > x) \\
&= 1 - [1 - F_{X_1}(x)][1 - F_{X_2}(x)] \cdots [1 - F_{X_n}(x)] \\
&= 1 - [1 - F_{X_1}(x)]^n.
\end{align*}
Again, if $F_{X_1}$ corresponds to an absolutely continuous distribution, we can differentiate this expression to obtain the density of $X_{(1)}$.
\end{example}

\begin{example}
\label{ex:2.8.9}
Let $X_1, \ldots, X_n$ be i.i.d.\ Uniform$[0, 1]$. Hence, for $0 \leqslant x \leqslant 1$,
\[
F_{X_{(1)}}(x) = \prb(X_{(1)} \leqslant x) = 1 - \prb(X_{(1)} > x) = 1 - (1 - x)^n.
\]
It then follows from Corollary~\ref{cor:2.5.1} that the density $f_{X_{(1)}}$ of $X_{(1)}$ satisfies $f_{X_{(1)}}(x) = F_{X_{(1)}}'(x) = n(1 - x)^{n-1}$ for $0 \leqslant x \leqslant 1$, with (of course) $f_{X_{(1)}}(x) = 0$ for $x < 0$ and $x > 1$. Note that, from Problem~\ref{exer:2.4.24}, we can write $X_{(1)} \sim \text{Beta}(1, n)$.
\end{example}

The sample median and sample quartiles are defined in terms of order statistics and used in statistical applications. These quantities, and their uses, are discussed in Section~\ref{sec:5.5}.

\subsection*{Summary of Section~\ref{sec:2.8}}

\begin{itemize}
\item If $X$ and $Y$ are discrete, then the conditional probability function of $Y$ given $X$ equals $p_{Y|X}(y \mid x) = p_{X,Y}(x, y)/p_X(x)$.
\item If $X$ and $Y$ are absolutely continuous, then the conditional density function of $Y$ given $X$ equals $f_{Y|X}(y \mid x) = f_{X,Y}(x, y)/f_X(x)$.
\item $X$ and $Y$ are independent if $\prb(X \in B_1, Y \in B_2) = \prb(X \in B_1) \, \prb(Y \in B_2)$ for all $B_1, B_2 \subseteq \mathbf{R}^1$.
\item Discrete $X$ and $Y$ are independent if and only if $p_{X,Y}(x, y) = p_X(x) \, p_Y(y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$, or, equivalently, $p_{Y|X}(y \mid x) = p_Y(y)$.
\item Absolutely continuous $X$ and $Y$ are independent if and only if $f_{X,Y}(x, y) = f_X(x) \, f_Y(y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$, or, equivalently, $f_{Y|X}(y \mid x) = f_Y(y)$.
\item A sequence $X_1, X_2, \ldots, X_n$ is i.i.d.\ if the random variables are independent, and each $X_i$ has the same distribution.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.8.1}
Suppose $X$ and $Y$ have joint probability function
\[
p_{X,Y}(x, y) = \begin{cases}
1/6 & x = 2, \, y = 3 \\
1/12 & x = 2, \, y = 5 \\
1/6 & x = 9, \, y = 3 \\
1/12 & x = 9, \, y = 5 \\
1/3 & x = 13, \, y = 3 \\
1/6 & x = 13, \, y = 5 \\
0 & \text{otherwise.}
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $p_X(x)$ for all $x \in \mathbf{R}^1$.
\item Compute $p_Y(y)$ for all $y \in \mathbf{R}^1$.
\item Determine whether or not $X$ and $Y$ are independent.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $p_X(-2) = p_{X,Y}(-2, 3) + p_{X,Y}(-2, 5) = 1/6 + 1/12 = 1/4$. $p_X(9) = p_{X,Y}(9, 3) + p_{X,Y}(9, 5) = 1/6 + 1/12 = 1/4$. $p_X(13) = p_{X,Y}(13, 3) + p_{X,Y}(13, 5) = 1/3 + 1/6 = 1/2$. Otherwise, $p_X(x) = 0$.
    \item $p_Y(3) = p_{X,Y}(-2, 3) + p_{X,Y}(9, 3) + p_{X,Y}(13, 3) = 1/6 + 1/6 + 1/3 = 2/3$. $p_Y(5) = p_{X,Y}(-2, 5) + p_{X,Y}(9, 5) + p_{X,Y}(13, 5) = 1/12 + 1/12 + 1/6 = 1/3$. Otherwise, $p_Y(y) = 0$.
    \item Yes, since $p_X(x)p_Y(y) = p_{X,Y}(x, y)$ for all $x$ and $y$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.2}
Suppose $X$ and $Y$ have joint probability function
\[
p_{X,Y}(x, y) = \begin{cases}
1/16 & x = 2, \, y = 3 \\
1/4 & x = 2, \, y = 5 \\
1/2 & x = 9, \, y = 3 \\
1/16 & x = 9, \, y = 5 \\
1/16 & x = 13, \, y = 3 \\
1/16 & x = 13, \, y = 5 \\
0 & \text{otherwise.}
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $p_X(x)$ for all $x \in \mathbf{R}^1$.
\item Compute $p_Y(y)$ for all $y \in \mathbf{R}^1$.
\item Determine whether or not $X$ and $Y$ are independent.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $p_X(-2) = p_{X,Y}(-2, 3) + p_{X,Y}(-2, 5) = 1/16 + 1/4 = 5/16$. $p_X(9) = p_{X,Y}(9, 3) + p_{X,Y}(9, 5) = 1/2 + 1/16 = 9/16$. $p_X(13) = p_{X,Y}(13, 3) + p_{X,Y}(13, 5) = 1/16 + 1/16 = 1/8$. Otherwise, $p_X(x) = 0$.
    \item $p_Y(3) = p_{X,Y}(-2, 3) + p_{X,Y}(9, 3) + p_{X,Y}(13, 3) = 1/16 + 1/2 + 1/16 = 5/8$. $p_Y(5) = p_{X,Y}(-2, 5) + p_{X,Y}(9, 5) + p_{X,Y}(13, 5) = 1/4 + 1/16 + 1/16 = 3/8$. Otherwise, $p_Y(y) = 0$.
    \item No, since, e.g., $p_X(-2)p_Y(3) \neq p_{X,Y}(-2, 3)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.3}
Suppose $X$ and $Y$ have joint density function
\[
f_{X,Y}(x, y) = \begin{cases}
\frac{12}{49}(2 + x + xy + 4y^2) & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $f_X(x)$ for all $x \in \mathbf{R}^1$.
\item Compute $f_Y(y)$ for all $y \in \mathbf{R}^1$.
\item Determine whether or not $X$ and $Y$ are independent.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item For $0 \leqslant x \leqslant 1$, $f_X(x) = \int_0^1(12/49)(2 + x + xy + 4y^2)\,\mathrm{d}y = (18x/49) + (40/49)$, otherwise $f_X(x) = 0$.
    \item For $0 \leqslant y \leqslant 1$, $f_Y(y) = \int_0^1(12/49)(2 + x + xy + 4y^2)\,\mathrm{d}x = (48y^2 + 6y + 30)/49$, otherwise $f_Y(y) = 0$.
    \item No, since $f_X(x)f_Y(y) \neq f_{X,Y}(x, y)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.4}
Suppose $X$ and $Y$ have joint density function
\[
f_{X,Y}(x, y) = \begin{cases}
\frac{2}{5}(2 - e^{-3} - e^{-x} - 3ye^{-y} + ye^{-x} + ye^{-x-y}) & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $f_X(x)$ for all $x \in \mathbf{R}^1$.
\item Compute $f_Y(y)$ for all $y \in \mathbf{R}^1$.
\item Determine whether or not $X$ and $Y$ are independent.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item For $0 \leqslant x \leqslant 1$, $f_X(x) = \int_0^1(2/5(2+e))(3 + e^x + 3y + 3ye^y + ye^x + ye^{x+y})\,\mathrm{d}y = (3 + e^x)/(2 + e)$, otherwise $f_X(x) = 0$.
    \item For $0 \leqslant y \leqslant 1$, $f_Y(y) = \int_0^1(2/5(2+e))(3 + e^x + 3y + 3ye^y + ye^x + ye^{x+y})\,\mathrm{d}x = 2(1 + y + ye^y)/5$, otherwise $f_Y(y) = 0$.
    \item Yes, since $f_X(x)f_Y(y) = f_{X,Y}(x, y)$ for all $x$ and $y$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.5}
Suppose $X$ and $Y$ have joint probability function
\[
p_{X,Y}(x, y) = \begin{cases}
1/9 & x = 4, \, y = 2 \\
2/9 & x = 5, \, y = 2 \\
3/9 & x = 9, \, y = 2 \\
2/9 & x = 9, \, y = 0 \\
1/9 & x = 9, \, y = -4 \\
0 & \text{otherwise.}
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $\prb(Y = 4 \mid X = 9)$.
\item Compute $\prb(Y = 2 \mid X = 9)$.
\item Compute $\prb(Y = 0 \mid X = 4)$.
\item Compute $\prb(Y = 2 \mid X = 5)$.
\item Compute $\prb(X = 5 \mid Y = 2)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\prb(Y = 4 \mid X = 9) = \prb(X = 9, Y = 4)/\prb(X = 9) = (1/9)/(3/9 + 2/9 + 1/9) = 1/6$.
    \item $\prb(Y = -2 \mid X = 9) = \prb(X = 9, Y = -2)/\prb(X = 9) = (3/9)/(3/9 + 2/9 + 1/9) = 1/2$.
    \item $\prb(Y = 0 \mid X = -4) = \prb(X = -4, Y = 0)/\prb(X = -4) = 0/(1/9) = 0$.
    \item $\prb(Y = -2 \mid X = 5) = \prb(X = 5, Y = -2)/\prb(X = 5) = (2/9)/(2/9) = 1$.
    \item $\prb(X = 5 \mid Y = -2) = \prb(X = 5, Y = -2)/\prb(Y = -2) = (2/9)/(1/9 + 2/9 + 3/9) = 1/3$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.6}
Let $X \sim \text{Bernoulli}(\theta)$ and $Y \sim \text{Geometric}(\theta)$, with $X$ and $Y$ independent. Let $Z = X + Y$. What is the probability function of $Z$?
\end{exercise}

\begin{solution}
$\prb(Z = 0) = \prb(X = 0, Y = 0) = (1-p)p$. For $z$, a positive integer, $\prb(Z = z) = \prb(X = 0, Y = z) + \prb(X = 1, Y = z - 1) = (1-p)(1-p)^z p + p(1-p)^{z-1}p = (1 - p)^{z-1}[(1 - p)^2 p + p^2] = p(1 - p)^{z-1}[1 - p + p^2]$.
\end{solution}

\begin{exercise}
\label{exer:2.8.7}
For each of the following joint density functions $f_{X,Y}$ (taken from Exercise~\ref{exer:2.7.4}), compute the conditional density $f_{Y|X}(y \mid x)$, and determine whether or not $X$ and $Y$ are independent.
\begin{enumerate}[(a)]
\item $f_{X,Y}(x, y) = \begin{cases} 2x^2 y + Cy^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\ 0 & \text{otherwise.} \end{cases}$
\item $f_{X,Y}(x, y) = \begin{cases} C(xy + x^5 y^5) & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\ 0 & \text{otherwise.} \end{cases}$
\item $f_{X,Y}(x, y) = \begin{cases} C(xy + x^5 y^5) & 0 \leqslant x \leqslant 4, \, 0 \leqslant y \leqslant 10 \\ 0 & \text{otherwise.} \end{cases}$
\item $f_{X,Y}(x, y) = \begin{cases} Cx^5 y^5 & 0 \leqslant x \leqslant 4, \, 0 \leqslant y \leqslant 10 \\ 0 & \text{otherwise.} \end{cases}$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Recall $C = 4$. Hence, $f_X(x) = \int_0^1(2x^2 y + 4y^5)\,\mathrm{d}y = x^2 + 2/3$ and $f_Y(y) = \int_0^1(2x^2 y + 4y^5)\,\mathrm{d}x = 4y^5 + 2y/3$. Then for $0 \leqslant x \leqslant 1$ and $0 \leqslant y \leqslant 1$, $f_{Y|X}(y \mid x) = f_{X,Y}(x, y)/f_X(x) = (2x^2 y + 4y^5)/(x^2 + 2/3)$ (otherwise $f_{Y|X}(y \mid x) = 0$). Thus, $X$ and $Y$ are not independent since $f_{Y|X}(y \mid x) \neq f_Y(y)$.
    \item Here $f_X(x) = \int_0^1 C(xy + x^5 y^5)\,\mathrm{d}y = C(x^5/6 + x/2)$ and $f_Y(y) = \int_0^1 C(xy + x^5 y^5)\,\mathrm{d}x = C(y^5/6 + y/2)$. Then for $0 \leqslant x \leqslant 1$ and $0 \leqslant y \leqslant 1$, $f_{Y|X}(y \mid x) = f_{X,Y}(x, y)/f_X(x) = C(xy + x^5 y^5)/C(x^5/6 + x/2) = (xy + x^5 y^5)/(x^5/6 + x/2)$ (otherwise $f_{Y|X}(y \mid x) = 0$). Thus, $X$ and $Y$ are not independent since $f_{Y|X}(y \mid x) \neq f_Y(y)$.
    \item Here $f_X(x) = \int_0^{10}C(xy + x^5 y^5)\,\mathrm{d}y = C(50000x^5/3 + 50x)$ and $f_Y(y) = \int_0^4 C(xy + x^5 y^5)\,\mathrm{d}x = C(2048y^5/3 + 8y)$. Then for $0 \leqslant x \leqslant 4$ and $0 \leqslant y \leqslant 10$, $f_{Y|X}(y \mid x) = f_{X,Y}(x, y)/f_X(x) = C(xy + x^5 y^5)/C(50000x^5/3 + 50x) = (xy + x^5 y^5)/(50000x^5/3 + 50x)$ (otherwise $f_{Y|X}(y \mid x) = 0$). Thus, $X$ and $Y$ are not independent since $f_{Y|X}(y \mid x) \neq f_Y(y)$.
    \item Here $f_X(x) = \int_0^{10}C(x^5 y^5)\,\mathrm{d}y = C(50000x^5/3)$ and $f_Y(y) = \int_0^4 C(xy + x^5 y^5)\,\mathrm{d}x = C(2048y^5/3)$. Then for $0 \leqslant x \leqslant 4$ and $0 \leqslant y \leqslant 10$, $f_{Y|X}(y \mid x) = f_{X,Y}(x, y)/f_X(x) = C(x^5 y^5)/C(50000x^5/3) = 3y^5/50000$ (otherwise $f_{Y|X}(y \mid x) = 0$). Here $X$ and $Y$ are independent since $f_{Y|X}(y \mid x) = f_Y(y)$ for all $x$ and $y$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.8}
Let $X$ and $Y$ be jointly absolutely continuous random variables. Suppose $X \sim \text{Exponential}(2)$ and that $\prb(Y > 5 \mid X = x) = e^{-3x}$. Compute $\prb(Y > 5)$.
\end{exercise}

\begin{solution}
We have that $e^{-3x} = \prb(Y > 5 \mid X = x) = \int_5^{\infty}f_{Y|X}(y \mid x)\,\mathrm{d}y$. Hence,
\[
    \prb(Y > 5) = \prb(Y > 5, X > 0) = \int_0^{\infty}\int_5^{\infty}f_X(x)f_{Y|X}(y \mid x)\,\mathrm{d}y\,\mathrm{d}x = \int_0^{\infty}2e^{-2x}e^{-3x}\,\mathrm{d}x = -\frac{2}{5}e^{-5x}\bigg|_{x=0}^{x=\infty} = \frac{2}{5}.
\]
\end{solution}

\begin{exercise}
\label{exer:2.8.9}
Give an example of two random variables $X$ and $Y$, each taking values in the set $\{1, 2, 3\}$, such that $\prb(X = 1, Y = 1) = \prb(X = 1) \, \prb(Y = 1)$, but $X$ and $Y$ are not independent.
\end{exercise}

\begin{solution}
For example, suppose $\prb(X = 1, Y = 1) = \prb(X = 1, Y = 2) = \prb(X = 2, Y = 1) = \prb(X = 3, Y = 3) = 1/4$. Then $\prb(X = 1) = \prb(Y = 1) = 1/2$, so $\prb(X = 1)\prb(Y = 1) = 1/4 = \prb(X = 1, Y = 1)$. On the other hand, $\prb(X = 3)\prb(Y = 3) = (1/4)(1/4) \neq 1/4 = \prb(X = 3, Y = 3)$, so $X$ and $Y$ are not independent.
\end{solution}

\begin{exercise}
\label{exer:2.8.10}
Let $X \sim \text{Bernoulli}(\theta)$ and $Y \sim \text{Bernoulli}(\rho)$, where $0 < \theta < 1$ and $0 < \rho < 1$. Suppose $\prb(X = 1, Y = 1) = \prb(X = 1) \, \prb(Y = 1)$. Prove that $X$ and $Y$ must be independent.
\end{exercise}

\begin{solution}
Here $\prb(X = 1, Y = 0) = \prb(X = 1) - \prb(X = 1, Y = 1) = \prb(X = 1) - \prb(X = 1)\prb(Y = 1) = \prb(X = 1)(1 - \prb(Y = 1)) = \prb(X = 1)\prb(Y = 0)$. Similarly, $\prb(X = 0, Y = 1) = \prb(Y = 1) - \prb(X = 1, Y = 1) = \prb(Y = 1)\prb(X = 0)$. Finally, $\prb(X = 0, Y = 0) = \prb(X = 0) - \prb(X = 0, Y = 1) = \prb(X = 0) - \prb(X = 0)\prb(Y = 1) = \prb(X = 0)(1 - \prb(Y = 1)) = \prb(X = 0)\prb(Y = 0)$. Hence, $\prb(X = x, Y = y) = \prb(X = x)\prb(Y = y)$ for all $x$ and $y$, so $X$ and $Y$ are independent.
\end{solution}

\begin{exercise}
\label{exer:2.8.11}
Suppose that $X$ is a constant random variable and that $Y$ is any random variable. Prove that $X$ and $Y$ must be independent.
\end{exercise}

\begin{solution}
If $X = C$ is constant, then $\prb(X \in B_1) = \indc_{B_1}(C)$ and $\prb(X \in B_1, Y \in B_2) = \indc_{B_1}(C)\prb(Y \in B_2)$. Hence, $\prb(X \in B_1, Y \in B_2) = \prb(X \in B_1)\prb(Y \in B_2) = \indc_{B_1}(C)\prb(Y \in B_2)$ for any subsets $B_1$ and $B_2$, so $X$ and $Y$ are independent.
\end{solution}

\begin{exercise}
\label{exer:2.8.12}
Suppose $X \sim \text{Bernoulli}(1/3)$ and $Y \sim \text{Poisson}(\lambda)$, with $X$ and $Y$ independent and with $\lambda > 0$. Compute $\prb(X + 1 > Y/5)$.
\end{exercise}

\begin{solution}
Since $X$ and $Y$ are independent, $\prb(X = 1 \mid Y = 5) = \prb(X = 1) = 1/3$.
\end{solution}

\begin{exercise}
\label{exer:2.8.13}
Suppose $\prb(X = x, Y = y) = 1/8$ for $x \in \{3, 5\}$ and $y \in \{1, 2, 4, 7\}$, otherwise $\prb(X = x, Y = y) = 0$.
\begin{enumerate}[(a)]
\item Compute the conditional probability function $p_{Y|X}(y \mid x)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$ with $p_X(x) > 0$.
\item Compute the conditional probability function $p_{X|Y}(x \mid y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$ with $p_Y(y) > 0$.
\item Are $X$ and $Y$ independent? Why or why not?
\end{enumerate}
\end{exercise}

\begin{solution}
In Exercise \ref{exer:2.7.6}, we show that $p_X(x) = 1/2$ for $x = 3$ or $x = 5$ and $p_X(x) = 0$ otherwise. Also $p_Y(y) = 1/4$ for $y = 1, 2, 4, 7$ and otherwise $p_Y(y) = 0$.
\begin{enumerate}[(a)]
    \item By definition, $p_{Y|X}(y|x) = p_{X,Y}(x, y)/p_X(x)$. Hence, we have the next conditional probability table.
    \begin{center}
    \begin{tabular}{c|ccccc}
        $p_{Y|X}(y|x)$ & $y = 1$ & $y = 2$ & $y = 4$ & $y = 7$ & others \\
        \hline
        $x = 3$ & $1/4$ & $1/4$ & $1/4$ & $1/4$ & 0 \\
        $x = 5$ & $1/4$ & $1/4$ & $1/4$ & $1/4$ & 0
    \end{tabular}
    \end{center}
    \item By definition, $p_{X|Y}(x|y) = p_{X,Y}(x, y)/p_Y(y)$. $p_{X|Y}(3|1) = p_{X,Y}(3, 1)/p_Y(1) = (1/8)/(1/4) = 1/2$. Similar calculation gives the next conditional probability table.
    \begin{center}
    \begin{tabular}{c|ccc}
        $p_{X|Y}(x|y)$ & $x = 3$ & $x = 5$ & others \\
        \hline
        $y = 1$ & $1/2$ & $1/2$ & 0 \\
        $y = 2$ & $1/2$ & $1/2$ & 0 \\
        $y = 4$ & $1/2$ & $1/2$ & 0 \\
        $y = 7$ & $1/2$ & $1/2$ & 0
    \end{tabular}
    \end{center}
  \item Note that $p_{Y|X}(y|x) = 1/4 = p_Y(y)$ for all $x = 3, 5$ and $y = 1, 2, 4, 7$. By Theorem \ref{thm:2.8.4}(a), $X$ and $Y$ are independent.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.14}
Let $X$ and $Y$ have joint density $f_{X,Y}(x, y) = (x^2 + y)/36$ for $-2 \leqslant x \leqslant 1$ and $0 \leqslant y \leqslant 4$, otherwise $f_{X,Y}(x, y) = 0$.
\begin{enumerate}[(a)]
\item Compute the conditional density $f_{Y|X}(y \mid x)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$ with $f_X(x) > 0$.
\item Compute the conditional density $f_{X|Y}(x \mid y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$ with $f_Y(y) > 0$.
\item Are $X$ and $Y$ independent? Why or why not?
\end{enumerate}
\end{exercise}

\begin{solution}
In Exercise \ref{exer:2.7.8}, we already showed that $f_X(x) = (x^2 + 2)/4$ for $-2 < x < 1$ and otherwise $f_X(x) = 0$. Also we showed that $f_Y(y) = (1 + y)/12$ for $0 < y < 4$, otherwise $f_Y(y) = 0$.
\begin{enumerate}[(a)]
    \item Since $f_X(x) > 0$ for $-2 < x < 1$, the conditional density is $f_{Y|X}(y|x) = (x^2 + y)/36/[(x^2 + 2)/9] = (x^2 + y)/(4x^2 + 8)$ for $0 < y < 4$, otherwise $f_{Y|X}(y|x) = 0$.
    \item Since $f_Y(y) > 0$ for $0 < y < 4$, the conditional density is $f_{X|Y}(x|y) = (x^2 + y)/36/[(1+y)/12] = (x^2 + y)/(3y + 3)$ for $-2 < x < 1$, otherwise $f_{X|Y}(x|y) = 0$.
    \item We compare $f_{Y|X}(y|x)$ and $f_Y(y)$. Note that $f_{Y|X}(y|x) = (x^2 + y)/(4x^2 + 8) \neq (1 + y)/12 = f_Y(y)$ for $-2 < x < 1$, $0 < y < 4$ except $x = -1$, $y = 2$. Hence, $X$ and $Y$ are not independent.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.15}
Let $X$ and $Y$ have joint density $f_{X,Y}(x, y) = (x^2 + y)/4$ for $0 \leqslant x \leqslant y \leqslant 2$, otherwise $f_{X,Y}(x, y) = 0$. Compute each of the following.
\begin{enumerate}[(a)]
\item The conditional density $f_{Y|X}(y \mid x)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$ with $f_X(x) > 0$.
\item The conditional density $f_{X|Y}(x \mid y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$ with $f_Y(y) > 0$.
\item Are $X$ and $Y$ independent? Why or why not?
\end{enumerate}
\end{exercise}

\begin{solution}
In Exercise \ref{exer:2.7.9}, we already showed that $f_X(x) = (4 + 3x^2 - 2x^3)/8$ for $0 < x < 2$ and otherwise $f_X(x) = 0$ as well as $f_Y(y) = (y^3 + 3y^2)/12$ for $0 < y < 2$, otherwise $f_Y(y) = 0$.
\begin{enumerate}[(a)]
    \item Since $f_X(x) > 0$ only for $0 < x < 2$, the conditional density is $f_{Y|X}(y|x) = (x^2 + y)/4/[(4 + 3x^2 - 2x^3)/8] = 2(x^2 + y)/(4 + 3x^2 - 2x^3)$ for $x < y < 2$, otherwise $f_{Y|X}(y|x) = 0$.
    \item Since $f_Y(y) > 0$ for $0 < y < 2$, the conditional density is $f_{X|Y}(x|y) = (x^2 + y)/4/[(y^3 + 3y^2)/12] = 3(x^2 + y)/(y^3 + 3y^2)$ for $0 < x < y$, otherwise $f_{X|Y}(x|y) = 0$.
    \item We compare $f_{Y|X}(y|x)$ and $f_Y(y)$. Note that $f_{Y|X}(y|x) = 2(x^2 + y)/(4 + 3x^2 - 2x^3) = (y^3 + 3y^2)/12 = f_Y(y)$ holds only on a curve amongst $0 < x < y < 2$. Hence, $X$ and $Y$ are not independent.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.8.16}
Suppose we obtain the following sample of size $n = 6$: $X_1 = 12$, $X_2 = 8$, $X_3 = X_4 = 9$, $X_5 = 7$, and $X_6 = 11$. Specify the order statistics $X_{(i)}$ for $1 \leqslant i \leqslant 6$.
\end{exercise}

\begin{solution}
The observed data $12, 8, 9, 9, 7, 11$ is sorted as $7, 8, 9, 9, 11, 12$. Hence, $X_{(1)} = 7$, $X_{(2)} = 8$, $X_{(3)} = 9$, $X_{(4)} = 9$, $X_{(5)} = 11$, and $X_{(6)} = 12$.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.8.17}
Let $X$ and $Y$ be jointly absolutely continuous random variables, having joint density of the form
\[
f_{X,Y}(x, y) = \begin{cases}
C_1(2x^2 + y) + C_2 y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]
Determine values of $C_1$ and $C_2$, such that $f_{X,Y}$ is a valid joint density function, and $X$ and $Y$ are independent.
\end{exercise}

\begin{solution}
We compute that $f_X(x) = C_1(x^2 + C_2/6)$ and $f_Y(y) = C_1(C_2 y^5 + 2y/3)$, with $\int_0^1\int_0^1 f_{X,Y}(x, y)\,\mathrm{d}x\,\mathrm{d}y = C_1(C_2/6 + 1/3)$. So, we require that $C_1(C_2/6 + 1/3) = 1$ and that $C_1(x^2 + C_2/6)C_1(C_2 y^5 + 2y/3) = C_1(2x^2 y + C_2 y^5)$ for $0 \leqslant x \leqslant 1$ and $0 \leqslant y \leqslant 1$. The second condition requires that $C_2 = 0$, while the first requires that $C_1 = 3$, which gives the solution.
\end{solution}

\begin{exercise}
\label{exer:2.8.18}
Let $X$ and $Y$ be discrete random variables. Suppose $p_{X,Y}(x, y) = g(x) \, h(y)$, for some functions $g$ and $h$. Prove that $X$ and $Y$ are independent. (Hint: Use Theorem~\ref{thm:2.8.3}(a) and Theorem~\ref{thm:2.7.4}.)
\end{exercise}

\begin{solution}
Let $C_1 = \sum_x g(x)$, and $C_2 = \sum_y h(y)$. Then $\sum_{x,y}p_{X,Y}(x, y) = C_1 C_2 = 1$. Also, $p_X(x) = \sum_y p_{X,Y}(x, y) = g(x)\sum_y h(y) = g(x)C_2$ and $p_Y(y) = \sum_x p_{X,Y}(x, y) = h(y)\sum_x g(x) = h(y)C_1$. Hence, $p_X(x)p_Y(y) = g(x)C_2 h(y)C_1 = (C_1 C_2)g(x)h(y) = g(x)h(y) = p_{X,Y}(x, y)$, so $X$ and $Y$ are independent.
\end{solution}

\begin{exercise}
\label{exer:2.8.19}
Let $X$ and $Y$ be jointly absolutely continuous random variables. Suppose $f_{X,Y}(x, y) = g(x) \, h(y)$, for some functions $g$ and $h$. Prove that $X$ and $Y$ are independent. (Hint: Use Theorem~\ref{thm:2.8.3}(b) and Theorem~\ref{thm:2.7.5}.)
\end{exercise}

\begin{solution}
Let $C_1 = \int_{-\infty}^{\infty}g(x)$, and $C_2 = \int_{-\infty}^{\infty}h(y)$. Then $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X,Y}(x, y) = C_1 C_2 = 1$. Also, $f_X(x) = \int_{-\infty}^{\infty}f_{X,Y}(x, y)\,\mathrm{d}y = g(x)\int_{-\infty}^{\infty}h(y)\,\mathrm{d}y = g(x)C_2$ and $f_Y(y) = \int_{-\infty}^{\infty}f_{X,Y}(x, y)\,\mathrm{d}x = h(y)\int_{-\infty}^{\infty}g(x)\,\mathrm{d}x = h(y)C_1$. Hence, $f_X(x)f_Y(y) = g(x)C_2 h(y)C_1 = (C_1 C_2)g(x)h(y) = g(x)h(y) = f_{X,Y}(x, y)$, so $X$ and $Y$ are independent.
\end{solution}

\begin{exercise}
\label{exer:2.8.20}
Let $X$ and $Y$ be discrete random variables, with $\prb(X = 1) > 0$ and $\prb(X = 2) > 0$. Suppose $\prb(Y = 1 \mid X = 1) = 3/4$ and $\prb(Y = 2 \mid X = 2) = 3/4$. Prove that $X$ and $Y$ cannot be independent.
\end{exercise}

\begin{solution}
If $X$ and $Y$ were independent, then we would have $\prb(Y = 1) = \prb(Y = 1 \mid X = 1) = 3/4$, and $\prb(Y = 2) = \prb(Y = 2 \mid X = 2) = 3/4$. This is impossible since we must always have $\prb(Y = 1) + \prb(Y = 2) \leqslant 1$.
\end{solution}

\begin{exercise}
\label{exer:2.8.21}
Let $X$ and $Y$ have the bivariate normal distribution, as in Example~\ref{ex:2.7.9}. Prove that $X$ and $Y$ are independent if and only if $\rho = 0$.
\end{exercise}

\begin{solution}
We have from Problem \ref{exer:2.7.13} that $f_X(x) = (\sigma_1\sqrt{2\pi})^{-1}e^{-(x-\mu_1)^2/2\sigma_1}$ and, similarly, $f_Y(y) = (\sigma_2\sqrt{2\pi})^{-1}e^{-(y-\mu_2)^2/2\sigma_2}$. Multiplying these together, we see that they are equal to the expression for $f_{X,Y}(x, y)$, except with $\rho = 0$. Hence, $f_X(x)f_Y(y) = f_{X,Y}(x, y)$ if and only if $\rho = 0$.
\end{solution}

\begin{exercise}
\label{exer:2.8.22}
Suppose that $(X_1, X_2, X_3) \sim \text{Multinomial}(n, \theta_1, \theta_2, \theta_3)$. Prove, by summing the joint probability function, that $X_1 \sim \text{Binomial}(n, \theta_1)$.
\end{exercise}

\begin{solution}
We have that $\prb(X_1 = f_1) = \sum_{f_2=0}^{n-f_1}\binom{n}{f_1\ f_2\ n-f_1-f_2}\theta_1^{f_1}\theta_2^{f_2}\theta_3^{n-f_1-f_2} = \sum_{f_2=0}^{n-f_1}\binom{n}{f_1}\binom{n-f_1}{f_2}\theta_1^{f_1}\theta_2^{f_2}(1 - \theta_1 - \theta_2)^{n-f_1-f_2} = \binom{n}{f_1}\theta_1^{f_1}(1 - \theta_1)^{n-f_1} \times \sum_{f_2=0}^{n-f_1}\binom{n-f_1}{f_2}\left(\frac{\theta_2}{1-\theta_1}\right)^{f_2}\left(1 - \frac{\theta_2}{1-\theta_1}\right)^{n-f_1-f_2} = \binom{n}{f_1}\theta_1^{f_1}(1 - \theta_1)^{n-f_1} \times \left(\frac{\theta_2}{1-\theta_1} + 1 - \frac{\theta_2}{1-\theta_1}\right)^{n-f_1} = \binom{n}{f_1}\theta_1^{f_1}(1 - \theta_1)^{n-f_1}$, so $X_1 \sim \text{Binomial}(n, \theta_1)$.
\end{solution}

\begin{exercise}
\label{exer:2.8.23}
Suppose that $(X_1, X_2, X_3) \sim \text{Multinomial}(n, \theta_1, \theta_2, \theta_3)$. Find the conditional distribution of $X_2$ given that $X_1 = x_1$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \prb(X_2 = f_2 \mid X_1 = f_1) &= \binom{n}{f_1\ f_2\ n - f_1 - f_2}\theta_1^{f_1}\theta_2^{f_2}\theta_3^{n-f_1-f_2}/\binom{n}{f_1}\theta_1^{f_1}(1 - \theta_1)^{n-f_1} \\
    &= \binom{n - f_1}{f_2}\left(\frac{\theta_2}{1 - \theta_1}\right)^{f_2}\left(1 - \frac{\theta_2}{1 - \theta_1}\right)^{n-f_1-f_2},
\end{align*}
so $X_2 \mid X_1 = f_1 \sim \text{Binomial}(n - f_1, \theta_2/(1 - \theta_1))$.
\end{solution}

\begin{exercise}
\label{exer:2.8.24}
Suppose that $X_1, \ldots, X_n$ is a sample from the Exponential$(\lambda)$ distribution. Find the densities $f_{X_{(1)}}$ and $f_{X_{(n)}}$.
\end{exercise}

\begin{solution}
The cdf of the $\text{Exponential}(\lambda)$ is given by $F(x) = 1 - e^{-\lambda x}$ for $x > 0$ and is 0 otherwise. Therefore for $X > 0$, $\prb(X_{(n)} \leqslant x) = (1 - e^{-\lambda x})^n$, so $f_{X_{(n)}}(x) = \frac{d}{\mathrm{d}x}(1 - e^{-\lambda x})^n = n\lambda(1 - e^{-\lambda x})^{n-1}$. Also, $\prb(X_{(1)} \leqslant x) = 1 - e^{-n\lambda x}$, so $f_{X_{(1)}}(x) = \frac{d}{\mathrm{d}x}(1 - e^{-n\lambda x}) = n\lambda(1 - e^{-\lambda x})^{n-1}$.
\end{solution}

\begin{exercise}
\label{exer:2.8.25}
Suppose that $X_1, \ldots, X_n$ is a sample from a distribution with cdf $F$. Prove that
\[
F_{X_{(i)}}(x) = \sum_{j=i}^{n} \binom{n}{j} [F(x)]^j [1 - F(x)]^{n-j}.
\]
(Hint: Note that $X_{(i)} \leqslant x$ if and only if at least $i$ of $X_1, \ldots, X_n$ are less than or equal to $x$.)
\end{exercise}

\begin{solution}
We have that $\prb(X_{(i)} \leqslant x) = \prb(\text{at least } i \text{ sample values are } \leqslant x) = \sum_{j=i}^{n}\prb(\text{exactly } j \text{ sample values are } \leqslant x) = \sum_{j=i}^{n}\binom{n}{j}F^j(x)(1 - F(x))^{n-j}$.
\end{solution}

\begin{exercise}
\label{exer:2.8.26}
Suppose that $X_1, \ldots, X_5$ is a sample from the Uniform$[0, 1]$ distribution. If we define the sample median to be $X_{(3)}$, find the density of the sample median. Can you identify this distribution? (Hint: Use Problem~\ref{exer:2.8.25}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.8.25} the distribution function of $X_{(3)}$, for $0 < x < 1$, is given by $\prb(X_{(3)} \leqslant x) = \sum_{j=3}^{5}\binom{5}{j}x^j(1 - x)^{5-j} = 10x^3(1 - x)^2 + 5x^4(1 - x) + x^5 = 10x^3 - 15x^4 + 6x^5$, so $f(x) = 30x^2 - 60x^3 + 30x^4 = 30x^2(x - 1)^2$. This is the $\text{Beta}(3, 3)$ density.
\end{solution}

\begin{exercise}
\label{exer:2.8.27}
Suppose that $(X, Y) \sim \text{Bivariate Normal}(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$. Prove that $Y$ given $X = x$ is distributed $N\left(\mu_2 + \sigma_2 \rho (x - \mu_1)/\sigma_1, (1 - \rho^2) \sigma_2^2\right)$. Establish the analogous result for the conditional distribution of $X$ given $Y = y$. (Hint: Use \eqref{eq:2.7.1} for $Y$ given $X = x$, and its analog for $X$ given $Y = y$.)
\end{exercise}

\begin{solution}
From (2.7.1) we have that $X = \mu_1 + \sigma_1 Z_1$, $Y = \mu_2 + \sigma_2(\rho Z_1 + \sqrt{1 - \rho^2}Z_2)$, so specifying $X = x$ implies that $Z_1 = (x - \mu_1)/\sigma_1$, so $Y = \mu_2 + \rho\sigma_2(x - \mu_1) + \sigma_2\sqrt{1 - \rho^2}Z_2$, and this immediately implies the result.

By symmetry we can also write that the distribution of $(X, Y)$ is obtained from $Y = \mu_2 + \sigma_2 Z_1$, $X = \mu_1 + \sigma_1(\rho Z_1 + \sqrt{1 - \rho^2}Z_2)$, so the conditional distribution of $X$ given $Y = y$ is $N(\mu_1 + \rho\sigma_1(y - \mu_2), (1 - \rho^2)\sigma_1^2)$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:2.8.28}
Let $X$ and $Y$ be random variables.
\begin{enumerate}[(a)]
\item Suppose $X$ and $Y$ are both discrete. Prove that $X$ and $Y$ are independent if and only if $\prb(Y = y \mid X = x) = \prb(Y = y)$ for all $x$ and $y$ such that $\prb(X = x) > 0$.
\item Suppose $X$ and $Y$ are jointly absolutely continuous. Prove that $X$ and $Y$ are independent if and only if $\prb(a \leqslant Y \leqslant b \mid X = x) = \prb(a \leqslant Y \leqslant b)$ for all $x$ and $y$ such that $f_X(x) > 0$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item The ``only if'' part follows from Theorem \ref{thm:2.8.4}(a). For the ``if'' part, the condition says that $\prb(X = x, Y = y) = \prb(X = x)\prb(Y = y)$ whenever $\prb(X = x) > 0$. But if $\prb(X = x) = 0$, then $\prb(X = x, Y = y) \leqslant \prb(X = x) = 0$, so $\prb(X = x, Y = y) = \prb(X = x)\prb(Y = y) = 0$. We conclude that $\prb(X = x, Y = y) = \prb(X = x)\prb(Y = y)$ for all $x$ and $y$. Hence, $X$ and $Y$ are independent.
    \item Very similar to (a).
\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multidimensional Change of Variable}
\label{sec:2.9}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $X$ and $Y$ be random variables with known joint distribution. Suppose that $Z = h_1(X, Y)$ and $W = h_2(X, Y)$, where $h_1, h_2 : \mathbf{R}^2 \to \mathbf{R}^1$ are two functions. What is the joint distribution of $Z$ and $W$?

This is similar to the problem considered in Section~\ref{sec:2.6}, except that we have moved from a one-dimensional to a two-dimensional setting. The two-dimensional setting is more complicated; however, the results remain essentially the same, as we shall see.

\subsection{The Discrete Case}
\label{ssec:2.9.1}

If $X$ and $Y$ are discrete random variables, then the distribution of $Z$ and $W$ is essentially straightforward.

\begin{theorem}
\label{thm:2.9.1}
Let $X$ and $Y$ be discrete random variables, with joint probability function $p_{X,Y}$. Let $Z = h_1(X, Y)$ and $W = h_2(X, Y)$, where $h_1, h_2 : \mathbf{R}^2 \to \mathbf{R}^1$ are some functions. Then $Z$ and $W$ are also discrete, and their joint probability function $p_{Z,W}$ satisfies
\[
p_{Z,W}(z, w) = \sum_{\substack{(x, y) : \\ h_1(x, y) = z, \, h_2(x, y) = w}} p_{X,Y}(x, y).
\]
Here, the sum is taken over all pairs $(x, y)$ such that $h_1(x, y) = z$ and $h_2(x, y) = w$.
\end{theorem}

\begin{proof}
We compute that $p_{Z,W}(z, w) = \prb(Z = z, W = w) = \prb(h_1(X, Y) = z, h_2(X, Y) = w)$. This equals
\[
\sum_{\substack{(x, y) : \\ h_1(x, y) = z, \, h_2(x, y) = w}} \prb(X = x, Y = y) = \sum_{\substack{(x, y) : \\ h_1(x, y) = z, \, h_2(x, y) = w}} p_{X,Y}(x, y),
\]
as claimed.
\end{proof}

As a special case, we note the following.

\begin{corollary}
\label{cor:2.9.1}
Suppose in the context of Theorem~\ref{thm:2.9.1} that the joint function $h = (h_1, h_2) : \mathbf{R}^2 \to \mathbf{R}^2$ defined by $h(x, y) = (h_1(x, y), h_2(x, y))$ is one-to-one, i.e., if $h_1(x_1, y_1) = h_1(x_2, y_2)$ and $h_2(x_1, y_1) = h_2(x_2, y_2)$, then $x_1 = x_2$ and $y_1 = y_2$. Then
\[
p_{Z,W}(z, w) = p_{X,Y}(h^{-1}(z, w)),
\]
where $h^{-1}(z, w)$ is the unique pair $(x, y)$ such that $h(x, y) = (z, w)$.
\end{corollary}

\begin{example}
\label{ex:2.9.1}
Suppose $X$ and $Y$ have joint density function
\[
p_{X,Y}(x, y) = \begin{cases}
1/6 & x = 2, \, y = 6 \\
1/12 & x = 2, \, y = -6 \\
1/4 & x = 3, \, y = 11 \\
1/2 & x = 3, \, y = 8 \\
0 & \text{otherwise.}
\end{cases}
\]
Let $Z = X + Y$ and $W = Y - X^2$. Then $p_{Z,W}(8, 2) = \prb(Z = 8, W = 2) = \prb(X = 2, Y = 6) + \prb(X = 3, Y = 11) = 1/6 + 1/4 = 5/12$. On the other hand, $p_{Z,W}(5, -17) = \prb(Z = 5, W = -17) = \prb(X = 3, Y = 8) = 1/2$.
\end{example}

\subsection{The Continuous Case (Advanced)}
\label{ssec:2.9.2}

If $X$ and $Y$ are continuous, and the function $h = (h_1, h_2)$ is one-to-one, then it is again possible to compute a formula for the joint density of $Z$ and $W$, as the following theorem shows. To state it, recall from multivariable calculus that, if $h = (h_1, h_2) : \mathbf{R}^2 \to \mathbf{R}^2$ is a differentiable function, then its \emph{Jacobian derivative} $J$ is defined by
\[
J(x, y) = \det \begin{pmatrix} \dfrac{\partial h_1}{\partial x} & \dfrac{\partial h_2}{\partial x} \\[10pt] \dfrac{\partial h_1}{\partial y} & \dfrac{\partial h_2}{\partial y} \end{pmatrix} = \frac{\partial h_1}{\partial x} \frac{\partial h_2}{\partial y} - \frac{\partial h_2}{\partial x} \frac{\partial h_1}{\partial y}.
\]

\begin{theorem}
\label{thm:2.9.2}
Let $X$ and $Y$ be jointly absolutely continuous, with joint density function $f_{X,Y}$. Let $Z = h_1(X, Y)$ and $W = h_2(X, Y)$, where $h_1, h_2 : \mathbf{R}^2 \to \mathbf{R}^1$ are differentiable functions. Define the joint function $h = (h_1, h_2) : \mathbf{R}^2 \to \mathbf{R}^2$ by
\[
h(x, y) = (h_1(x, y), h_2(x, y)).
\]
Assume that $h$ is one-to-one, at least on the region $\{(x, y) : f(x, y) > 0\}$, i.e., if $h_1(x_1, y_1) = h_1(x_2, y_2)$ and $h_2(x_1, y_1) = h_2(x_2, y_2)$, then $x_1 = x_2$ and $y_1 = y_2$. Then $Z$ and $W$ are also jointly absolutely continuous, with joint density function $f_{Z,W}$ given by
\[
f_{Z,W}(z, w) = f_{X,Y}(h^{-1}(z, w)) \, |J(h^{-1}(z, w))|,
\]
where $J$ is the Jacobian derivative of $h$, and where $h^{-1}(z, w)$ is the unique pair $(x, y)$ such that $h(x, y) = (z, w)$.
\end{theorem}

\begin{proof}
See Section~\ref{sec:2.11} for the proof of this result.
\end{proof}

\begin{example}
\label{ex:2.9.2}
Let $X$ and $Y$ be jointly absolutely continuous, with joint density function $f_{X,Y}$ given by
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise}
\end{cases}
\]
as in Example~\ref{ex:2.7.6}. Let $Z = X + Y^2$ and $W = X - Y^2$. What is the joint density of $Z$ and $W$?

We first note that $Z = h_1(X, Y)$ and $W = h_2(X, Y)$, where $h_1(x, y) = x + y^2$ and $h_2(x, y) = x - y^2$. Hence,
\[
J(x, y) = \frac{\partial h_1}{\partial x} \frac{\partial h_2}{\partial y} - \frac{\partial h_2}{\partial x} \frac{\partial h_1}{\partial y} = (1)(-2y) - (1)(2y) = -4y.
\]

We may invert the relationship $h$ by solving for $X$ and $Y$, to obtain that
\[
X = \frac{1}{2}(Z + W) \quad \text{and} \quad Y = \sqrt{\frac{Z - W}{2}}.
\]
This means that $h = (h_1, h_2)$ is invertible, with
\[
h^{-1}(z, w) = \left(\frac{1}{2}(z + w), \sqrt{\frac{z - w}{2}}\right).
\]
Hence, using Theorem~\ref{thm:2.9.2}, we see that
\begin{align*}
f_{Z,W}(z, w) &= f_{X,Y}(h^{-1}(z, w)) \, |J(h^{-1}(z, w))| \\
&= f_{X,Y}\left(\frac{1}{2}(z + w), \sqrt{\frac{z - w}{2}}\right) \, \left|J\left(h^{-1}(z, w)\right)\right| \\
&= \begin{cases}
\left[4\left(\frac{1}{2}(z + w)\right)^2 \sqrt{\frac{z - w}{2}} + 2\left(\sqrt{\frac{z - w}{2}}\right)^5\right] \cdot \left|{-4\sqrt{\frac{z - w}{2}}}\right| & \substack{0 \leqslant \frac{1}{2}(z + w) \leqslant 1, \\ 0 \leqslant \sqrt{\frac{z - w}{2}} \leqslant 1} \\[6pt]
0 & \text{otherwise}
\end{cases} \\
&= \begin{cases}
\left[(z + w)^2 \sqrt{\frac{z - w}{2}} + 2\left(\frac{z - w}{2}\right)^{5/2}\right] \cdot 4\sqrt{\frac{z - w}{2}} & 0 \leqslant z + w \leqslant 2, \, 0 \leqslant z - w \leqslant 2 \\[6pt]
0 & \text{otherwise.}
\end{cases}
\end{align*}
We have thus obtained the joint density function for $Z$ and $W$.
\end{example}

\begin{example}
\label{ex:2.9.3}
Let $U_1$ and $U_2$ be independent, each having the Uniform$[0, 1]$ distribution. (We could write this as $U_1, U_2$ are i.i.d.\ Uniform$[0, 1]$.) Thus,
\[
f_{U_1, U_2}(u_1, u_2) = \begin{cases}
1 & 0 \leqslant u_1 \leqslant 1, \, 0 \leqslant u_2 \leqslant 1 \\
0 & \text{otherwise.}
\end{cases}
\]
Then define $X$ and $Y$ by
\[
X = \sqrt{-2\log(1 - U_1)} \cos(2\pi U_2), \quad Y = \sqrt{-2\log(1 - U_1)} \sin(2\pi U_2).
\]
What is the joint density of $X$ and $Y$?

We see that here $X = h_1(U_1, U_2)$ and $Y = h_2(U_1, U_2)$, where
\[
h_1(u_1, u_2) = \sqrt{-2\log(1 - u_1)} \cos(2\pi u_2), \quad h_2(u_1, u_2) = \sqrt{-2\log(1 - u_1)} \sin(2\pi u_2).
\]
Therefore,
\[
\frac{\partial h_1}{\partial u_1}(u_1, u_2) = \frac{1}{2}[-2\log(1 - u_1)]^{-1/2} \cdot \frac{2}{u_1} \cdot \cos(2\pi u_2).
\]
Continuing in this way, we eventually compute (see Exercise~\ref{exer:2.9.1}) that
\[
J(u_1, u_2) = \frac{\partial h_1}{\partial u_1} \frac{\partial h_2}{\partial u_2} - \frac{\partial h_2}{\partial u_1} \frac{\partial h_1}{\partial u_2} = \frac{2\pi}{u_1}[\cos^2(2\pi u_2) + \sin^2(2\pi u_2)] = \frac{2\pi}{u_1}.
\]

Next, we set $R = \sqrt{X^2 + Y^2}$, so that $R^2 = X^2 + Y^2 = -2\log(1 - U_1)$. Then, inverting the relationship $h$, we compute that
\[
U_1(X, Y) = e^{-R^2/2}, \quad \cos(2\pi U_2(X, Y)) = X/R, \quad \sin(2\pi U_2(X, Y)) = Y/R.
\]
Here $U_1(X, Y) \in [0, 1]$ is defined directly, while $U_2(X, Y) \in [0, 1]$ is defined implicitly to make $2\pi U_2(X, Y) \in [0, 2\pi)$ be the unique angle which satisfies the above relationships. Then, by Theorem~\ref{thm:2.9.2}, for any $(x, y) \in \mathbf{R}^2$,
\begin{align*}
f_{X,Y}(x, y) &= f_{U_1, U_2}(h^{-1}(x, y)) \, |J(h^{-1}(x, y))|^{-1} \\
&= 1 \cdot f_{U_1, U_2}(U_1(x, y), U_2(x, y)) \, |J(U_1(x, y), U_2(x, y))|^{-1} \\
&= 1 \cdot \frac{1}{2\pi} U_1(x, y) = \frac{1}{2\pi} e^{-R^2(x, y)/2} = \frac{1}{2\pi} e^{-(x^2 + y^2)/2}.
\end{align*}

We conclude that
\[
f_{X,Y}(x, y) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \cdot \frac{1}{\sqrt{2\pi}} e^{-y^2/2}.
\]
We recognize this as a product of two standard normal densities. We thus conclude that $X \sim N(0, 1)$ and $Y \sim N(0, 1)$ and that, furthermore, $X$ and $Y$ are independent.
\end{example}

\subsection{Convolution}
\label{ssec:2.9.3}

Suppose now that $X$ and $Y$ are independent, with known distributions, and that $Z = X + Y$. What is the distribution of $Z$? In this case, the distribution of $Z$ is called the \emph{convolution} of the distributions of $X$ and of $Y$. Fortunately, the convolution is often reasonably straightforward to compute.

\begin{theorem}
\label{thm:2.9.3}
Let $X$ and $Y$ be independent, and let $Z = X + Y$.
\begin{enumerate}[(a)]
\item If $X$ and $Y$ are both discrete, with probability functions $p_X$ and $p_Y$, then $Z$ is also discrete, with probability function $p_Z$ given by
\[
p_Z(z) = \sum_w p_X(z - w) \, p_Y(w).
\]
\item If $X$ and $Y$ are jointly absolutely continuous, with density functions $f_X$ and $f_Y$, then $Z$ is also absolutely continuous, with density function $f_Z$ given by
\[
f_Z(z) = \int_{-\infty}^{\infty} f_X(z - w) \, f_Y(w) \, \mathrm{d}w.
\]
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}[(a)]
  \item We let $W = Y$ and consider the two-dimensional transformation from $(X, Y)$ to $(Z, W) = (X + Y, Y)$.

In the discrete case, by Corollary~\ref{cor:2.9.1}, $p_{Z,W}(z, w) = p_{X,Y}(z - w, w)$. Then from Theorem~\ref{thm:2.7.4}, $p_Z(z) = \sum_w p_{Z,W}(z, w) = \sum_w p_{X,Y}(z - w, w)$. But because $X$ and $Y$ are independent, $p_{X,Y}(x, y) = p_X(x) \, p_Y(y)$, so $p_{X,Y}(z - w, w) = p_X(z - w) \, p_Y(w)$. This proves part (a).

  \item In the continuous case, we must compute the Jacobian derivative $J(x, y)$ of the transformation from $(X, Y)$ to $(Z, W) = (X + Y, Y)$. Fortunately, this is very easy, as we obtain
\[
J(x, y) = \begin{vmatrix} \dfrac{\partial (x + y)}{\partial x} & \dfrac{\partial y}{\partial x} \\[10pt] \dfrac{\partial (x + y)}{\partial y} & \dfrac{\partial y}{\partial y} \end{vmatrix} = \begin{vmatrix} 1 & 0 \\ 1 & 1 \end{vmatrix} = 1.
\]
Hence, from Theorem~\ref{thm:2.9.2}, $f_{Z,W}(z, w) = f_{X,Y}(z - w, w) \cdot 1 = f_{X,Y}(z - w, w)$, and from Theorem~\ref{thm:2.7.5},
\[
f_Z(z) = \int_{-\infty}^{\infty} f_{Z,W}(z, w) \, \mathrm{d}w = \int_{-\infty}^{\infty} f_{X,Y}(z - w, w) \, \mathrm{d}w.
\]
But because $X$ and $Y$ are independent, we may take $f_{X,Y}(x, y) = f_X(x) \, f_Y(y)$, so $f_{X,Y}(z - w, w) = f_X(z - w) \, f_Y(w)$. This proves part (b).
\end{enumerate}
\end{proof}

\begin{example}
\label{ex:2.9.4}
Let $X \sim \text{Binomial}(4, 1/5)$ and $Y \sim \text{Bernoulli}(1/4)$, with $X$ and $Y$ independent. Let $Z = X + Y$. Then
\begin{align*}
p_Z(3) &= \prb(X + Y = 3) = \prb(X = 3, Y = 0) + \prb(X = 2, Y = 1) \\
&= \binom{4}{3}(1/5)^3(4/5)^1 \cdot (3/4) + \binom{4}{2}(1/5)^2(4/5)^2 \cdot (1/4) \\
&= 4 \cdot (1/5)^3 \cdot (4/5)^1 \cdot (3/4) + 6 \cdot (1/5)^2 \cdot (4/5)^2 \cdot (1/4) = 0.0576.
\end{align*}
\end{example}

\begin{example}
\label{ex:2.9.5}
Let $X \sim \text{Uniform}[3, 7]$ and $Y \sim \text{Exponential}(6)$, with $X$ and $Y$ independent. Let $Z = X + Y$. Then
\[
f_Z(5) = \int_{-\infty}^{\infty} f_X(x) \, f_Y(5 - x) \, \mathrm{d}x = \int_3^5 \frac{1}{4} \cdot 6 e^{-6(5 - x)} \, \mathrm{d}x = \frac{1}{4}[e^{-6(5 - x)}]_{x=5}^{x=3} = \frac{1}{4} e^{-12} - \frac{1}{4} e^0 \approx 0.2499985.
\]
Note that here the limits of integration go from $3$ to $5$ only, because $f_X(x) = 0$ for $x < 3$, while $f_Y(5 - x) = 0$ for $x > 5$.
\end{example}

\subsection*{Summary of Section~\ref{sec:2.9}}

\begin{itemize}
\item If $X$ and $Y$ are discrete, and $Z = h_1(X, Y)$ and $W = h_2(X, Y)$, then
\[
p_{Z,W}(z, w) = \sum_{(x, y) : h_1(x, y) = z, \, h_2(x, y) = w} p_{X,Y}(x, y).
\]
\item If $X$ and $Y$ are absolutely continuous, if $Z = h_1(X, Y)$ and $W = h_2(X, Y)$, and if $h = (h_1, h_2) : \mathbf{R}^2 \to \mathbf{R}^2$ is one-to-one with Jacobian $J(x, y)$, then
\[
f_{Z,W}(z, w) = f_{X,Y}(h^{-1}(z, w)) \, |J(h^{-1}(z, w))|.
\]
\item This allows us to compute the joint distribution of functions of pairs of random variables.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.9.1}
Verify explicitly in Example~\ref{ex:2.9.3} that $J(u_1, u_2) = 2\pi/u_1$.
\end{exercise}

\begin{solution}
We compute that
\begin{align*}
    \frac{\partial h_1}{\partial u_1} &= -\frac{\cos(2\pi u_2)}{u_1\sqrt{2\log(1/u_1)}}, & \frac{\partial h_1}{\partial u_2} &= -2\sqrt{2}\pi\sin(2\pi u_2)\sqrt{2\log(1/u_1)} \\
    \frac{\partial h_2}{\partial u_1} &= -\frac{\sin(2\pi u_2)}{u_1\sqrt{2\log(1/u_1)}}, & \frac{\partial h_2}{\partial u_2} &= -2\sqrt{2}\pi\cos(2\pi u_2)\sqrt{2\log(1/u_1)}.
\end{align*}
Then $J(u_1, u_2) = \frac{\partial h_1}{\partial u_1}\frac{\partial h_2}{\partial u_2} - \frac{\partial h_2}{\partial u_1}\frac{\partial h_1}{\partial u_2} = -2\pi/u_1$.
\end{solution}

\begin{exercise}
\label{exer:2.9.2}
Let $X \sim \text{Exponential}(3)$ and $Y \sim \text{Uniform}[1, 4]$, with $X$ and $Y$ independent. Let $Z = X + Y$ and $W = X - Y$.
\begin{enumerate}[(a)]
\item Write down the joint density $f_{X,Y}(x, y)$ of $X$ and $Y$. (Be sure to consider the ranges of valid $x$ and $y$ values.)
\item Find a two-dimensional function $h$ such that $(Z, W) = h(X, Y)$.
\item Find a two-dimensional function $h^{-1}$ such that $(X, Y) = h^{-1}(Z, W)$.
\item Compute the joint density $f_{Z,W}(z, w)$ of $Z$ and $W$. (Again, be sure to consider the ranges of valid $z$ and $w$ values.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $f_{X,Y}(x, y) = e^{-x}$ for $x \geqslant 0$ and $1 \leqslant y \leqslant 4$, otherwise $f_{X,Y}(x, y) = 0$.
    \item $h(x, y) = (x + y, x - y)$.
    \item $h^{-1}(z, w) = ((z + w)/2, (z - w)/2)$.
    \item Here $J(x, y) = \frac{\partial h_1}{\partial x}\frac{\partial h_2}{\partial y} - \frac{\partial h_2}{\partial x}\frac{\partial h_1}{\partial y} = |(1)(-1) - (1)(1)| = 2$, so $f_{Z,W}(z, w) = f_{X,Y}(h^{-1}(z, w))/|J(h^{-1}(z, w))| = f_{X,Y}((z+w)/2, (z-w)/2)/2$, which equals $e^{-(z+w)/2}$ for $(z + w)/2 \geqslant 0$ and $1 \leqslant (z - w)/2 \leqslant 4$, i.e., for $z \geqslant 1$ and $\max(-z, z - 8) \leqslant w \leqslant z - 2$, otherwise $f_{Z,W}(z, w) = 0$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.9.3}
Repeat parts (b) through (d) of Exercise~\ref{exer:2.9.2}, for the same random variables $X$ and $Y$, if instead $Z = X^2 + Y^2$ and $W = X^2 - Y^2$.
\end{exercise}

\begin{solution}
\begin{enumerate}[(b)]
    \item $h(x, y) = (x^2 + y^2, x^2 - y^2)$
    \item $h^{-1}(z, w) = (\sqrt{(z + w)/2}, \sqrt{(z - w)/2})$, at least for $z + w \geqslant 0$ and $z - w \geqslant 0$
    \item Here $J(x, y) = \frac{\partial h_1}{\partial x}\frac{\partial h_2}{\partial y} - \frac{\partial h_2}{\partial x}\frac{\partial h_1}{\partial y} = |(2x)(-2y) - (2y)(2x)| = 4xy$ for $x, y \geqslant 0$, so
    \begin{align*}
        f_{Z,W}(z, w) &= f_{X,Y}(h^{-1}(z, w))/|J(h^{-1}(z, w))| \\
        &= f_{X,Y}(\sqrt{(z + w)/2}, \sqrt{(z - w)/2})/4\sqrt{(z + w)/2}\sqrt{(z - w)/2} \\
        &= f_{X,Y}(\sqrt{(z + w)/2}, \sqrt{(z - w)/2})/2\sqrt{z^2 - w^2}
    \end{align*}
    which equals $e^{-\sqrt{(z+w)/2}}/2\sqrt{z^2 - w^2}$ for $\sqrt{(z + w)/2} \geqslant 0$ and $1 \leqslant \sqrt{(z - w)/2} \leqslant 4$, i.e., for $z \geqslant 4$ and $\max(-z, z - 64) \leqslant w \leqslant z - 4$, otherwise $f_{Z,W}(z, w) = 0$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.9.4}
Repeat parts (b) through (d) of Exercise~\ref{exer:2.9.2}, for the same random variables $X$ and $Y$, if instead $Z = X + 4$ and $W = Y - 3$.
\end{exercise}

\begin{solution}
\begin{enumerate}[(b)]
    \item $h(x, y) = (x + 4, y - 3)$
    \item $h^{-1}(z, w) = (z - 4, w + 3)$
    \item Here $J(x, y) = \frac{\partial h_1}{\partial x}\frac{\partial h_2}{\partial y} - \frac{\partial h_2}{\partial x}\frac{\partial h_1}{\partial y} = |(1)(1) - (0)(0)| = 1$, so $f_{Z,W}(z, w) = f_{X,Y}(h^{-1}(z, w))/|J(h^{-1}(z, w))| = f_{X,Y}(z - 4, w + 3)/1$, which equals $e^{-(z-4)}$ for $z - 4 \geqslant 0$ and $1 \leqslant w + 3 \leqslant 4$, i.e., for $z \geqslant 4$ and $-2 \leqslant w \leqslant 1$, otherwise $f_{Z,W}(z, w) = 0$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.9.5}
Repeat parts (b) through (d) of Exercise~\ref{exer:2.9.2}, for the same random variables $X$ and $Y$, if instead $Z = Y^4$ and $W = X^4$.
\end{exercise}

\begin{solution}
\begin{enumerate}[(b)]
    \item $h(x, y) = (y^4, x^4)$
    \item $h^{-1}(z, w) = (w^{1/4}, z^{1/4})$
    \item Here $J(x, y) = \frac{\partial h_1}{\partial x}\frac{\partial h_2}{\partial y} - \frac{\partial h_2}{\partial x}\frac{\partial h_1}{\partial y} = |(0)(0) - (4y^3)(4x^3)| = 4x^3 y^3$, at least for $x, y \geqslant 0$, so $f_{Z,W}(z, w) = f_{X,Y}(h^{-1}(z, w))/|J(h^{-1}(z, w))| = f_{X,Y}(w^{1/4}, z^{1/4})/4w^{3/4}z^{3/4}$, which equals $e^{-w^{1/4}}$ for $w^{1/4} \geqslant 0$ and $1 \leqslant z^{1/4} \leqslant 4$, i.e., for $w \geqslant 0$ and $1 \leqslant z \leqslant 256$, otherwise $f_{Z,W}(z, w) = 0$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.9.6}
Suppose the joint probability function of $X$ and $Y$ is given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/7 & x = 5, \, y = 0 \\
1/7 & x = 5, \, y = 3 \\
1/7 & x = 5, \, y = 4 \\
3/7 & x = 8, \, y = 0 \\
1/7 & x = 8, \, y = 4 \\
0 & \text{otherwise.}
\end{cases}
\]
Let $Z = X + Y$, $W = X - Y$, $A = X^2 + Y^2$, and $B = 2X + 3Y^2$.
\begin{enumerate}[(a)]
\item Compute the joint probability function $p_{Z,W}(z, w)$.
\item Compute the joint probability function $p_{A,B}(a, b)$.
\item Compute the joint probability function $p_{Z,A}(z, a)$.
\item Compute the joint probability function $p_{W,B}(w, b)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $p_{Z,W}(5, 5) = 1/7$; $p_{Z,W}(8, 2) = 1/7$; $p_{Z,W}(9, 1) = 1/7$; $p_{Z,W}(8, 0) = 3/7$; $p_{Z,W}(12, 4) = 1/7$; $p_{Z,W}(z, w) = 0$ otherwise.
    \item $p_{A,B}(25, 10) = 1/7$; $p_{A,B}(34, -17) = 1/7$; $p_{A,B}(41, -38) = 1/7$; $p_{A,B}(64, 16) = 3/7$; $p_{A,B}(80, -32) = 1/7$; $p_{A,B}(a, b) = 0$ otherwise.
    \item $p_{Z,A}(5, 25) = 1/7$; $p_{Z,A}(8, 34) = 1/7$; $p_{Z,A}(9, 41) = 1/7$; $p_{Z,A}(8, 64) = 3/7$; $p_{Z,A}(12, 80) = 1/7$; $p_{Z,A}(z, a) = 0$ otherwise.
    \item $p_{Z,B}(5, 10) = 1/7$; $p_{Z,B}(8, -17) = 1/7$; $p_{Z,B}(9, -38) = 1/7$; $p_{Z,B}(8, 16) = 3/7$; $p_{Z,B}(12, -32) = 1/7$; $p_{Z,B}(z, b) = 0$ otherwise.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.9.7}
Let $X$ have probability function
\[
p_X(x) = \begin{cases}
1/3 & x = 0 \\
1/2 & x = 2 \\
1/6 & x = 3 \\
0 & \text{otherwise}
\end{cases}
\]
and let $Y$ have probability function
\[
p_Y(y) = \begin{cases}
1/6 & y = 2 \\
1/12 & y = 5 \\
3/4 & y = 9 \\
0 & \text{otherwise.}
\end{cases}
\]
Suppose $X$ and $Y$ are independent. Let $Z = X + Y$. Compute $p_Z(z)$ for all $z \in \mathbf{R}^1$.
\end{exercise}

\begin{solution}
$p_Z(2) = (1/3)(1/6) = 1/18$; $p_Z(4) = (1/2)(1/6) = 1/12$; $p_Z(5) = (1/3)(1/12) + (1/6)(1/6) = 1/18$; $p_Z(7) = (1/2)(1/12) = 1/24$; $p_Z(8) = (1/6)(1/12) = 1/72$; $p_Z(9) = (1/3)(3/4) = 1/4$; $p_Z(11) = (1/2)(3/4) = 3/8$; $p_Z(12) = (1/6)(3/4) = 1/8$; $p_Z(z) = 0$ otherwise.
\end{solution}

\begin{exercise}
\label{exer:2.9.8}
Let $X \sim \text{Geometric}(1/4)$, and let $Y$ have probability function
\[
p_Y(y) = \begin{cases}
1/6 & y = 2 \\
1/12 & y = 5 \\
3/4 & y = 9 \\
0 & \text{otherwise.}
\end{cases}
\]
Let $W = X + Y$. Suppose $X$ and $Y$ are independent. Compute $p_W(w)$ for all $w \in \mathbf{R}^1$.
\end{exercise}

\begin{solution}
If $w$ is an integer between 2 and 4, then $p_W(w) = \prb(Y = 2, X = w - 2) = (1/6)(3/4)^{w-2}(1/4) = (3/4)^{w-2}/24$. If $w$ is an integer between 5 and 8, then $p_W(w) = \prb(Y = 2, X = w - 2) + \prb(Y = 5, X = w - 5) = (1/6)(3/4)^{w-2}(1/4) + (1/12)(3/4)^{w-5}(1/4)$. If $w$ is an integer $\geqslant 9$, then $p_W(w) = \prb(Y = 2, X = w - 2) + \prb(Y = 5, X = w - 5) + \prb(Y = 9, X = w - 9) = (1/6)(3/4)^{w-2}(1/4) + (1/12)(3/4)^{w-5}(1/4) + (3/4)(3/4)^{w-9}(1/4)$. Otherwise, $p_W(w) = 0$.
\end{solution}

\begin{exercise}
\label{exer:2.9.9}
Suppose $X$ and $Y$ are discrete, with $\prb(X = 1, Y = 1) = \prb(X = 1, Y = 2) = \prb(X = 1, Y = 3) = \prb(X = 2, Y = 2) = \prb(X = 2, Y = 3) = 1/5$, otherwise $\prb(X = x, Y = y) = 0$. Let $Z = X + Y^2$ and $W = X^2 + 5Y$.
\begin{enumerate}[(a)]
\item Compute the joint probability function $p_{Z,W}(z, w)$ for all $(z, w) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\item Compute the marginal probability function $p_Z(z)$ for $Z$.
\item Compute the marginal probability function $p_W(w)$ for $W$.
\end{enumerate}
\end{exercise}

\begin{solution}
From the given probability measure, we have
\begin{center}
\begin{tabular}{c|cccccc}
    $(x, y)$ & $(1, 1)$ & $(1, 2)$ & $(1, 3)$ & $(2, 2)$ & $(2, 3)$ & otherwise \\
    \hline
    $\prb(X = x, Y = y)$ & $1/5$ & $1/5$ & $1/5$ & $1/5$ & $1/5$ & 0 \\
    $Z(x, y)$ & 0 & $-3$ & $-8$ & $-2$ & $-7$ & $x - y^2$ \\
    $W(x, y)$ & 6 & 11 & 16 & 14 & 19 & $x^2 + 5y$
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
    \item From the above table we have
    \begin{center}
    \begin{tabular}{c|cccccc}
        $(z, w)$ & $(-8, 16)$ & $(-7, 19)$ & $(-3, 11)$ & $(-2, 14)$ & $(0, 6)$ & otherwise \\
        \hline
        $\prb(Z = z, W = w)$ & $1/5$ & $1/5$ & $1/5$ & $1/5$ & $1/5$ & 0
    \end{tabular}
    \end{center}
    \item From the probability table we have $p_Z(z) = 1/5$ for $z = -8, -7, -3, -2, 0$, otherwise $p_Z(z) = 0$.
    \item From the probability table we have $p_W(w) = 1/5$ for $w = 6, 11, 14, 16, 19$, otherwise $p_W(w) = 0$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.9.10}
Suppose $X$ has density $f_X(x) = x^3/4$ for $0 \leqslant x \leqslant 2$, otherwise $f_X(x) = 0$, and $Y$ has density $f_Y(y) = 5y^4/32$ for $0 \leqslant y \leqslant 2$, otherwise $f_Y(y) = 0$. Assume $X$ and $Y$ are independent, and let $Z = X + Y$.
\begin{enumerate}[(a)]
\item Compute the joint density $f_{X,Y}(x, y)$ for all $(x, y) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\item Compute the density $f_Z(z)$ for $Z$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item From Theorem \ref{thm:2.8.3}(b), $f_{X,Y}(x, y) = f_X(x)f_Y(y)$. Hence, $f_{X,Y}(x, y) = 5x^3 y^4/128$ for $0 < x < 2$, $0 < y < 2$, otherwise $f_{X,Y}(x, y) = 0$.
  \item The density of $f_Z(z)$ can be obtained using Theorem \ref{thm:2.9.3}(b). Since $X$ and $Y$ have positive density only when $0 < x, y < 2$, new random variable $Z$ has positive density only when $0 < z = x + y < 4$. Thus, $f_Z(z) = 0$ for $z \notin (0, 4)$. For $0 < z < 4$,
    \[
        f_Z(z) = \int_{-\infty}^{\infty}f_X(x)f_Y(z - x)\,\mathrm{d}x = \int_{\max(0,z-2)}^{\min(2,z)}5x^3(z - x)^4/128\,\mathrm{d}x.
    \]
    For $0 < z < 2$, the integration range is $(\max(0, z - 2), \min(2, z)) = (0, z)$. Let $u = x/z$. Then,
    \[
        f_Z(z) = \frac{5}{128}\int_0^z x^3(z - x)^4\,\mathrm{d}x = \frac{5z^8}{128}\int_0^1 u^3(1 - u)^4\,\mathrm{d}u = \frac{5z^8}{128}\text{Beta}(4, 5) = \frac{z^8}{7168}.
    \]
    For $2 \leqslant z < 4$, the integration range is $(\max(0, z - 2), \min(2, z)) = (z - 2, 2)$.
    \begin{align*}
        f_Z(z) &= \frac{5}{128}\int_{z-2}^{2}x^7 - 4zx^6 + 6z^2 x^5 - 4z^3 x^4 + z^4 x^3\,\mathrm{d}x \\
        &= \frac{5}{128}\left[\frac{x^8}{8} - \frac{4zx^7}{7} + z^2 x^6 - \frac{4z^3 x^5}{5} + \frac{z^4 x^4}{4}\right]_{x=z-2}^{x=2} \\
        &= \frac{1}{28}\left(-20z + 35z^2 - 21z^3 + \frac{35}{8}z^4 - \frac{z^8}{28}\right).
    \end{align*}
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.9.11}
Suppose again that $X$ has density $f_X(x) = x^3/4$ for $0 \leqslant x \leqslant 2$, otherwise $f_X(x) = 0$, that $Y$ has density $f_Y(y) = 5y^4/32$ for $0 \leqslant y \leqslant 2$, otherwise $f_Y(y) = 0$, and that $X$ and $Y$ are independent. Let $Z = X + Y$ and $W = 4X - 3Y$.
\begin{enumerate}[(a)]
\item Compute the joint density $f_{Z,W}(z, w)$ for all $(z, w) \in \mathbf{R}^1 \times \mathbf{R}^1$.
\item Compute the marginal density $f_Z(z)$ for $Z$.
\item Compute the marginal density $f_W(w)$ for $W$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item The transformation $h : (x, y) \mapsto (z, w) = (x - y, 4x + 3y)$ has inverse $h^{-1}(z, w) = ((3z + w)/7, (w - 4z)/7)$. $J(x, y) = \frac{\partial z}{\partial x}\frac{\partial w}{\partial y} - \frac{\partial z}{\partial y}\frac{\partial w}{\partial x} = 1 \cdot 3 - (-1) \cdot 4 = 7$. From Theorem \ref{thm:2.9.2}, $f_{Z,W}(z, w) = f_{X,Y}(h^{-1}(z, w))/|J(h^{-1}(z, w))| = (5/128)((3z + w)/7)^3((w - 4z)/7)^4/7 = 5(3z + w)^3(w - 4z)^4/(27 \cdot 7^8)$ for $0 < 3z + w < 14$, $0 < w - 4z < 14$, otherwise $f_{Z,W}(z, w) = 0$.
    \item By integrating $w$ out, we have
    \[
        f_Z(z) = \int_{\mathbb{R}}f_{Z,W}(z, w)\,\mathrm{d}w = \int_{\max(-3z,4z)}^{\min(14-3z,14+4z)}5(3z + w)^3(w - 4z)^4/(27 \cdot 7^8)\,\mathrm{d}w.
    \]
    For $-2 < z < 0$, the integration range is $(\max(-3z, 4z), \min(14 - 3z, 14 + 4z)) = (-3z, 14 + 4z)$. Hence,
    \begin{align*}
        f_Z(z) &= \frac{5}{27 \cdot 7^8}\int_{-3z}^{14+4z}\left(w^7 - 7zw^6 - 21z^2 w^5 + 203z^3 w^4 + 112z^4 w^3 - 2016z^5 w^2 + 6912z^7\right)\mathrm{d}w \\
        &= \frac{1}{28}\left(35 + 60z + 35z^2 + 7z^3 + \frac{z^8}{28}\right).
    \end{align*}
    For $0 \leqslant z < 2$, the integration range is $(\max(-3z, 4z), \min(14 - 3z, 14 + 4z)) = (4z, 14 - 3z)$. Hence,
    \begin{align*}
        f_Z(z) &= \frac{5}{27 \cdot 7^8}\int_{4z}^{14-3z}\left(w^7 - 7zw^6 - 21z^2 w^5 + 203z^3 w^4 + 112z^4 w^3 - 2016z^5 w^2 + 6912z^7\right)\mathrm{d}w \\
        &= \frac{1}{28}\left(35 - 80z + 70z^2 - 28z^3 + \frac{z^4}{2^5} - \frac{z^8}{28}\right).
    \end{align*}
    \item By integrating $z$ out, we have
    \[
        f_W(w) = \int_{\mathbb{R}}f_{Z,W}(z, w)\,dz = \int_{\max(-w/3,(w-14)/4)}^{\min((14-w)/3,w/4)}5(3z + w)^3(w - 4z)^4/(27 \cdot 7^8)\,dz.
    \]
    For $0 < w < 6$, the integration range is $(\max(-w/3, (w - 14)/4), \min((14 - w)/3, w/4)) = (-w/3, w/4)$. Hence,
    \begin{align*}
        f_W(w) &= \frac{5}{27 \cdot 7^8}\int_{-w/3}^{w/4}\left(w^7 - 7zw^6 - 21z^2 w^5 + 203z^3 w^4 + 112z^4 w^3 - 2016z^5 w^2 + 6912z^7\right)dz \\
        &= \frac{w^8}{2^{18} \cdot 3 \cdot 5 \cdot 7}.
    \end{align*}
    For $6 \leqslant w < 8$, the integration range is $(\max(-w/3, (w - 14)/4), \min((14 - w)/3, w/4)) = ((w - 14)/4, w/4)$. Hence,
    \begin{align*}
        f_W(w) &= \frac{5}{27 \cdot 7^8}\int_{(w-14)/4}^{w/4}\left(w^7 - 7zw^6 - 21z^2 w^5 + 203z^3 w^4 + 112z^4 w^3 - 2016z^5 w^2 + 6912z^7\right)dz \\
        &= \frac{1}{2^{10} \cdot 7}\left(-945 + 540w - 105w^2 + 7w^3\right).
    \end{align*}
    For $8 \leqslant w < 14$, the integration range is $(\max(-w/3, (w - 14)/4), \min((14 - w)/3, w/4)) = ((w - 14)/4, (14 - w)/3)$. Hence,
    \begin{align*}
        f_W(w) &= \frac{5}{27 \cdot 7^8}\int_{(w-14)/4}^{(14-w)/3}\left(w^7 - 7zw^6 - 21z^2 w^5 + 203z^3 w^4 + 112z^4 w^3 - 2016z^5 w^2 + 6912z^7\right)dz \\
        &= \frac{z^8}{2^{10} \cdot 3^5}\left(294875 - 168500w + 37315w^2 - 3853w^3 + 160w^4 - \frac{w^8}{2^8 \cdot 7}\right).
    \end{align*}
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.9.12}
Let $X \sim \text{Binomial}(n_1, \theta)$ independent of $Y \sim \text{Binomial}(n_2, \theta)$. Let $Z = X + Y$. Use Theorem~\ref{thm:2.9.3}(a) to prove that $Z \sim \text{Binomial}(n_1 + n_2, \theta)$.
\end{exercise}

\begin{solution}
For $z$, an integer between 0 and $n_1 + n_2$,
\begin{align*}
    \prb(Z = z) &= \sum_x p_X(x)p_Y(z - x) \\
    &= \sum_{x=\max(0, z-n_2)}^{\min(z, n_1)}\binom{n_1}{x}p^x(1 - p)^{n_1-x}\binom{n_2}{z - x}p^{z-x}(1 - p)^{n_2-(z-x)} \\
    &= p^z(1 - p)^{n_1+n_2-z}\sum_{x=\max(0, z-n_2)}^{\min(z, n_1)}\binom{n_1}{x}\binom{n_2}{z - x}.
\end{align*}
Now this sum represents the number of ways of choosing $z$ positions out of $n_1 + n_2$ positions, so it equals $\binom{n_1+n_2}{z}$. (Indeed, of the $z$ positions chosen, some number $x$ of them must be among the first $n_1$ positions, with the remaining $z - x$ choices among the final $n_2$ positions.) Thus, $\prb(Z = z) = \binom{n_1+n_2}{z}p^z(1 - p)^{n_1+n_2-z}$ for $z$, an integer between 0 and $n_1 + n_2$. Hence, $Z \sim \text{Binomial}(n_1 + n_2, p)$.
\end{solution}

\begin{exercise}
\label{exer:2.9.13}
Let $X$ and $Y$ be independent, with $X \sim \text{Negative-Binomial}(r_1, \theta)$ and $Y \sim \text{Negative-Binomial}(r_2, \theta)$. Let $Z = X + Y$. Use Theorem~\ref{thm:2.9.3}(a) to prove that $Z \sim \text{Negative-Binomial}(r_1 + r_2, \theta)$.
\end{exercise}

\begin{solution}
For $z$ a non-negative integer,
\begin{align*}
    \prb(Z = z) &= \sum_x p_X(x)p_Y(z - x) \\
    &= \sum_{x=0}^{z}\binom{r_1 - 1 + x}{x}p^{r_1}(1 - p)^x\binom{r_2 - 1 + z - x}{z - x}p^{r_2}(1 - p)^{z-x} \\
    &= p^{r_1+r_2}(1 - p)^z\sum_{x=0}^{z}\binom{r_1 - 1 + x}{x}\binom{r_2 - 1 + z - x}{z - x}.
\end{align*}
Now this sum represents the number of ways of lining up $z$ red balls and $r_1 + r_2$ black balls, such that a black ball comes last. (Indeed, all balls up to and including the $r_1$th black ball are responsible for the first factor, with the remaining balls responsible for the second factor.) Thus,
\[
    \sum_{x=0}^{z}\binom{r_1 - 1 + x}{x}\binom{r_2 - 1 + z - x}{z - x} = \binom{r_1 + r_2 - 1 + z}{z}.
\]
Hence, $\prb(Z = z) = p^{r_1+r_2}(1 - p)^z\binom{r_1+r_2-1+z}{z}$, for $z$, a non-negative integer. Hence, $Z \sim \text{Negative-Binomial}(r_1 + r_2, p)$.
\end{solution}

\begin{exercise}
\label{exer:2.9.14}
Let $X$ and $Y$ be independent, with $X \sim N(\mu_1, \sigma_1^2)$ and $Y \sim N(\mu_2, \sigma_2^2)$. Let $Z = X + Y$. Use Theorem~\ref{thm:2.9.3}(b) to prove that $Z \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$.
\end{exercise}

\begin{solution}
We have that $f_Z(z) = \int_{-\infty}^{\infty}f_X(x)f_Y(z-x)\,\mathrm{d}x = \int_{-\infty}^{\infty}\frac{1}{\sigma_1\sqrt{2\pi}}e^{-(x-\mu_1)^2/2\sigma_1} \times \frac{1}{\sigma_2\sqrt{2\pi}}e^{-(z-x-\mu_2)^2/2\sigma_2}\,\mathrm{d}x$. Squaring out the exponents, and remembering that $\int_{-\infty}^{\infty}e^{-t^2/2}\,dt = \sqrt{2\pi}$, we compute that
\[
    f_Z(z) = \bigl(2\pi(\sigma_1^2 + \sigma_2^2)\bigr)^{-1/2}\exp\left(-\frac{(z - \mu_1 - \mu_2)^2/2}{\sqrt{\sigma_1^2 + \sigma_2^2}}\right)
\]
so that $Z \sim \text{Normal}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$.
\end{solution}

\begin{exercise}
\label{exer:2.9.15}
Let $X$ and $Y$ be independent, with $X \sim \text{Gamma}(\alpha_1, \lambda)$ and $Y \sim \text{Gamma}(\alpha_2, \lambda)$. Let $Z = X + Y$. Use Theorem~\ref{thm:2.9.3}(b) to prove that $Z \sim \text{Gamma}(\alpha_1 + \alpha_2, \lambda)$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    f_Z(z) &= \int_{-\infty}^{\infty}f_X(x)f_Y(z - x)\,\mathrm{d}x \\
    &= \int_0^z \Gamma(\alpha_1)^{-1}\lambda^{\alpha_1}x^{\alpha_1-1}e^{-\lambda x}\Gamma(\alpha_2)^{-1}\lambda^{\alpha_2}(z - x)^{\alpha_2-1}e^{-\lambda(z-x)}\,\mathrm{d}x \\
    &= \frac{1}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\lambda^{\alpha_1+\alpha_2}e^{-\lambda z}\int_0^z x^{\alpha_1-1}(z - x)^{\alpha_2-1}\,\mathrm{d}x.
\end{align*}
We recognize this integral as a Beta integral, with
\[
    \int_0^z x^{\alpha_1-1}(z - x)^{\alpha_2-1}\,\mathrm{d}x = z^{\alpha_1+\alpha_2-1}\Gamma(\alpha_1)\Gamma(\alpha_2)/\Gamma(\alpha_1 + \alpha_2).
\]
Hence, $f_Z(z) = \Gamma(\alpha_1 + \alpha_2)^{-1}\lambda^{\alpha_1+\alpha_2}z^{\alpha_1+\alpha_2-1}e^{-\lambda z}$, so that $Z \sim \text{Gamma}(\alpha_1 + \alpha_2, \lambda)$.
\end{solution}

\begin{exercise}
\label{exer:2.9.16}
(MV) Show that when $Z_1, Z_2$ are i.i.d.\ $N(0, 1)$ and $X, Y$ are given by \eqref{eq:2.7.1}, then $(X, Y) \sim \text{Bivariate Normal}(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$.
\end{exercise}

\begin{solution}
The joint density of $(Z_1, Z_2)$ is $(2\pi)^{-1}\exp\{-(z_1^2 + z_2^2)/2\}$. The inverse of the transformation given by (2.7.1) is $Z_1 = (X - \mu_1)/\sigma_1$, $Z_2 = ((Y - \mu_2)/\sigma_2 - \rho(X - \mu_1)/\sigma_1)/\sqrt{1 - \rho^2}$, and this has Jacobian
\[
    \left|\det\begin{pmatrix} 1/\sigma_1 & 0 \\ -\rho/(\sigma_1\sqrt{1 - \rho^2}) & 1/(\sigma_2\sqrt{1 - \rho^2}) \end{pmatrix}\right| = (\sigma_1\sigma_2\sqrt{1 - \rho^2})^{-1}.
\]
So the joint density of $(X, Y)$ is given by
\begin{align*}
    &\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}\exp\left\{-\frac{1}{2(1 - \rho^2)}\left[(1 - \rho^2)\left(\frac{X-\mu_1}{\sigma_1}\right)^2 + \rho^2\left(\frac{X-\mu_1}{\sigma_1}\right)^2 - 2\rho\left(\frac{X-\mu_1}{\sigma_1}\right)\left(\frac{Y-\mu_2}{\sigma_2}\right) + \left(\frac{Y-\mu_2}{\sigma_2}\right)^2\right]\right\} \\
    &= \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}\exp\left\{-\frac{1}{2(1 - \rho^2)}\left[\left(\frac{X-\mu_1}{\sigma_1}\right)^2 - 2\left(\frac{X-\mu_1}{\sigma_1}\right)\left(\frac{Y-\mu_2}{\sigma_2}\right) + \left(\frac{Y-\mu_2}{\sigma_2}\right)^2\right]\right\}
\end{align*}
and this proves the result.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simulating Probability Distributions}
\label{sec:2.10}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

So far, we have been concerned primarily with mathematical theory and manipulations of probabilities and random variables. However, modern high-speed computers can be used to simulate probabilities and random variables numerically. Such simulations have many applications, including:

\begin{itemize}
\item To approximate quantities that are too difficult to compute mathematically
\item To graphically simulate complicated physical or biological systems
\item To randomly sample from large data sets to search for errors or illegal activities, etc.
\item To implement complicated algorithms to sharpen pictures, recognize speech, etc.
\item To simulate intelligent behavior
\item To encrypt data or generate passwords
\item To solve puzzles or break codes by trying lots of random solutions
\item To generate random choices for online quizzes, computer games, etc.
\end{itemize}

Indeed, as computers become faster and more widespread, probabilistic simulations are becoming more and more common in software applications, scientific research, quality control, marketing, law enforcement, etc.

In most applications of probabilistic simulation, the first step is to simulate random variables having certain distributions. That is, a certain probability distribution will be specified, and we want to generate one or more random variables having that distribution.

Now, nearly all modern computer languages come with a \emph{pseudorandom number generator}, which is a device for generating a sequence $U_1, U_2, \ldots$ of random values that are approximately independent and have approximately the uniform distribution on $[0, 1]$. Now, in fact, the $U_i$ are usually generated from some sort of deterministic iterative procedure, which is designed to ``appear'' random. So the $U_i$ are, in fact, not random, but rather \emph{pseudorandom}.

Nevertheless, we shall ignore any concerns about pseudorandomness and shall simply assume that
\begin{equation}
\label{eq:2.10.1}
U_1, U_2, U_3, \ldots \sim \text{Uniform}[0, 1],
\end{equation}
i.e., the $U_i$ are i.i.d.\ Uniform$[0, 1]$.

Hence, if all we ever need are Uniform$[0, 1]$ random variables, then according to \eqref{eq:2.10.1}, we are all set. However, in most applications, other kinds of randomness are also required. We therefore consider how to use the uniform random variables of \eqref{eq:2.10.1} to generate random variables having other distributions.

\begin{example}[{The Uniform$[L, R]$ Distribution}]
\label{ex:2.10.1}
Suppose we want to generate $X \sim \text{Uniform}[L, R]$. According to Exercise~\ref{exer:2.6.1}, we can simply set
\[
X = (R - L) U_1 + L
\]
to ensure that $X \sim \text{Uniform}[L, R]$.
\end{example}

\subsection{Simulating Discrete Distributions}
\label{ssec:2.10.1}

We now consider the question of how to simulate from discrete distributions.

\begin{example}[The Bernoulli$(\theta)$ Distribution]
\label{ex:2.10.2}
Suppose we want to generate $X \sim \text{Bernoulli}(\theta)$, where $0 < \theta < 1$. We can simply set
\[
X = \begin{cases}
1 & U_1 \leqslant \theta \\
0 & U_1 > \theta.
\end{cases}
\]
Then clearly, we always have either $X = 0$ or $X = 1$. Furthermore, $\prb(X = 1) = \prb(U_1 \leqslant \theta) = \theta$ because $U_1 \sim \text{Uniform}[0, 1]$. Hence, we see that $X \sim \text{Bernoulli}(\theta)$.
\end{example}

\begin{example}[The Binomial$(n, \theta)$ Distribution]
\label{ex:2.10.3}
Suppose we want to generate $Y \sim \text{Binomial}(n, \theta)$, where $0 < \theta < 1$ and $n \geqslant 1$. There are two natural methods for doing this.

First, we can simply define $Y$ as follows:
\[
Y = \min\left\{j : \sum_{k=0}^{j} \binom{n}{k} \theta^k (1 - \theta)^{n-k} \geqslant U_1\right\}.
\]
That is, we let $Y$ be the largest value of $j$ such that the sum of the binomial probabilities up to $j - 1$ is still no more than $U_1$. In that case,
\[
\prb(Y = y) = \prb\left(\sum_{k=0}^{y-1} \binom{n}{k} \theta^k (1 - \theta)^{n-k} < U_1 \leqslant \sum_{k=0}^{y} \binom{n}{k} \theta^k (1 - \theta)^{n-k}\right) = \binom{n}{y} \theta^y (1 - \theta)^{n-y}.
\]
Hence, we have $Y \sim \text{Binomial}(n, \theta)$, as desired.

Alternatively, we can set
\[
X_i = \begin{cases}
1 & U_i \leqslant \theta \\
0 & U_i > \theta
\end{cases}
\]
for $i = 1, 2, 3, \ldots$. Then, by Example~\ref{ex:2.10.2}, we have $X_i \sim \text{Bernoulli}(\theta)$ for each $i$, with the $X_i$ independent because the $U_i$ are independent. Hence, by the observation at the end of Example~2.3.3, if we set $Y = X_1 + \cdots + X_n$, then we will again have $Y \sim \text{Binomial}(n, \theta)$.
\end{example}

In Example~\ref{ex:2.10.3}, the second method is more elegant and is also simpler computationally (as it does not require computing any binomial coefficients). On the other hand, the first method of Example~\ref{ex:2.10.3} is more general, as the following theorem shows.

\begin{theorem}
\label{thm:2.10.1}
Let $p$ be a probability function for a discrete probability distribution. Let $x_1, x_2, x_3, \ldots$ be all the values for which $p(x_i) > 0$. Let $U_1 \sim \text{Uniform}[0, 1]$. Define $Y$ by
\[
Y = \min\left\{x_j : \sum_{k=1}^{j} p(x_k) \geqslant U_1\right\}.
\]
Then $Y$ is a discrete random variable, having probability function $p$.
\end{theorem}

\begin{proof}
We have
\[
\prb(Y = x_i) = \prb\left(\sum_{k=1}^{i-1} p(x_k) < U_1 \leqslant \sum_{k=1}^{i} p(x_k)\right) = \sum_{k=1}^{i} p(x_k) - \sum_{k=1}^{i-1} p(x_k) = p(x_i).
\]
Also, clearly $\prb(Y = y) = 0$ if $y \notin \{x_1, x_2, \ldots\}$. Hence, for all $y \in \mathbf{R}^1$, we have $\prb(Y = y) = p(y)$, as desired.
\end{proof}

\begin{example}[The Geometric$(\theta)$ Distribution]
\label{ex:2.10.4}
To simulate $Y \sim \text{Geometric}(\theta)$, we again have two choices. Using Theorem~\ref{thm:2.10.1}, we can let $U_1 \sim \text{Uniform}[0, 1]$ and then set
\[
Y = \min\left\{j : \sum_{k=0}^{j} (1 - \theta)^k \theta \geqslant U_1\right\} = \min\left\{j : 1 - (1 - \theta)^{j+1} \geqslant U_1\right\} = \left\lfloor\frac{\log(1 - U_1)}{\log(1 - \theta)}\right\rfloor,
\]
where $\lfloor r \rfloor$ means to round down $r$ to the next integer value, i.e., $\lfloor r \rfloor$ is the greatest integer not exceeding $r$ (sometimes called the \emph{floor} of $r$).

Alternatively, using the definition of Geometric$(\theta)$ from Example~2.3.4, we can set
\[
X_i = \begin{cases}
1 & U_i \leqslant \theta \\
0 & U_i > \theta
\end{cases}
\]
for $i = 1, 2, 3, \ldots$ (where $U_i \sim \text{Uniform}[0, 1]$), and then let $Y = \min\{i : X_i = 1\}$.

Either way, we have $Y \sim \text{Geometric}(\theta)$, as desired.
\end{example}

\subsection{Simulating Continuous Distributions}
\label{ssec:2.10.2}

We next turn to the subject of simulating absolutely continuous distributions. In general, this is not an easy problem. However, for certain particular continuous distributions, it is not difficult, as we now demonstrate.

\begin{example}[{The Uniform$[L, R]$ Distribution}]
\label{ex:2.10.5}
We have already seen in Example~\ref{ex:2.10.1} that if $U_1 \sim \text{Uniform}[0, 1]$, and we set
\[
X = (R - L) U_1 + L,
\]
then $X \sim \text{Uniform}[L, R]$. Thus, simulating from any uniform distribution is straightforward.
\end{example}

\begin{example}[The Exponential$(\lambda)$ Distribution]
\label{ex:2.10.6}
We have also seen, in Example~\ref{ex:2.6.6}, that if $U_1 \sim \text{Uniform}[0, 1]$, and we set
\[
Y = -\ln(1 - U_1),
\]
then $Y \sim \text{Exponential}(1)$. Thus, simulating from the Exponential$(1)$ distribution is straightforward.

Furthermore, we know from Exercise~\ref{exer:2.6.4} that once $Y \sim \text{Exponential}(1)$, then if $\lambda > 0$ and we set
\[
Z = Y/\lambda = -\ln(1 - U_1)/\lambda,
\]
then $Z \sim \text{Exponential}(\lambda)$. Thus, simulating from any Exponential distribution is also straightforward.
\end{example}

\begin{example}[The $N(\mu, \sigma^2)$ Distribution]
\label{ex:2.10.7}
Simulating from the standard normal distribution, $N(0, 1)$, may appear to be more difficult. However, by Example~\ref{ex:2.9.3}, if $U_1 \sim \text{Uniform}[0, 1]$ and $U_2 \sim \text{Uniform}[0, 1]$, with $U_1$ and $U_2$ independent, and we set
\begin{equation}
\label{eq:2.10.2}
X = \sqrt{-2\log(1 - U_1)} \cos(2\pi U_2), \quad Y = \sqrt{-2\log(1 - U_1)} \sin(2\pi U_2),
\end{equation}
then $X \sim N(0, 1)$ and $Y \sim N(0, 1)$ (and furthermore, $X$ and $Y$ are independent). So, using this trick, the standard normal distribution can be easily simulated as well.

It then follows from Exercise~\ref{exer:2.6.3} that, once we have $X \sim N(0, 1)$, if we set $Z = \sigma X + \mu$, then $Z \sim N(\mu, \sigma^2)$. Hence, it is straightforward to sample from any normal distribution.
\end{example}

These examples illustrate that, for certain special continuous distributions, sampling from them is straightforward. To provide a general method of sampling from a continuous distribution, we first state the following definition.

\begin{definition}
\label{def:2.10.1}
Let $X$ be a random variable, with cumulative distribution function $F$. Then the \emph{inverse cdf} (or \emph{quantile function}) of $X$ is the function $F^{-1}$ defined by
\[
F^{-1}(t) = \min\{x : F(x) \geqslant t\}
\]
for $0 < t < 1$.
\end{definition}

In Figure~\ref{fig:2.10.1}, we have provided a plot of the inverse cdf of an $N(0, 1)$ distribution. Note that this function goes to $-\infty$ as the argument goes to $0$, and goes to $\infty$ as the argument goes to $1$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig2-10-1.pdf}
\caption{The inverse cdf of the $N(0, 1)$ distribution.}
\label{fig:2.10.1}
\end{figure}

Using the inverse cdf, we obtain a general method of sampling from a continuous distribution, as follows.

\begin{theorem}[Inversion method for generating random variables]
\label{thm:2.10.2}
Let $F$ be any cumulative distribution function, and let $U \sim \text{Uniform}[0, 1]$. Define a random variable $Y$ by $Y = F^{-1}(U)$. Then $\prb(Y \leqslant y) = F(y)$, i.e., $Y$ has cumulative distribution function given by $F$.
\end{theorem}

\begin{proof}
We begin by noting that $\prb(Y \leqslant y) = \prb(F^{-1}(U) \leqslant y)$. But $F^{-1}(U)$ is the smallest value $x$ such that $F(x) \geqslant U$. Hence, $F^{-1}(U) \leqslant y$ if and only if $F(y) \geqslant U$, i.e., $U \leqslant F(y)$. Therefore,
\[
\prb(Y \leqslant y) = \prb(F^{-1}(U) \leqslant y) = \prb(U \leqslant F(y)).
\]
But $0 \leqslant F(y) \leqslant 1$, and $U \sim \text{Uniform}[0, 1]$, so $\prb(U \leqslant F(y)) = F(y)$. Thus,
\[
\prb(Y \leqslant y) = \prb(U \leqslant F(y)) = F(y).
\]
It follows that $F$ is the cdf of $Y$, as claimed.
\end{proof}

We note that Theorem~\ref{thm:2.10.2} is valid for any cumulative distribution function, whether it corresponds to a continuous distribution, a discrete distribution, or a mixture of the two (as in Section~\ref{ssec:2.5.4}). In fact, this was proved for discrete distributions in Theorem~\ref{thm:2.10.1}.

\begin{example}[Generating from an Exponential Distribution]
\label{ex:2.10.8}
Let $F$ be the cdf of an Exponential$(1)$ random variable. Then
\[
F(x) = \int_0^x e^{-t} \, \mathrm{d}t = 1 - e^{-x}.
\]
It then follows that
\[
F^{-1}(t) = \min\{x : F(x) \geqslant t\} = \min\{x : 1 - e^{-x} \geqslant t\} = -\ln(1 - t) = \ln(1/(1 - t)).
\]
Therefore, by Theorem~\ref{thm:2.10.2}, if $U \sim \text{Uniform}[0, 1]$, and we set
\begin{equation}
\label{eq:2.10.3}
Y = F^{-1}(U) = \ln(1/(1 - U)),
\end{equation}
then $Y \sim \text{Exponential}(1)$.

Now, we have already seen from Example~\ref{ex:2.6.6} that, if $U \sim \text{Uniform}[0, 1]$, and we set $Y = -\ln(1 - U)$, then $Y \sim \text{Exponential}(1)$. This is essentially the same as \eqref{eq:2.10.3}, except that we have replaced $U$ by $1 - U$. On the other hand, this is not surprising, because we already know by Exercise~\ref{exer:2.6.2} that, if $U \sim \text{Uniform}[0, 1]$, then also $1 - U \sim \text{Uniform}[0, 1]$.
\end{example}

\begin{example}[Generating from the Standard Normal Distribution]
\label{ex:2.10.9}
Let $\Phi$ be the cdf of a $N(0, 1)$ random variable, as in Definition~\ref{def:2.5.2}. Then
\[
\Phi^{-1}(t) = \min\{x : \Phi(x) \geqslant t\},
\]
and there is no simpler formula for $\Phi^{-1}(t)$. By Theorem~\ref{thm:2.10.2}, if $U \sim \text{Uniform}[0, 1]$, and we set
\begin{equation}
\label{eq:2.10.4}
Y = \Phi^{-1}(U),
\end{equation}
then $Y \sim N(0, 1)$.

On the other hand, due to the difficulties of computing with $\Phi$ and $\Phi^{-1}$, the method of \eqref{eq:2.10.4} is not very practical. It is far better to use the method of \eqref{eq:2.10.2}, to simulate a normal random variable.
\end{example}

For distributions that are too complicated to sample using the inversion method of Theorem~\ref{thm:2.10.2}, and for which no simple trick is available, it may still be possible to do sampling using Markov chain methods, which we will discuss in later chapters, or by rejection sampling (see Challenge~\ref{exer:2.10.21}).

\subsection*{Summary of Section~\ref{sec:2.10}}

\begin{itemize}
\item It is important to be able to simulate probability distributions.
\item If $X$ is discrete, taking the value $x_i$ with probability $p_i$, where $x_1 < x_2 < \cdots$, and $U \sim \text{Uniform}[0, 1]$, and $Y = \min\{x_j : \sum_{k=1}^{j} p_k \geqslant U\}$, then $Y$ has the same distribution as $X$. This method can be used to simulate virtually any discrete distribution.
\item If $F$ is any cumulative distribution with inverse cdf $F^{-1}$, $U \sim \text{Uniform}[0, 1]$, and $Y = F^{-1}(U)$, then $Y$ has cumulative distribution function $F$. This allows us to simulate virtually any continuous distribution.
\item There are simple methods of simulating many standard distributions, including the binomial, uniform, exponential, and normal.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:2.10.1}
Let $Y$ be a discrete random variable with $\prb(Y = 7) = 1/2$, $\prb(Y = -2) = 1/3$, and $\prb(Y = 5) = 1/6$. Find a formula for $Z$ in terms of $U$, such that if $U \sim \text{Uniform}[0, 1]$, then $Z$ has the same distribution as $Y$.
\end{exercise}

\begin{solution}
We can let $Z = -7$ if $U \leqslant 1/2$, $Z = -2$ if $1/2 < U \leqslant 5/6$, and $Z = 5$ if $U > 5/6$.
\end{solution}

\begin{exercise}
\label{exer:2.10.2}
For each of the following cumulative distribution functions $F$, find a formula for $X$ in terms of $U$, such that if $U \sim \text{Uniform}[0, 1]$, then $X$ has cumulative distribution function $F$.
\begin{enumerate}[(a)]
\item $F(x) = \begin{cases} 0 & x < 0 \\ x & 0 \leqslant x \leqslant 1 \\ 1 & x > 1. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < 0 \\ x^2 & 0 \leqslant x \leqslant 1 \\ 1 & x > 1. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < 0 \\ x^2/9 & 0 \leqslant x \leqslant 3 \\ 1 & x > 3. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < 1 \\ x^2/9 & 1 \leqslant x \leqslant 3 \\ 1 & x > 3. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < 0 \\ x^5/32 & 0 \leqslant x \leqslant 2 \\ 1 & x > 2. \end{cases}$
\item $F(x) = \begin{cases} 0 & x < 0 \\ 1/3 & 0 \leqslant x < 7 \\ 3/4 & 7 \leqslant x < 11 \\ 1 & x \geqslant 11. \end{cases}$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Here $F^{-1}(t) = t$, so we can let $X = U$.
    \item Here $F^{-1}(t) = \sqrt{t}$, so we can let $X = \sqrt{U}$.
    \item Here $F^{-1}(t) = 3\sqrt{t}$, so we can let $X = 3\sqrt{U}$.
    \item Here $F^{-1}(t) = 3\sqrt{t}$ for $t \geqslant 1/9$, with $F^{-1}(t) = 1$ for $t \leqslant 1/9$. Hence, we can let $X = 1$ for $U \leqslant 1/9$, with $X = 3\sqrt{U}$ for $U > 1/9$.
    \item Here $F^{-1}(t) = 5t^{1/5}$, so we can let $X = 5U^{1/5}$.
    \item Here $F^{-1}(t)$ equals 0 for $t \leqslant 1/3$, and equals 7 for $1/3 < t \leqslant 3/4$, and equals 11 for $t > 3/4$. Hence, we can let $X = 0$ for $U \leqslant 1/3$, $X = 7$ for $1/3 < U \leqslant 3/4$, and $X = 11$ for $U > 3/4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.10.3}
Suppose $U \sim \text{Uniform}[0, 1]$, and $Y = -\ln(1 - U)/3$. What is the distribution of $Y$?
\end{exercise}

\begin{solution}
Since $U \in [0, 1]$, the range $Y$ is $[0, \infty)$. For $y \in [0, \infty)$,
\[
    \prb(Y \leqslant y) = \prb(\ln(1/U)/3 \leqslant y) = \prb(1/U \leqslant e^{3y}) = \prb(U \geqslant e^{-3y}) = 1 - e^{-3y}.
\]
Hence, the density of $Y$ is $f_Y(y) = \frac{d}{\mathrm{d}y}\prb(Y \leqslant y) = \frac{d}{\mathrm{d}y}(1 - e^{-3y}) = 3e^{-3y}$ that is a density of $\text{Exponential}(3)$. Therefore, $Y \sim \text{Exponential}(3)$.
\end{solution}

\begin{exercise}
\label{exer:2.10.4}
Generalizing the previous question, suppose $U \sim \text{Uniform}[0, 1]$ and $W = -\ln(1 - U)/\lambda$ for some fixed $\lambda > 0$.
\begin{enumerate}[(a)]
\item What is the distribution of $W$?
\item Does this provide a way of simulating from a certain well-known distribution? Explain.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item From Exercise \ref{exer:2.10.3}, $Y = \ln(1/U)/3 \sim \text{Exponential}(3)$. Note $W = \ln(1/U)/\lambda = Y(3/\lambda)$. It is not hard to show that $\ln(1/U) \sim \text{Exponential}(1)$.
    \[
        \prb(W \leqslant w) = \prb(Y(3/\lambda) \leqslant w) = \prb(Y \leqslant w\lambda/3) = 1 - e^{-3(w\lambda/3)} = 1 - e^{-\lambda w}.
    \]
    Hence, the density of $W$ is $f_W(w) = \frac{d}{\mathrm{d}w}\prb(W \leqslant w) = \frac{d}{\mathrm{d}w}(1 - e^{-\lambda w}) = \lambda e^{-\lambda w}$ that is a density of $\text{Exponential}(\lambda)$. Therefore, $W \sim \text{Exponential}(\lambda)$.
    \item It is not difficult to generate a pseudo random number $u$ having $\text{Uniform}[0, 1]$ distribution. Then, $y = \ln(1/u)/\lambda$ has an $\text{Exponential}(\lambda)$ distribution.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.10.5}
Let $U_1 \sim \text{Uniform}[0, 1]$ and $U_2 \sim \text{Uniform}[0, 1]$ be independent, and let $X = c_1 \sqrt{-\log(1 - U_1)} \cos(2\pi U_2) + c_2$. Find values of $c_1$ and $c_2$ such that $X \sim N(5, 9)$.
\end{exercise}

\begin{solution}
In Example \ref{ex:2.10.7}, it is shown that $X_1 = \sqrt{2\ln(1/U_1)}\cos(2\pi U_2)$ has a $N(0, 1)$ distribution and $X = X_1 c_1/\sqrt{2} + c_2 \sim N(c_2, c_1^2/2)$. Hence, $c_2 = 5$ and $c_1^2/2 = 9$. The solution is $c_1 = \pm 3\sqrt{2}$ and $c_2 = 5$.
\end{solution}

\begin{exercise}
\label{exer:2.10.6}
Let $U \sim \text{Uniform}[0, 1]$. Find a formula for $Y$ in terms of $U$, such that $\prb(Y = 3) = \prb(Y = 4) = 2/5$ and $\prb(Y = 7) = 1/5$, otherwise $\prb(Y = y) = 0$.
\end{exercise}

\begin{solution}
Let $Y = 3$ if $0 \leqslant U \leqslant 2/5$, $Y = 4$ if $2/5 < U \leqslant 4/5$, and $Y = 7$ if $U > 4/5$. Then, $Y = 3\indc_{[0,2/5]}(U) + 4\indc_{(2/5,4/5]}(U) + 7\indc_{(4/5,1]}(U)$. Hence, $\prb(Y = 3) = \prb(0 \leqslant U \leqslant 2/5) = 2/5$, $\prb(Y = 4) = \prb(2/5 < U \leqslant 4/5) = \prb(U \leqslant 4/5) - \prb(U \leqslant 2/5) = 4/5 - 2/5 = 2/5$, and $\prb(Y = 7) = \prb(4/5 < U \leqslant 1) = \prb(U \leqslant 1) - \prb(U \leqslant 4/5) = 1 - 4/5 = 1/5$. For any $y \notin \{3, 4, 7\}$, $\prb(Y = y) = \prb(U \notin [0, 1]) = 0$.
\end{solution}

\begin{exercise}
\label{exer:2.10.7}
Suppose $\prb(X = 1) = 1/3$, $\prb(X = 2) = 1/6$, $\prb(X = 4) = 1/2$, and $\prb(X = x) = 0$ otherwise.
\begin{enumerate}[(a)]
\item Compute the cdf $F_X(x)$ for all $x \in \mathbf{R}^1$.
\item Compute the inverse cdf $F_X^{-1}(t)$ for all $t \in \mathbf{R}^1$.
\item Let $U \sim \text{Uniform}[0, 1]$. Find a formula for $Y$ in terms of $U$, such that $Y$ has cdf $F_X$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item By definition, $F_X(x) = \prb(X \leqslant x)$. Hence, $F_X(x) = 0$ for $x < 1$. For $1 \leqslant x < 2$, $F_X(x) = \prb(X \leqslant x) = \prb(X = 1) = 1/3$. For $2 \leqslant x < 4$, $F_X(x) = \prb(X \leqslant x) = \prb(X = 1 \text{ or } X = 2) = \prb(X = 1) + \prb(X = 2) = 1/2$. For $x \geqslant 4$, $F_X(x) = \prb(X \leqslant x) \geqslant \prb(X \leqslant 4) \geqslant \prb(X = 1) + \prb(X = 2) + \prb(X = 4) = 1$ implies $F_X(x) = 1$.
    \item The range of $t$ must be restricted on $(0, 1]$ because $F_X^{-1}(0) = -\infty$. $F_X^{-1}(t) = 1$ for $t \in (0, 1/3]$, $F_X^{-1}(t) = 2$ for $t \in (1/3, 1/2]$, and $F_X^{-1}(t) = 4$ for $t \in (1/2, 1]$.
    \item Let $Y = F_X^{-1}(U)$. Then $F_Y(y) = \prb(Y \leqslant y)$ is the same to $F_X$. For $y < 1$, $F_Y(y) = \prb(Y \leqslant y) = \prb(F_X^{-1}(U) \leqslant y) = \prb(\varnothing) = 0$. For $1 \leqslant y < 2$, $F_Y(y) = \prb(F_X^{-1}(U) \leqslant y) = \prb(F_X^{-1}(U) = 1) = \prb(U \in (0, 1/3]) = 1/3$. For $2 \leqslant y < 4$, $F_Y(y) = \prb(F_X^{-1}(U) \leqslant y) = \prb(F_X^{-1}(U) = 1 \text{ or } 2) = \prb(U \in (0, 1/2]) = 1/2$. For $y \geqslant 4$, $F_Y(y) = \prb(F_X^{-1}(U) \leqslant y) = \prb(F_X^{-1}(U) = 1 \text{ or } 2 \text{ or } 4) = \prb(U \in (0, 1]) = 1$. Hence, the cdf $F_Y$ of $Y$ is the same to $F_X$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.10.8}
Let $X$ have density function $f_X(x) = 3(x - 2)$ for $0 \leqslant x \leqslant 1$, otherwise $f_X(x) = 0$.
\begin{enumerate}[(a)]
\item Compute the cdf $F_X(x)$ for all $x \in \mathbf{R}^1$.
\item Compute the inverse cdf $F_X^{-1}(t)$ for all $t \in \mathbf{R}^1$.
\item Let $U \sim \text{Uniform}[0, 1]$. Find a formula for $Y$ in terms of $U$, such that $Y$ has density $f$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item From the density, $F_X(x) = 0$ for all $x \leqslant 0$, and $F_X(x) = 1$ for all $x \geqslant 1$. For $x \in (0, 1)$,
    \[
        F_X(x) = \prb(X \leqslant x) = \int_0^x f_X(y)\,\mathrm{d}y = \frac{3}{4}\int_0^x \sqrt{y}\,\mathrm{d}y = \frac{3}{4}\cdot\frac{2}{3}y^{3/2}\bigg|_{y=0}^{y=x} = x^{3/2}/2.
    \]
    \item For $t \in (0, 1]$, we will find $x$ satisfying $t = F_X(x) = x^{3/2}/2$. Hence, $F_X^{-1}(t) = x = (2t)^{2/3}$.
    \item Let $Y = F_X^{-1}(U)$. Then, by Theorem \ref{thm:2.10.2}, $Y$ has the cdf $F_X$. The density $f_Y$ of $Y$ is
    \[
        f_Y(y) = \frac{d}{\mathrm{d}y}\prb(Y \leqslant y) = \frac{d}{\mathrm{d}y}F_X(y) = \frac{d}{\mathrm{d}y}\frac{y^{3/2}}{2} = \frac{y^{1/2} \cdot 3}{4}.
    \]
    Hence, $f_Y = f_X$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:2.10.9}
Let $U \sim \text{Uniform}[0, 1]$. Find a formula for $Z$ in terms of $U$, such that $Z$ has density $f_Z(z) = 4z^3$ for $0 \leqslant z \leqslant 1$, otherwise $f_Z(z) = 0$.
\end{exercise}

\begin{solution}
The cdf of $Z$ is given by, for $z \in (0, 1)$,
\[
    F_Z(z) = \prb(Z \leqslant z) = \int_0^z 4y^3\,\mathrm{d}y = y^4\big|_{y=0}^{y=z} = z^4.
\]
For $t \in (0, 1]$, we can solve the equation $t = F_Z(z) = z^4$ for the inverse cdf $F_Z^{-1}(t) = z = t^{1/4}$. Hence, $Y = F_Z^{-1}(U) = U^{1/4}$ has the cdf $F_Z$ and the density $f_Z$.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:2.10.10}
For each of the following distributions, use the computer (you can use any algorithms available to you as part of a software package) to simulate $X_1, X_2, \ldots, X_N$ i.i.d.\ having the given distribution. (Take $N = 1000$ at least, with $N = 10{,}000$ or $N = 100{,}000$ if possible.) Then compute $\bar{X} = (1/N) \sum_{i=1}^{N} X_i$ and $(1/N) \sum_{i=1}^{N} (X_i - \bar{X})^2$.
\begin{enumerate}[(a)]
\item Uniform$[0, 1]$
\item Uniform$[5, 8]$
\item Bernoulli$(1/3)$
\item Binomial$(12, 1/3)$
\item Geometric$(1/5)$
\item Exponential$(1)$
\item Exponential$(13)$
\item $N(0, 1)$
\item $N(5, 9)$
\end{enumerate}
\end{exercise}

\subsection*{Problems}

\begin{exercise}
\label{exer:2.10.11}
Let $G(x) = p_1 F_1(x) + p_2 F_2(x) + \cdots + p_k F_k(x)$, where $p_i > 0$, $\sum_i p_i = 1$, and $F_i$ are cdfs, as in \eqref{eq:2.5.3}. Suppose we can generate $X_i$ to have cdf $F_i$, for $i = 1, 2, \ldots, k$. Describe a procedure for generating a random variable $Y$ that has cdf $G$.
\end{exercise}

\begin{solution}
First choose a random variable $I$, independent of all the $X_i$, such that $I \in \{1, 2, \ldots, k\}$, with $\prb(I = i) = \alpha_i$. Then set $Y = X_I$. [That is, $Y$ is equal to $X_i$ for the choice $i = I$.] Then $\prb(Y \leqslant y) = \sum_i \prb(I = i)\prb(Y \leqslant y \mid I = i) = \sum_i \alpha_i F_i(y) = G(y)$, as desired.
\end{solution}

\begin{exercise}
\label{exer:2.10.12}
Let $X$ be an absolutely continuous random variable, with density given by $f_X(x) = x^{-2}$ for $x \geqslant 1$, with $f_X(x) = 0$ otherwise. Find a formula for $Z$ in terms of $U$, such that if $U \sim \text{Uniform}[0, 1]$, then $Z$ has the same distribution as $X$.
\end{exercise}

\begin{solution}
Here $F_X(x) = 0$ for $x < 1$, while for $x \geqslant 1$, $F_X(x) = \int_{-\infty}^{x}f_X(t)\,dt = \int_1^x t^{-2}\,dt = -t^{-1}\big|_{t=1}^{t=x} = 1 - (1/x)$. Hence, $F^{-1}(t) = 1/(1 - t)$. Thus, we can let $Z = 1/(1 - U)$.
\end{solution}

\begin{exercise}
\label{exer:2.10.13}
Find the inverse cdf of the logistic distribution of Problem~\ref{exer:2.4.18}. (Hint: See Problem~\ref{exer:2.5.20}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.5.20} we have that $F(x) = (1 + e^{-x})^{-1} = u$, so inverting this we have that $x = F^{-1}(u) = \ln(u/(1 - u))$ for $0 \leqslant u \leqslant 1$.
\end{solution}

\begin{exercise}
\label{exer:2.10.14}
Find the inverse cdf of the Weibull distribution of Problem~\ref{exer:2.4.19}. (Hint: See Problem~\ref{exer:2.5.21}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.5.21} we have that $F(x) = 1 - \exp\{-x^{\alpha}\} = u$ for $x > 0$, so inverting this we have that $x = F^{-1}(u) = (-\ln(1 - u))^{1/\alpha}$ for $0 \leqslant u \leqslant 1$.
\end{solution}

\begin{exercise}
\label{exer:2.10.15}
Find the inverse cdf of the Pareto distribution of Problem~\ref{exer:2.4.20}. (Hint: See Problem~\ref{exer:2.5.22}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.5.22} we have that $F(x) = 1 - (1 + x)^{-\alpha} = u$ for $x > 0$, so inverting this we have that $x = F^{-1}(u) = (1 - u)^{-1/\alpha} - 1$ for $0 \leqslant u \leqslant 1$.
\end{solution}

\begin{exercise}
\label{exer:2.10.16}
Find the inverse cdf of the Cauchy distribution of Problem~\ref{exer:2.4.21}. (Hint: See Problem~\ref{exer:2.5.23}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.5.23} we have that $F(x) = (\arctan(x) + \pi/2)/\pi = u$, so inverting this we have that $x = F^{-1}(u) = \tan(\pi u - \pi/2)$ for $0 \leqslant u \leqslant 1$.
\end{solution}

\begin{exercise}
\label{exer:2.10.17}
Find the inverse cdf of the Laplace distribution of Problem~\ref{exer:2.4.22}. (Hint: See Problem~\ref{exer:2.5.24}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.5.24} we have that $F(x) = \frac{1}{2}\int_{-\infty}^{x}e^z\,dz = \frac{1}{2}e^x = u$ for $x \leqslant 0$, and $F(x) = \frac{1}{2} + \frac{1}{2}\int_0^x e^{-z}\,dz = \frac{1}{2} + \frac{1}{2}(1 - e^{-x}) = u$ for $x > 0$. So, for $0 \leqslant u \leqslant 1/2$, inverting this we have that $x = F^{-1}(u) = \ln(2u)$ and, for $1/2 \leqslant u \leqslant 1$, $x = F^{-1}(u) = -\ln 2(1 - u)$.
\end{solution}

\begin{exercise}
\label{exer:2.10.18}
Find the inverse cdf of the extreme value distribution of Problem~\ref{exer:2.4.23}. (Hint: See Problem~\ref{exer:2.5.25}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.5.25} we have that $F(x) = \exp\{-e^{-x}\} = u$, so inverting this we have that $x = F^{-1}(u) = -\ln(-\ln u)$ for $0 \leqslant u \leqslant 1$.
\end{solution}

\begin{exercise}
\label{exer:2.10.19}
Find the inverse cdfs of the beta distributions in Problem~\ref{exer:2.4.24}(b) through (d). (Hint: See Problem~\ref{exer:2.5.26}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.5.26} we have that
\begin{enumerate}[(b)]
    \item $u = F(x) = x$ for $0 < x < 1$, so $x = u$ for $0 \leqslant u \leqslant 1$.
    \item $u = F(x) = x^2$ for $0 < x < 1$, so $x = \sqrt{u}$ for $0 \leqslant u \leqslant 1$.
    \item $u = F(x) = 1 - (1 - x)^2$ for $0 < x < 1$, so $x = 1 - \sqrt{1 - u}$ for $0 \leqslant u \leqslant 1$.
\end{enumerate}
\end{solution}

\begin{exercise}[Method of composition]
\label{exer:2.10.20}
If we generate $X \sim f_X$ obtaining $x$ and then generate $Y$ from $f_{Y|X}(\cdot \mid x)$, prove that $Y \sim f_Y$.
\end{exercise}

\begin{solution}
We have that $\prb(Y \leqslant y) = \int_{-\infty}^{\infty}\bigl(\int_{-\infty}^{y}f_{Y|X}(z \mid x)\,dz\bigr)f_X(x)\,\mathrm{d}x = \int_{-\infty}^{y}\int_{-\infty}^{\infty}f(x, z)\,\mathrm{d}x\,dz = \int_{-\infty}^{y}f_Y(z)\,dz = F_Y(y)$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}[Rejection sampling]
\label{exer:2.10.21}
Suppose $f$ is a complicated density function. Suppose $g$ is a density function from which it is easy to sample (e.g., the density of a uniform or exponential or normal distribution). Suppose we know a value of $c$ such that $f(x) \leqslant c \, g(x)$ for all $x \in \mathbf{R}^1$. The following provides a method, called \emph{rejection sampling}, for sampling from a complicated density $f$ by using a simpler density $g$, provided only that we know $f(x) \leqslant c \, g(x)$ for all $x \in \mathbf{R}^1$.
\begin{enumerate}[(a)]
\item Suppose $Y$ has density $g$. Let $U \sim \text{Uniform}[0, c]$, with $U$ and $Y$ independent. Prove that
\[
\prb(a \leqslant Y \leqslant b, f(Y) \geqslant Ug(Y)) = \int_a^b f(x) \, \mathrm{d}x.
\]
(Hint: Use Theorem~\ref{thm:2.8.1} to show that $\prb(a \leqslant Y \leqslant b, f(Y) \geqslant cUg(Y)) = \int_a^b g(y) \, \prb(f(Y) \geqslant cUg(Y) \mid Y = y) \, \mathrm{d}y$.)
\item Suppose that $Y_1, Y_2, \ldots$ are i.i.d., each with density $g$, and independently $U_1, U_2, \ldots$ are i.i.d.\ Uniform$[0, c]$. Let $i_0 = 0$, and for $n \geqslant 1$, let $i_n = \min\{j > i_{n-1} : U_j \leqslant f(Y_j)/(cg(Y_j))\}$. Prove that $X_{i_1}, X_{i_2}, \ldots$ are i.i.d., each with density $f$. (Hint: Prove this for $X_{i_1}, X_{i_2}$.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item
    \begin{align*}
        \prb(a \leqslant Y \leqslant b \mid f(Y) \geqslant Ucg(Y)) &= \frac{\prb(a \leqslant Y \leqslant b, f(Y) \geqslant Ucg(Y))}{\prb(f(Y) \geqslant Ucg(Y))} \\
        &= \frac{\expc(\prb(a \leqslant y \leqslant b, f(Y) \geqslant Ucg(u) \mid Y = y))}{\expc(f(y) \geqslant Ucg(y) \mid Y = y)} = \frac{\expc(\indc_{(a,b)}(Y)f(Y)/cg(Y))}{\expc(f(Y)/cg(Y))} \\
        &= \frac{\expc(\prb(a \leqslant y \leqslant b, f(Y) \geqslant Ucg(u) \mid Y = y))}{\expc(f(y) \geqslant Ucg(y) \mid Y = y)} = \frac{\expc(\indc_{(a,b)}(Y)f(Y)/cg(Y))}{\expc(f(Y)/cg(Y))} \\
        &= \int_a^b \frac{f(y)}{cg(y)}g(y)\,\mathrm{d}y \bigg/ \int_{-\infty}^{\infty}\frac{f(y)}{cg(y)}g(y)\,\mathrm{d}y = \int_a^b f(y)\,\mathrm{d}y.
    \end{align*}
    \item Let $p = \prb(f(Y) \geqslant Ucg(Y))$. Then, using (a) and the independence of the $U_i$ and $Y_i$, we have that
    \begin{align*}
        \prb(X_{i_1} \leqslant x) &= \sum_{j=1}^{\infty}\prb(Y_j \leqslant x, i_1 = j) \\
        &= \sum_{j=1}^{\infty}\prb\left(Y_j \leqslant x, f(Y_1) < U_1 cg(Y_1), \ldots, f(Y_{j-1}) < U_{j-1}cg(Y_{j-1}), f(Y_j) \geqslant U_{j-1}cg(Y_j)\right) \\
        &= \sum_{j=1}^{\infty}\prb(Y_j \leqslant x, f(Y_j) \geqslant U_j cg(Y_j))\prb\left(f(Y_1) < U_1 cg(Y_1), \ldots, f(Y_{j-1}) < U_{j-1}cg(Y_{j-1})\right) \\
        &= \sum_{j=1}^{\infty}\prb(Y_j \leqslant x \mid f(Y_j) \geqslant U_j cg(Y_j))p(1 - p)^{j-1} = \int_{-\infty}^{x}f(y)\,\mathrm{d}y,
    \end{align*}
    so $X_{i_1} \sim f$.
    
    Further, we have that
    \begin{align*}
        &\prb(X_{i_1} \leqslant x_1, X_{i_2} \leqslant x_2) = \sum_{j_1=1}^{\infty}\sum_{j_2=j_1+1}^{\infty}\prb(Y_{j_1} \leqslant x_1, i_1 = j_1, Y_{j_2} \leqslant x_2, i_2 = j_2) \\
        &= \sum_{j_1=1}^{\infty}\sum_{j_2=j_1+1}^{\infty}\prb\left(\begin{array}{l}
            Y_{j_1} \leqslant x_1, f(Y_1) < U_1 cg(Y_1), \ldots, f(Y_{j_1-1}) < U_{j_1-1}cg(Y_{j_1-1}), f(Y_{j_1}) \geqslant U_{j_1}cg(Y_{j_1}), \\
            Y_{j_2} \leqslant x_2, f(Y_{j_1+1}) < U_{j_1+1}cg(Y_{j_1+1}), \ldots, f(Y_{j_2-1}) < U_{j_2-1}cg(Y_{j_2-1}), f(Y_{j_2}) \geqslant U_{j_2}cg(Y_{j_2})
        \end{array}\right) \\
        &= \sum_{j_1=1}^{\infty}\sum_{j_2=j_1+1}^{\infty}\left\{\prb(Y_{j_1} \leqslant x_1, f(Y_{j_1}) \geqslant U_{j_1}cg(Y_{j_1})) \times \right. \\
        &\quad \prb(f(Y_1) < U_1 cg(Y_1), \ldots, f(Y_{j_1-1}) < U_{j_1-1}cg(Y_{j_1-1})) \times \\
        &\quad \prb(Y_{j_2} \leqslant x_2, f(Y_{j_2}) \leqslant U_{j_2-1}cg(Y_{j_2})) \times \\
        &\quad \left.\prb(f(Y_{j_1+1}) < U_{j_1+1}cg(Y_{j_1+1}), \ldots, f(Y_{j_2-1}) < U_{j_2-1}cg(Y_{j_2-1}))\right\} \\
        &= \sum_{j_1=1}^{\infty}\sum_{j_2=j_1+1}^{\infty}\left\{\prb(Y_{j_1} \leqslant x_1 \mid f(Y_{j_1}) \geqslant U_{j_1}cg(Y_{j_1}))p(1 - p)^{j_1-1} \times \right. \\
        &\quad \left.\prb(Y_{j_2} \leqslant x_2 \mid f(Y_{j_2}) \leqslant U_{j_2-1}cg(Y_{j_2}))p(1 - p)^{j_2-j_1-1}\right\} \\
        &= \left(\int_{-\infty}^{x_1}f(y)\,\mathrm{d}y\right)\left(\int_{-\infty}^{x_2}f(y)\,\mathrm{d}y\right).
    \end{align*}
    so $X_{i_1} \sim f$ independently of $X_{i_2} \sim f$. Continuing in this fashion proves that $X_{i_1}, X_{i_2}, \ldots$ is an i.i.d.\ sequence from the distribution, with density given by $f$.
\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Further Proofs (Advanced)}
\label{sec:2.11}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Proof of Theorem~\ref{thm:2.4.2}}

We want to prove that the function $\phi$ given by \eqref{eq:2.4.9} is a density function.

Clearly $\phi(x) \geqslant 0$ for all $x$. To proceed, we set $I = \int_{-\infty}^{\infty} \phi(x) \, \mathrm{d}x$. Then, using multivariable calculus,
\[
I^2 = \left[\int_{-\infty}^{\infty} \phi(x) \, \mathrm{d}x\right]^2 = \int_{-\infty}^{\infty} \phi(x) \, \mathrm{d}x \int_{-\infty}^{\infty} \phi(y) \, \mathrm{d}y = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \phi(x) \, \phi(y) \, \mathrm{d}x \, \mathrm{d}y = \frac{1}{2\pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-(x^2 + y^2)/2} \, \mathrm{d}x \, \mathrm{d}y.
\]

We now switch to polar coordinates $(r, \theta)$, so that $x = r\cos\theta$ and $y = r\sin\theta$, where $r \geqslant 0$ and $0 \leqslant \theta < 2\pi$. Then $x^2 + y^2 = r^2$ and, by the multivariable change of variable theorem from calculus, $\mathrm{d}x \, \mathrm{d}y = r \, \mathrm{d}r \, \mathrm{d}\theta$. Hence,
\[
I^2 = \int_0^{2\pi} \int_0^{\infty} \frac{1}{2\pi} e^{-r^2/2} r \, \mathrm{d}r \, \mathrm{d}\theta = \int_0^{\infty} e^{-r^2/2} r \, \mathrm{d}r = \left[-e^{-r^2/2}\right]_{r=0}^{r=\infty} = 0 - (-1) = 1,
\]
and we have $I^2 = 1$. But clearly $I > 0$ (because $\phi > 0$), so we must have $I = 1$, as claimed.

\subsection*{Proof of Theorem~\ref{thm:2.6.2}}

We want to prove that, when $X$ is an absolutely continuous random variable, with density function $f_X$, and $Y = h(X)$, where $h : \mathbf{R}^1 \to \mathbf{R}^1$ is a function that is differentiable and strictly increasing, then $Y$ is also absolutely continuous, and its density function $f_Y$ is given by
\begin{equation}
\label{eq:2.11.1}
f_Y(y) = f_X(h^{-1}(y)) \, |h'(h^{-1}(y))|,
\end{equation}
where $h'$ is the derivative of $h$, and where $h^{-1}(y)$ is the unique number $x$ such that $h(x) = y$.

We must show that whenever $a < b$, we have
\[
\prb(a \leqslant Y \leqslant b) = \int_a^b f_Y(y) \, \mathrm{d}y,
\]
where $f_Y$ is given by \eqref{eq:2.11.1}. To that end, we note that, because $h$ is strictly increasing, so is $h^{-1}$. Hence, applying $h^{-1}$ preserves inequalities, so that
\[
\prb(a \leqslant Y \leqslant b) = \prb(h^{-1}(a) \leqslant h^{-1}(Y) \leqslant h^{-1}(b)) = \prb(h^{-1}(a) \leqslant X \leqslant h^{-1}(b)) = \int_{h^{-1}(a)}^{h^{-1}(b)} f_X(x) \, \mathrm{d}x.
\]

We then make the substitution $y = h(x)$, so that $x = h^{-1}(y)$, and
\[
\mathrm{d}x = \frac{\mathrm{d}}{\mathrm{d}y} h^{-1}(y) \, \mathrm{d}y.
\]
But by the inverse function theorem from calculus, $\frac{\mathrm{d}}{\mathrm{d}y} h^{-1}(y) = 1/h'(h^{-1}(y))$. Furthermore, as $x$ goes from $h^{-1}(a)$ to $h^{-1}(b)$, we see that $y = h(x)$ goes from $a$ to $b$. We conclude that
\[
\prb(a \leqslant Y \leqslant b) = \int_{h^{-1}(a)}^{h^{-1}(b)} f_X(x) \, \mathrm{d}x = \int_a^b f_X(h^{-1}(y)) \cdot \frac{1}{|h'(h^{-1}(y))|} \, \mathrm{d}y = \int_a^b f_Y(y) \, \mathrm{d}y,
\]
as required.

\subsection*{Proof of Theorem~\ref{thm:2.6.3}}

We want to prove that when $X$ is an absolutely continuous random variable, with density function $f_X$, and $Y = h(X)$, where $h : \mathbf{R}^1 \to \mathbf{R}^1$ is a function that is differentiable and strictly decreasing, then $Y$ is also absolutely continuous, and its density function $f_Y$ may again be defined by \eqref{eq:2.11.1}.

We note that, because $h$ is strictly decreasing, so is $h^{-1}$. Hence, applying $h^{-1}$ \emph{reverses} the inequalities, so that
\[
\prb(a \leqslant Y \leqslant b) = \prb(h^{-1}(b) \leqslant h^{-1}(Y) \leqslant h^{-1}(a)) = \prb(h^{-1}(b) \leqslant X \leqslant h^{-1}(a)) = \int_{h^{-1}(b)}^{h^{-1}(a)} f_X(x) \, \mathrm{d}x.
\]

We then make the substitution $y = h(x)$, so that $x = h^{-1}(y)$, and
\[
\mathrm{d}x = \frac{\mathrm{d}}{\mathrm{d}y} h^{-1}(y) \, \mathrm{d}y.
\]
But by the inverse function theorem from calculus,
\[
\frac{\mathrm{d}}{\mathrm{d}y} h^{-1}(y) = \frac{1}{h'(h^{-1}(y))}.
\]
Furthermore, as $x$ goes from $h^{-1}(b)$ to $h^{-1}(a)$, we see that $y = h(x)$ goes from $a$ to $b$. We conclude that
\[
\prb(a \leqslant Y \leqslant b) = \int_{h^{-1}(b)}^{h^{-1}(a)} f_X(x) \, \mathrm{d}x = \int_b^a f_X(h^{-1}(y)) \cdot \frac{1}{|h'(h^{-1}(y))|} \, \mathrm{d}y = \int_a^b f_Y(y) \, \mathrm{d}y,
\]
as required.

\subsection*{Proof of Theorem~\ref{thm:2.9.2}}

We want to prove the following result. Let $X$ and $Y$ be jointly absolutely continuous, with joint density function $f_{X,Y}$. Let $Z = h_1(X, Y)$ and $W = h_2(X, Y)$, where $h_1, h_2 : \mathbf{R}^2 \to \mathbf{R}^1$ are differentiable functions. Define the joint function $h = (h_1, h_2) : \mathbf{R}^2 \to \mathbf{R}^2$ by
\[
h(x, y) = (h_1(x, y), h_2(x, y)).
\]
Assume that $h$ is one-to-one, at least on the region $\{(x, y) : f(x, y) > 0\}$, i.e., if $h_1(x_1, y_1) = h_1(x_2, y_2)$ and $h_2(x_1, y_1) = h_2(x_2, y_2)$, then $x_1 = x_2$ and $y_1 = y_2$. Then $Z$ and $W$ are also jointly absolutely continuous, with joint density function $f_{Z,W}$ given by
\[
f_{Z,W}(z, w) = f_{X,Y}(h^{-1}(z, w)) \, |J(h^{-1}(z, w))|,
\]
where $J$ is the Jacobian derivative of $h$, and where $h^{-1}(z, w)$ is the unique pair $(x, y)$ such that $h(x, y) = (z, w)$.

We must show that whenever $a \leqslant b$ and $c \leqslant d$, we have
\[
\prb(a \leqslant Z \leqslant b, c \leqslant W \leqslant d) = \int_c^d \int_a^b f_{Z,W}(z, w) \, \mathrm{d}z \, \mathrm{d}w.
\]

If we let $S = [a, b] \times [c, d]$ be the two-dimensional rectangle, then we can rewrite this as
\[
\prb((Z, W) \in S) = \iint_S f_{Z,W}(z, w) \, \mathrm{d}z \, \mathrm{d}w.
\]

Now, using the theory of multivariable calculus, and making the substitution $(x, y) = h^{-1}(z, w)$ (which is permissible because $h$ is one-to-one), we have
\begin{align*}
\iint_S f_{Z,W}(z, w) \, \mathrm{d}z \, \mathrm{d}w &= \iint_S f_{X,Y}(h^{-1}(z, w)) \, |J(h^{-1}(z, w))| \, \mathrm{d}z \, \mathrm{d}w \\
&= \iint_{h^{-1}(S)} f_{X,Y}(x, y) \, |J(x, y)| \cdot |J(x, y)|^{-1} \, \mathrm{d}x \, \mathrm{d}y \\
&= \iint_{h^{-1}(S)} f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \prb((X, Y) \in h^{-1}(S)) \\
&= \prb(h^{-1}(Z, W) \in h^{-1}(S)) = \prb((Z, W) \in S),
\end{align*}
as required.
