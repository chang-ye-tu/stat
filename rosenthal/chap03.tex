\chapter{Expectation}
\label{ch:3}

\section*{Chapter Outline}
\begin{itemize}
\item Section 1: The Discrete Case
\item Section 2: The Absolutely Continuous Case
\item Section 3: Variance, Covariance, and Correlation
\item Section 4: Generating Functions
\item Section 5: Conditional Expectation
\item Section 6: Inequalities
\item Section 7: General Expectations (Advanced)
\item Section 8: Further Proofs (Advanced)
\end{itemize}

In the first two chapters we learned about probability models, random variables, and distributions. There is one more concept that is fundamental to all of probability theory, that of expected value.

Intuitively, the expected value of a random variable is the average value that the random variable takes on. For example, if half the time $X = 0$, and the other half of the time $X = 10$, then the average value of $X$ is $5$. We shall write this as $\expc[X] = 5$. Similarly, if one-third of the time $Y = 6$ while two-thirds of the time $Y = 15$, then $\expc[Y] = 12$.

Another interpretation of expected value is in terms of fair gambling. Suppose someone offers you a ticket (e.g., a lottery ticket) worth a certain random amount $X$. How much would you be willing to pay to buy the ticket? It seems reasonable that you would be willing to pay the expected value $\expc[X]$ of the ticket, but no more. However, this interpretation does have certain limitations; see Example~\ref{ex:3.1.12}.

To understand expected value more precisely, we consider discrete and absolutely continuous random variables separately.

\section{The Discrete Case}
\label{sec:3.1}

We begin with a definition.

\begin{definition}
\label{def:3.1.1}
Let $X$ be a discrete random variable. Then the \emph{expected value} (or \emph{mean value} or \emph{mean}) of $X$, written $\expc[X]$ (or $\mu_X$), is defined by
\[
\expc[X] = \sum_{x \in \mathbf{R}^1} x \, \prb(X = x) = \sum_{x \in \mathbf{R}^1} x \, p_X(x).
\]
\end{definition}

We will have $\prb(X = x) = 0$ except for those values $x$ that are possible values of $X$. Hence, an equivalent definition is the following.

\begin{definition}
\label{def:3.1.2}
Let $X$ be a discrete random variable, taking on distinct values $x_1, x_2, \ldots$, with $p_i = \prb(X = x_i)$. Then the expected value of $X$ is given by
\[
\expc[X] = \sum_i x_i p_i.
\]
\end{definition}

The definition (in either form) is best understood through examples.

\begin{example}
\label{ex:3.1.1}
Suppose, as above, that $\prb(X = 0) = \prb(X = 10) = 1/2$. Then
\[
\expc[X] = 0 \cdot (1/2) + 10 \cdot (1/2) = 5
\]
as predicted.
\end{example}

\begin{example}
\label{ex:3.1.2}
Suppose, as above, that $\prb(Y = 6) = 1/3$, and $\prb(Y = 15) = 2/3$. Then
\[
\expc[Y] = 6 \cdot (1/3) + 15 \cdot (2/3) = 2 + 10 = 12
\]
again as predicted.
\end{example}

\begin{example}
\label{ex:3.1.3}
Suppose that $\prb(Z = 3) = 0.2$, and $\prb(Z = 11) = 0.7$, and $\prb(Z = 31) = 0.1$. Then
\[
\expc[Z] = 3 \cdot (0.2) + 11 \cdot (0.7) + 31 \cdot (0.1) = 0.6 + 7.7 + 3.1 = 10.2.
\]
\end{example}

\begin{example}
\label{ex:3.1.4}
Suppose that $\prb(W = -3) = 0.2$, and $\prb(W = -11) = 0.7$, and $\prb(W = 31) = 0.1$. Then
\[
\expc[W] = (-3) \cdot (0.2) + (-11) \cdot (0.7) + 31 \cdot (0.1) = -0.6 - 7.7 + 3.1 = -5.2.
\]
In this case, the expected value of $W$ is negative.
\end{example}

We thus see that, for a discrete random variable $X$, once we know the probabilities that $X = x$ (or equivalently, once we know the probability function $p_X$), it is straightforward (at least in simple cases) to compute the expected value of $X$.

We now consider some of the common discrete distributions introduced in Section~\ref{sec:2.3}.

\begin{example}[Degenerate Distributions]
\label{ex:3.1.5}
If $X = c$ is a constant, then $\prb(X = c) = 1$, so
\[
\expc[X] = c \cdot 1 = c
\]
as it should.
\end{example}

\begin{example}[The Bernoulli Distribution and Indicator Functions]
\label{ex:3.1.6}
If $X \sim \text{Bernoulli}(\theta)$, then $\prb(X = 1) = \theta$ and $\prb(X = 0) = 1 - \theta$, so
\[
\expc[X] = 1 \cdot \theta + 0 \cdot (1 - \theta) = \theta.
\]
As a particular application of this, suppose we have a response $s$ taking values in a sample $S$ and $A \subseteq S$. Letting $X(s) = \indc_A(s)$, we have that $X$ is the indicator function of the set $A$ and so takes the values $0$ and $1$. Then we have that $\prb(X = 1) = \prb(A)$, and so $X \sim \text{Bernoulli}(\prb(A))$. This implies that
\[
\expc[X] = \expc[\indc_A] = \prb(A).
\]
Therefore, we have shown that the expectation of the indicator function of the set $A$ is equal to the probability of $A$.
\end{example}

\begin{example}[The Binomial$(n, \theta)$ Distribution]
\label{ex:3.1.7}
If $Y \sim \text{Binomial}(n, \theta)$, then
\[
\prb(Y = k) = \binom{n}{k} \theta^k (1 - \theta)^{n-k}
\]
for $k = 0, 1, \ldots, n$. Hence,
\begin{align*}
\expc[Y] &= \sum_{k=0}^{n} k \, \prb(Y = k) = \sum_{k=0}^{n} k \binom{n}{k} \theta^k (1 - \theta)^{n-k} \\
&= \sum_{k=0}^{n} k \frac{n!}{k!(n-k)!} \theta^k (1 - \theta)^{n-k} = \sum_{k=1}^{n} \frac{n!}{(k-1)!(n-k)!} \theta^k (1 - \theta)^{n-k} \\
&= \sum_{k=1}^{n} \frac{n(n-1)!}{(k-1)!(n-k)!} \theta^k (1 - \theta)^{n-k} = \sum_{k=1}^{n} n \binom{n-1}{k-1} \theta^k (1 - \theta)^{n-k}.
\end{align*}
Now, the binomial theorem says that for any $a$ and $b$ and any positive integer $m$,
\[
(a + b)^m = \sum_{j=0}^{m} \binom{m}{j} a^j b^{m-j}.
\]
Using this, and setting $j = k - 1$, we see that
\begin{align*}
\expc[Y] &= \sum_{k=1}^{n} n \binom{n-1}{k-1} \theta^k (1 - \theta)^{n-k} = \sum_{j=0}^{n-1} n \binom{n-1}{j} \theta^{j+1} (1 - \theta)^{(n-1)-j} \\
&= n\theta \sum_{j=0}^{n-1} \binom{n-1}{j} \theta^j (1 - \theta)^{(n-1)-j} = n\theta (\theta + 1 - \theta)^{n-1} = n\theta.
\end{align*}
Hence, the expected value of $Y$ is $n\theta$. Note that this is precisely $n$ times the expected value of $X$, where $X \sim \text{Bernoulli}(\theta)$ as in Example~\ref{ex:3.1.6}. We shall see in Example~\ref{ex:3.1.15} that this is not a coincidence.
\end{example}

\begin{example}[The Geometric Distribution]
\label{ex:3.1.8}
If $Z \sim \text{Geometric}(\theta)$, then $\prb(Z = k) = (1 - \theta)^k \theta$ for $k = 0, 1, 2, \ldots$. Hence,
\begin{equation}
\label{eq:3.1.1}
\expc[Z] = \sum_{k=0}^{\infty} k (1 - \theta)^k \theta.
\end{equation}
Therefore, we can write
\[
(1 - \theta) \expc[Z] = \sum_{\ell=0}^{\infty} \ell (1 - \theta)^{\ell+1} \theta.
\]
Using the substitution $\ell = k - 1$, we compute that
\begin{equation}
\label{eq:3.1.2}
(1 - \theta) \expc[Z] = \sum_{k=1}^{\infty} (k - 1)(1 - \theta)^k \theta.
\end{equation}
Subtracting \eqref{eq:3.1.2} from \eqref{eq:3.1.1}, we see that
\begin{align*}
\expc[Z] - (1 - \theta) \expc[Z] = \theta \expc[Z] &= \sum_{k=1}^{\infty} [k - (k - 1)](1 - \theta)^k \theta \\
&= \sum_{k=1}^{\infty} (1 - \theta)^k \theta = \theta (1 - \theta) \cdot \frac{1}{1 - (1 - \theta)} = \frac{1 - \theta}{\theta} \cdot \theta.
\end{align*}
Hence, $\theta \expc[Z] = 1 - \theta$, and we obtain $\expc[Z] = (1 - \theta)/\theta$.
\end{example}

\begin{example}[The Poisson Distribution]
\label{ex:3.1.9}
If $X \sim \text{Poisson}(\lambda)$, then $\prb(X = k) = e^{-\lambda} \lambda^k / k!$ for $k = 0, 1, 2, \ldots$. Hence, setting $\ell = k - 1$,
\begin{align*}
\expc[X] &= \sum_{k=0}^{\infty} k e^{-\lambda} \frac{\lambda^k}{k!} = \sum_{k=1}^{\infty} e^{-\lambda} \frac{\lambda^k}{(k-1)!} = e^{-\lambda} \sum_{k=1}^{\infty} \lambda \frac{\lambda^{k-1}}{(k-1)!} \\
&= \lambda e^{-\lambda} \sum_{\ell=0}^{\infty} \frac{\lambda^\ell}{\ell!} = \lambda e^{-\lambda} e^{\lambda} = \lambda
\end{align*}
and we conclude that $\expc[X] = \lambda$.
\end{example}

It should be noted that expected values can sometimes be infinite, as the following example demonstrates.

\begin{example}
\label{ex:3.1.10}
Let $X$ be a discrete random variable, with probability function $p_X$ given by
\[
p_X(2^k) = 2^{-k}
\]
for $k = 1, 2, 3, \ldots$, with $p_X(x) = 0$ for other values of $x$. That is, $p_X(2) = 1/2$, $p_X(4) = 1/4$, $p_X(8) = 1/8$, etc., while $p_X(1) = p_X(3) = p_X(5) = p_X(6) = 0$.

Then it is easily checked that $p_X$ is indeed a valid probability function (i.e., $p_X(x) \geqslant 0$ for all $x$, with $\sum_x p_X(x) = 1$). On the other hand, we compute that
\[
\expc[X] = \sum_{k=1}^{\infty} 2^k \cdot 2^{-k} = \sum_{k=1}^{\infty} 1 = \infty.
\]
We therefore say that $\expc[X] = \infty$, i.e., that the expected value of $X$ is infinite.
\end{example}

Sometimes the expected value simply does not exist, as in the following example.

\begin{example}
\label{ex:3.1.11}
Let $Y$ be a discrete random variable, with probability function $p_Y$ given by
\[
p_Y(y) = \begin{cases}
1/(2|y|) & y = 2, 4, 8, 16, \ldots \\
1/(2|y|) & y = -2, -4, -8, -16, \ldots \\
0 & \text{otherwise}.
\end{cases}
\]
That is, $p_Y(2) = p_Y(-2) = 1/4$, $p_Y(4) = p_Y(-4) = 1/8$, $p_Y(8) = p_Y(-8) = 1/16$, etc. Then it is easily checked that $p_Y$ is indeed a valid probability function (i.e., $p_Y(y) \geqslant 0$ for all $y$, with $\sum_y p_Y(y) = 1$).

On the other hand, we compute that
\[
\expc[Y] = \sum_y y \, p_Y(y) = \sum_{k=1}^{\infty} 2^k \cdot \frac{1}{2 \cdot 2^k} - \sum_{k=1}^{\infty} 2^k \cdot \frac{1}{2 \cdot 2^k} = \sum_{k=1}^{\infty} \frac{1}{2} - \sum_{k=1}^{\infty} \frac{1}{2}
\]
which is undefined. We therefore say that $\expc[Y]$ does not exist, i.e., that the expected value of $Y$ is undefined in this case.
\end{example}

\begin{example}[The St.\ Petersburg Paradox]
\label{ex:3.1.12}
Suppose someone makes you the following deal. You will repeatedly flip a fair coin and will receive an award of $2^Z$ pennies, where $Z$ is the number of tails that appear before the first head. How much would you be willing to pay for this deal?

Well, the probability that the award will be $2^z$ pennies is equal to the probability that you will flip $z$ tails and then one head, which is equal to $(1/2)^{z+1}$. Hence, the expected value of the award (in pennies) is equal to
\[
\sum_{z=0}^{\infty} 2^z (1/2)^{z+1} = \sum_{z=0}^{\infty} (1/2) = \infty.
\]
In words, the average amount of the award is infinite!

Hence, according to the ``fair gambling'' interpretation of expected value, as discussed at the beginning of this chapter, it seems that you should be willing to pay an infinite amount (or, at least, any finite amount no matter how large) to get the award promised by this deal! How much do you think you should really be willing to pay for it?\footnote{When one of the authors first heard about this deal, he decided to try it and agreed to pay \$1. In fact, he got four tails before the first head, so his award was 16 cents, but he still lost 84 cents overall.}
\end{example}

\begin{example}[The St.\ Petersburg Paradox, Truncated]
\label{ex:3.1.13}
Suppose in the St.\ Petersburg paradox (Example~\ref{ex:3.1.12}), it is agreed that the award will be truncated at $2^{30}$ cents (which is just over \$10 million!). That is, the award will be the same as for the original deal, except the award will be frozen once it exceeds $2^{30}$ cents. Formally, the award is now equal to $2^{\min(30,Z)}$ pennies, where $Z$ is as before.

How much would you be willing to pay for this new award? Well, the expected value of the new award (in cents) is equal to
\begin{align*}
\sum_{z=1}^{\infty} 2^{\min(30,z)} (1/2)^{z+1} &= \sum_{z=1}^{30} 2^z (1/2)^{z+1} + \sum_{z=31}^{\infty} 2^{30} (1/2)^{z+1} \\
&= \sum_{z=1}^{30} (1/2) + 2^{30} \cdot (1/2)^{31} = 30/2 + 15.5.
\end{align*}
That is, truncating the award at just over \$10 million changes its expected value enormously, from infinity to less than 16 cents!
\end{example}

In utility theory, it is often assumed that each person has a utility function $U$ such that, if they win $x$ cents, their amount of ``utility'' (i.e., benefit or joy or pleasure) is equal to $U(x)$. In this context, the truncation of Example~\ref{ex:3.1.13} may be thought of not as changing the rules of the game but as corresponding to a utility function of the form $U(x) = \min(x, 2^{30})$. In words, this says that your utility is equal to the amount of money you get, until you reach $2^{30}$ cents (approximately \$10 million), after which point you don't care about money\footnote{Or, perhaps, you think it is unlikely you will be able to collect the money!} anymore. The result of Example~\ref{ex:3.1.13} then says that, with this utility function, the St.\ Petersburg paradox is only worth 15.5 cents to you --- even though its expected value is infinite.

We often need to compute expected values of functions of random variables. Fortunately, this is not too difficult, as the following theorem shows.

\begin{theorem}
\label{thm:3.1.1}
\leavevmode
\begin{enumerate}[(a)]
\item Let $X$ be a discrete random variable, and let $g : \mathbf{R}^1 \to \mathbf{R}^1$ be some function such that the expectation of the random variable $g(X)$ exists. Then
\[
\expc[g(X)] = \sum_x g(x) \, \prb(X = x).
\]
\item Let $X$ and $Y$ be discrete random variables, and let $h : \mathbf{R}^2 \to \mathbf{R}^1$ be some function such that the expectation of the random variable $h(X, Y)$ exists. Then
\[
\expc[h(X, Y)] = \sum_{x,y} h(x, y) \, \prb(X = x, Y = y).
\]
\end{enumerate}
\end{theorem}

\begin{proof}
We prove part (b) here. Part (a) then follows by simply setting $h(x, y) = g(x)$ and noting that
\[
\sum_{x,y} g(x) \, \prb(X = x, Y = y) = \sum_x g(x) \, \prb(X = x).
\]
Let $Z = h(X, Y)$. We have that
\begin{align*}
\expc[Z] &= \sum_z z \, \prb(Z = z) = \sum_z z \, \prb(h(X, Y) = z) \\
&= \sum_z z \sum_{\substack{x,y \\ h(x,y) = z}} \prb(X = x, Y = y) = \sum_{x,y} \sum_{\substack{z \\ h(x,y) = z}} z \, \prb(X = x, Y = y) \\
&= \sum_{x,y} h(x, y) \, \prb(X = x, Y = y)
\end{align*}
as claimed.
\end{proof}

One of the most important properties of expected value is that it is linear, stated as follows.

\begin{theorem}[Linearity of expected values]
\label{thm:3.1.2}
Let $X$ and $Y$ be discrete random variables, let $a$ and $b$ be real numbers, and put $Z = aX + bY$. Then $\expc[Z] = a\expc[X] + b\expc[Y]$.
\end{theorem}

\begin{proof}
Let $p_{X,Y}$ be the joint probability function of $X$ and $Y$. Then using Theorem~\ref{thm:3.1.1},
\begin{align*}
\expc[Z] &= \sum_{x,y} (ax + by) \, p_{X,Y}(x, y) = a \sum_{x,y} x \, p_{X,Y}(x, y) + b \sum_{x,y} y \, p_{X,Y}(x, y) \\
&= a \sum_x x \sum_y p_{X,Y}(x, y) + b \sum_y y \sum_x p_{X,Y}(x, y).
\end{align*}
Because $\sum_y p_{X,Y}(x, y) = p_X(x)$ and $\sum_x p_{X,Y}(x, y) = p_Y(y)$, we have that
\[
\expc[Z] = a \sum_x x \, p_X(x) + b \sum_y y \, p_Y(y) = a\expc[X] + b\expc[Y]
\]
as claimed.
\end{proof}

\begin{example}
\label{ex:3.1.14}
Let $X \sim \text{Binomial}(n, \theta_1)$, and let $Y \sim \text{Geometric}(\theta_2)$. What is $\expc[3X - 2Y]$?

We already know (Examples~\ref{ex:3.1.6} and \ref{ex:3.1.7}) that $\expc[X] = n\theta_1$ and $\expc[Y] = (1 - \theta_2)/\theta_2$. Hence, by Theorem~\ref{thm:3.1.2}, $\expc[3X - 2Y] = 3\expc[X] - 2\expc[Y] = 3n\theta_1 - 2(1 - \theta_2)/\theta_2$.
\end{example}

\begin{example}
\label{ex:3.1.15}
Let $Y \sim \text{Binomial}(n, \theta)$. Then we know (cf.\ Example~\ref{ex:2.3.3}) that we can think of $Y = X_1 + \cdots + X_n$, where each $X_i \sim \text{Bernoulli}(\theta)$ (in fact, $X_i = 1$ if the $i$th coin is heads, otherwise $X_i = 0$). Because $\expc[X_i] = \theta$ for each $i$, it follows immediately from Theorem~\ref{thm:3.1.2} that
\[
\expc[Y] = \expc[X_1] + \cdots + \expc[X_n] = n\theta.
\]
This gives the same answer as Example~\ref{ex:3.1.7}, but much more easily.
\end{example}

Suppose that $X$ is a random variable and $Y = c$ is a constant. Then from Theorem~\ref{thm:3.1.2}, we have that $\expc[X + c] = \expc[X] + c$. From this we see that the mean value $\mu_X$ of $X$ is a measure of the location of the probability distribution of $X$. For example, if $X$ takes the value $x$ with probability $p$ and the value $y$ with probability $1 - p$, then the mean of $X$ is $\mu_X = px + (1 - p)y$, which is a value between $x$ and $y$. For a constant $c$, the probability distribution of $X + c$ is concentrated on the points $x + c$ and $y + c$ with probabilities $p$ and $1 - p$, respectively. The mean of $X + c$ is $\mu_X + c$, which is between the points $x + c$ and $y + c$, i.e., the mean shifts with the probability distribution. It is also true that if $X$ is concentrated on the finite set of points $x_1 \leqslant x_2 \leqslant \cdots \leqslant x_k$, then $x_1 \leqslant \mu_X \leqslant x_k$ and the mean shifts exactly as we shift the distribution. This is depicted in Figure~\ref{fig:3.1.1} for a distribution concentrated on $k = 4$ points. Using the results of Section~\ref{ssec:2.6.1}, we have that $p_{X+c}(x) = p_X(x - c)$.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_1_1.pdf}
  \caption{The probability functions and means of discrete random variables $X$ and $X + c$.}
  \label{fig:3.1.1}
\end{figure}

Theorem~\ref{thm:3.1.2} says, in particular, that $\expc[X + Y] = \expc[X] + \expc[Y]$, i.e., that expectation preserves sums. It is reasonable to ask whether the same property holds for products. That is, do we necessarily have $\expc[XY] = \expc[X]\expc[Y]$? In general, the answer is no, as the following example shows.

\begin{example}
\label{ex:3.1.16}
Let $X$ and $Y$ be discrete random variables, with joint probability function given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/2 & x = 3, y = 5 \\
1/6 & x = 3, y = 9 \\
1/6 & x = 6, y = 5 \\
1/6 & x = 6, y = 9 \\
0 & \text{otherwise}.
\end{cases}
\]
Then
\[
\expc[X] = \sum_x x \, \prb(X = x) = 3 \cdot (1/2 + 1/6) + 6 \cdot (1/6 + 1/6) = 4
\]
and
\[
\expc[Y] = \sum_y y \, \prb(Y = y) = 5 \cdot (1/2 + 1/6) + 9 \cdot (1/6 + 1/6) = 19/3
\]
while
\begin{align*}
\expc[XY] &= \sum_z z \, \prb(XY = z) \\
&= 3 \cdot 5 \cdot (1/2) + 3 \cdot 9 \cdot (1/6) + 6 \cdot 5 \cdot (1/6) + 6 \cdot 9 \cdot (1/6) = 26.
\end{align*}
Because $4 \cdot (19/3) \neq 26$, we see that $\expc[X]\expc[Y] \neq \expc[XY]$ in this case.
\end{example}

On the other hand, if $X$ and $Y$ are independent, then we do have $\expc[X]\expc[Y] = \expc[XY]$.

\begin{theorem}
\label{thm:3.1.3}
Let $X$ and $Y$ be discrete random variables that are independent. Then $\expc[XY] = \expc[X]\expc[Y]$.
\end{theorem}

\begin{proof}
Independence implies (see Theorem~\ref{thm:2.8.3}) that $\prb(X = x, Y = y) = \prb(X = x)\prb(Y = y)$. Using this, we compute by Theorem~\ref{thm:3.1.1} that
\begin{align*}
\expc[XY] &= \sum_{x,y} xy \, \prb(X = x, Y = y) = \sum_{x,y} xy \, \prb(X = x) \prb(Y = y) \\
&= \sum_x x \, \prb(X = x) \sum_y y \, \prb(Y = y) = \expc[X]\expc[Y]
\end{align*}
as claimed.
\end{proof}

Theorem~\ref{thm:3.1.3} will be used often in subsequent chapters, as will the following important property.

\begin{theorem}[Monotonicity]
\label{thm:3.1.4}
Let $X$ and $Y$ be discrete random variables, and suppose that $X \leqslant Y$. (Remember that this means $X(s) \leqslant Y(s)$ for all $s \in S$.) Then $\expc[X] \leqslant \expc[Y]$.
\end{theorem}

\begin{proof}
Let $Z = Y - X$. Then $Z$ is also discrete. Furthermore, because $X \leqslant Y$, we have $Z \geqslant 0$, so that all possible values of $Z$ are nonnegative. Hence, if we list the possible values of $Z$ as $z_1, z_2, \ldots$, then $z_i \geqslant 0$ for all $i$, so that
\[
\expc[Z] = \sum_i z_i \, \prb(Z = z_i) \geqslant 0.
\]
But by Theorem~\ref{thm:3.1.2}, $\expc[Z] = \expc[Y] - \expc[X]$. Hence, $\expc[Y] - \expc[X] \geqslant 0$, so that $\expc[Y] \geqslant \expc[X]$.
\end{proof}

\subsection*{Summary of Section~\ref{sec:3.1}}

\begin{itemize}
\item The expected value $\expc[X]$ of a random variable $X$ represents the long-run average value that it takes on.
\item If $X$ is discrete, then $\expc[X] = \sum_x x \, \prb(X = x)$.
\item The expected values of the Bernoulli, binomial, geometric, and Poisson distributions were computed.
\item Expected value has an interpretation in terms of fair gambling, but such interpretations require utility theory to accurately reflect human behavior.
\item Expected values of functions of one or two random variables can also be computed by summing the function values times the probabilities.
\item Expectation is linear and monotone.
\item If $X$ and $Y$ are independent, then $\expc[XY] = \expc[X]\expc[Y]$. But without independence, this property may fail.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:3.1.1}
Compute $\expc[X]$ when the probability function of $X$ is given by each of the following.
\begin{enumerate}[(a)]
\item
\[
p_X(x) = \begin{cases}
1/7 & x = -4 \\
2/7 & x = 0 \\
4/7 & x = 3 \\
0 & \text{otherwise}
\end{cases}
\]
\item
\[
p_X(x) = \begin{cases}
2^{-x-1} & x = 0, 1, 2, \ldots \\
0 & \text{otherwise}
\end{cases}
\]
\item
\[
p_X(x) = \begin{cases}
2^{x-6} & x = 7, 8, 9, \ldots \\
0 & \text{otherwise}
\end{cases}
\]
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X) = (-4)(1/7) + (0)(2/7) + (3)(4/7) = 8/7$.
    \item We recognize that $X \sim \text{Geometric}(1/2)$. Hence, $\expc(X) = (1 - (1/2))/(1/2) = 1$.
    \item Using the substitution $y = x - 7$, we have $\expc(X) = \sum_{x=7}^{\infty} x \, 2^{-x+6} = \sum_{y=0}^{\infty} (y + 7) \, 2^{-y-1} = 7 + \sum_{y=0}^{\infty} y \, 2^{-y-1} = 7 + 1 = 8$ since $\sum_{y=0}^{\infty} y \, 2^{-y-1} = 1$ is the mean of a $\text{Geometric}(1/2)$ distribution.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.1.2}
Let $X$ and $Y$ have joint probability function given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/7 & x = 5, y = 0 \\
1/7 & x = 5, y = 3 \\
1/7 & x = 5, y = 4 \\
3/7 & x = 8, y = 0 \\
1/7 & x = 8, y = 4 \\
0 & \text{otherwise}
\end{cases}
\]
as in Example~\ref{ex:2.7.5}. Compute each of the following.
\begin{enumerate}[(a)]
\item $\expc[X]$
\item $\expc[Y]$
\item $\expc[3X - 7Y]$
\item $\expc[X^2]$
\item $\expc[Y^2]$
\item $\expc[XY]$
\item $\expc[XY - 14]$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X) = (5)(1/7) + (5)(1/7) + (5)(1/7) + (8)(3/7) + (8)(1/7) = 47/7$.
    \item $\expc(Y) = (0)(1/7) + (3)(1/7) + (4)(1/7) + (0)(3/7) + (4)(1/7) = 11/7$.
    \item By linearity, $\expc(3X + 7Y) = 3\expc(X) + 7\expc(Y) = 3(47/7) + 7(11/7) = 218/7$.
    \item $\expc(X^2) = (5)^2(1/7) + (5)^2(1/7) + (5)^2(1/7) + (8)^2(3/7) + (8)^2(1/7) = 331/7$.
    \item $\expc(Y^2) = (0)^2(1/7) + (3)^2(1/7) + (4)^2(1/7) + (0)^2(3/7) + (4)^2(1/7) = 41/7$.
    \item $\expc(XY) = (5)(0)(1/7) + (5)(3)(1/7) + (5)(4)(1/7) + (8)(0)(3/7) + (8)(4)(1/7) = 67/7$.
    \item By linearity, $\expc(XY + 14) = \expc(XY) + 14 = 67/7 + 14 = 165/7$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.1.3}
Let $X$ and $Y$ have joint probability function given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/2 & x = 2, y = 10 \\
1/6 & x = 7, y = 10 \\
1/12 & x = 2, y = 12 \\
1/12 & x = 7, y = 12 \\
1/12 & x = 2, y = 14 \\
1/12 & x = 7, y = 14 \\
0 & \text{otherwise}
\end{cases}
\]
Compute each of the following.
\begin{enumerate}[(a)]
\item $\expc[X]$
\item $\expc[Y]$
\item $\expc[X^2]$
\item $\expc[Y^2]$
\item $\expc[X^2 + Y^2]$
\item $\expc[XY + 4Y]$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X) = (2)(1/2) + (-7)(1/6) + (2)(1/12) + (-7)(1/12) + (2)(1/12) + (-7)(1/12) = -173/12 = -14.4$.
    \item $\expc(Y) = (10)(1/2) + (10)(1/6) + (12)(1/12) + (12)(1/12) + (14)(1/12) + (14)(1/12) = 11$.
    \item $\expc(X^2) = (2)^2(1/2) + (-7)^2(1/6) + (2)^2(1/12) + (-7)^2(1/12) + (2)^2(1/12) + (-7)^2(1/12) = 19$.
    \item $\expc(Y^2) = (10)^2(1/2) + (10)^2(1/6) + (12)^2(1/12) + (12)^2(1/12) + (14)^2(1/12) + (14)^2(1/12) = 370/3 = 123.3$.
    \item $\expc(X^2 + Y^2) = \expc(X^2) + \expc(Y^2) = 19 + 370/3 = 427/3 = 142.3$.
    \item $\expc(XY - 4Y) = (2 \cdot 10 - 4 \cdot 10)(1/2) + ((-7) \cdot 10 - 4 \cdot 10)(1/6) + (2 \cdot 12 - 4 \cdot 12)(1/12) + ((-7) \cdot 12 - 4 \cdot 12)(1/12) + (2 \cdot 14 - 4 \cdot 14)(1/12) + ((-7) \cdot 14 - 4 \cdot 14)(1/12) = -113/2 = -56.5$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.1.4}
Let $X \sim \text{Bernoulli}(\theta_1)$ and $Y \sim \text{Binomial}(n, \theta_2)$. Compute $\expc[4X - 3Y]$.
\end{exercise}

\begin{solution}
$\expc(4X - 3Y) = 4\expc(X) - 3\expc(Y) = 4(p_1) - 3(np_2)$.
\end{solution}

\begin{exercise}
\label{exer:3.1.5}
Let $X \sim \text{Geometric}(\theta)$ and $Y \sim \text{Poisson}(\lambda)$. Compute $\expc[8X - Y + 12]$.
\end{exercise}

\begin{solution}
$\expc(8X - Y + 12) = 8\expc(X) - \expc(Y) + 12 = 8((1 - p)/p) - \lambda + 12$.
\end{solution}

\begin{exercise}
\label{exer:3.1.6}
Let $Y \sim \text{Binomial}(100, 0.3)$, and $Z \sim \text{Poisson}(7)$. Compute $\expc[Y - Z]$.
\end{exercise}

\begin{solution}
$\expc(Y + Z) = \expc(Y) + \expc(Z) = (100)(0.3) + (7) = 37$.
\end{solution}

\begin{exercise}
\label{exer:3.1.7}
Let $X \sim \text{Binomial}(80, 1/4)$, and let $Y \sim \text{Poisson}(3/2)$. Assume $X$ and $Y$ are independent. Compute $\expc[XY]$.
\end{exercise}

\begin{solution}
Since $X$ and $Y$ are independent, $\expc(XY) = \expc(X)\expc(Y) = ((80)(1/4))(3/2) = 30$.
\end{solution}

\begin{exercise}
\label{exer:3.1.8}
Starting with one penny, suppose you roll one fair six-sided die and get paid an additional number of pennies equal to three times the number showing on the die. Let $X$ be the total number of pennies you have at the end. Compute $\expc[X]$.
\end{exercise}

\begin{solution}
Let $Z$ be the number showing on the die. Then $X = 1 + 3Z$, so $\expc(X) = 1 + 3\expc(Z) = 1 + 3(3.5) = 11.5$.
\end{solution}

\begin{exercise}
\label{exer:3.1.9}
Suppose you start with eight pennies and flip one fair coin. If the coin comes up heads, you get to keep all your pennies; if the coin comes up tails, you have to give half of them back. Let $X$ be the total number of pennies you have at the end. Compute $\expc[X]$.
\end{exercise}

\begin{solution}
Let $Y = 1$ if the coin comes up tails, otherwise $Y = 0$ if the coin comes up heads. Then $X = 8 - 4Y$ and $\expc(Y) = 1(1/2) + 0(1/2) = 1/2$. Hence, $\expc(X) = 8 - 4\expc(Y) = 8 - 4(1/2) = 6$.
\end{solution}

\begin{exercise}
\label{exer:3.1.10}
Suppose you flip two fair coins. Let $Y = 3$ if the two coins show the same result, otherwise let $Y = -5$. Compute $\expc[Y]$.
\end{exercise}

\begin{solution}
$\prb(Y = 3) = \prb(\text{the same face}) = \prb(HH \text{ or } TT) = \prb(HH) + \prb(TT) = (1/2)(1/2) + (1/2)(1/2) = 1/2$. Hence, $\prb(Y = 5) = 1 - \prb(Y = 3) = 1 - 1/2 = 1/2$. The expectation is
\[
    \expc(Y) = 3\prb(Y = 3) + 5\prb(Y = 5) = 3 \cdot (1/2) + 5 \cdot (1/2) = 4.
\]
\end{solution}

\begin{exercise}
\label{exer:3.1.11}
Suppose you roll two fair six-sided dice.
\begin{enumerate}[(a)]
\item Let $Z$ be the sum of the two numbers showing. Compute $\expc[Z]$.
\item Let $W$ be the product of the two numbers showing. Compute $\expc[W]$.
\end{enumerate}
\end{exercise}

\begin{solution}
Let $X_1$ and $X_2$ be the two numbers showing on two dice. The expectation of $X_1$ is
\[
    \expc(X_1) = \sum_{i=1}^{6} i \, \prb(X_1 = i) = \sum_{i=1}^{6} i \cdot \frac{1}{6} = \frac{6 \cdot 7}{2} \cdot \frac{1}{6} = \frac{7}{2}.
\]
Since $X_1$ and $X_2$ are identically distributed, $\expc(X_1) = \expc(X_2) = 7/2$.
\begin{enumerate}[(a)]
  \item The random variable $Z$ becomes $Z = X_1 + X_2$. From Theorem \ref{thm:3.1.2}, $\expc(Z) = \expc(X_1 + X_2) = \expc(X_1) + \expc(X_2) = 2\expc(X_1) = 2(7/2) = 7$.
  \item The random variable $W = X_1 X_2$. Since $X_1$ and $X_2$ are independent, Theorem \ref{thm:3.1.3} is applicable. Hence, we get $\expc(W) = \expc(X_1 X_2) = \expc(X_1)\expc(X_2) = (7/2)^2 = 49/4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.1.12}
Suppose you flip one fair coin and roll one fair six-sided die. Let $X$ be the product of the numbers of heads (i.e., $0$ or $1$) times the number showing on the die. Compute $\expc[X]$. (Hint: Do not forget Theorem~\ref{thm:3.1.3}.)
\end{exercise}

\begin{solution}
Let $Y$ be the number of heads and $Z$ be the number showing on the die. The expectations of $Y$ and $Z$ are $\expc(Y) = 0 \cdot \prb(Y = 0) + 1 \cdot \prb(Y = 1) = 1/2$ and $\expc(Z) = 1 \cdot \prb(Z = 1) + \cdots + 6 \cdot \prb(Z = 6) = 7/2$. Then, $X = YZ$. Note $Y$ and $Z$ are independent. From Theorem \ref{thm:3.1.3}, we have $\expc(X) = \expc(Y)\expc(Z) = (1/2)(7/2) = 7/4$.
\end{solution}

\begin{exercise}
\label{exer:3.1.13}
Suppose you roll one fair six-sided die and then flip as many coins as the number showing on the die. (For example, if the die shows $4$, then you flip four coins.) Let $Y$ be the number of heads obtained. Compute $\expc[Y]$.
\end{exercise}

\begin{solution}
Let $X$ be the number showing on the die. When $X = x$ is shown on the die, the distribution of $Y$ is $Y \sim \text{Binomial}(x, 1/2)$. Hence,
\begin{align*}
    \expc(Y) &= \sum_{y=0}^{6} y \, \prb(Y = y) = \sum_{y=0}^{6} y \sum_{x=1}^{6} \prb(Y = y, X = x) \\
    &= \sum_{y=0}^{6} y \sum_{x=y}^{6} \prb(Y = y, X = x) = \sum_{y=0}^{6} y \sum_{x=y}^{6} \binom{x}{y} \left(\frac{1}{2}\right)^y \left(\frac{1}{2}\right)^{x-y} \frac{1}{6} \\
    &= \frac{1}{6} \sum_{x=1}^{6} \sum_{y=0}^{x} y \binom{x}{y} (1/2)^x = \frac{1}{6} \sum_{x=1}^{6} \frac{x}{2} = \frac{1}{6} \cdot \frac{6 \cdot 7}{4} = \frac{7}{4}.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:3.1.14}
Suppose you roll three fair coins, and let $X$ be the cube of the number of heads showing. Compute $\expc[X]$.
\end{exercise}

\begin{solution}
Let $T$ be the number of heads. Then, $X = T^3$. Hence, the expectation of $X$ is
\[
    \expc(X) = \expc(T^3) = \sum_{t=0}^{3} t^3 \cdot \binom{3}{t} \left(\frac{1}{2}\right)^3 = (0^3)\left(\frac{1}{8}\right) + (1^3)\left(\frac{3}{8}\right) + (2^3)\left(\frac{3}{8}\right) + (3^3)\left(\frac{1}{8}\right) = \frac{27}{4}.
\]
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:3.1.15}
Suppose you start with one penny and repeatedly flip a fair coin. Each time you get heads, before the first time you get tails, you get two more pennies. Let $X$ be the total number of pennies you have at the end. Compute $\expc[X]$.
\end{exercise}

\begin{solution}
Let $Z$ be the number of heads before the first tail. Then we know that $\prb(Z = k) = 1/2^{k+1}$ for $k = 0, 1, 2, \ldots$, and $\expc(Z) = (1 - (1/2))/(1/2) = 1$. Now, $X = 1 + 2Z$, so $\expc(X) = 1 + 2\expc(Z) = 1 + 2(1) = 3$.
\end{solution}

\begin{exercise}
\label{exer:3.1.16}
Suppose you start with one penny and repeatedly flip a fair coin. Each time you get heads, before the first time you get tails, your number of pennies is doubled. Let $X$ be the total number of pennies you have at the end. Compute $\expc[X]$.
\end{exercise}

\begin{solution}
Again, let $Z$ be the number of heads before the first tail, so $\prb(Z = k) = 1/2^{k+1}$ for $k = 0, 1, 2, \ldots$. Then $X = 2^Z$, so $\expc(X) = \sum_{k=0}^{\infty} 2^k (1/2^{k+1}) = \sum_{k=0}^{\infty} (1/2) = \infty$. Hence, $\expc(X)$ is infinite in this case.
\end{solution}

\begin{exercise}
\label{exer:3.1.17}
Let $X \sim \text{Geometric}(\theta)$, and let $Y = \min(X, 100)$.
\begin{enumerate}[(a)]
\item Compute $\expc[Y]$.
\item Compute $\expc[Y - X]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(Y) = \sum_{x=0}^{\infty} \min(x, 100) (1 - \theta)^x \theta = \theta \sum_{x=0}^{100} x (1 - \theta)^x + \theta(100) \sum_{x=101}^{\infty} (1 - \theta)^x = \theta S + 100(1 - \theta)^{101}$, where $S = \sum_{x=0}^{100} x (1 - \theta)^x$. Then $(1 - \theta)S = \sum_{x=0}^{100} x (1 - \theta)^{x+1} = \sum_{y=1}^{101} (y - 1) (1 - \theta)^y$. Hence, $\theta S = S - (1 - \theta)S = \sum_{x=1}^{100} (1 - \theta)^x - 100(1 - \theta)^{101} = \theta^{-1}(1 - \theta - (1 - \theta)^{101}) - 100(1 - \theta)^{101}$.
    \item $\expc(Y - X) = \expc(Y) - \expc(X) = -(1 - \theta)^{101}(1/\theta + 100)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.1.18}
Give an example of a random variable $X$ such that $\expc[\min(X, 100)] = \expc[X]$.
\end{exercise}

\begin{solution}
Any $X$ with $X \leqslant 100$ will do since then $\min(X, 100) = X$. For example, $X = 29$, or $X \sim \text{Bernoulli}(80, 1/3)$.
\end{solution}

\begin{exercise}
\label{exer:3.1.19}
Give an example of a random variable $X$ such that $\expc[\min(X, 100)] = \expc[X] - 2$.
\end{exercise}

\begin{solution}
For one example, let $\prb(X = 200) = 1$. For another, let $\prb(X = 300) = \prb(X = 100) = 1/2$.
\end{solution}

\begin{exercise}
\label{exer:3.1.20}
Give an example of a joint probability function $p_{X,Y}$ for random variables $X$ and $Y$, such that $X \sim \text{Bernoulli}(1/4)$ and $Y \sim \text{Bernoulli}(1/2)$, but $\expc[XY] \neq 1/8$.
\end{exercise}

\begin{solution}
Let $p_{X,Y}(1, 1) = p_{X,Y}(1, 0) = 1/4$, with $p_{X,Y}(0, 0) = 1/2$. Then $\prb(X = 1) = 1/2$ and $\prb(Y = 1) = 1/4$, but $\expc(XY) = 1/4$.
\end{solution}

\begin{exercise}
\label{exer:3.1.21}
For $X \sim \text{Hypergeometric}(N, M, n)$, prove that $\expc[X] = nM/N$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(X) &= \sum_{x=\max(0, n+M-N)}^{\min(n,M)} x \frac{\binom{M}{x} \binom{N-M}{n-x}}{\binom{N}{n}} = \sum_{x=\max(1, n+M-N)}^{\min(n,M)} x \frac{\binom{M}{x} \binom{N-M}{n-x}}{\binom{N}{n}} \\
    &= n \frac{M}{N} \sum_{x=\max(1, n+M-N)}^{\min(n,M)} \frac{\binom{M-1}{x-1} \binom{N-1-(M-1)}{n-1-(x-1)}}{\binom{N-1}{n-1}} \\
    &= n \frac{M}{N} \sum_{x=\max(0, n-1+(M-1)-(N-1))}^{\min(n-1,M-1)} \frac{\binom{M-1}{x} \binom{N-1-(M-1)}{n-1-x}}{\binom{N-1}{n-1}} = n \frac{M}{N}
\end{align*}
since the final sum is the sum of all $\text{Hypergeometric}(N - 1, M - 1, n)$ probabilities.
\end{solution}

\begin{exercise}
\label{exer:3.1.22}
For $X \sim \text{Negative-Binomial}(r, \theta)$, prove that $\expc[X] = r(1 - \theta)/\theta$. (Hint: Argue that if $X_1, \ldots, X_r$ are independent and identically distributed $\text{Geometric}(\theta)$, then $X = X_1 + \cdots + X_r \sim \text{Negative-Binomial}(r, \theta)$.)
\end{exercise}

\begin{solution}
We have that if $X_1, \ldots, X_r$ are i.i.d.\ $\text{Geometric}(\theta)$, then $X = X_1 + \cdots + X_r \sim \text{Negative Binomial}(r, \theta)$ so $\expc(X) = \expc(X_1 + \cdots + X_r) = r(1 - \theta)/\theta$.
\end{solution}

\begin{exercise}
\label{exer:3.1.23}
Suppose that $(X_1, X_2, X_3) \sim \text{Multinomial}(n; \theta_1, \theta_2, \theta_3)$. Prove that $\expc[X_i] = n\theta_i$.
\end{exercise}

\begin{solution}
This follows immediately since $X_i \sim \text{Binomial}(n, \theta_i)$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:3.1.24}
Let $X \sim \text{Geometric}(\theta)$. Compute $\expc[X^2]$.
\end{exercise}

\begin{solution}
Here $\expc(X^2) = \sum_k k^2 \prb(X = k) = \sum_{k=0}^{\infty} k^2 (1 - p)^k p$. Hence, $(1 - p)\expc(X^2) = \sum_{k=0}^{\infty} k^2 (1 - p)^{k+1} p = \sum_{j=1}^{\infty} (j - 1)^2 (1 - p)^j p$. Then $p\expc(X^2) = \expc(X^2) - (1 - p)\expc(X^2) = \sum_{k=1}^{\infty} [k^2 - (k - 1)^2] (1 - p)^k p = \sum_{k=1}^{\infty} [2k - 1] (1 - p)^k p = 2\expc(X) - (1 - p) = 2(1 - p)/p - (1 - p) = 2(1 - p)/p - (1 - p)$. Hence, $\expc(X^2) = 2(1 - p)/p^2 - (1 - p)/p$.
\end{solution}

\begin{exercise}
\label{exer:3.1.25}
Suppose $X$ is a discrete random variable, such that $\expc[\min(X, M)] = \expc[X]$. Prove that $\prb(X > M) = 0$.
\end{exercise}

\begin{solution}
Let $Y = X - \min(X, M)$. Then $Y$ is also discrete. Also, since $\min(X, M) \leqslant X$, we have $Y \geqslant 0$. Now, if $\expc(\min(X, M)) = \expc(X)$, then $\expc(Y) = 0$, so that $0 = \sum_y y \, \prb(Y = y) = \sum_{y \geqslant 0} y \, \prb(Y = y)$. But the only way a sum of nonnegative terms can be $0$ is if each term is $0$, i.e., $y \, \prb(Y = y) = 0$ for all $y \in \mathbb{R}^1$. This means that $\prb(Y = y) = 0$ for $y \neq 0$, so that $\prb(Y = 0) = 1$. But $\{Y = 0\} = \{\min(X, M) = X\} = \{X \leqslant M\}$, so $\prb(X \leqslant M) = 1$, i.e., $\prb(X > M) = 0$.
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:3.1.26}
How much would you be willing to pay for the deal corresponding to the St.\ Petersburg paradox (see Example~\ref{ex:3.1.12})? Justify your answer.
\end{exercise}

\begin{exercise}
\label{exer:3.1.27}
What utility function $U$ (as in the text following Example~\ref{ex:3.1.13}) best describes your own personal attitude toward money? Why?
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Absolutely Continuous Case}
\label{sec:3.2}

Suppose now that $X$ is absolutely continuous, with density function $f_X$. How can we compute $\expc[X]$ then? By analogy with the discrete case, we might try computing $\sum_x x \, \prb(X = x)$, but because $\prb(X = x)$ is always zero, this sum is always zero as well.

On the other hand, if $\epsilon$ is a small positive number, then we could try approximating $\expc[X]$ by
\[
\expc[X] \approx \sum_i i\epsilon \, \prb(i\epsilon \leqslant X \leqslant (i+1)\epsilon)
\]
where the sum is over all integers $i$. This makes sense because, if $\epsilon$ is small and $i\epsilon \leqslant X \leqslant (i+1)\epsilon$, then $X \approx i\epsilon$.

Now, we know that
\[
\prb(i\epsilon \leqslant X \leqslant (i+1)\epsilon) = \int_{i\epsilon}^{(i+1)\epsilon} f_X(x) \, \mathrm{d}x.
\]
This tells us that
\[
\expc[X] \approx \sum_i \int_{i\epsilon}^{(i+1)\epsilon} i\epsilon \, f_X(x) \, \mathrm{d}x.
\]
Furthermore, in this integral, $i\epsilon \leqslant x \leqslant (i+1)\epsilon$. Hence, $i\epsilon \approx x$. We therefore see that
\[
\expc[X] \approx \sum_i \int_{i\epsilon}^{(i+1)\epsilon} x \, f_X(x) \, \mathrm{d}x \to \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x.
\]
This prompts the following definition.

\begin{definition}
\label{def:3.2.1}
Let $X$ be an absolutely continuous random variable, with density function $f_X$. Then the expected value of $X$ is given by
\[
\expc[X] = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x.
\]
\end{definition}

From this definition, it is not too difficult to compute the expected values of many of the standard absolutely continuous distributions.

\begin{example}[{The Uniform$[0, 1]$ Distribution}]
\label{ex:3.2.1}
Let $X \sim \text{Uniform}[0, 1]$, so that the density of $X$ is given by
\[
f_X(x) = \begin{cases}
1 & 0 \leqslant x \leqslant 1 \\
0 & \text{otherwise}.
\end{cases}
\]
Hence,
\[
\expc[X] = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x = \int_0^1 x \, \mathrm{d}x = \left. \frac{x^2}{2} \right|_{x=0}^{x=1} = 1/2
\]
as one would expect.
\end{example}

\begin{example}[{The Uniform$[L, R]$ Distribution}]
\label{ex:3.2.2}
Let $X \sim \text{Uniform}[L, R]$, so that the density of $X$ is given by
\[
f_X(x) = \begin{cases}
1/(R - L) & L \leqslant x \leqslant R \\
0 & \text{otherwise}.
\end{cases}
\]
Hence,
\begin{align*}
\expc[X] &= \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x = \int_L^R x \cdot \frac{1}{R - L} \, \mathrm{d}x = \left. \frac{x^2}{2(R - L)} \right|_{x=L}^{x=R} \\
&= \frac{R^2 - L^2}{2(R - L)} = \frac{(R - L)(R + L)}{2(R - L)} = \frac{R + L}{2}
\end{align*}
again as one would expect.
\end{example}

\begin{example}[The Exponential Distribution]
\label{ex:3.2.3}
Let $Y \sim \text{Exponential}(\lambda)$, so that the density of $Y$ is given by
\[
f_Y(y) = \begin{cases}
\lambda e^{-\lambda y} & y \geqslant 0 \\
0 & y < 0.
\end{cases}
\]
Hence, integration by parts, with $u = y$ and $\mathrm{d}v = \lambda e^{-\lambda y}$ (so $\mathrm{d}u = \mathrm{d}y$, $v = -e^{-\lambda y}$), leads to
\begin{align*}
\expc[Y] &= \int_{-\infty}^{\infty} y \, f_Y(y) \, \mathrm{d}y = \int_0^{\infty} y \lambda e^{-\lambda y} \, \mathrm{d}y = \left[ -y e^{-\lambda y} \right]_0^{\infty} + \int_0^{\infty} e^{-\lambda y} \, \mathrm{d}y \\
&= \int_0^{\infty} e^{-\lambda y} \, \mathrm{d}y = \left[ -\frac{1}{\lambda} e^{-\lambda y} \right]_0^{\infty} = 0 - \left( -\frac{1}{\lambda} \right) = \frac{1}{\lambda}.
\end{align*}
In particular, if $\lambda = 1$, then $Y \sim \text{Exponential}(1)$ and $\expc[Y] = 1$.
\end{example}

\begin{example}[The $N(0, 1)$ Distribution]
\label{ex:3.2.4}
Let $Z \sim N(0, 1)$, so that the density of $Z$ is given by
\[
f_Z(z) = \phi(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}.
\]
Hence,
\begin{equation}
\label{eq:3.2.1}
\expc[Z] = \int_{-\infty}^{\infty} z \, f_Z(z) \, \mathrm{d}z = \int_{-\infty}^{0} z \cdot \frac{1}{\sqrt{2\pi}} e^{-z^2/2} \, \mathrm{d}z + \int_0^{\infty} z \cdot \frac{1}{\sqrt{2\pi}} e^{-z^2/2} \, \mathrm{d}z.
\end{equation}
But using the substitution $\zeta = -z$, we see that
\[
\int_{-\infty}^{0} z \cdot \frac{1}{\sqrt{2\pi}} e^{-z^2/2} \, \mathrm{d}z = -\int_0^{\infty} \zeta \cdot \frac{1}{\sqrt{2\pi}} e^{-\zeta^2/2} \, \mathrm{d}\zeta.
\]
Then the two integrals in \eqref{eq:3.2.1} cancel each other out, and leaving us with $\expc[Z] = 0$.
\end{example}

As with discrete variables, means of absolutely continuous random variables can also be infinite or undefined.

\begin{example}
\label{ex:3.2.5}
Let $X$ have density function given by
\[
f_X(x) = \begin{cases}
1/x^2 & x \geqslant 1 \\
0 & \text{otherwise}.
\end{cases}
\]
Then
\[
\expc[X] = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x = \int_1^{\infty} x \cdot \frac{1}{x^2} \, \mathrm{d}x = \int_1^{\infty} \frac{1}{x} \, \mathrm{d}x = \left[ \log x \right]_{x=1}^{x=\infty} = \infty.
\]
Hence, the expected value of $X$ is infinite.
\end{example}

\begin{example}
\label{ex:3.2.6}
Let $Y$ have density function given by
\[
f_Y(y) = \begin{cases}
1/(2y^2) & y \geqslant 1 \\
1/(2y^2) & y \leqslant -1 \\
0 & \text{otherwise}.
\end{cases}
\]
Then
\[
\expc[Y] = \int_{-\infty}^{\infty} y \, f_Y(y) \, \mathrm{d}y = \int_1^{\infty} y \cdot \frac{1}{y^2} \, \mathrm{d}y + \int_{-\infty}^{-1} y \cdot \frac{1}{y^2} \, \mathrm{d}y = \int_1^{\infty} \frac{1}{y} \, \mathrm{d}y - \int_1^{\infty} \frac{1}{y} \, \mathrm{d}y
\]
which is undefined. Hence, the expected value of $Y$ is undefined (i.e., does not exist) in this case.
\end{example}

Theorem~\ref{thm:3.1.1} remains true in the continuous case, as follows.

\begin{theorem}
\label{thm:3.2.1}
\begin{enumerate}[(a)]
\item Let $X$ be an absolutely continuous random variable, with density function $f_X$, and let $g : \mathbf{R}^1 \to \mathbf{R}^1$ be some function. Then when the expectation of $g(X)$ exists,
\[
\expc[g(X)] = \int_{-\infty}^{\infty} g(x) \, f_X(x) \, \mathrm{d}x.
\]
\item Let $X$ and $Y$ be jointly absolutely continuous random variables, with joint density function $f_{X,Y}$, and let $h : \mathbf{R}^2 \to \mathbf{R}^1$ be some function. Then when the expectation of $h(X, Y)$ exists,
\[
\expc[h(X, Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} h(x, y) \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y.
\]
\end{enumerate}
\end{theorem}

We do not prove Theorem~\ref{thm:3.2.1} here; however, we shall use it often. For a first use of this result, we prove that expected values for absolutely continuous random variables are still linear.

\begin{theorem}[Linearity of expected values]
\label{thm:3.2.2}
Let $X$ and $Y$ be jointly absolutely continuous random variables, and let $a$ and $b$ be real numbers. Then $\expc[aX + bY] = a\expc[X] + b\expc[Y]$.
\end{theorem}

\begin{proof}
Let $f_{X,Y}$ be the joint density function of $X$ and $Y$. Then using Theorem~\ref{thm:3.2.1}, we compute that
\begin{align*}
\expc[Z] &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ax + by) \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y \\
&= a \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y + b \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y \\
&= a \int_{-\infty}^{\infty} x \left( \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y \right) \mathrm{d}x + b \int_{-\infty}^{\infty} y \left( \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}x \right) \mathrm{d}y.
\end{align*}
But $\int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y = f_X(x)$ and $\int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}x = f_Y(y)$, so
\[
\expc[Z] = a \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x + b \int_{-\infty}^{\infty} y \, f_Y(y) \, \mathrm{d}y = a\expc[X] + b\expc[Y]
\]
as claimed.
\end{proof}

Just as in the discrete case, we have that $\expc[X + c] = \expc[X] + c$ for an absolutely continuous random variable $X$. Note, however, that this is not implied by Theorem~\ref{thm:3.2.2} because the constant $c$ is a discrete, not absolutely continuous, random variable. In fact, we need a more general treatment of expectation to obtain this result (see Section~\ref{sec:3.7}). In any case, the result is true and we again have that the mean of a random variable serves as a measure of the location of the probability distribution of $X$. In Figure~\ref{fig:3.2.1}, we have plotted the densities and means of the absolutely continuous random variables $X$ and $X + c$. The change of variable results from Section~\ref{ssec:2.6.2} give $f_{X+c}(x) = f_X(x - c)$.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_2_1.pdf}
  \caption{The densities and means of absolutely continuous random variables $X$ and $X + c$.}
  \label{fig:3.2.1}
\end{figure}

\begin{example}[The $N(\mu, \sigma^2)$ Distribution]
\label{ex:3.2.7}
Let $X \sim N(\mu, \sigma^2)$. Then we know (cf.\ Exercise~\ref{exer:2.6.3}) that if $Z = (X - \mu)/\sigma$, then $Z \sim N(0, 1)$. Hence, we can write $X = \mu + \sigma Z$, where $Z \sim N(0, 1)$. But we know (see Example~\ref{ex:3.2.4}) that $\expc[Z] = 0$ and (see Example~\ref{ex:3.1.5}) that $\expc[\mu] = \mu$. Hence, using Theorem~\ref{thm:3.2.2}, $\expc[X] = \expc[\mu + \sigma Z] = \expc[\mu] + \expc[\sigma Z] = \mu + \sigma \cdot 0 = \mu$.
\end{example}

If $X$ and $Y$ are independent, then the following results show that we again have $\expc[XY] = \expc[X]\expc[Y]$.

\begin{theorem}
\label{thm:3.2.3}
Let $X$ and $Y$ be jointly absolutely continuous random variables that are independent. Then $\expc[XY] = \expc[X]\expc[Y]$.
\end{theorem}

\begin{proof}
Independence implies (Theorem~\ref{thm:2.8.3}) that $f_{X,Y}(x, y) = f_X(x) f_Y(y)$. Using this, along with Theorem~\ref{thm:3.2.1}, we compute
\begin{align*}
\expc[XY] &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy \, f_X(x) f_Y(y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x \int_{-\infty}^{\infty} y \, f_Y(y) \, \mathrm{d}y = \expc[X]\expc[Y]
\end{align*}
as claimed.
\end{proof}

The monotonicity property (Theorem~\ref{thm:3.1.4}) still holds as well.

\begin{theorem}[Monotonicity]
\label{thm:3.2.4}
Let $X$ and $Y$ be jointly continuous random variables, and suppose that $X \leqslant Y$. Then $\expc[X] \leqslant \expc[Y]$.
\end{theorem}

\begin{proof}
Let $f_{X,Y}$ be the joint density function of $X$ and $Y$. Because $X \leqslant Y$, the density $f_{X,Y}$ can be chosen so that $f_{X,Y}(x, y) = 0$ whenever $x > y$. Now let $Z = Y - X$. Then by Theorem~\ref{thm:3.2.1}(b),
\[
\expc[Z] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (y - x) \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y.
\]
Because $f_{X,Y}(x, y) = 0$ whenever $x > y$, this implies that $\expc[Z] \geqslant 0$. But by Theorem~\ref{thm:3.2.2}, $\expc[Z] = \expc[Y] - \expc[X]$. Hence, $\expc[Y] - \expc[X] \geqslant 0$, so that $\expc[Y] \geqslant \expc[X]$.
\end{proof}

\subsection*{Summary of Section~\ref{sec:3.2}}

\begin{itemize}
\item If $X$ is absolutely continuous, then $\expc[X] = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x$.
\item The expected values of the uniform, exponential, and normal distributions were computed.
\item Expectation for absolutely continuous random variables is linear and monotone.
\item If $X$ and $Y$ are independent, then we still have $\expc[XY] = \expc[X]\expc[Y]$.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:3.2.1}
Compute $C$ and $\expc[X]$ when the density function of $X$ is given by each of the following.
\begin{enumerate}[(a)]
\item
\[
f_X(x) = \begin{cases}
C & 5 \leqslant x \leqslant 9 \\
0 & \text{otherwise}
\end{cases}
\]
\item
\[
f_X(x) = \begin{cases}
C(x - 1) & 6 \leqslant x \leqslant 8 \\
0 & \text{otherwise}
\end{cases}
\]
\item
\[
f_X(x) = \begin{cases}
C x^4 & -5 \leqslant x \leqslant 2 \\
0 & \text{otherwise}
\end{cases}
\]
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $1 = \int_{-\infty}^{\infty} f_X(x) \, \mathrm{d}x = \int_5^9 C \, \mathrm{d}x = 4C$, where $C = 1/4$. Then $\expc(X) = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x = \int_5^9 x (1/4) \, \mathrm{d}x = (9^2 - 5^2)/8 = 7$.
    \item $1 = \int_{-\infty}^{\infty} f_X(x) \, \mathrm{d}x = \int_6^8 C(x + 1) \, \mathrm{d}x = C(9^2 - 7^2)/2 = 16C$, where $C = 1/16$. Then $\expc(X) = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x = \int_6^8 x (1/16)(x + 1) \, \mathrm{d}x = (8^3 - 6^3)/48 + (8^2 - 6^2)/32 = 169/24 = 7.04$.
    \item $1 = \int_{-\infty}^{\infty} f_X(x) \, \mathrm{d}x = \int_{-5}^{-2} Cx^4 \, \mathrm{d}x = C((-2)^5 - (-5)^5)/5 = C \cdot 3093/5$, where $C = 5/3093$. Then $\expc(X) = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x = \int_{-5}^{-2} x (5/3093)(x^4) \, \mathrm{d}x = (5/3093)((-2)^6 - (-5)^6)/6 = -8645/2062 = -4.19$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.2.2}
Let $X$ and $Y$ have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise}
\end{cases}
\]
as in Examples~\ref{ex:2.7.6} and \ref{ex:2.7.7}. Compute each of the following.
\begin{enumerate}[(a)]
\item $\expc[X]$
\item $\expc[Y]$
\item $\expc[3X - 7Y]$
\item $\expc[X^2]$
\item $\expc[Y^2]$
\item $\expc[XY]$
\item $\expc[XY - 14]$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X) = \int_0^1 \int_0^1 x (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = 2/3$.
    \item $\expc(Y) = \int_0^1 \int_0^1 y (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = 46/63$.
    \item $\expc(3X + 7Y) = 3\expc(X) + 7\expc(Y) = 3(2/3) + 7(46/63) = 64/9$.
    \item $\expc(X^2) = \int_0^1 \int_0^1 x^2 (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = 23/45$.
    \item $\expc(Y^2) = \int_0^1 \int_0^1 y^2 (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = 7/12$.
    \item $\expc(XY) = \int_0^1 \int_0^1 xy (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = 10/21$.
    \item $\expc(XY + 14) = \expc(XY) + 14 = (10/21) + 14 = 304/21$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.2.3}
Let $X$ and $Y$ have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
(4xy + 3x^2 y^2)/18 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 3 \\
0 & \text{otherwise}
\end{cases}
\]
Compute each of the following.
\begin{enumerate}[(a)]
\item $\expc[X]$
\item $\expc[Y]$
\item $\expc[X^2]$
\item $\expc[Y^2]$
\item $\expc[Y^4]$
\item $\expc[X^2 Y^3]$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^3 \int_0^1 x \left(\frac{4xy + 3x^2 y^2}{18}\right) \mathrm{d}x \, \mathrm{d}y = 17/24$.
    \item $\expc(Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^3 \int_0^1 y \left(\frac{4xy + 3x^2 y^2}{18}\right) \mathrm{d}x \, \mathrm{d}y = 17/8$.
    \item $\expc(X^2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^2 \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^3 \int_0^1 x^2 \left(\frac{4xy + 3x^2 y^2}{18}\right) \mathrm{d}x \, \mathrm{d}y = 11/20$.
    \item $\expc(Y^2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y^2 \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^3 \int_0^1 y^2 \left(\frac{4xy + 3x^2 y^2}{18}\right) \mathrm{d}x \, \mathrm{d}y = 99/20$.
    \item $\expc(Y^4) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y^4 \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^3 \int_0^1 y^4 \left(\frac{4xy + 3x^2 y^2}{18}\right) \mathrm{d}x \, \mathrm{d}y = 216/7$.
    \item $\expc(X^2 Y^3) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^2 y^3 \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^3 \int_0^1 x^2 y^3 \left(\frac{4xy + 3x^2 y^2}{18}\right) \mathrm{d}x \, \mathrm{d}y = 27/4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.2.4}
Let $X$ and $Y$ have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
(6xy + 9/2 x^2 y^2) & 0 \leqslant y \leqslant x \leqslant 1 \\
0 & \text{otherwise}
\end{cases}
\]
Compute each of the following.
\begin{enumerate}[(a)]
\item $\expc[X]$
\item $\expc[Y]$
\item $\expc[X^2]$
\item $\expc[Y^2]$
\item $\expc[Y^4]$
\item $\expc[X^2 Y^3]$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \int_y^1 x (6xy + (9/2)x^2 y^2) \, \mathrm{d}x \, \mathrm{d}y = 57/70$.
    \item $\expc(Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \int_y^1 y (6xy + (9/2)x^2 y^2) \, \mathrm{d}x \, \mathrm{d}y = 157/280$.
    \item $\expc(X^2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^2 \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \int_y^1 x^2 (6xy + (9/2)x^2 y^2) \, \mathrm{d}x \, \mathrm{d}y = 11/16$.
    \item $\expc(Y^2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y^2 \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \int_y^1 y^2 (6xy + (9/2)x^2 y^2) \, \mathrm{d}x \, \mathrm{d}y = 29/80$.
    \item $\expc(Y^4) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y^4 \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \int_y^1 y^4 (6xy + (9/2)x^2 y^2) \, \mathrm{d}x \, \mathrm{d}y = 53/280$.
    \item $\expc(X^2 Y^3) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^2 y^3 \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \int_y^1 x^2 y^3 (6xy + (9/2)x^2 y^2) \, \mathrm{d}x \, \mathrm{d}y = 133/660$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.2.5}
Let $X \sim \text{Uniform}[3, 7]$ and $Y \sim \text{Exponential}(9)$. Compute $\expc[5X - 6Y]$.
\end{exercise}

\begin{solution}
$\expc(-5X - 6Y) = -5\expc(X) - 6\expc(Y) = -5((3 + 7)/2) - 3(1/9) = -76/3$.
\end{solution}

\begin{exercise}
\label{exer:3.2.6}
Let $X \sim \text{Uniform}[-12, 9]$ and $Y \sim N(8, 9)$. Compute $\expc[11X - 14Y + 3]$.
\end{exercise}

\begin{solution}
$\expc(11X + 14Y + 3) = 11\expc(X) + 14\expc(Y) + 3 = 11(((-12) + (-9))/2) + 14(-8) + 3 = -449/2$.
\end{solution}

\begin{exercise}
\label{exer:3.2.7}
Let $Y \sim \text{Exponential}(9)$ and $Z \sim \text{Exponential}(8)$. Compute $\expc[Y - Z]$.
\end{exercise}

\begin{solution}
$\expc(Y + Z) = \expc(Y) + \expc(Z) = (1/9) + (1/8) = 17/72$.
\end{solution}

\begin{exercise}
\label{exer:3.2.8}
Let $Y \sim \text{Exponential}(9)$ and $Z \sim \text{Gamma}(5, 4)$. Compute $\expc[Y - Z]$. (You may use Problem~\ref{exer:3.2.16} below.)
\end{exercise}

\begin{solution}
$\expc(Y + Z) = \expc(Y) + \expc(Z) = (1/9) + (5/4) = 49/36$.
\end{solution}

\begin{exercise}
\label{exer:3.2.9}
Suppose $X$ has density function $f(x) = (3/20)(x^2 + x^3)$ for $0 \leqslant x \leqslant 2$, otherwise $f(x) = 0$. Compute each of $\expc[X]$, $\expc[X^2]$, and $\expc[X^3]$, and rank them from largest to smallest.
\end{exercise}

\begin{solution}
Let $\mu_k = \expc(X^k)$ for $k > -3$.
\[
    \mu_k = \int_{\mathbb{R}^1} x^k f(x) \, \mathrm{d}x = \int_0^2 x^k \cdot \frac{3}{20}(x^2 + x^3) \, \mathrm{d}x = \frac{3}{20} \left[\frac{x^{k+3}}{k+3} + \frac{x^{k+4}}{k+4}\right]_{x=0}^{x=2} = \frac{3}{20} \left(\frac{2^{k+3}}{k+3} + \frac{2^{k+4}}{k+4}\right) = \frac{3 \cdot 2^{k+1}(3k + 10)}{5(k+3)(k+4)}.
\]
Hence, $\mu_1 = 39/25 = 1.56$, $\mu_2 = 64/25 = 2.56$, $\mu_3 = 152/35 = 4.34$. Therefore, $\expc(X^3) > \expc(X^2) > \expc(X)$.
\end{solution}

\begin{exercise}
\label{exer:3.2.10}
Suppose $X$ has density function $f(x) = (12/7)(x^2 + x^3)$ for $0 \leqslant x \leqslant 1$, otherwise $f(x) = 0$. Compute each of $\expc[X]$, $\expc[X^2]$, and $\expc[X^3]$ and rank them from largest to smallest.
\end{exercise}

\begin{solution}
Let $\mu_k = \expc(X^k)$ for $k > -3$.
\[
    \mu_k = \int_{\mathbb{R}^1} x^k f(x) \, \mathrm{d}x = \int_0^1 x^k \cdot \frac{12}{7}(x^2 + x^3) \, \mathrm{d}x = \frac{12}{7} \left[\frac{x^{k+3}}{k+3} + \frac{x^{k+4}}{k+4}\right]_{x=0}^{x=1} = \frac{12}{7} \left(\frac{1}{k+3} + \frac{1}{k+4}\right) = \frac{12(2k + 7)}{7(k+3)(k+4)}.
\]
Hence, $\mu_1 = 54/70 = 0.771$, $\mu_2 = 22/35 = 0.629$, and $\mu_3 = 26/49 = 0.531$. Therefore, $\expc(X) > \expc(X^2) > \expc(X^3)$.
\end{solution}

\begin{exercise}
\label{exer:3.2.11}
Suppose men's heights (in centimeters) follow the distribution $N(174, 20^2)$, while those of women follow the distribution $N(160, 15^2)$. Compute the mean total height of a man--woman married couple.
\end{exercise}

\begin{solution}
Let $X$ and $Y$ be the height of wife and husband. The expected value of $Z = X + Y$ is
\[
    \expc(Z) = \expc(X + Y) = \expc(X) + \expc(Y) = 174 + 160 = 334.
\]
Here, we used Theorem \ref{thm:3.2.2} and Example \ref{ex:3.2.7}.
\end{solution}

\begin{exercise}
\label{exer:3.2.12}
Suppose $X$ and $Y$ are independent, with $\expc[X] = 5$ and $\expc[Y] = 6$. For each of the following variables $Z$, either compute $\expc[Z]$ or explain why we cannot determine $\expc[Z]$ from the available information:
\begin{enumerate}[(a)]
\item $Z = X + Y$
\item $Z = XY$
\item $Z = 2X + 4Y$
\item $Z = 2X^3 + 4Y$
\item $Z = 2(X + 3)(4Y)$
\item $Z = 2(X + 3X)(4Y)$
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item From Theorem \ref{thm:3.2.2}, $\expc(Z) = \expc(X + Y) = \expc(X) + \expc(Y) = 5 + 6 = 11$.
  \item We have $\expc(Z) = \expc(XY) = \expc(X)\expc(Y) = 5 \times 6 = 30$ by Theorem \ref{thm:3.2.3} based on the independence of $X$ and $Y$.
  \item From Theorem \ref{thm:3.2.2}, we have $\expc(Z) = \expc(2X - 4Y) = 2\expc(X) - 4\expc(Y) = 2 \cdot 5 - 4 \cdot 6 = -14$.
  \item From Theorem \ref{thm:3.2.2}, $\expc(Z) = \expc(2X(3 + 4Y)) = \expc(6X + 8XY) = 6\expc(X) + 8\expc(XY) = 6 \cdot 5 + 8 \cdot 30 = 270$. The result in part (b) was also used in this computation.
  \item The formula of $Z$ is simplified as $Z = (2 + X)(3 + 4Y) = 6 + 3X + 8Y + 4XY$. By Theorem \ref{thm:3.2.2}, $\expc(Z) = 6 + 3\expc(X) + 8\expc(Y) + 4\expc(XY) = 6 + 3 \cdot 5 + 8 \cdot 6 + 4 \cdot 30 = 189$.
  \item The formula is simplified as $Z = (2 + X)(3X + 4Y) = 6X + 8Y + 4XY + 3X^2$. By Theorem \ref{thm:3.2.2}, $\expc(Z) = 6\expc(X) + 8\expc(Y) + 4\expc(XY) + 3\expc(X^2) = 6 \cdot 5 + 8 \cdot 6 + 4 \cdot 30 + 3\expc(X^2) = 198 + 3\expc(X^2)$. The value $\expc(X^2)$ is unknown. Hence, $\expc(Z)$ can be determined based on the given information.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.2.13}
Suppose darts are randomly thrown at a wall. Let $X$ be the distance (in centimeters) from the left edge of the dart's point to the left end of the wall, and let $Y$ be the distance from the right edge of the dart's point to the left end of the wall. Assume the dart's point is $0.1$ centimeters thick, and that $\expc[X] = 214$. Compute $\expc[Y]$.
\end{exercise}

\begin{solution}
Since the dart's point is 0.1 centimeters thick, the random variable $Y$ must be $Y = X + 0.1$. By Theorem \ref{thm:3.2.2}, $\expc(Y) = \expc(X + 0.1) = \expc(X) + 0.1 = 214.1$.
\end{solution}

\begin{exercise}
\label{exer:3.2.14}
Let $X$ be the mean height of all citizens measured from the top of their head, and let $Y$ be the mean height of all citizens measured from the top of their head or hat (whichever is higher). Must we have $\expc[Y] \geqslant \expc[X]$? Why or why not?
\end{exercise}

\begin{solution}
Let $X$ be the citizen's height from the top of his/her head and $Y$ be the citizen's height from the top of his/her head or hat. Then, $Y \geqslant X$. Therefore, we have $\expc(Y) \geqslant \expc(X)$ by Theorem \ref{thm:3.2.4}.
\end{solution}

\begin{exercise}
\label{exer:3.2.15}
Suppose basketball teams A and B each have five players and that each member of team A is being ``guarded'' by a unique member of team B. Suppose it is noticed that each member of team A is taller than the corresponding guard from team B. Does it necessarily follow that the mean height of team A is larger than the mean height of team B? Why or why not?
\end{exercise}

\begin{solution}
Let $x_1, \ldots, x_5$ be the heights of the members of team A. Let $y_1, \ldots, y_5$ be the heights of the member of team B who is guarding $x_1, \ldots, x_5$ respectively. From the assumption, $x_i > y_i$. Hence, the mean height of team A $= (x_1 + \cdots + x_5)/5 > (y_1 + \cdots + y_5)/5 =$ the mean height of team B. Therefore, the mean height of team A is larger than the mean height of team B.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:3.2.16}
Let $\alpha > 0$ and $\lambda > 0$, and let $X \sim \text{Gamma}(\alpha, \lambda)$. Prove that $\expc[X] = \alpha/\lambda$. (Hint: The computations are somewhat similar to those of Problem~\ref{exer:2.4.15}. You will also need property \eqref{eq:2.4.7} of the gamma function.)
\end{exercise}

\begin{solution}
Letting $t = \lambda x$, we have that $\expc(X) = \int_0^{\infty} x \frac{\lambda^{\alpha} x^{\alpha-1}}{\Gamma(\alpha)} e^{-\lambda x} \, \mathrm{d}x = \int_0^{\infty} \frac{\lambda^{\alpha} x^{\alpha}}{\Gamma(\alpha)} e^{-\lambda x} \, \mathrm{d}x = \int_0^{\infty} \frac{\lambda^{\alpha} t^{\alpha}}{\lambda^{\alpha} \Gamma(\alpha)} e^{-t} (1/\lambda) \, \mathrm{d}t = \frac{1}{\lambda \Gamma(\alpha)} \int_0^{\infty} t^{\alpha} e^{-t} \, \mathrm{d}x = \frac{1}{\lambda \Gamma(\alpha)} \Gamma(\alpha + 1) = \frac{1}{\lambda \Gamma(\alpha)} \alpha \Gamma(\alpha) = \alpha/\lambda$.
\end{solution}

\begin{exercise}
\label{exer:3.2.17}
Suppose that $X$ follows the logistic distribution (see Problem~\ref{exer:2.4.18}). Prove that $\expc[X] = 0$.
\end{exercise}

\begin{solution}
We have $\expc(X) = \int_0^{-\infty} x e^{-x} (1 + e^{-x})^{-2} \, \mathrm{d}x + \int_0^{\infty} x e^{-x} (1 + e^{-x})^{-2} \, \mathrm{d}x = -\int_0^{\infty} x e^{-x} (1 + e^{-x})^{-2} \, \mathrm{d}x + \int_0^{\infty} x e^{-x} (1 + e^{-x})^{-2} \, \mathrm{d}x$, so $\expc(X) = 0$, provided $\int_0^{\infty} x e^{-x} (1 + e^{-x})^{-2} \, \mathrm{d}x < \infty$. This is the case because $\int_0^{\infty} x e^{-x} (1 + e^{-x})^{-2} \, \mathrm{d}x \leqslant \int_0^{\infty} x e^{-x} \, \mathrm{d}x = 1$.
\end{solution}

\begin{exercise}
\label{exer:3.2.18}
Suppose that $X$ follows the Weibull$(\lambda, \beta)$ distribution (see Problem~\ref{exer:2.4.19}). Prove that $\expc[X] = \lambda^{-1/\beta} \Gamma(1 + 1/\beta)$.
\end{exercise}

\begin{solution}
We have that $\expc(X) = \int_0^{\infty} x \alpha x^{\alpha-1} e^{-x^{\alpha}} \, \mathrm{d}x = \int_0^{\infty} \alpha x^{\alpha} e^{-x^{\alpha}} \, \mathrm{d}x$ and putting $u = x^{\alpha}$, $x = u^{1/\alpha}$, $\mathrm{d}u = \alpha x^{\alpha-1} \, \mathrm{d}x$ we have that $\expc(X) = \int_0^{\infty} u^{1/\alpha} e^{-u} \, \mathrm{d}u = \Gamma(1/\alpha + 1)$.
\end{solution}

\begin{exercise}
\label{exer:3.2.19}
Suppose that $X$ follows the Pareto$(\alpha)$ distribution (see Problem~\ref{exer:2.4.20}) for $\alpha > 1$. Prove that $\expc[X] = \alpha/(\alpha - 1)$. What is $\expc[X]$ when $0 < \alpha \leqslant 1$?
\end{exercise}

\begin{solution}
We have that
\[
    \expc(X) = \int_0^{\infty} x \alpha (1 + x)^{-\alpha-1} \, \mathrm{d}x = \int_0^{\infty} \alpha (1 + x)^{-\alpha} \, \mathrm{d}x - 1 = \begin{cases} \dfrac{\alpha}{-\alpha+1} (1 + x)^{-\alpha+1} \Big|_0^{\infty} - 1 & \alpha \neq 1 \\[1em] \alpha \ln(1 + x) \Big|_0^{\infty} & \alpha = 1 \end{cases} = \begin{cases} \infty & 0 < \alpha \leqslant 1 \\[0.5em] 1/(\alpha - 1) & \text{if } \alpha > 1. \end{cases}
\]
\end{solution}

\begin{exercise}
\label{exer:3.2.20}
Suppose that $X$ follows the Cauchy distribution (see Problem~\ref{exer:2.4.21}). Argue that $\expc[X]$ does not exist. (Hint: Compute the integral in two parts, where the integrand is positive and where the integrand is negative.)
\end{exercise}

\begin{solution}
We have that $\int_0^{\infty} x \pi^{-1} (1 + x^2)^{-1} \, \mathrm{d}x = (\ln(1 + x^2))/2 \big|_0^{\infty} = \infty$ and $\int_{-\infty}^{0} x \pi^{-1} (1 + x^2)^{-1} \, \mathrm{d}x = -\infty$, so $\expc(X)$ doesn't exist.
\end{solution}

\begin{exercise}
\label{exer:3.2.21}
Suppose that $X$ follows the Laplace distribution (see Problem~\ref{exer:2.4.22}). Prove that $\expc[X] = 0$.
\end{exercise}

\begin{solution}
We have that
\[
    \expc(X) = \int_{-\infty}^{0} x e^x \, \mathrm{d}x + \int_0^{\infty} x e^{-x} \, \mathrm{d}x = -\int_0^{\infty} x e^{-x} \, \mathrm{d}x + \int_0^{\infty} x e^{-x} \, \mathrm{d}x = -1 + 1 = 0.
\]
\end{solution}

\begin{exercise}
\label{exer:3.2.22}
Suppose that $X$ follows the Beta$(a, b)$ distribution (see Problem~\ref{exer:2.4.24}). Prove that $\expc[X] = a/(a + b)$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(X) &= \int_0^1 x \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1 - x)^{b-1} \, \mathrm{d}x = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \int_0^1 x^a (1 - x)^{b-1} \, \mathrm{d}x \\
    &= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \cdot \frac{\Gamma(a + 1) \Gamma(b)}{\Gamma(a + b + 1)} = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \cdot \frac{a \Gamma(a) \Gamma(b)}{(a + b) \Gamma(a + b)} = \frac{a}{a + b}.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:3.2.23}
Suppose that $(X_1, X_2) \sim \text{Dirichlet}(\alpha_1, \alpha_2, \alpha_3)$ (see Problem~\ref{exer:2.7.17}). Prove that $\expc[X_i] = \alpha_i / (\alpha_1 + \alpha_2 + \alpha_3)$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(X_1) &= \int_0^1 \int_0^{1-x_2} x_1 \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1) \Gamma(\alpha_2) \Gamma(\alpha_3)} x_1^{\alpha_1-1} x_2^{\alpha_2-1} (1 - x_1 - x_2)^{\alpha_3-1} \, \mathrm{d}x_1 \, \mathrm{d}x_2 \\
    &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1) \Gamma(\alpha_2) \Gamma(\alpha_3)} \int_0^1 \int_0^{1-x_2} x_1^{\alpha_1} x_2^{\alpha_2-1} (1 - x_1 - x_2)^{\alpha_3-1} \, \mathrm{d}x_1 \, \mathrm{d}x_2 \\
    &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1) \Gamma(\alpha_2) \Gamma(\alpha_3)} \cdot \frac{\Gamma(\alpha_1 + 1) \Gamma(\alpha_2) \Gamma(\alpha_3)}{\Gamma(\alpha_1 + \alpha_2 + \alpha_3 + 1)} = \frac{\alpha_1}{\alpha_1 + \alpha_2 + \alpha_3}.
\end{align*}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variance, Covariance, and Correlation}
\label{sec:3.3}

Now that we understand expected value, we can use it to define various other quantities of interest. The numerical values of these quantities provide information about the distribution of random variables.

Given a random variable $X$, we know that the average value of $X$ will be $\expc[X]$. However, this tells us nothing about how far $X$ tends to be from $\expc[X]$. For that, we have the following definition.

\begin{definition}
\label{def:3.3.1}
The \emph{variance} of a random variable $X$ is the quantity
\begin{equation}
\label{eq:3.3.1}
\sigma_X^2 = \var(X) = \expc[(X - \mu_X)^2]
\end{equation}
where $\mu_X = \expc[X]$ is the mean of $X$.
\end{definition}

We note that it is also possible to write \eqref{eq:3.3.1} as $\var(X) = \expc[(X - \expc[X])^2]$; however, the multiple uses of ``$\expc$'' may be confusing. Also, because $(X - \mu_X)^2$ is always nonnegative, its expectation is always defined, so the variance of $X$ is always defined.

Intuitively, the variance $\var(X)$ is a measure of how spread out the distribution of $X$ is, or how random $X$ is, or how much $X$ varies, as the following example illustrates.

\begin{example}
\label{ex:3.3.1}
Let $X$ and $Y$ be two discrete random variables, with probability functions
\[
p_X(x) = \begin{cases}
1 & x = 10 \\
0 & \text{otherwise}
\end{cases}
\quad \text{and} \quad
p_Y(y) = \begin{cases}
1/2 & y = 5 \\
1/2 & y = 15 \\
0 & \text{otherwise}
\end{cases}
\]
respectively.

Then $\expc[X] = \expc[Y] = 10$. However,
\[
\var(X) = (10 - 10)^2 \cdot 1 = 0
\]
while
\[
\var(Y) = (5 - 10)^2 \cdot (1/2) + (15 - 10)^2 \cdot (1/2) = 25.
\]
We thus see that, while $X$ and $Y$ have the same expected value, the variance of $Y$ is much greater than that of $X$. This corresponds to the fact that $Y$ is more random than $X$; that is, it varies more than $X$ does.
\end{example}

\begin{example}
\label{ex:3.3.2}
Let $X$ have probability function given by
\[
p_X(x) = \begin{cases}
1/2 & x = 2 \\
1/6 & x = 3 \\
1/6 & x = 4 \\
1/6 & x = 5 \\
0 & \text{otherwise}.
\end{cases}
\]
Then $\expc[X] = 2 \cdot (1/2) + 3 \cdot (1/6) + 4 \cdot (1/6) + 5 \cdot (1/6) = 3$. Hence,
\[
\var(X) = (2 - 3)^2 \cdot \frac{1}{2} + (3 - 3)^2 \cdot \frac{1}{6} + (4 - 3)^2 \cdot \frac{1}{6} + (5 - 3)^2 \cdot \frac{1}{6} = 4/3.
\]
\end{example}

\begin{example}
\label{ex:3.3.3}
Let $Y \sim \text{Bernoulli}(\theta)$. Then $\expc[Y] = \theta$. Hence,
\[
\var(Y) = \expc[(Y - \theta)^2] = (1 - \theta)^2 \cdot \theta + (0 - \theta)^2 \cdot (1 - \theta) = \theta^2 - 3\theta^3 + 3\theta^2 - \theta^3 = \theta(1 - \theta).
\]
\end{example}

The square in \eqref{eq:3.3.1} implies that the ``scale'' of $\var(X)$ is different from the scale of $X$. For example, if $X$ were measuring a distance in meters (m), then $\var(X)$ would be measuring in meters squared (m$^2$). If we then switched from meters to feet, we would have to multiply $X$ by about $3.28084$ but would have to multiply $\var(X)$ by about $3.28084^2$.

To correct for this ``scale'' problem, we can simply take the square root, as follows.

\begin{definition}
\label{def:3.3.2}
The \emph{standard deviation} of a random variable $X$ is the quantity
\[
\sigma_X = \text{Sd}(X) = \sqrt{\var(X)} = \sqrt{\expc[(X - \mu_X)^2]}.
\]
\end{definition}

It is reasonable to ask why, in \eqref{eq:3.3.1}, we need the square at all. Now, if we simply omitted the square and considered $\expc[X - \mu_X]$, we would always get zero (because $\mu_X = \expc[X]$), which is useless. On the other hand, we could instead use $\expc[|X - \mu_X|]$. This would, like \eqref{eq:3.3.1}, be a valid measure of the average distance of $X$ from $\mu_X$. Furthermore, it would not have the ``scale problem'' that $\var(X)$ does. However, we shall see that $\var(X)$ has many convenient properties. By contrast, $\expc[|X - \mu_X|]$ is very difficult to work with. Thus, it is purely for convenience that we define variance by $\expc[(X - \mu_X)^2]$ instead of $\expc[|X - \mu_X|]$.

Variance will be very important throughout the remainder of this book. Thus, we pause to present some important properties of $\var$.

\begin{theorem}
\label{thm:3.3.1}
Let $X$ be any random variable, with expected value $\mu_X = \expc[X]$, and variance $\var(X)$. Then the following hold true:
\begin{enumerate}[(a)]
\item $\var(X) \geqslant 0$.
\item If $a$ and $b$ are real numbers, $\var(aX + b) = a^2 \var(X)$.
\item $\var(X) = \expc[X^2] - \mu_X^2 = \expc[X^2] - (\expc[X])^2$. (That is, variance is equal to the second moment minus the square of the first moment.)
\item $\var(X) \leqslant \expc[X^2]$.
\end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}[(a)]
    \item This is immediate, because we always have $(X - \mu_X)^2 \geqslant 0$.
    \item We note that $\mu_{aX+b} = \expc[aX + b] = a\expc[X] + b = a\mu_X + b$, by linearity. Hence, again using linearity,
\[
\var(aX + b) = \expc[(aX + b - a\mu_X - b)^2] = \expc[(aX - a\mu_X)^2] = a^2 \expc[(X - \mu_X)^2] = a^2 \var(X).
\]
    \item Again, using linearity,
\begin{align*}
\var(X) &= \expc[(X - \mu_X)^2] = \expc[X^2 - 2X\mu_X + \mu_X^2] \\
&= \expc[X^2] - 2\expc[X]\mu_X + \mu_X^2 = \expc[X^2] - 2\mu_X^2 + \mu_X^2 = \expc[X^2] - \mu_X^2.
\end{align*}
    \item This follows immediately from part (c) because we have $\mu_X^2 \geqslant 0$.
  \end{enumerate}
\end{proof}

Theorem~\ref{thm:3.3.1} often provides easier ways of computing variance, as in the following examples.

\begin{example}[Variance of the Exponential Distribution]
\label{ex:3.3.4}
Let $W \sim \text{Exponential}(\lambda)$, so that $f_W = \lambda e^{-\lambda\omega}$. Then $\expc[W] = 1/\lambda$. Also, using integration by parts,
\begin{align*}
\expc[W^2] &= \int_0^{\infty} \omega^2 \lambda e^{-\lambda\omega} \, \mathrm{d}\omega = \int_0^{\infty} \omega^2 \, \mathrm{d}(-e^{-\lambda\omega}) \\
&= \frac{2}{\lambda} \int_0^{\infty} \omega e^{-\lambda\omega} \, \mathrm{d}\omega = \frac{2}{\lambda} \expc[W] = \frac{2}{\lambda^2}.
\end{align*}
Hence, by part (c) of Theorem~\ref{thm:3.3.1},
\[
\var(W) = \expc[W^2] - (\expc[W])^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}.
\]
\end{example}

\begin{example}
\label{ex:3.3.5}
Let $W \sim \text{Exponential}(\lambda)$, and let $Y = 5W + 3$. Then from the above example, $\var(W) = 1/\lambda^2$. Then, using part (b) of Theorem~\ref{thm:3.3.1},
\[
\var(Y) = \var(5W + 3) = 25 \var(W) = 25/\lambda^2.
\]
\end{example}

Because $|a|^2 = a^2$, part (b) of Theorem~\ref{thm:3.3.1} immediately implies a corresponding fact about standard deviation.

\begin{corollary}
\label{cor:3.3.1}
Let $X$ be any random variable, with standard deviation $\text{Sd}(X)$, and let $a$ be any real number. Then $\text{Sd}(aX) = |a| \, \text{Sd}(X)$.
\end{corollary}

\begin{example}
\label{ex:3.3.6}
Let $W \sim \text{Exponential}(\lambda)$, and let $Y = 5W + 3$. Then using the above examples, we see that $\text{Sd}(W) = \sqrt{\var(W)} = (1/\lambda^2)^{1/2} = 1/\lambda$. Also, $\text{Sd}(Y) = \sqrt{\var(Y)} = (25/\lambda^2)^{1/2} = 5/\lambda$. This agrees with Corollary~\ref{cor:3.3.1}, since $\text{Sd}(Y) = 5 \, \text{Sd}(W)$.
\end{example}

\begin{example}[Variance and Standard Deviation of the $N(\mu, \sigma^2)$ Distribution]
\label{ex:3.3.7}
Suppose that $X \sim N(\mu, \sigma^2)$. In Example~\ref{ex:3.2.7} we established that $\expc[X] = \mu$. Now we compute $\var(X)$.

First consider $Z \sim N(0, 1)$. Then from Theorem~\ref{thm:3.3.1}(c) we have that
\[
\var(Z) = \expc[Z^2] = \int_{-\infty}^{\infty} z^2 \cdot \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{z^2}{2} \right) \mathrm{d}z.
\]
Then, putting $u = z$, $\mathrm{d}v = z \exp(-z^2/2)$ (so $\mathrm{d}u = 1$, $v = -\exp(-z^2/2)$), and using integration by parts, we obtain
\[
\var(Z) = \frac{1}{\sqrt{2\pi}} \left[ -z \exp\left( -\frac{z^2}{2} \right) \right]_{-\infty}^{\infty} + \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp\left( -\frac{z^2}{2} \right) \mathrm{d}z = 1
\]
and $\text{Sd}(Z) = 1$.

Now, for $\sigma > 0$, put $X = \mu + \sigma Z$. We then have $X \sim N(\mu, \sigma^2)$. From Theorem~\ref{thm:3.3.1}(b) we have that
\[
\var(X) = \var(\mu + \sigma Z) = \sigma^2 \var(Z) = \sigma^2
\]
and $\text{Sd}(X) = \sigma$. This establishes the variance of the $N(\mu, \sigma^2)$ distribution as $\sigma^2$ and the standard deviation as $\sigma$.
\end{example}

In Figure~\ref{fig:3.3.1}, we have plotted three normal distributions, all with mean $0$ but different variances.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_3_1.pdf}
  \caption{Plots of the $N(0, 1)$ (solid line), the $N(0, 1/4)$ (dashed line) and the $N(0, 4)$ (dotted line) density functions.}
  \label{fig:3.3.1}
\end{figure}

The effect of the variance on the amount of spread of the distribution about the mean is quite clear from these plots. As $\sigma^2$ increases, the distribution becomes more diffuse; as it decreases, it becomes more concentrated about the mean $0$.

So far we have considered the variance of one random variable at a time. However, the related concept of covariance measures the relationship between two random variables.

\begin{definition}
\label{def:3.3.3}
The \emph{covariance} of two random variables $X$ and $Y$ is given by
\[
\cov(X, Y) = \expc[(X - \mu_X)(Y - \mu_Y)]
\]
where $\mu_X = \expc[X]$ and $\mu_Y = \expc[Y]$.
\end{definition}

\begin{example}
\label{ex:3.3.8}
Let $X$ and $Y$ be discrete random variables, with joint probability function $p_{X,Y}$ given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/2 & x = 3, y = 4 \\
1/3 & x = 3, y = 6 \\
1/6 & x = 5, y = 6 \\
0 & \text{otherwise}.
\end{cases}
\]
Then $\expc[X] = 3 \cdot (1/2 + 1/3) + 5 \cdot (1/6) = 10/3$, and $\expc[Y] = 4 \cdot (1/2) + 6 \cdot (1/3 + 1/6) = 5$. Hence,
\begin{align*}
\cov(X, Y) &= \expc[(X - 10/3)(Y - 5)] \\
&= (3 - 10/3)(4 - 5) \cdot \frac{1}{2} + (3 - 10/3)(6 - 5) \cdot \frac{1}{3} + (5 - 10/3)(6 - 5) \cdot \frac{1}{6} \\
&= 1/3.
\end{align*}
\end{example}

\begin{example}
\label{ex:3.3.9}
Let $X$ be any random variable with $\var(X) \neq 0$. Let $Y = 3X$, and let $Z = -4X$. Then $\mu_Y = 3\mu_X$ and $\mu_Z = -4\mu_X$. Hence,
\begin{align*}
\cov(X, Y) &= \expc[(X - \mu_X)(Y - \mu_Y)] = \expc[(X - \mu_X)(3X - 3\mu_X)] \\
&= 3 \expc[(X - \mu_X)^2] = 3 \var(X)
\end{align*}
while
\begin{align*}
\cov(X, Z) &= \expc[(X - \mu_X)(Z - \mu_Z)] = \expc[(X - \mu_X)(-4X + 4\mu_X)] \\
&= -4 \expc[(X - \mu_X)^2] = -4 \var(X).
\end{align*}
Note in particular that $\cov(X, Y) > 0$, while $\cov(X, Z) < 0$. Intuitively, this says that $Y$ increases when $X$ increases, whereas $Z$ decreases when $X$ increases.
\end{example}

We begin with some simple facts about covariance. Obviously, we always have $\cov(X, Y) = \cov(Y, X)$. We also have the following result.

\begin{theorem}[Linearity of covariance]
\label{thm:3.3.2}
Let $X$, $Y$, and $Z$ be three random variables. Let $a$ and $b$ be real numbers. Then
\[
\cov(aX + bY, Z) = a \, \cov(X, Z) + b \, \cov(Y, Z).
\]
\end{theorem}

\begin{proof}
Note that by linearity, $\mu_{aX+bY} = \expc[aX + bY] = a\expc[X] + b\expc[Y] = a\mu_X + b\mu_Y$. Hence,
\begin{align*}
\cov(aX + bY, Z) &= \expc[(aX + bY - a\mu_X - b\mu_Y)(Z - \mu_Z)] \\
&= \expc[(aX - a\mu_X + bY - b\mu_Y)(Z - \mu_Z)] \\
&= a \expc[(X - \mu_X)(Z - \mu_Z)] + b \expc[(Y - \mu_Y)(Z - \mu_Z)] \\
&= a \, \cov(X, Z) + b \, \cov(Y, Z)
\end{align*}
and the result is established.
\end{proof}

We also have the following identity, which is similar to Theorem~\ref{thm:3.3.1}(c).

\begin{theorem}
\label{thm:3.3.3}
Let $X$ and $Y$ be two random variables. Then
\[
\cov(X, Y) = \expc[XY] - \expc[X]\expc[Y].
\]
\end{theorem}

\begin{proof}
Using linearity, we have
\begin{align*}
\cov(X, Y) &= \expc[(X - \mu_X)(Y - \mu_Y)] = \expc[XY - \mu_X Y - X \mu_Y + \mu_X \mu_Y] \\
&= \expc[XY] - \mu_X \expc[Y] - \expc[X] \mu_Y + \mu_X \mu_Y \\
&= \expc[XY] - \mu_X \mu_Y - \mu_X \mu_Y + \mu_X \mu_Y = \expc[XY] - \mu_X \mu_Y.
\end{align*}
\end{proof}

\begin{corollary}
\label{cor:3.3.2}
If $X$ and $Y$ are independent, then $\cov(X, Y) = 0$.
\end{corollary}

\begin{proof}
Because $X$ and $Y$ are independent, we know (Theorems~\ref{thm:3.1.3} and \ref{thm:3.2.3}) that $\expc[XY] = \expc[X]\expc[Y]$. Hence, the result follows immediately from Theorem~\ref{thm:3.3.3}.
\end{proof}

We note that the converse to Corollary~\ref{cor:3.3.2} is false, as the following example shows.

\begin{example}[Covariance $0$ Does Not Imply Independence.]
\label{ex:3.3.10}
Let $X$ and $Y$ be discrete random variables, with joint probability function $p_{X,Y}$ given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/4 & x = 3, y = 5 \\
1/4 & x = 4, y = 9 \\
1/4 & x = 7, y = 5 \\
1/4 & x = 6, y = 9 \\
0 & \text{otherwise}.
\end{cases}
\]
Then $\expc[X] = 3 \cdot (1/4) + 4 \cdot (1/4) + 7 \cdot (1/4) + 6 \cdot (1/4) = 5$, $\expc[Y] = 5 \cdot (1/4) + 9 \cdot (1/4) + 5 \cdot (1/4) + 9 \cdot (1/4) = 7$, and $\expc[XY] = 3 \cdot 5 \cdot (1/4) + 4 \cdot 9 \cdot (1/4) + 7 \cdot 5 \cdot (1/4) + 6 \cdot 9 \cdot (1/4) = 35$. We obtain $\cov(X, Y) = \expc[XY] - \expc[X]\expc[Y] = 35 - 5 \cdot 7 = 0$.

On the other hand, $X$ and $Y$ are clearly not independent. For example, $\prb(X = 4) \neq 0$ and $\prb(Y = 5) \neq 0$, but $\prb(X = 4, Y = 5) = 0$, so $\prb(X = 4, Y = 5) \neq \prb(X = 4)\prb(Y = 5)$.
\end{example}

There is also an important relationship between variance and covariance.

\begin{theorem}
\label{thm:3.3.4}
\begin{enumerate}[(a)]
\item For any random variables $X$ and $Y$,
\[
\var(X + Y) = \var(X) + \var(Y) + 2\,\cov(X, Y).
\]
\item More generally, for any random variables $X_1, \ldots, X_n$,
\[
\var\left( \sum_i X_i \right) = \sum_i \var(X_i) + 2 \sum_{i < j} \cov(X_i, X_j).
\]
\end{enumerate}
\end{theorem}

\begin{proof}
We prove part (b) here; part (a) then follows as the special case $n = 2$.

Note that by linearity,
\[
\mu_{\sum_i X_i} = \expc\left[ \sum_i X_i \right] = \sum_i \expc[X_i] = \sum_i \mu_{X_i}.
\]
Therefore, we have that
\begin{align*}
\var\left( \sum_i X_i \right) &= \expc\left[ \left( \sum_i X_i - \sum_i \mu_{X_i} \right)^2 \right] = \expc\left[ \left( \sum_i (X_i - \mu_i) \right)^2 \right] \\
&= \expc\left[ \sum_i (X_i - \mu_i)^2 + \sum_i (X_i - \mu_i) \sum_{j \neq i} (X_j - \mu_j) \right] \\
&= \expc\left[ \sum_{i,j} (X_i - \mu_i)(X_j - \mu_j) \right] \\
&= \sum_{i,j} \expc[(X_i - \mu_i)(X_j - \mu_j)] \\
&= \sum_{i,j: i=j} \expc[(X_i - \mu_i)(X_j - \mu_j)] + 2 \sum_{i < j} \expc[(X_i - \mu_i)(X_j - \mu_j)] \\
&= \sum_i \var(X_i) + 2 \sum_{i < j} \cov(X_i, X_j).
\end{align*}
\end{proof}

Combining Theorem~\ref{thm:3.3.4} with Corollary~\ref{cor:3.3.2}, we obtain the following.

\begin{corollary}
\label{cor:3.3.3}
\begin{enumerate}[(a)]
\item If $X$ and $Y$ are independent, then $\var(X + Y) = \var(X) + \var(Y)$.
\item If $X_1, \ldots, X_n$ are independent, then $\var\left( \sum_{i=1}^{n} X_i \right) = \sum_{i=1}^{n} \var(X_i)$.
\end{enumerate}
\end{corollary}

One use of Corollary~\ref{cor:3.3.3} is the following.

\begin{example}
\label{ex:3.3.11}
Let $Y \sim \text{Binomial}(n, \theta)$. What is $\var(Y)$? Recall that we can write
\[
Y = X_1 + X_2 + \cdots + X_n
\]
where the $X_i$ are independent, with $X_i \sim \text{Bernoulli}(\theta)$. We have already seen that $\var(X_i) = \theta(1 - \theta)$. Hence, from Corollary~\ref{cor:3.3.3},
\[
\var(Y) = \var(X_1) + \var(X_2) + \cdots + \var(X_n) = \theta(1 - \theta) + \cdots + \theta(1 - \theta) = n\theta(1 - \theta).
\]
\end{example}

Another concept very closely related to covariance is correlation.

\begin{definition}
\label{def:3.3.4}
The \emph{correlation} of two random variables $X$ and $Y$ is given by
\[
\cor(X, Y) = \frac{\cov(X, Y)}{\text{Sd}(X) \, \text{Sd}(Y)} = \frac{\cov(X, Y)}{\sqrt{\var(X) \var(Y)}}
\]
provided $0 < \var(X) < \infty$ and $0 < \var(Y) < \infty$.
\end{definition}

\begin{example}
\label{ex:3.3.12}
As in Example~\ref{ex:3.3.2}, let $X$ be any random variable with $\var(X) \neq 0$, let $Y = 3X$, and let $Z = -4X$. Then $\cov(X, Y) = 3\var(X)$ and $\cov(X, Z) = -4\var(X)$. But by Corollary~\ref{cor:3.3.1}, $\text{Sd}(Y) = 3\,\text{Sd}(X)$ and $\text{Sd}(Z) = 4\,\text{Sd}(X)$. Hence,
\[
\cor(X, Y) = \frac{\cov(X, Y)}{\text{Sd}(X) \, \text{Sd}(Y)} = \frac{3\var(X)}{\text{Sd}(X) \cdot 3\,\text{Sd}(X)} = \frac{\var(X)}{(\text{Sd}(X))^2} = 1
\]
because $(\text{Sd}(X))^2 = \var(X)$. Also, we have that
\[
\cor(X, Z) = \frac{\cov(X, Z)}{\text{Sd}(X) \, \text{Sd}(Z)} = \frac{-4\var(X)}{\text{Sd}(X) \cdot 4\,\text{Sd}(X)} = -\frac{\var(X)}{(\text{Sd}(X))^2} = -1.
\]
Intuitively, this again says that $Y$ increases when $X$ increases, whereas $Z$ decreases when $X$ increases. However, note that the scale factors $3$ and $4$ have cancelled out; only their signs were important.
\end{example}

We shall see later, in Section~\ref{sec:3.6}, that we always have $-1 \leqslant \cor(X, Y) \leqslant 1$, for any random variables $X$ and $Y$. Hence, in Example~\ref{ex:3.3.12}, $Y$ has the largest possible correlation with $X$ (which makes sense because $Y$ increases whenever $X$ does, without exception), while $Z$ has the smallest possible correlation with $X$ (which makes sense because $Z$ decreases whenever $X$ does). We will also see that $\cor(X, Y)$ is a measure of the extent to which a linear relationship exists between $X$ and $Y$.

\begin{example}[The Bivariate Normal$(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$ Distribution]
\label{ex:3.3.13}
We defined this distribution in Example~\ref{ex:2.7.9}. It turns out that when $(X, Y)$ follows this joint distribution then, (from Problem~\ref{exer:2.7.13}) $X \sim N(\mu_1, \sigma_1^2)$ and $Y \sim N(\mu_2, \sigma_2^2)$. Further, we have that (see Problem~\ref{exer:3.3.17}) $\cor(X, Y) = \rho$. In the following graphs, we have plotted samples of $n = 1000$ values of $(X, Y)$ from bivariate normal distributions with $\mu_1 = \mu_2 = 0$, $\sigma_1^2 = \sigma_2^2 = 1$, and various values of $\rho$. Note that we used \eqref{eq:2.7.1} to generate these samples.

From these plots we can see the effect of $\rho$ on the joint distribution. Figure~\ref{fig:3.3.2} shows that when $\rho = 0$ the point cloud is roughly circular. It becomes elliptical in Figure~\ref{fig:3.3.3} with $\rho = 0.5$ and more tightly concentrated about a line in Figure~\ref{fig:3.3.4} with $\rho = 0.9$. As we will see in Section~\ref{sec:3.6}, the points will lie exactly on a line when $|\rho| = 1$.

Figure~\ref{fig:3.3.5} demonstrates the effect of a negative correlation. With positive correlations, the value of $Y$ tends to increase with $X$ as reflected in the upward slope of the point cloud. With negative correlations, $Y$ tends to decrease with $X$ as reflected in the negative slope of the point cloud.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_3_2.pdf}
  \caption{A sample of $n = 1000$ values $(X, Y)$ from the Bivariate Normal$(0, 0, 1, 1, 0)$ distribution.}
  \label{fig:3.3.2}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_3_3.pdf}
  \caption{A sample of $n = 1000$ values $(X, Y)$ from the Bivariate Normal$(0, 0, 1, 1, 0.5)$ distribution.}
  \label{fig:3.3.3}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_3_4.pdf}
  \caption{A sample of $n = 1000$ values $(X, Y)$ from the Bivariate Normal$(0, 0, 1, 1, 0.9)$ distribution.}
  \label{fig:3.3.4}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_3_5.pdf}
  \caption{A sample of $n = 1000$ values $(X, Y)$ from the Bivariate Normal$(0, 0, 1, 1, -0.9)$ distribution.}
  \label{fig:3.3.5}
\end{figure}

\subsection*{Summary of Section~\ref{sec:3.3}}

\begin{itemize}
\item The variance of a random variable $X$ measures how far it tends to be from its mean and is given by $\var(X) = \expc[(X - \mu_X)^2] = \expc[X^2] - (\expc[X])^2$.
\item The variances of many standard distributions were computed.
\item The standard deviation of $X$ equals $\text{Sd}(X) = \sqrt{\var(X)}$.
\item $\var(X) \geqslant 0$, and $\var(aX + b) = a^2 \var(X)$; also $\text{Sd}(aX + b) = |a| \, \text{Sd}(X)$.
\item The covariance of random variables $X$ and $Y$ measures how they are related and is given by $\cov(X, Y) = \expc[(X - \mu_X)(Y - \mu_y)] = \expc[XY] - \expc[X]\expc[Y]$.
\item If $X$ and $Y$ are independent, then $\cov(X, Y) = 0$.
\item $\var(X + Y) = \var(X) + \var(Y) + 2\,\cov(X, Y)$. If $X$ and $Y$ are independent, this equals $\var(X) + \var(Y)$.
\item The correlation of $X$ and $Y$ is $\cor(X, Y) = \cov(X, Y) / (\text{Sd}(X) \, \text{Sd}(Y))$.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:3.3.1}
Suppose the joint probability function of $X$ and $Y$ is given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/2 & x = 3, y = 5 \\
1/6 & x = 3, y = 9 \\
1/6 & x = 6, y = 5 \\
1/6 & x = 6, y = 9 \\
0 & \text{otherwise}
\end{cases}
\]
with $\expc[X] = 4$, $\expc[Y] = 19/3$, and $\expc[XY] = 26$, as in Example~\ref{ex:3.1.16}.
\begin{enumerate}[(a)]
\item Compute $\cov(X, Y)$.
\item Compute $\var(X)$ and $\var(Y)$.
\item Compute $\cor(X, Y)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = 26 - (4)(19/3) = 2/3$.
    \item $\expc(X^2) = 3^2(1/2) + 3^2(1/6) + 6^2(1/6) + 6^2(1/6) = 18$, so $\var(X) = \expc(X^2) - \expc(X)^2 = 18 - 4^2 = 2$. Also $\expc(Y^2) = 5^2(1/2) + 9^2(1/6) + 5^2(1/6) + 9^2(1/6) = 131/3$, so $\var(Y) = \expc(Y^2) - \expc(Y)^2 = 131/3 - (19/3)^2 = 32/9$.
    \item $\cor(X, Y) = \cov(X, Y)/\sqrt{\var(X) \var(Y)} = (2/3)/\sqrt{(2)(32/9)} = 1/4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.3.2}
Suppose the joint probability function of $X$ and $Y$ is given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/7 & x = 5, y = 0 \\
1/7 & x = 5, y = 3 \\
1/7 & x = 5, y = 4 \\
3/7 & x = 8, y = 0 \\
1/7 & x = 8, y = 4 \\
0 & \text{otherwise}
\end{cases}
\]
as in Example~\ref{ex:2.7.5}.
\begin{enumerate}[(a)]
\item Compute $\expc[X]$ and $\expc[Y]$.
\item Compute $\cov(X, Y)$.
\item Compute $\var(X)$ and $\var(Y)$.
\item Compute $\cor(X, Y)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X) = (5)(1/7) + (5)(1/7) + (5)(1/7) + (8)(3/7) + (8)(1/7) = 47/7$. Also, $\expc(Y) = (0)(1/7) + (3)(1/7) + (4)(1/7) + (0)(3/7) + (4)(1/7) = 11/7$.
    \item $\expc(XY) = (5)(0)(1/7) + (5)(3)(1/7) + (5)(4)(1/7) + (8)(0)(3/7) + (8)(4)(1/7) = 67/7$. Then $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = 67/7 - (47/7)(11/7) = -48/49$.
    \item $\expc(X^2) = (5)^2(1/7) + (5)^2(1/7) + (5)^2(1/7) + (8)^2(3/7) + (8)^2(1/7) = 331/7$. Then $\var(X) = \expc(X^2) - \expc(X)^2 = 331/7 - (47/7)^2 = 108/49$. Also, $\expc(Y^2) = (0)^2(1/7) + (3)^2(1/7) + (4)^2(1/7) + (0)^2(3/7) + (4)^2(1/7) = 41/7$. Then $\var(Y) = \expc(Y^2) - \expc(Y)^2 = 41/7 - (11/7)^2 = 166/49$.
    \item $\cor(X, Y) = \cov(X, Y)/\sqrt{\var(X) \var(Y)} = (-48/49)/\sqrt{(108/49)(166/49)} = -4\sqrt{2/249} = -0.3585$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.3.3}
Let $X$ and $Y$ have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise}
\end{cases}
\]
as in Exercise~\ref{exer:3.2.2}. Compute $\cor(X, Y)$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(X) &= \int_0^1 \int_0^1 x (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = 2/3, \\
    \expc(Y) &= \int_0^1 \int_0^1 y (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = 46/63, \\
    \expc(X^2) &= \int_0^1 \int_0^1 x^2 (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \left(\frac{4}{5} y + \frac{2}{3} y^5\right) \mathrm{d}y = \frac{2}{5} + \frac{2}{9} = \frac{23}{45}, \\
    \expc(Y^2) &= \int_0^1 \int_0^1 y^2 (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 \left(\frac{4}{3} y^3 + 2y^7\right) \mathrm{d}y = \frac{7}{12}, \\
    \expc(XY) &= \int_0^1 \int_0^1 xy (4x^2 y + 2y^5) \, \mathrm{d}x \, \mathrm{d}y = \int_0^1 (y^2 + y^6) \, \mathrm{d}y = \frac{10}{21}, \\
    \cor(X, Y) &= \frac{\frac{10}{21} - \left(\frac{2}{3}\right)\left(\frac{46}{63}\right)}{\sqrt{\frac{23}{45} - \left(\frac{2}{3}\right)^2} \sqrt{\frac{7}{12} - \left(\frac{46}{63}\right)^2}} = -0.18292.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:3.3.4}
Let $X$ and $Y$ have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
15x^3 y^4 + 6x^2 y^7 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise}
\end{cases}
\]
Compute $\expc[X]$, $\expc[Y]$, $\var(X)$, $\var(Y)$, $\cov(X, Y)$, and $\cor(X, Y)$.
\end{exercise}

\begin{solution}
Here
\begin{align*}
    \expc(X) &= \int_0^1 \int_0^1 x (15x^3 y^4 + 6x^2 y^7) \, \mathrm{d}x \, \mathrm{d}y = 63/80, \\
    \expc(Y) &= \int_0^1 \int_0^1 y (15x^3 y^4 + 6x^2 y^7) \, \mathrm{d}x \, \mathrm{d}y = 61/72.
\end{align*}
$\expc(X^2) = \int_0^1 \int_0^1 x^2 (15x^3 y^4 + 6x^2 y^7) \, \mathrm{d}x \, \mathrm{d}y = 13/20$.
$\expc(Y^2) = \int_0^1 \int_0^1 y^2 (15x^3 y^4 + 6x^2 y^7) \, \mathrm{d}x \, \mathrm{d}y = 103/140$. $\var(X) = \expc(X^2) - \expc(X)^2 = (13/20) - (63/80)^2 = 191/6400$.
$\var(Y) = \expc(Y^2) - \expc(Y)^2 = (103/140) - (61/72)^2 = 3253/181440$. $\expc(XY) = \int_0^1 \int_0^1 xy (15x^3 y^4 + 6x^2 y^7) \, \mathrm{d}x \, \mathrm{d}y = 2/3$.
$\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = (2/3) - (63/80)(61/72) = -1/1920$.
$\cor(X, Y) = \cov(X, Y)/\sqrt{\var(X) \var(Y)} = (-1/1920)/\sqrt{(191/6400)(3253/181440)} = -3\sqrt{35/621323} = -0.0225$.
\end{solution}

\begin{exercise}
\label{exer:3.3.5}
Let $Y$ and $Z$ be two independent random variables, each with positive variance. Prove that $\cor(Y, Z) = 0$.
\end{exercise}

\begin{solution}
If $X$ and $Y$ are independent, then $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = \expc(X)\expc(Y) - \expc(X)\expc(Y) = 0$, so $\cor(X, Y) = \cov(X, Y)/\sqrt{\var(X) \var(Y)} = 0$.
\end{solution}

\begin{exercise}
\label{exer:3.3.6}
Let $X$, $Y$, and $Z$ be three random variables, and suppose that $X$ and $Z$ are independent. Prove that $\cov(X + Y, Z) = \cov(Y, Z)$.
\end{exercise}

\begin{solution}
If $X$ and $Z$ are independent, then $\cov(X + Y, Z) = \cov(X, Z) + \cov(Y, Z) = 0 + \cov(Y, Z) = \cov(Y, Z)$.
\end{solution}

\begin{exercise}
\label{exer:3.3.7}
Let $X \sim \text{Exponential}(3)$ and $Y \sim \text{Poisson}(5)$. Assume $X$ and $Y$ are independent. Let $Z = X + Y$.
\begin{enumerate}[(a)]
\item Compute $\cov(X, Z)$.
\item Compute $\cor(X, Z)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\cov(X, Z) = \cov(X, X + Y) = \cov(X, X) + \cov(X, Y) = \var(X) + 0 = 1/3^2 = 1/9$.
    \item $\cor(X, Z) = \cov(X, Z)/\sqrt{\var(X) \var(Z)} = (1/9)/\sqrt{(1/9)((1/9) + 5)} = 1/\sqrt{46} = 0.147$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.3.8}
Prove that the variance of the Uniform$[L, R]$ distribution is given by the expression $(R - L)^2/12$.
\end{exercise}

\begin{solution}
We can write $X = L + (R - L)U$, where $U \sim \text{Uniform}[0, 1]$. Then $\expc(X) = L + (R - L)\expc(U) = L + (R - L)/2 = (L + R)/2$ and $\var(X) = (R - L)^2 \var(U)$. Now $\expc(U^2) = \int_0^1 u^2 \, \mathrm{d}u = 1/3$, so $\var(U) = 1/3 - 1/4 = 1/12$.
\end{solution}

\begin{exercise}
\label{exer:3.3.9}
Prove that $\var(X) = \expc[X(X - 1)] + \expc[X] - (\expc[X])^2$. Use this to compute directly from the probability function that when $X \sim \text{Binomial}(n, \theta)$ then $\var(X) = n\theta(1 - \theta)$.
\end{exercise}

\begin{solution}
$\expc(X(X - 1)) = \expc(X^2) - \expc(X)$, so $\expc(X(X - 1)) - \expc(X)(\expc(X) - 1) = \expc(X^2) - (\expc(X))^2 = \var(X)$. Then, when $X \sim \text{Binomial}(n, \theta)$, we have that,
\begin{align*}
    \expc(X(X - 1)) &= \sum_{x=0}^{n} x(x - 1) \binom{n}{x} \theta^x (1 - \theta)^{n-x} \\
    &= n(n - 1) \theta^2 \sum_{x=2}^{n} \binom{n-2}{x-2} \theta^{x-2} (1 - \theta)^{n-2-(x-2)} \\
    &= n(n - 1) \theta^2 \sum_{x=0}^{n-2} \binom{n-2}{x} \theta^x (1 - \theta)^{n-2-x} = n(n - 1) \theta^2,
\end{align*}
so $\var(X) = n(n - 1)\theta^2 - n\theta(n\theta - 1) = n\theta(1 - \theta)$.
\end{solution}

\begin{exercise}
\label{exer:3.3.10}
Suppose you flip three fair coins. Let $X$ be the number of heads showing, and let $Y = X^2$. Compute $\expc[X]$, $\expc[Y]$, $\var(X)$, $\var(Y)$, $\cov(X, Y)$, and $\cor(X, Y)$.
\end{exercise}

\begin{solution}
Since $X \sim \text{Binomial}(3, 1/2)$, the probability is given by $\prb(X = 0) = \prb(X = 3) = 1/8$ and $\prb(X = 1) = \prb(X = 2) = 3/8$. Thus, $\expc(X) = (0 + 3)(1/8) + (1 + 2)(3/8) = 3/2$, $\expc(X^2) = (0^2 + 3^2)(1/8) + (1^2 + 2^2)(3/8) = 3$, $\expc(X^3) = (0^3 + 3^3)(1/8) + (1^3 + 2^3)(3/8) = 27/4$, $\expc(X^4) = (0^4 + 3^4)(1/8) + (1^4 + 2^4)(3/8) = 33/2$, $\expc(X^5) = (0^5 + 3^5)(1/8) + (1^5 + 2^5)(3/8) = 171/4$, and $\expc(X^6) = (0^6 + 3^6)(1/8) + (1^6 + 2^6)(3/8) = 231/2$. Hence, we get $\expc(X) = 3/2$, $\expc(Y) = \expc(X^2) = 3$, $\var(X) = \expc(X^2) - (\expc(X))^2 = 3 - (3/2)^2 = 3/4$, $\var(Y) = \expc(Y^2) - (\expc(Y))^2 = \expc(X^4) - (\expc(X^2))^2 = 33/2 - 3^2 = 15/2$, $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = \expc(X^3) - \expc(X)\expc(X^2) = 27/4 - (3/2)(3) = 9/4$, and $\cor(X, Y) = \cov(X, Y)/\sqrt{\var(X)\var(Y)} = (9/4)/\sqrt{(3/4)(15/2)} = 3\sqrt{10}/10 = 0.9487$.
\end{solution}

\begin{exercise}
\label{exer:3.3.11}
Suppose you roll two fair six-sided dice. Let $X$ be the number showing on the first die, and let $Y$ be the sum of the numbers showing on the two dice. Compute $\expc[X]$, $\expc[Y]$, $\expc[XY]$, and $\cov(X, Y)$.
\end{exercise}

\begin{solution}
We know $\prb(X = x) = 1/6$ for $x = 1, \ldots, 6$, otherwise $\prb(X = x) = 0$. Since $Y$ is also a fair die, $\prb(Y = y) = \prb(X = y)$. Two dice cannot affect each other, so $X$ and $Y$ are independent. Thus, $\expc(X) = \expc(Y) = (1)(1/6) + \cdots + (6)(1/6) = 7/2$. From Theorem \ref{thm:3.2.3}, $\expc(XY) = \expc(X)\expc(Y) = (7/2)(7/2) = 49/4$. Hence, $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = 0$.
\end{solution}

\begin{exercise}
\label{exer:3.3.12}
Suppose you flip four fair coins. Let $X$ be the number of heads showing, and let $Y$ be the number of tails showing. Compute $\cov(X, Y)$ and $\cor(X, Y)$.
\end{exercise}

\begin{solution}
The distribution of $X$ is $\text{Binomial}(4, 1/2)$. Since $X + Y = 4$, the distribution of $Y$ is the same to the distribution of $4 - X$. In Example \ref{ex:3.1.7}, $\expc(X) = 4(1/2) = 2$. The expectation of $Y$ is $\expc(Y) = \expc(4 - X) = 4 - \expc(X) = 4 - 2 = 2$. For the covariance, $\expc(X^2)$ is required because $\expc(XY) = \expc(X(4 - X)) = 4\expc(X) - \expc(X^2) = 8 - \expc(X^2)$. Theorem \ref{thm:3.3.1} and Example \ref{ex:3.3.11} implies $\var(X) = \expc(X^2) - (\expc(X))^2 = 4(1/2)(1 - (1/2)) = 1$. Hence, $\expc(X^2) = 1 + (2)^2 = 5$. Thus, $\expc(XY) = 8 - \expc(X^2) = 3$. By the definition of the covariance, $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = 3 - 2 \cdot 2 = -1$. Since $\var(Y) = \var(4 - X) = \var(X) = 4(1/2)(1 - 1/2) = 1$, we have $\cor(X, Y) = \cov(X, Y)/\sqrt{\var(X)\var(Y)} = -1/1 = -1$.
\end{solution}

\begin{exercise}
\label{exer:3.3.13}
Let $X$ and $Y$ be independent, with $X \sim \text{Bernoulli}(1/2)$ and $Y \sim \text{Bernoulli}(1/3)$. Let $Z = X + Y$ and $W = X - Y$. Compute $\cov(Z, W)$ and $\cor(Z, W)$.
\end{exercise}

\begin{solution}
It is known that for $U \sim \text{Bernoulli}(\theta)$, $\expc(U) = \expc(U^2) = \theta$ and $\var(U) = \theta(1 - \theta)$. The expectations are $\expc(Z) = \expc(X + Y) = \expc(X) + \expc(Y) = 1/2 + 1/3 = 5/6$ and $\expc(W) = \expc(X - Y) = \expc(X) - \expc(Y) = 1/2 - 1/3 = 1/6$. The variances are $\var(Z) = \var(X + Y) = \var(X) + \var(Y) = 1/4 + 2/9 = 17/36$ and $\var(W) = \var(X - Y) = \var(X) + \var(Y) = 1/4 + 2/9 = 17/36$. $\expc(ZW) = \expc((X + Y)(X - Y)) = \expc(X^2 - Y^2) = \expc(X^2) - \expc(Y^2) = 1/2 - 1/3 = 1/6$. Hence, $\cov(Z, W) = \expc(ZW) - \expc(Z)\expc(W) = 1/6 - (1/2)(1/3) = 0$ and $\cor(Z, W) = \cov(Z, W)/\sqrt{\var(Z)\var(W)} = 0$.
\end{solution}

\begin{exercise}
\label{exer:3.3.14}
Let $X$ and $Y$ be independent, with $X \sim \text{Bernoulli}(1/2)$ and $Y \sim N(0, 1)$. Let $Z = X + Y$ and $W = X - Y$. Compute $\var(Z)$, $\var(W)$, $\cov(Z, W)$, and $\cor(Z, W)$.
\end{exercise}

\begin{solution}
It is known that $\expc(X) = 1/2$, $\expc(Y) = 0$, $\var(X) = 1/4$, and $\var(Y) = 1$. Hence, $\expc(Z) = \expc(X + Y) = \expc(X) + \expc(Y) = 1/2$, $\expc(W) = \expc(X - Y) = \expc(X) - \expc(Y) = 1/2$, $\var(Z) = \var(X + Y) = \var(X) + \var(Y) = 5/4$, $\var(W) = \var(X - Y) = \var(X) + \var(Y) = 5/4$, and $\expc(ZW) = \expc(X^2 - Y^2) = \expc(X) - \var(Y) = 1/2 - 1 = -1/2$. Thus, $\cov(Z, W) = \expc(ZW) - \expc(Z)\expc(W) = -1/2 - (1/2)(0) = -1/2$ and $\cor(Z, W) = \cov(Z, W)/\sqrt{\var(Z)\var(W)} = -2/5$.
\end{solution}

\begin{exercise}
\label{exer:3.3.15}
Suppose you roll one fair six-sided die and then flip as many coins as the number showing on the die. (For example, if the die shows $4$, then you flip four coins.) Let $X$ be the number showing on the die, and $Y$ be the number of heads obtained. Compute $\cov(X, Y)$.
\end{exercise}

\begin{solution}
The joint probability $\prb(X = x, Y = y) = (1/6) \cdot \binom{x}{y} (1/2)^x$ for $x = 1, \ldots, 6$, $y = 0, \ldots, x$, otherwise $\prb(X = x, Y = y) = 0$. The expectations are $\expc(X) = \sum_{x=1}^{6} \sum_{y=0}^{x} x (1/6) \binom{x}{y} 2^{-x} = \sum_{x=1}^{6} x/6 = 7/2$ and $\expc(Y) = \sum_{x=1}^{6} \sum_{y=0}^{x} y (1/6) \binom{x}{y} 2^{-x} = \sum_{x=1}^{6} x/12 = 7/4$. $\expc(XY) = \sum_{x=1}^{6} \sum_{y=0}^{x} xy (1/6) \binom{x}{y} 2^{-x} = \sum_{x=1}^{6} x^2/12 = 91/12$. Hence, $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = 91/12 - (7/2)(7/4) = 35/24$.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:3.3.16}
Let $X \sim N(0, 1)$, and let $Y = cX$.
\begin{enumerate}[(a)]
\item Compute $\lim_{c \to 0^+} \cov(X, Y)$.
\item Compute $\lim_{c \to 0^-} \cov(X, Y)$.
\item Compute $\lim_{c \to 0^+} \cor(X, Y)$.
\item Compute $\lim_{c \to 0^-} \cor(X, Y)$.
\item Explain why the answers in parts (c) and (d) are not the same.
\end{enumerate}
\end{exercise}

\begin{solution}
Here $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = \expc(cX^2) - \expc(X)\expc(cX) = c(1) - (0)(0) = c$, and $\cor(X, Y) = \cov(X, Y)/\sqrt{\var(X) \var(Y)} = c/\sqrt{(1)(c^2)} = c/|c| = \mathrm{sgn}(c)$, where $\mathrm{sgn}(c) = 1$ for $c > 0$, $\mathrm{sgn}(c) = 0$ for $c = 0$, and $\mathrm{sgn}(c) = -1$ for $c < 0$. Hence,
\begin{enumerate}[(a)]
    \item $\lim_{c \searrow 0} \cov(X, Y) = \lim_{c \searrow 0} c = 0$.
    \item $\lim_{c \nearrow 0} \cov(X, Y) = \lim_{c \nearrow 0} c = 0$.
    \item $\lim_{c \searrow 0} \cor(X, Y) = \lim_{c \searrow 0} \mathrm{sgn}(c) = 1$.
    \item $\lim_{c \nearrow 0} \cor(X, Y) = \lim_{c \nearrow 0} \mathrm{sgn}(c) = -1$.
    \item As $c$ passes from positive to negative, $\cor(X, Y)$ is not continuous but rather ``jumps'' from $+1$ to $-1$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.3.17}
Let $X$ and $Y$ have the bivariate normal distribution, as in Example~\ref{ex:2.7.9}. Prove that $\cor(X, Y) = \rho$. (Hint: Use \eqref{eq:2.7.1}.)
\end{exercise}

\begin{solution}
We have that $\expc(X) = \mu_1$, $\var(X) = \sigma_1^2$, $\expc(Y) = \mu_2$, $\var(Y) = \sigma_2^2$, and using (2.7.1) we have that
\begin{align*}
    \expc(XY) &= \expc\left((\mu_1 + \sigma_1 Z_1)\left(\mu_2 + \sigma_2\left(\rho Z_1 + \sqrt{1 - \rho^2} Z_2\right)\right)\right) \\
    &= \expc\left(\mu_1 \mu_2 + \sigma_1 \mu_2 Z_1 + \rho \sigma_2 \mu_1 Z_1 + \rho \sigma_1 \sigma_2 Z_1^2 + \sigma_2 \mu_1 \sqrt{1 - \rho^2} Z_2 + \sigma_1 \sigma_2 \sqrt{1 - \rho^2} Z_1 Z_2\right) \\
    &= \mu_1 \mu_2 + \rho \sigma_1 \sigma_2
\end{align*}
where we have used $\expc(Z_1) = \expc(Z_2) = \expc(Z_1 Z_2) = \expc(Z_1)\expc(Z_2) = 0$, $\expc(Z_1^2) = 1$. So $\cov(X, Y) = \rho \sigma_1 \sigma_2$ and $\cor(X, Y) = \cov(X, Y)/\sqrt{\var(X) \var(Y)} = (\sigma_1 \sigma_2 \rho)/\sqrt{(\sigma_1^2)(\sigma_2^2)} = \rho$.
\end{solution}

\begin{exercise}
\label{exer:3.3.18}
Prove that the variance of the Geometric$(\theta)$ distribution is given by $(1 - \theta)/\theta^2$. (Hint: Use Exercise~\ref{exer:3.3.9} and $\sum_{\ell=1}^{\infty} \ell(\ell - 1)(1 - x)^{\ell - 2} = 2/(x)^3$.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(X(X - 1)) &= \theta \sum_{x=0}^{\infty} x(x - 1)(1 - \theta)^x = \theta(1 - \theta)^2 \sum_{x=2}^{\infty} x(x - 1)(1 - \theta)^{x-2} \\
    &= \theta(1 - \theta)^2 \sum_{x=2}^{\infty} \frac{\mathrm{d}^2 (1 - \theta)^x}{\mathrm{d}\theta^2} = \theta(1 - \theta)^2 \frac{\mathrm{d}^2}{\mathrm{d}\theta^2} \sum_{x=2}^{\infty} (1 - \theta)^x \\
    &= \theta(1 - \theta)^2 \frac{\mathrm{d}^2}{\mathrm{d}\theta^2} \left(\frac{1}{\theta} - 1 - (1 - \theta)\right) = \theta(1 - \theta)^2 \frac{2}{\theta^3} = \frac{(1 - \theta)^2}{\theta}.
\end{align*}
Therefore
\[
    \var(X) = \frac{2(1 - \theta)^2}{\theta^2} - \frac{(1 - \theta)}{\theta} \left(\frac{(1 - \theta)}{\theta} - 1\right) = \frac{(1 - \theta)^2}{\theta^2} + \frac{(1 - \theta)}{\theta} = \frac{(1 - \theta)}{\theta} \left(\frac{(1 - \theta)}{\theta} + 1\right) = \frac{(1 - \theta)}{\theta^2}.
\]
\end{solution}

\begin{exercise}
\label{exer:3.3.19}
Prove that the variance of the Negative-Binomial$(r, \theta)$ distribution is given by $r(1 - \theta)/\theta^2$. (Hint: Use Problem~\ref{exer:3.3.18}.)
\end{exercise}

\begin{solution}
We have that when $X_1, \ldots, X_r$ are i.i.d.\ $\text{Geometric}(\theta)$ then $X = X_1 + \cdots + X_r \sim \text{Negative Binomial}(r, \theta)$. Therefore, $\var(X) = r(1 - \theta)/\theta^2$.
\end{solution}

\begin{exercise}
\label{exer:3.3.20}
Let $\alpha > 0$ and $\lambda > 0$, and let $X \sim \text{Gamma}(\alpha, \lambda)$. Prove that $\var(X) = \alpha/\lambda^2$. (Hint: Recall Problem~\ref{exer:3.2.16}.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(X^2) &= \int_0^{\infty} x^2 \frac{\lambda^{\alpha} x^{\alpha-1}}{\Gamma(\alpha)} e^{-\lambda x} \, \mathrm{d}x = \int_0^{\infty} \frac{\lambda^{\alpha} x^{\alpha+1}}{\Gamma(\alpha)} e^{-\lambda x} \, \mathrm{d}x \\
    &= \int_0^{\infty} \frac{\lambda^{\alpha} t^{\alpha+1}}{\lambda^{\alpha+1} \Gamma(\alpha)} e^{-t} (1/\lambda) \, \mathrm{d}t = \frac{1}{\lambda^2 \Gamma(\alpha)} \int_0^{\infty} t^{\alpha+1} e^{-t} \, \mathrm{d}x = \frac{\Gamma(\alpha + 2)}{\lambda^2 \Gamma(\alpha)} = \frac{\alpha(\alpha + 1)}{\lambda^2},
\end{align*}
so $\var(X) = \alpha(\alpha + 1)/\lambda^2 - \alpha^2/\lambda^2 = \alpha/\lambda^2$.
\end{solution}

\begin{exercise}
\label{exer:3.3.21}
Suppose that $X \sim \text{Weibull}(\lambda, \beta)$ distribution (see Problem~\ref{exer:2.4.19}). Prove that $\var(X) = \lambda^{-2/\beta} [\Gamma(1 + 2/\beta) - (\Gamma(1 + 1/\beta))^2]$. (Hint: Recall Problem~\ref{exer:3.2.18}.)
\end{exercise}

\begin{solution}
We have that $\expc(X^2) = \int_0^{\infty} x^2 \alpha x^{\alpha-1} e^{-x^{\alpha}} \, \mathrm{d}x = \int_0^{\infty} \alpha x^{\alpha+1} e^{-x^{\alpha}} \, \mathrm{d}x$, and putting $u = x^{\alpha}$, $x = u^{1/\alpha}$, $\mathrm{d}u = \alpha x^{\alpha-1} \, \mathrm{d}x$ we have that $\expc(X^2) = \int_0^{\infty} u^{2/\alpha} e^{-u} \, \mathrm{d}u = \Gamma(2/\alpha + 1)$ and $\var(X) = \expc(X^2) - (\expc(X))^2 = \Gamma(2/\alpha + 1) - \Gamma^2(1/\alpha + 1)$.
\end{solution}

\begin{exercise}
\label{exer:3.3.22}
Suppose that $X \sim \text{Pareto}(\alpha)$ (see Problem~\ref{exer:2.4.20}) for $\alpha > 2$. Prove that $\var(X) = \alpha / [(\alpha - 1)^2(\alpha - 2)]$. (Hint: Recall Problem~\ref{exer:3.2.19}.)
\end{exercise}

\begin{solution}
We have that
\[
    \expc\left((X + 1)^2\right) = \int_0^{\infty} (x + 1)^2 \alpha (1 + x)^{-\alpha-1} \, \mathrm{d}x = \int_0^{\infty} \alpha (1 + x)^{-\alpha+1} \, \mathrm{d}x = \begin{cases} \dfrac{\alpha}{-\alpha+2} (1 + x)^{-\alpha+2} \Big|_0^{\infty} & \alpha \neq 2 \\[1em] \alpha \ln(1 + x) \Big|_0^{\infty} & \alpha = 2 \end{cases} = \begin{cases} \infty & 0 < \alpha \leqslant 2 \\[0.5em] \alpha/(\alpha - 2) & \text{if } \alpha > 2. \end{cases}
\]
Therefore, when $\alpha > 2$,
\begin{align*}
    \var(X) &= \expc\left((X + 1)^2\right) - 2\expc(X) - 1 - (\expc(X))^2 \\
    &= \frac{\alpha}{\alpha - 2} - \frac{2}{\alpha - 1} - 1 - \frac{1}{(\alpha - 1)^2} \\
    &= \frac{\alpha(\alpha - 1)^2 - 2(\alpha - 1)(\alpha - 2) - (\alpha - 1)^2(\alpha - 2) - (\alpha - 2)}{(\alpha - 1)^2(\alpha - 2)} \\
    &= \frac{\alpha}{(\alpha - 1)^2(\alpha - 2)}.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:3.3.23}
Suppose that $X$ follows the Laplace distribution (see Problem~\ref{exer:2.4.22}). Prove that $\var(X) = 2$. (Hint: Recall Problem~\ref{exer:3.2.21}.)
\end{exercise}

\begin{solution}
We have that $\expc(X^2) = \int_0^{\infty} x^2 e^{-x} \, \mathrm{d}x = \Gamma(3) = 2 = \var(X)$ since $\expc(X) = 0$.
\end{solution}

\begin{exercise}
\label{exer:3.3.24}
Suppose that $X \sim \text{Beta}(a, b)$ (see Problem~\ref{exer:2.4.24}). Prove that $\var(X) = ab / [(a + b)^2(a + b + 1)]$. (Hint: Recall Problem~\ref{exer:3.2.22}.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(X^2) &= \int_0^1 x^2 \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1 - x)^{b-1} \, \mathrm{d}x = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \int_0^1 x^{a+1} (1 - x)^{b-1} \, \mathrm{d}x \\
    &= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \cdot \frac{\Gamma(a + 2) \Gamma(b)}{\Gamma(a + b + 2)} = \frac{a(a + 1)}{(a + b)(a + b + 1)}.
\end{align*}
Therefore
\[
    \var(X) = \expc(X^2) - (\expc(X))^2 = \frac{a(a + 1)}{(a + b)(a + b + 1)} - \left(\frac{a}{a + b}\right)^2 = \frac{a(a + 1)(a + b) - a(a + b + 1)}{(a + b)^2(a + b + 1)} = \frac{ab}{(a + b)^2(a + b + 1)}.
\]
\end{solution}

\begin{exercise}
\label{exer:3.3.25}
Suppose that $(X_1, X_2, X_3) \sim \text{Multinomial}(n; \theta_1, \theta_2, \theta_3)$. Prove that
\[
\var(X_i) = n\theta_i(1 - \theta_i), \qquad \cov(X_i, X_j) = -n\theta_i\theta_j \text{ when } i \neq j.
\]
(Hint: Recall Problem~\ref{exer:3.1.23}.)
\end{exercise}

\begin{solution}
We have that $X_i \sim \text{Binomial}(n, \theta_i)$ so that $\expc(X_i) = n\theta_i$, $\var(X_i) = n\theta_i(1 - \theta_i)$. Also,
\begin{align*}
    \expc(X_1 X_2) &= \sum_{x_1=0}^{n} \sum_{x_2=0}^{n-x_1} x_1 x_2 \binom{n}{x_1 \; x_2 \; n - x_1 - x_2} \theta_1^{x_1} \theta_2^{x_2} \theta_3^{n-x_1-x_2} \\
    &= n(n - 1) \theta_1 \theta_2 \sum_{x_1=1}^{n} \sum_{x_2=1}^{n-x_1} \binom{n - 2}{x_1 - 1 \; x_2 - 1 \; n - 2 - (x_1 - 1) - (x_2 - 1)} \theta_1^{x_1-1} \theta_2^{x_2-1} \theta_3^{n-2-(x_1-1)-(x_2-1)} \\
    &= n(n - 1) \theta_1 \theta_2 \sum_{x_1=0}^{n-2} \sum_{x_2=0}^{n-2-x_1} \binom{n - 2}{x_1 \; x_2 \; n - 2 - x_1 - x_2} \theta_1^{x_1} \theta_2^{x_2} \theta_3^{n-2-x_1-x_2} = n(n - 1) \theta_1 \theta_2
\end{align*}
since the sum is the sum of all $\text{Multinomial}(n - 2, \theta_1, \theta_2, \theta_3)$ probabilities. Therefore, $\cov(X_1, X_2) = n(n - 1)\theta_1 \theta_2 - n^2 \theta_1 \theta_2 = -n\theta_1 \theta_2$.
\end{solution}

\begin{exercise}
\label{exer:3.3.26}
Suppose that $(X_1, X_2) \sim \text{Dirichlet}(\alpha_1, \alpha_2, \alpha_3)$ (see Problem~\ref{exer:2.7.17}). Prove that
\begin{align*}
\var(X_i) &= \frac{\alpha_i(\alpha_1 + \alpha_2 + \alpha_3 - \alpha_i)}{(\alpha_1 + \alpha_2 + \alpha_3)^2(\alpha_1 + \alpha_2 + \alpha_3 + 1)}, \\
\cov(X_1, X_2) &= \frac{-\alpha_1 \alpha_2}{(\alpha_1 + \alpha_2 + \alpha_3)^2(\alpha_1 + \alpha_2 + \alpha_3 + 1)}.
\end{align*}
(Hint: Recall Problem~\ref{exer:3.2.23}.)
\end{exercise}

\begin{solution}
We have that $X_1 \sim \text{Beta}(\alpha_1, \alpha_2 + \alpha_3)$, so $\expc(X_1) = \alpha_1/(\alpha_1 + \alpha_2 + \alpha_3)$ and $\var(X_1) = \alpha_1(\alpha_2 + \alpha_3)/(\alpha_1 + \alpha_2 + \alpha_3)^2(\alpha_1 + \alpha_2 + \alpha_3 + 1)$ by Problem \ref{exer:3.3.24}. Also,
\begin{align*}
    \expc(X_1 X_2) &= \int_0^1 \int_0^{1-x_2} x_1 x_2 \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1) \Gamma(\alpha_2) \Gamma(\alpha_3)} x_1^{\alpha_1-1} x_2^{\alpha_2-1} (1 - x_1 - x_2)^{\alpha_3-1} \, \mathrm{d}x_1 \, \mathrm{d}x_2 \\
    &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1) \Gamma(\alpha_2) \Gamma(\alpha_3)} \int_0^1 \int_0^{1-x_2} x_1^{\alpha_1} x_2^{\alpha_2} (1 - x_1 - x_2)^{\alpha_3-1} \, \mathrm{d}x_1 \, \mathrm{d}x_2 \\
    &= \frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1) \Gamma(\alpha_2) \Gamma(\alpha_3)} \cdot \frac{\Gamma(\alpha_1 + 1) \Gamma(\alpha_2 + 1) \Gamma(\alpha_3)}{\Gamma(\alpha_1 + \alpha_2 + \alpha_3 + 2)} = \frac{\alpha_1 \alpha_2}{(\alpha_1 + \alpha_2 + \alpha_3)(\alpha_1 + \alpha_2 + \alpha_3 + 1)}
\end{align*}
so
\[
    \cov(X_1, X_2) = \frac{\alpha_1 \alpha_2}{(\alpha_1 + \alpha_2 + \alpha_3)(\alpha_1 + \alpha_2 + \alpha_3 + 1)} - \frac{\alpha_1 \alpha_2}{(\alpha_1 + \alpha_2 + \alpha_3)^2} = \frac{-\alpha_1 \alpha_2}{(\alpha_1 + \alpha_2 + \alpha_3)^2(\alpha_1 + \alpha_2 + \alpha_3 + 1)}.
\]
\end{solution}

\begin{exercise}
\label{exer:3.3.27}
Suppose that $X \sim \text{Hypergeometric}(N, M, n)$. Prove that
\[
\var(X) = n \cdot \frac{M}{N} \cdot \left( 1 - \frac{M}{N} \right) \cdot \frac{N - n}{N - 1}.
\]
(Hint: Recall Problem~\ref{exer:3.1.21} and use Exercise~\ref{exer:3.3.9}.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(X(X - 1)) &= \sum_{x=\max(0, n+M-N)}^{\min(n,M)} x(x - 1) \frac{\binom{M}{x} \binom{N-M}{n-x}}{\binom{N}{n}} = \sum_{x=\max(2, n+M-N)}^{\min(n,M)} x(x - 1) \frac{\binom{M}{x} \binom{N-M}{n-x}}{\binom{N}{n}} \\
    &= \frac{n(n - 1) M(M - 1)}{N(N - 1)} \sum_{x=\max(2, n+M-N)}^{\min(n,M)} \frac{\binom{M-2}{x-2} \binom{N-2-(M-2)}{n-2-(x-2)}}{\binom{N-2}{n-2}} \\
    &= \frac{n(n - 1) M(M - 1)}{N(N - 1)} \sum_{x=\max(0, n-2+(M-2)-(N-2))}^{\min(n-2,M-2)} \frac{\binom{M-2}{x} \binom{N-2-(M-2)}{n-2-x}}{\binom{N-2}{n-2}} = \frac{n(n - 1) M(M - 1)}{N(N - 1)}
\end{align*}
as we are summing all $\text{Hypergeometric}(N - 2, M - 2, n - 2)$ probabilities. Therefore,
\[
    \var(X) = \frac{n(n - 1) M(M - 1)}{N(N - 1)} - \frac{nM}{N} \left(\frac{nM}{N} - 1\right) = \frac{nM}{N} \cdot \frac{(n - 1)(M - 1)N - (N - 1)(nM - N)}{N(N - 1)} = \frac{nM}{N} \left(1 - \frac{M}{N}\right) \frac{(N - n)}{(N - 1)}.
\]
\end{solution}

\begin{exercise}
\label{exer:3.3.28}
Suppose you roll one fair six-sided die and then flip as many coins as the number showing on the die. (For example, if the die shows $4$, then you flip four coins.) Let $X$ be the number showing on the die, and $Y$ be the number of heads obtained. Compute $\cor(X, Y)$.
\end{exercise}

\begin{solution}
In Exercise \ref{exer:3.3.15}, we showed that (1) the joint probability $\prb(X = x, Y = y) = (1/6) \cdot \binom{x}{y} (1/2)^x$ for $x = 1, \ldots, 6$, $y = 0, \ldots, x$, otherwise $\prb(X = x, Y = y) = 0$ and (2) $\expc(X) = 7/2$, $\expc(Y) = 7/4$, $\expc(XY) = 91/12$ and $\cov(X, Y) = 35/24$. To compute $\cor(X, Y)$, the variances are required. $\expc(X^2) = \sum_{x=1}^{6} \sum_{y=0}^{x} x^2 (1/6) \binom{x}{y} 2^{-x} = \sum_{x=1}^{6} x^2/6 = 91/6$ and $\expc(Y^2) = \sum_{x=1}^{6} \sum_{y=0}^{x} y^2 (1/6) \binom{x}{y} 2^{-x} = \sum_{i=1}^{6} x(x + 1)/24 = 14/3$. Hence, $\var(X) = \expc(X^2) - (\expc(X))^2 = 35/12$ and $\var(Y) = \expc(Y^2) - (\expc(Y))^2 = 77/48$. Therefore, $\cor(X, Y) = (35/24)/\sqrt{(35/12)(77/48)} = \sqrt{55}/11 = 0.6742$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:3.3.29}
Let $Y$ be a nonnegative random variable. Prove that $\expc[Y] = 0$ if and only if $\prb(Y = 0) = 1$. (You may assume for simplicity that $Y$ is discrete, but the result is true for any $Y$.)
\end{exercise}

\begin{solution}
Assume $Y$ is discrete, with $Y \geqslant 0$ and $\expc(Y) = 0$. Then $0 = \sum_y y \, \prb(Y = y) = \sum_{y \geqslant 0} y \, \prb(Y = y)$. But the only way a sum of non-negative terms can be $0$ is if each term is $0$, i.e., $y \, \prb(Y = y) = 0$ for all $y \in \mathbb{R}^1$. This means that $\prb(Y = y) = 0$ for $y \neq 0$, so that $\prb(Y = 0) = 1$.
\end{solution}

\begin{exercise}
\label{exer:3.3.30}
Prove that $\var(X) = 0$ if and only if there is a real number $c$ with $\prb(X = c) = 1$. (You may use the result of Challenge~\ref{exer:3.3.29}.)
\end{exercise}

\begin{solution}
Let $C = \expc(X)$, and let $Y = (X - C)^2$. Then $Y \geqslant 0$, and $\expc(Y) = \var(X) = 0$. Hence, from the previous challenge, $\prb(Y = 0) = 1$. But $Y = 0$ if and only if $X = C$. Hence, $\prb(X = C) = 1$.
\end{solution}

\begin{exercise}
\label{exer:3.3.31}
Give an example of a random variable $X$, such that $\expc[X] = 5$, and $\var(X) = \infty$.
\end{exercise}

\begin{solution}
Let $C = \sum_{k=1}^{\infty} 1/k^3$. (Then $C = \zeta(3) = 1.202$, but $C$ cannot be expressed precisely in elementary terms.) Let $\prb(Y = k) = 1/Ck^3$ for $k = 1, 2, 3, \ldots$. Let $X = Y + 5 - \pi^2/6$. Then $\expc(Y) = \sum_{k=1}^{\infty} k(1/k^3) = \sum_{k=1}^{\infty} (1/k^2) = \pi^2/6$, so $\expc(X) = \expc(Y) + 5 - \pi^2/6 = \pi^2/6 + 5 - \pi^2/6 = 5$. On the other hand, $\expc(Y^2) = \sum_{k=1}^{\infty} k^2(1/k^3) = \sum_{k=1}^{\infty} (1/k) = \infty$. It follows that $\expc(X^2) = \infty$ and that $\var(X) = \infty$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generating Functions}
\label{sec:3.4}

Let $X$ be a random variable. Recall that the cumulative distribution function of $X$, defined by $F_X(x) = \prb(X \leqslant x)$, contains all the information about the distribution of $X$ (see Theorem~\ref{thm:2.5.1}). It turns out that there are other functions --- the probability-generating function and the moment-generating function --- that also provide information (sometimes all the information) about $X$ and its expected values.

\begin{definition}
\label{def:3.4.1}
Let $X$ be a random variable (usually discrete). Then we define its \emph{probability-generating function}, $r_X$, by $r_X(t) = \expc[t^X]$ for $t \in \mathbf{R}^1$.
\end{definition}

Consider the following examples of probability-generating functions.

\begin{example}[The Binomial$(n, \theta)$ Distribution]
\label{ex:3.4.1}
If $X \sim \text{Binomial}(n, \theta)$, then
\begin{align*}
r_X(t) &= \expc[t^X] = \sum_{i=0}^{n} \prb(X = i) t^i = \sum_{i=0}^{n} \binom{n}{i} \theta^i (1 - \theta)^{n-i} t^i \\
&= \sum_{i=0}^{n} \binom{n}{i} (t\theta)^i (1 - \theta)^{n-i} = (t\theta + 1 - \theta)^n
\end{align*}
using the binomial theorem.
\end{example}

\begin{example}[The Poisson Distribution]
\label{ex:3.4.2}
If $Y \sim \text{Poisson}(\lambda)$, then
\begin{align*}
r_Y(t) &= \expc[t^Y] = \sum_{i=0}^{\infty} \prb(Y = i) t^i = \sum_{i=0}^{\infty} e^{-\lambda} \frac{\lambda^i}{i!} t^i \\
&= \sum_{i=0}^{\infty} e^{-\lambda} \frac{(t\lambda)^i}{i!} = e^{-\lambda} e^{t\lambda} = e^{\lambda(t-1)}.
\end{align*}
\end{example}

The following theorem tells us that once we know the probability-generating function $r_X(t)$, then we can compute all the probabilities $\prb(X = 0)$, $\prb(X = 1)$, $\prb(X = 2)$, etc.

\begin{theorem}
\label{thm:3.4.1}
Let $X$ be a discrete random variable, whose possible values are all nonnegative integers. Assume that $r_X(t_0) < \infty$ for some $t_0 > 0$. Then
\begin{align*}
r_X(0) &= \prb(X = 0) \\
r_X'(0) &= \prb(X = 1) \\
r_X''(0) &= 2 \prb(X = 2)
\end{align*}
etc. In general,
\[
r_X^{(k)}(0) = k! \, \prb(X = k)
\]
where $r_X^{(k)}$ is the $k$th derivative of $r_X$.
\end{theorem}

\begin{proof}
Because the possible values are all nonnegative integers of the form $i = 0, 1, 2, \ldots$, we have
\begin{align*}
r_X(t) &= \expc[t^X] = \sum_x t^x \prb(X = x) = \sum_{i=0}^{\infty} t^i \prb(X = i) \\
&= t^0 \prb(X = 0) + t^1 \prb(X = 1) + t^2 \prb(X = 2) + t^3 \prb(X = 3) + \cdots
\end{align*}
so that
\begin{equation}
\label{eq:3.4.1}
r_X(t) = 1 \cdot \prb(X = 0) + t^1 \prb(X = 1) + t^2 \prb(X = 2) + t^3 \prb(X = 3) + \cdots.
\end{equation}
Substituting $t = 0$ into \eqref{eq:3.4.1}, every term vanishes except the first one, and we obtain $r_X(0) = \prb(X = 0)$. Taking derivatives of both sides of \eqref{eq:3.4.1}, we obtain
\[
r_X'(t) = 1 \cdot \prb(X = 1) + 2t^1 \prb(X = 2) + 3t^2 \prb(X = 3) + \cdots
\]
and setting $t = 0$ gives $r_X'(0) = \prb(X = 1)$. Taking another derivative of both sides gives
\[
r_X''(t) = 2 \prb(X = 2) + 3 \cdot 2 \, t^1 \prb(X = 3) + \cdots
\]
and setting $t = 0$ gives $r_X''(0) = 2 \prb(X = 2)$. Continuing in this way, we obtain the general formula.
\end{proof}

We now apply Theorem~\ref{thm:3.4.1} to the binomial and Poisson distributions.

\begin{example}[The Binomial$(n, \theta)$ Distribution]
\label{ex:3.4.3}
From Example~\ref{ex:3.4.1}, we have that
\begin{align*}
r_X(0) &= (1 - \theta)^n \\
r_X'(0) &= n\theta(t\theta + 1 - \theta)^{n-1} \big|_{t=0} = n\theta(1 - \theta)^{n-1} \\
r_X''(0) &= n(n-1)\theta^2(t\theta + 1 - \theta)^{n-2} \big|_{t=0} = n(n-1)\theta^2(1 - \theta)^{n-2}
\end{align*}
etc. It is thus verified directly that
\begin{align*}
\prb(X = 0) &= r_X(0) \\
\prb(X = 1) &= r_X'(0) \\
2\prb(X = 2) &= r_X''(0)
\end{align*}
etc.
\end{example}

\begin{example}[The Poisson Distribution]
\label{ex:3.4.4}
From Example~\ref{ex:3.4.2}, we have that
\begin{align*}
r_X(0) &= e^{-\lambda} \\
r_X'(0) &= \lambda e^{-\lambda} \\
r_X''(0) &= \lambda^2 e^{-\lambda}
\end{align*}
etc. It is again verified that
\begin{align*}
\prb(X = 0) &= r_X(0) \\
\prb(X = 1) &= r_X'(0) \\
2\prb(X = 2) &= r_X''(0)
\end{align*}
etc.
\end{example}

From Theorem~\ref{thm:3.4.1}, we can see why $r_X$ is called the probability-generating function. For, at least in the discrete case with the distribution concentrated on the nonnegative integers, we can indeed generate the probabilities for $X$ from $r_X$. We thus see immediately that for a random variable $X$ that takes values only in $\{0, 1, 2, \ldots\}$, $r_X$ is unique. By this we mean that if $X$ and $Y$ are concentrated on $\{0, 1, 2, \ldots\}$ and $r_X = r_Y$, then $X$ and $Y$ have the same distribution. This uniqueness property of the probability-generating function can be very useful in trying to determine the distribution of a random variable that takes only values in $\{0, 1, 2, \ldots\}$.

It is clear that the probability-generating function tells us a lot --- in fact, everything --- about the distribution of random variables concentrated on the nonnegative integers. But what about other random variables? It turns out that there are other quantities, called moments, associated with random variables that are quite informative about their distributions.

\begin{definition}
\label{def:3.4.2}
Let $X$ be a random variable, and let $k$ be a positive integer. Then the \emph{$k$th moment} of $X$ is the quantity $\expc[X^k]$, provided this expectation exists.
\end{definition}

Note that if $\expc[X^k]$ exists and is finite, it can be shown that $\expc[X^\ell]$ exists and is finite when $0 \leqslant \ell \leqslant k$.

The first moment is just the mean of the random variable. This can be taken as a measure of where the central mass of probability for $X$ lies in the real line, at least when this distribution is unimodal (has a single peak) and is not too highly skewed. The second moment $\expc[X^2]$, together with the first moment, gives us the variance through $\var(X) = \expc[X^2] - (\expc[X])^2$. Therefore, the first two moments of the distribution tell us about the location of the distribution and the spread, or degree of concentration, of that distribution about the mean. In fact, the higher moments also provide information about the distribution.

Many of the most important distributions of probability and statistics have all of their moments finite; in fact, they have what is called a moment-generating function.

\begin{definition}
\label{def:3.4.3}
Let $X$ be any random variable. Then its \emph{moment-generating function} $m_X$ is defined by $m_X(s) = \expc[e^{sX}]$ at $s \in \mathbf{R}^1$.
\end{definition}

The following example computes the moment-generating function of a well-known distribution.

\begin{example}[The Exponential Distribution]
\label{ex:3.4.5}
Let $X \sim \text{Exponential}(\lambda)$. Then for $s < \lambda$,
\begin{align*}
m_X(s) &= \expc[e^{sX}] = \int_{-\infty}^{\infty} e^{sx} f_X(x) \, \mathrm{d}x = \int_0^{\infty} e^{sx} \lambda e^{-\lambda x} \, \mathrm{d}x \\
&= \int_0^{\infty} \lambda e^{-((\lambda - s)x)} \, \mathrm{d}x = \left[ -\frac{\lambda}{\lambda - s} e^{-(\lambda - s)x} \right]_{x=0}^{x=\infty} \\
&= -\frac{\lambda}{\lambda - s} \cdot 0 - \left( -\frac{\lambda}{\lambda - s} \right) = \frac{\lambda}{\lambda - s}.
\end{align*}
\end{example}

A comparison of Definitions~\ref{def:3.4.1} and \ref{def:3.4.3} immediately gives the following.

\begin{theorem}
\label{thm:3.4.2}
Let $X$ be any random variable. Then $m_X(s) = r_X(e^s)$.
\end{theorem}

This result can obviously help us evaluate some moment-generating functions when we have $r_X$ already.

\begin{example}
\label{ex:3.4.6}
Let $Y \sim \text{Binomial}(n, \theta)$. Then we know that $r_Y(t) = (t\theta + 1 - \theta)^n$. Hence, $m_Y(s) = r_Y(e^s) = (e^s\theta + 1 - \theta)^n$.
\end{example}

\begin{example}
\label{ex:3.4.7}
Let $Z \sim \text{Poisson}(\lambda)$. Then we know that $r_Z(t) = e^{\lambda(t-1)}$. Hence, $m_Z(s) = r_Z(e^s) = e^{\lambda(e^s - 1)}$.
\end{example}

The following theorem tells us that once we know the moment-generating function $m_X(t)$, we can compute all the moments $\expc[X]$, $\expc[X^2]$, $\expc[X^3]$, etc.

\begin{theorem}
\label{thm:3.4.3}
Let $X$ be any random variable. Suppose that for some $s_0 > 0$, it is true that $m_X(s) < \infty$ whenever $s \in (-s_0, s_0)$. Then
\begin{align*}
m_X(0) &= 1 \\
m_X'(0) &= \expc[X] \\
m_X''(0) &= \expc[X^2]
\end{align*}
etc. In general,
\[
m_X^{(k)}(0) = \expc[X^k]
\]
where $m_X^{(k)}$ is the $k$th derivative of $m_X$.
\end{theorem}

\begin{proof}
We know that $m_X(s) = \expc[e^{sX}]$. We have
\[
m_X(0) = \expc[e^{0 \cdot X}] = \expc[e^0] = \expc[1] = 1.
\]
Also, taking derivatives, we see\footnote{Strictly speaking, interchanging the order of derivative and expectation is justified by analytic function theory and requires that $m_X(s) < \infty$ whenever $|s| < s_0$.} that $m_X'(s) = \expc[X e^{sX}]$, so
\[
m_X'(0) = \expc[X e^{0 \cdot X}] = \expc[X e^0] = \expc[X].
\]
Taking derivatives again, we see that $m_X''(s) = \expc[X^2 e^{sX}]$, so
\[
m_X''(0) = \expc[X^2 e^{0 \cdot X}] = \expc[X^2 e^0] = \expc[X^2].
\]
Continuing in this way, we obtain the general formula.
\end{proof}

We now consider an application of Theorem~\ref{thm:3.4.3}.

\begin{example}[The Mean and Variance of the Exponential Distribution]
\label{ex:3.4.8}
Using the moment-generating function computed in Example~\ref{ex:3.4.5}, we have
\[
m_X'(s) = \frac{\lambda}{(\lambda - s)^2} = \lambda(\lambda - s)^{-2}.
\]
Therefore,
\[
\expc[X] = m_X'(0) = \lambda \cdot (\lambda - 0)^{-2} = \lambda^{-1} = \frac{1}{\lambda}
\]
as it should. Also,
\[
\expc[X^2] = m_X''(0) = 2\lambda(\lambda - 0)^{-3} \cdot 1 = 2\lambda^{-3} \cdot \lambda = \frac{2}{\lambda^2}
\]
so we have
\[
\var(X) = \expc[X^2] - (\expc[X])^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}.
\]
This provides an easy way of computing the variance of $X$.
\end{example}

\begin{example}[The Mean and Variance of the Poisson Distribution]
\label{ex:3.4.9}
In Example~\ref{ex:3.4.7}, we obtained $m_Z(s) = \exp(\lambda(e^s - 1))$. So we have
\begin{align*}
\expc[X] &= m_X'(0) = \lambda e^0 \exp(\lambda(e^0 - 1)) = \lambda \\
\expc[X^2] &= m_X''(0) = \lambda e^0 \exp(\lambda(e^0 - 1)) + (\lambda e^0)^2 \exp(\lambda(e^0 - 1)) = \lambda + \lambda^2.
\end{align*}
Therefore, $\var(X) = \expc[X^2] - (\expc[X])^2 = \lambda + \lambda^2 - \lambda^2 = \lambda$.
\end{example}

Computing the moment-generating function of a normal distribution is also important, but it is somewhat more difficult.

\begin{theorem}
\label{thm:3.4.4}
If $X \sim N(0, 1)$, then $m_X(s) = e^{s^2/2}$.
\end{theorem}

\begin{proof}
Because $X$ has density $\phi(x) = (2\pi)^{-1/2} e^{-x^2/2}$, we have that
\begin{align*}
m_X(s) &= \expc[e^{sX}] = \int_{-\infty}^{\infty} e^{sx} \phi(x) \, \mathrm{d}x = \int_{-\infty}^{\infty} e^{sx} \cdot \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \, \mathrm{d}x \\
&= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{sx - x^2/2} \, \mathrm{d}x = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-(x-s)^2/2 + s^2/2} \, \mathrm{d}x \\
&= e^{s^2/2} \cdot \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-(x-s)^2/2} \, \mathrm{d}x.
\end{align*}
Setting $y = x - s$ (so that $\mathrm{d}y = \mathrm{d}x$), this becomes (using Theorem~\ref{thm:2.4.2})
\[
m_X(s) = e^{s^2/2} \cdot \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-y^2/2} \, \mathrm{d}y = e^{s^2/2} \int_{-\infty}^{\infty} \phi(y) \, \mathrm{d}y = e^{s^2/2}
\]
as claimed.
\end{proof}

One useful property of both probability-generating and moment-generating functions is the following.

\begin{theorem}
\label{thm:3.4.5}
Let $X$ and $Y$ be random variables that are independent. Then we have
\begin{enumerate}[(a)]
\item $r_{X+Y}(t) = r_X(t) \, r_Y(t)$, and
\item $m_{X+Y}(t) = m_X(t) \, m_Y(t)$.
\end{enumerate}
\end{theorem}

\begin{proof}
Because $X$ and $Y$ are independent, so are $t^X$ and $t^Y$ (by Theorem~\ref{thm:2.8.5}). Hence, we know (by Theorems~\ref{thm:3.1.3} and \ref{thm:3.2.3}) that $\expc[t^X t^Y] = \expc[t^X]\expc[t^Y]$. Using this, we have
\[
r_{X+Y}(t) = \expc[t^{X+Y}] = \expc[t^X \cdot t^Y] = \expc[t^X]\expc[t^Y] = r_X(t) \, r_Y(t).
\]
Similarly,
\[
m_{X+Y}(t) = \expc[e^{t(X+Y)}] = \expc[e^{tX} e^{tY}] = \expc[e^{tX}]\expc[e^{tY}] = m_X(t) \, m_Y(t).
\]
\end{proof}

\begin{example}
\label{ex:3.4.10}
Let $Y \sim \text{Binomial}(n, \theta)$. Then, as in Example~\ref{ex:3.1.15}, we can write
\[
Y = X_1 + \cdots + X_n
\]
where the $X_i$ are i.i.d.\ with $X_i \sim \text{Bernoulli}(\theta)$. Hence, Theorem~\ref{thm:3.4.5} says we must have $r_Y(t) = r_{X_1}(t) \, r_{X_2}(t) \cdots r_{X_n}(t)$. But for any $i$,
\[
r_{X_i}(t) = \sum_x t^x \prb(X = x) = t^1 \cdot \theta + t^0 \cdot (1 - \theta) = t\theta + 1 - \theta.
\]
Hence, we must have
\[
r_Y(t) = (t\theta + 1 - \theta)(t\theta + 1 - \theta) \cdots (t\theta + 1 - \theta) = (t\theta + 1 - \theta)^n
\]
as already verified in Example~\ref{ex:3.4.1}.
\end{example}

Moment-generating functions, when defined in a neighborhood of $0$, completely define a distribution in the following sense. (We omit the proof, which is advanced.)

\begin{theorem}[Uniqueness theorem]
\label{thm:3.4.6}
Let $X$ be a random variable, such that for some $s_0 > 0$, we have $m_X(s) < \infty$ whenever $s \in (-s_0, s_0)$. Then if $Y$ is some other random variable with $m_Y(s) = m_X(s)$ whenever $s \in (-s_0, s_0)$, then $X$ and $Y$ have the same distribution.
\end{theorem}

Theorems~\ref{thm:3.4.1} and \ref{thm:3.4.6} provide a powerful technique for identifying distributions. For example, if we determine that the moment-generating function of $X$ is $m_X(t) = \exp(s^2/2)$, then we know, from Theorems~\ref{thm:3.4.4} and \ref{thm:3.4.6}, that $X \sim N(0, 1)$. We can use this approach to determine the distributions of some complicated random variables.

\begin{example}
\label{ex:3.4.11}
Suppose that $X_i \sim N(\mu_i, \sigma_i^2)$ for $i = 1, \ldots, n$ and that these random variables are independent. Consider the distribution of $Y = \sum_{i=1}^{n} X_i$.

When $n = 1$, we have (from Problem~\ref{exer:3.4.15})
\[
m_Y(s) = \exp\left( \mu_1 s + \frac{\sigma_1^2 s^2}{2} \right).
\]
Then, using Theorem~\ref{thm:3.4.5}, we have that
\begin{align*}
m_Y(s) &= \prod_{i=1}^{n} m_{X_i}(s) = \prod_{i=1}^{n} \exp\left( \mu_i s + \frac{\sigma_i^2 s^2}{2} \right) \\
&= \exp\left( \sum_{i=1}^{n} \mu_i \cdot s + \sum_{i=1}^{n} \sigma_i^2 \cdot \frac{s^2}{2} \right).
\end{align*}
From Problem~\ref{exer:3.4.15}, and applying Theorem~\ref{thm:3.4.6}, we have that
\[
Y \sim N\left( \sum_{i=1}^{n} \mu_i, \sum_{i=1}^{n} \sigma_i^2 \right).
\]
\end{example}

Generating functions can also help us with compound distributions, which are defined as follows.

\begin{definition}
\label{def:3.4.4}
Let $X_1, X_2, \ldots$ be i.i.d., and let $N$ be a nonnegative, integer-valued random variable which is independent of the $X_i$. Let
\begin{equation}
\label{eq:3.4.2}
S = \sum_{i=1}^{N} X_i.
\end{equation}
Then $S$ is said to have a \emph{compound distribution}.
\end{definition}

A compound distribution is obtained from a sum of i.i.d.\ random variables, where the number of terms in the sum is randomly distributed independently of the terms in the sum. Note that $S = 0$ when $N = 0$. Such distributions have applications in areas like insurance, where the $X_1, X_2, \ldots$ are claims and $N$ is the number of claims presented to an insurance company during a period. Therefore, $S$ represents the total amount claimed against the insurance company during the period. Obviously, the insurance company wants to study the distribution of $S$, as this will help determine what it has to charge for insurance to ensure a profit.

The following theorem is important in the study of compound distributions.

\begin{theorem}
\label{thm:3.4.7}
If $S$ has a compound distribution as in \eqref{eq:3.4.2}, then
\begin{enumerate}[(a)]
\item $\expc[S] = \expc[X_1]\expc[N]$.
\item $m_S(s) = r_N(m_{X_1}(s))$.
\end{enumerate}
\end{theorem}

\begin{proof}
See Section~\ref{sec:3.8} for the proof of this result.
\end{proof}

\subsection{Characteristic Functions (Advanced)}
\label{ssec:3.4.1}

One problem with moment-generating functions is that they can be infinite in any open interval about $s = 0$. Consider the following example.

\begin{example}
\label{ex:3.4.12}
Let $X$ be a random variable having density
\[
f_X(x) = \begin{cases}
1/x^2 & x \geqslant 1 \\
0 & \text{otherwise}.
\end{cases}
\]
Then
\[
m_X(s) = \expc[e^{sX}] = \int_1^{\infty} e^{sx} \cdot \frac{1}{x^2} \, \mathrm{d}x.
\]
For any $s > 0$, we know that $e^{sx}$ grows faster than $x^2$, so that $\lim_{x \to \infty} e^{sx}/x^2 = \infty$. Hence, $m_X(s) = \infty$ whenever $s > 0$.

Does $X$ have any finite moments? We have that
\[
\expc[X] = \int_1^{\infty} x \cdot \frac{1}{x^2} \, \mathrm{d}x = \int_1^{\infty} \frac{1}{x} \, \mathrm{d}x = \left[ \ln x \right]_{x=1}^{x=\infty} = \infty
\]
so, in fact, the first moment does not exist. From this we conclude that $X$ does not have any moments.
\end{example}

The random variable $X$ in the above example does not satisfy the condition of Theorem~\ref{thm:3.4.3} that $m_X(s) < \infty$ whenever $|s| < s_0$, for some $s_0 > 0$. Hence, Theorem~\ref{thm:3.4.3} (like most other theorems that make use of moment-generating functions) does not apply. There is, however, a similarly defined function that does not suffer from this defect, given by the following definition.

\begin{definition}
\label{def:3.4.5}
Let $X$ be any random variable. Then we define its \emph{characteristic function}, $c_X$, by
\begin{equation}
\label{eq:3.4.3}
c_X(s) = \expc[e^{isX}]
\end{equation}
for $s \in \mathbf{R}^1$.
\end{definition}

So the definition of $c_X$ is just like the definition of $m_X$, except for the introduction of the imaginary number $i = \sqrt{-1}$. Using properties of complex numbers, we see that \eqref{eq:3.4.3} can also be written as $c_X(s) = \expc[\cos(sX)] + i\expc[\sin(sX)]$ for $s \in \mathbf{R}^1$.

Consider the following examples.

\begin{example}[The Bernoulli Distribution]
\label{ex:3.4.13}
Let $X \sim \text{Bernoulli}(\theta)$. Then
\begin{align*}
c_X(s) &= \expc[e^{isX}] = e^{is \cdot 0}(1 - \theta) + e^{is \cdot 1} \theta \\
&= 1 \cdot (1 - \theta) + e^{is} \theta = (1 - \theta) + \theta e^{is} \\
&= (1 - \theta) + \theta(\cos s + i \sin s).
\end{align*}
\end{example}

\begin{example}
\label{ex:3.4.14}
Let $X$ have probability function given by
\[
p_X(x) = \begin{cases}
1/6 & x = 2 \\
1/3 & x = 3 \\
1/2 & x = 4 \\
0 & \text{otherwise}.
\end{cases}
\]
Then
\begin{align*}
c_X(s) &= \expc[e^{isX}] = e^{is \cdot 2} \cdot (1/6) + e^{is \cdot 3} \cdot (1/3) + e^{is \cdot 4} \cdot (1/2) \\
&= (1/6)\cos(2s) + (1/3)\cos(3s) + (1/2)\cos(4s) \\
&\quad + (1/6)i\sin(2s) + (1/3)i\sin(3s) + i(1/2)\sin(4s).
\end{align*}
\end{example}

\begin{example}
\label{ex:3.4.15}
Let $Z$ have probability function given by
\[
p_Z(z) = \begin{cases}
1/2 & z = 1 \\
1/2 & z = -1 \\
0 & \text{otherwise}.
\end{cases}
\]
Then
\begin{align*}
c_Z(s) &= \expc[e^{isZ}] = e^{is} \cdot (1/2) + e^{-is} \cdot (1/2) \\
&= (1/2)\cos s + (1/2)\cos(-s) + (1/2)\sin s + (1/2)\sin(-s) \\
&= (1/2)\cos s + (1/2)\cos s + (1/2)\sin s - (1/2)\sin s = \cos s.
\end{align*}
Hence, in this case, $c_Z(s)$ is a real (not complex) number for all $s \in \mathbf{R}^1$.
\end{example}

Once we overcome our ``fear'' of imaginary and complex numbers, we can see that the characteristic function is actually much better in some ways than the moment-generating function. The main advantage is that, because $|e^{isX}| = |\cos(sX) + i\sin(sX)| = 1$ and $|e^{isX}| \leqslant 1$, the characteristic function (unlike the moment-generating function) is always finite (although it could be a complex number).

\begin{theorem}
\label{thm:3.4.8}
Let $X$ be any random variable, and let $s$ be any real number. Then $|c_X(s)|$ is finite.
\end{theorem}

The characteristic function has many properties similar to the moment-generating function. In particular, we have the following. (The proof is just like the proof of Theorem~\ref{thm:3.4.3}.)

\begin{theorem}
\label{thm:3.4.9}
Let $X$ be any random variable with its first $k$ moments finite. Then $c_X(0) = 1$, $c_X'(0) = i\expc[X]$, $c_X''(0) = i^2 \expc[X^2] = -\expc[X^2]$, etc. In general,
\[
c_X^{(k)}(0) = i^k \expc[X^k],
\]
where $i = \sqrt{-1}$, and where $c_X^{(k)}$ is the $k$th derivative of $c_X$.
\end{theorem}

We also have the following. (The proof is just like the proof of Theorem~\ref{thm:3.4.5}.)

\begin{theorem}
\label{thm:3.4.10}
Let $X$ and $Y$ be random variables which are independent. Then $c_{X+Y}(s) = c_X(s) \, c_Y(s)$.
\end{theorem}

For simplicity, we shall generally not use characteristic functions in this book. However, it is worth keeping in mind that whenever we do anything with moment-generating functions, we could usually do the same thing in greater generality using characteristic functions.

\subsection*{Summary of Section~\ref{sec:3.4}}

\begin{itemize}
\item The probability-generating function of a random variable $X$ is $r_X(t) = \expc[t^X]$.
\item If $X$ is discrete, then the derivatives of $r_X$ satisfy $r_X^{(k)}(0) = k! \, \prb(X = k)$.
\item The $k$th moment of a random variable $X$ is $\expc[X^k]$.
\item The moment-generating function of a random variable $X$ is $m_X(s) = \expc[e^{sX}] = r_X(e^s)$.
\item The derivatives of $m_X$ satisfy $m_X^{(k)}(0) = \expc[X^k]$, for $k = 0, 1, 2, \ldots$.
\item If $X$ and $Y$ are independent, then $r_{X+Y}(t) = r_X(t) \, r_Y(y)$ and $m_{X+Y}(s) = m_X(s) \, m_Y(s)$.
\item If $m_X(s)$ is finite in a neighborhood of $s = 0$, then it uniquely characterizes the distribution of $X$.
\item The characteristic function $c_X(s) = \expc[e^{isX}]$ can be used in place of $m_X(s)$ to avoid infinities.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:3.4.1}
Let $Z$ be a discrete random variable with $\prb(Z = z) = (1/2)^z$ for $z = 1, 2, 3, \ldots$.
\begin{enumerate}[(a)]
\item Compute $r_Z(t)$. Verify that $r_Z'(0) = \prb(Z = 1)$ and $r_Z''(0) = 2\prb(Z = 2)$.
\item Compute $m_Z(t)$. Verify that $m_Z'(0) = \expc[Z]$ and $m_Z''(0) = \expc[Z^2]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $r_Z(t) = \expc(t^Z) = \sum_{z=1}^{\infty} t^z (1/2^z) = (t/2)/[1 - (t/2)] = t/(2 - t)$. Hence, $r_Z'(t) = ((2 - t)(1) - (t)(-1))/(2 - t)^2 = 2/(2 - t)^2$, so $r_Z'(0) = 2/2^2 = 1/2 = \prb(Z = 1)$. Also, $r_Z''(t) = \frac{\mathrm{d}}{\mathrm{d}t}[2/(2 - t)^2] = -4/(t - 2)^3$, so $r_Z''(0) = (-4)/(-2)^3 = 1/2 = 2(1/4) = 2\prb(Z = 2)$.
    \item Note that $Z = X + 1$, where $X \sim \text{Geometric}(1/2)$. Hence, $\expc(Z) = \expc(X) + 1 = 1 + 1 = 2$, and $\var(Z) = \var(X) = (1 - (1/2))/(1/2)^2 = 2$, where $\expc(Z^2) = \var(Z) + \expc(Z)^2 = 2 + 2^2 = 6$. On the other hand, $m_Z(t) = \expc(e^{tZ}) = r_Z(e^t) = e^t/(2 - e^t)$. Hence, $m_Z'(t) = [(2 - e^t)(e^t) - e^t(-e^t)]/(2 - e^t)^2 = 2e^t/(2 - e^t)^2$, so $m_Z'(0) = 2 = \expc(Z)$. Also, $m_Z''(t) = 2e^t(2 + e^t)/(2 - e^t)^3$, so $m_Z''(0) = 2(3)/1^3 = 6 = \expc(Z^2)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.4.2}
Let $X \sim \text{Binomial}(n, \theta)$. Use $m_X$ to prove that $\var(X) = n\theta(1 - \theta)$.
\end{exercise}

\begin{solution}
Here $m_X(s) = (e^s \theta + 1 - \theta)^n$. Hence, $m_X'(s) = e^s \theta n(e^s \theta + 1 - \theta)^{n-1}$, so $m_X'(0) = n\theta$. Then $m_X''(s) = e^{2s} \theta^2 n(n - 1)(e^s \theta + 1 - \theta)^{n-2} + e^s \theta n(e^s \theta + 1 - \theta)^{n-1}$, so $m_X''(0) = \theta^2 n(n - 1) + \theta n$. Hence, $\var(X) = \expc(X^2) - \expc(X)^2 = m_X''(0) - (m_X'(0))^2 = \theta^2 n(n - 1) + \theta n - (n\theta)^2 = n\theta(1 - \theta)$.
\end{solution}

\begin{exercise}
\label{exer:3.4.3}
Let $Y \sim \text{Poisson}(\lambda)$. Use $m_Y$ to compute the mean and variance of $Y$.
\end{exercise}

\begin{solution}
Here $m_Y(s) = e^{\lambda(e^s - 1)}$. Hence, $m_Y'(s) = \lambda e^s e^{\lambda(e^s - 1)}$, so $m_Y'(0) = \lambda$. Also, $m_Y''(s) = (\lambda e^s + \lambda^2 e^{2s}) e^{\lambda(e^s - 1)}$, so $m_Y''(0) = \lambda + \lambda^2$. Hence, $\var(Y) = \expc(Y^2) - \expc(Y)^2 = m_Y''(0) - (m_Y'(0))^2 = \lambda + \lambda^2 - (\lambda)^2 = \lambda$.
\end{solution}

\begin{exercise}
\label{exer:3.4.4}
Let $Y = 3X + 4$. Compute $r_Y(t)$ in terms of $r_X$.
\end{exercise}

\begin{solution}
$r_Y(t) = \expc(t^Y) = \expc(t^{3X+4}) = t^4 \expc((t^3)^X) = t^4 r_X(t^3)$.
\end{solution}

\begin{exercise}
\label{exer:3.4.5}
Let $Y = 3X + 4$. Compute $m_Y(s)$ in terms of $m_X$.
\end{exercise}

\begin{solution}
$m_Y(s) = \expc(e^{sY}) = \expc(e^{s(3X+4)}) = e^{4s} \expc(e^{3sX}) = e^{4s} m_X(3s)$.
\end{solution}

\begin{exercise}
\label{exer:3.4.6}
Let $X \sim \text{Binomial}(n, \theta)$. Compute $\expc[X^3]$, the third moment of $X$.
\end{exercise}

\begin{solution}
We know $m_X''(s) = e^{2s} \theta^2 n(n - 1)(e^s \theta + 1 - \theta)^{n-2} + e^s \theta n(e^s \theta + 1 - \theta)^{n-1}$, so $m_X'''(s) = e^{sn} \theta(1 - (e^s - 1)\theta)^{n-3}[1 - (e^s(3n - 1) + 2)\theta + (1 - e^s(3n - 1) + e^{2s} n^2)\theta^2]$, so $\expc(X^3) = m_X'''(0) = n\theta[1 - 3(n - 1)\theta + (n^2 - 3n + 2)\theta^2]$.
\end{solution}

\begin{exercise}
\label{exer:3.4.7}
Let $Y \sim \text{Poisson}(\lambda)$. Compute $\expc[Y^3]$, the third moment of $Y$.
\end{exercise}

\begin{solution}
We know from previously that $m_Y''(s) = (\lambda e^s + \lambda^2 e^{2s}) e^{\lambda(e^s - 1)}$ so $m_Y'''(s) = e^{\lambda(e^s - 1)} e^s \lambda(1 + 3e^s \lambda + e^{2s} \lambda^2)$, and $\expc(Y^3) = m_Y'''(0) = \lambda(1 + 3\lambda + \lambda^2)$.
\end{solution}

\begin{exercise}
\label{exer:3.4.8}
Suppose $\prb(X = 2) = 1/2$, $\prb(X = 5) = 1/3$, and $\prb(X = 7) = 1/6$.
\begin{enumerate}[(a)]
\item Compute $r_X(t)$ for $t \in \mathbf{R}^1$.
\item Verify that $r_X'(0) = \prb(X = 1)$ and $r_X''(0) = 2\prb(X = 2)$.
\item Compute $m_X(s)$ for $s \in \mathbf{R}^1$.
\item Verify that $m_X'(0) = \expc[X]$ and $m_X''(0) = \expc[X^2]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $r_X(t) = \expc(t^X) = (t^2)(1/2) + (t^5)(1/3) + (t^7)(1/6)$.
    \item $r_X'(t) = (2t)(1/2) + (5t^4)(1/3) + (7t^6)(1/6)$. Hence, $r_X'(0) = 0 = \prb(X = 1)$. Also, $r_X''(t) = (2)(1/2) + (20t^3)(1/3) + (42t^5)(1/6)$. Hence, $r_X''(0) = (2)(1/2) = 1 = 2\prb(X = 2)$.
    \item $m_X(s) = \expc(e^{sX}) = (e^{2s})(1/2) + (e^{5s})(1/3) + (e^{7s})(1/6)$. $m_X'(s) = (2e^{2s})(1/2) + (5e^{5s})(1/3) + (7e^{7s})(1/6)$. Hence, $m_X'(0) = (2)(1/2) + (5)(1/3) + (7)(1/6) = \expc(X)$. Also, $m_X''(s) = (2^2 e^{2s})(1/2) + (5^2 e^{5s})(1/3) + (7^2 e^{7s})(1/6)$. Hence, $m_X''(0) = (2^2)(1/2) + (5^2)(1/3) + (7^2)(1/6) = \expc(X^2)$.
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:3.4.9}
Suppose $f_X(x) = 1/10$ for $0 \leqslant x \leqslant 10$, with $f_X(x) = 0$ otherwise.
\begin{enumerate}[(a)]
\item Compute $m_X(s)$ for $s \in \mathbf{R}^1$.
\item Verify that $m_X'(0) = \expc[X]$. (Hint: L'H\^opital's rule.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $m_X(s) = \expc(e^{sX}) = \int_0^{10} e^{sx} (1/10) \, \mathrm{d}x = (1/10)(e^{10s} - 1)/s$ for $s \neq 0$, with (of course) $m_X(0) = 1$.
    \item For $s \neq 0$, $m_X'(s) = (1/10)(s(10e^{10s}) - (e^{10s} - 1))/s^2$. We then compute using L'Hpital's Rule (twice) that $m_X'(0) = \lim_{s \to \infty} m_X'(s) = 5 = \expc(X)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.4.10}
Let $X \sim \text{Geometric}(\theta)$. Compute $r_X(t)$ and $r_X''(0)$.
\end{exercise}

\begin{solution}
We have that $r_X(t) = \sum_{x=0}^{\infty} (t(1 - \theta))^x \theta = \theta(1 - t(1 - \theta))^{-1}$, provided $|t(1 - \theta)| < 1$. Then $r_X'(t) = \theta(1 - \theta)(1 - t(1 - \theta))^{-2}$, $r_X''(t) = 2\theta(1 - \theta)^2(1 - t(1 - \theta))^{-3}$, so $r_X''(0)/2 = \theta(1 - \theta)^2$.
\end{solution}

\begin{exercise}
\label{exer:3.4.11}
Let $X \sim \text{Negative-Binomial}(r, \theta)$. Compute $r_X(t)$ and $r_X''(0)$.
\end{exercise}

\begin{solution}
We have that $r_X(t) = \sum_{x=0}^{\infty} \binom{r+x-1}{x} (t(1 - \theta))^x \theta^r = \theta^r(1 - t(1 - \theta))^{-r}$, provided $|t(1 - \theta)| < 1$. Then $r_X'(t) = r\theta^r(1 - \theta)(1 - t(1 - \theta))^{-r-1}$, $r_X''(t) = r(r - 1)\theta^r(1 - \theta)^2(1 - t(1 - \theta))^{-r-2}$, so $r_X''(0)/2 = r(r + 1)\theta^r(1 - \theta)^2/2 = \binom{r+2-1}{2} \theta^r(1 - \theta)^2$.
\end{solution}

\begin{exercise}
\label{exer:3.4.12}
Let $X \sim \text{Geometric}(\theta)$.
\begin{enumerate}[(a)]
\item Compute $m_X(s)$.
\item Use $m_X$ to compute the mean of $X$.
\item Use $m_X$ to compute the variance of $X$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $m_X(s) = \expc(e^{sX}) = \sum_{x=0}^{\infty} e^{sx} (1 - \theta)^x \theta = \theta/(1 - e^s(1 - \theta))$, provided $|e^s(1 - \theta)| < 1$, i.e., $s < -\log(1 - \theta)$.
    \item $m_X'(s) = \frac{\mathrm{d}}{\mathrm{d}s}(\theta/[1 - e^s(1 - \theta)]) = \theta(1 - \theta)e^s/[1 - e^s(1 - \theta)]^2$. Hence, $\expc(X) = m_X'(0) = \theta(1 - \theta)/[1 - (1 - \theta)]^2 = (1 - \theta)/\theta$.
    \item $m_X''(s) = (e^s \theta(1 - \theta)(1 + e^s(1 - \theta)))/[1 - e^s(1 - \theta)]^3$. Hence, $\expc(X^2) = m_X''(0) = (\theta(1 - \theta)(2 - \theta))/[\theta]^3 = (1 - \theta)(2 - \theta)/\theta^2$, so $\var(X) = \expc(X^2) - \expc(X)^2 = [(1 - \theta)(2 - \theta)/\theta^2] - [(1 - \theta)/\theta]^2 = (1 - \theta)/\theta^2$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.4.13}
Let $X \sim \text{Negative-Binomial}(r, \theta)$.
\begin{enumerate}[(a)]
\item Compute $m_X(s)$.
\item Use $m_X$ to compute the mean of $X$.
\item Use $m_X$ to compute the variance of $X$.
\end{enumerate}
\end{exercise}

\begin{solution}
We use the result of 3.4.12 and the fact that if $X_1, \ldots, X_r$ is a sample from the $\text{Geometric}(\theta)$ distribution, then $X = X_1 + \cdots + X_r \sim \text{Negative Binomial}(r, \theta)$.
\begin{enumerate}[(a)]
    \item $m_X(s) = m_{X_1}(s) \cdots m_{X_r}(s) = \theta^r/(1 - e^s(1 - \theta))^r$.
    \item $m_X'(s) = re^s(1 - \theta)\theta^r/(1 - e^s(1 - \theta))^{r+1}$, so $\expc(X) = m_X'(0) = r(1 - \theta)\theta^r/\theta^{r+1} = r(1 - \theta)/\theta$.
    \item $m_X''(s) = re^s(1 - \theta)\theta^r/(1 - e^s(1 - \theta))^{r+1} + r(r + 1)e^{2s}(1 - \theta)^2 \theta^r/(1 - e^s(1 - \theta))^{r+2}$, so $\var(X) = m_X''(0) - (r(1 - \theta)/\theta)^2 = r(1 - \theta)\theta^r/\theta^{r+1} + r(r + 1)(1 - \theta)^2 \theta^r/\theta^{r+2} - (r(1 - \theta)/\theta)^2 = r(1 - \theta)/\theta + r(1 - \theta)^2/\theta^2 = r(1 - \theta)/\theta^2$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.4.14}
If $Y = a + bX$ where $a$ and $b$ are constants, then show that $r_Y(t) = t^a r_X(t^b)$ and $m_Y(t) = e^{at} m_X(bt)$.
\end{exercise}

\begin{solution}
$r_Y(t) = \expc(t^Y) = \expc(t^{a+bX}) = \expc(t^a t^{bX}) = t^a \expc(t^{bX}) = t^a \expc((t^b)^X) = t^a r_X(t^b)$ and $m_Y(t) = \expc(e^{tY}) = \expc(e^{at+btX}) = \expc(e^{at} e^{btX}) = e^{at} \expc(e^{btX}) = e^{at} \expc(e^{(bt)X}) = e^{at} m_X(bt)$.
\end{solution}

\begin{exercise}
\label{exer:3.4.15}
Let $Z \sim N(\mu, \sigma^2)$. Show that
\[
m_Z(s) = \exp\left( \mu s + \frac{\sigma^2 s^2}{2} \right).
\]
(Hint: Write $Z = \mu + \sigma X$ where $X \sim N(0, 1)$, and use Theorem~\ref{thm:3.4.4}.)
\end{exercise}

\begin{solution}
Write $Z = \mu + \sigma X$, where $X \sim \text{Normal}(0, 1)$. Then $m_Z(s) = \expc(e^{sZ}) = \expc(e^{s(\mu + \sigma X)}) = e^{s\mu} \expc(e^{s\sigma X}) = e^{s\mu} m_X(\sigma s) = e^{s\mu} e^{(\sigma s)^2/2} = e^{s\mu + \sigma^2 s^2/2}$.
\end{solution}

\begin{exercise}
\label{exer:3.4.16}
Let $Y$ be distributed according to the Laplace distribution (see Problem~\ref{exer:2.4.22}).
\begin{enumerate}[(a)]
\item Compute $m_Y(s)$. (Hint: Break up the integral into two pieces.)
\item Use $m_Y$ to compute the mean of $Y$.
\item Use $m_Y$ to compute the variance of $Y$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $m_Y(s) = \expc(e^{sY}) = \int_{-\infty}^{\infty} e^{sy} e^{|y|}/2 \, \mathrm{d}y = \int_0^{\infty} e^{sy} e^y/2 \, \mathrm{d}y + \int_{-\infty}^{0} e^{sy} e^{-y}/2 \, \mathrm{d}y = \int_0^{\infty} e^{(s-1/2)y} \, \mathrm{d}y + \int_{-\infty}^{0} e^{(s+1/2)y} \, \mathrm{d}y = 1/(1/2 - s) + 1/(1/2 + s)$, provided $|s| < 1/2$.
    \item $m_Y'(s) = (1/2 - s)^{-2} - (1/2 + s)^{-2}$, so $\expc(Y) = m_Y'(0) = 4 - 4 = 0$.
    \item $m_Y''(s) = 2(1/2 - s)^{-3} + 2(1/2 + s)^{-3}$, so $\expc(Y^2) = m_Y''(0) = 16 + 16 = 32$, where $\var(Y) = \expc(Y^2) - \expc(Y)^2 = 32$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.4.17}
Compute the $k$th moment of the Weibull$(\lambda, \beta)$ distribution in terms of $\Gamma$ (see Problem~\ref{exer:2.4.19}).
\end{exercise}

\begin{solution}
$\expc(X^k) = \int_0^{\infty} x^k \alpha x^{\alpha-1} e^{-x^{\alpha}} \, \mathrm{d}x = \int_0^{\infty} \alpha x^{\alpha+k-1} e^{-x^{\alpha}} \, \mathrm{d}x$ and putting $u = x^{\alpha}$, $x = u^{1/\alpha}$, $\mathrm{d}u = \alpha x^{\alpha-1} \, \mathrm{d}x$ we have that $\expc(X) = \int_0^{\infty} u^{k/\alpha} e^{-u} \, \mathrm{d}u = \Gamma(k/\alpha + 1)$.
\end{solution}

\begin{exercise}
\label{exer:3.4.18}
Compute the $k$th moment of the Pareto$(\alpha)$ distribution (see Problem~\ref{exer:2.4.20}). (Hint: Make the transformation $u = 1 - x^{-1}$ and recall the beta distribution.)
\end{exercise}

\begin{solution}
We put $u = 1/(1 + x)$ so $x = (1/u) - 1$, $\mathrm{d}x = -\mathrm{d}u/u^2$ so
\begin{align*}
    \expc(X^k) &= \int_0^{\infty} x^k \alpha (1 + x)^{-\alpha-1} \, \mathrm{d}x = \alpha \int_0^{\infty} \left(\frac{1 - u}{u}\right)^k u^{\alpha-1} \, \mathrm{d}u \\
    &= \alpha \int_0^{\infty} u^{\alpha-k-1} (1 - u)^k \, \mathrm{d}u = \begin{cases} \infty & 0 < \alpha \leqslant k \\[0.5em] \alpha \dfrac{\Gamma(\alpha - k)\Gamma(k + 1)}{\Gamma(\alpha + 1)} & \alpha > k \end{cases} = \begin{cases} \infty & 0 < \alpha \leqslant k \\[0.5em] \dfrac{\Gamma(\alpha - k)\Gamma(k + 1)}{\Gamma(\alpha)} & \alpha > k. \end{cases}
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:3.4.19}
Compute the $k$th moment of the Log-normal distribution (see Problem~\ref{exer:2.6.17}). (Hint: Make the transformation $z = \ln x$ and use Problem~\ref{exer:3.4.15}.)
\end{exercise}

\begin{solution}
Putting $z = \ln x$ so that $x = \exp(z)$, $\mathrm{d}x = \exp(z) \, \mathrm{d}z$, we have that
\begin{align*}
    \expc(X^k) &= \int_0^{\infty} x^k \frac{1}{\sqrt{2\pi}\tau} \exp\left(\frac{-(\ln x)^2}{2\tau^2}\right) \frac{1}{x} \, \mathrm{d}x = \int_0^{\infty} x^{k-1} \frac{1}{\sqrt{2\pi}\tau} \exp\left(\frac{-(\ln x)^2}{2\tau^2}\right) \mathrm{d}x \\
    &= \int_0^{\infty} \exp(kz) \frac{1}{\sqrt{2\pi}\tau} \exp\left(-\frac{z^2}{2\tau^2}\right) \mathrm{d}x = \exp\left(\frac{\tau^2 k^2}{2}\right)
\end{align*}
since this is the moment-generating function of the $N(0, \tau^2)$ distribution at $k$.
\end{solution}

\begin{exercise}
\label{exer:3.4.20}
Prove that the moment-generating function of the Gamma$(\alpha, \lambda)$ distribution is given by $(\lambda/(\lambda - t))^\alpha$ when $t < \lambda$.
\end{exercise}

\begin{solution}
\[
    m(s) = \int_0^{\infty} e^{xt} \frac{(\lambda x)^{\alpha-1}}{\Gamma(\alpha)} e^{-\lambda x} \lambda \, \mathrm{d}x = \lambda \int_0^{\infty} \frac{(\lambda x)^{\alpha-1}}{\Gamma(\alpha)} e^{-(\lambda - t)x} \, \mathrm{d}x = \begin{cases} \infty & t \geqslant \lambda \\[0.5em] \dfrac{\lambda^{\alpha}}{(\lambda - t)^{\alpha}} & t < \lambda \end{cases}
\]
\end{solution}

\begin{exercise}
\label{exer:3.4.21}
Suppose that $X_i \sim \text{Poisson}(\lambda_i)$ and $X_1, \ldots, X_n$ are independent. Using moment-generating functions, determine the distribution of $Y = \sum_{i=1}^{n} X_i$.
\end{exercise}

\begin{solution}
The mgf of the $\text{Poisson}(\lambda_i)$ equals $m_i(s) = \exp\{\lambda_i(e^s - 1)\}$. Then the mgf of $Y = X_1 + \cdots + X_n$ is given by (Theorem \ref{thm:3.4.5})
\[
    m_Y(s) = \prod_{i=1}^{n} m_i(s) = \prod_{i=1}^{n} \exp\{\lambda_i(e^s - 1)\} = \exp\left(\sum_{i=1}^{n} \lambda_i(e^s - 1)\right)
\]
and we recognize this as the mgf of the $\text{Poisson}(\sum_{i=1}^{n} \lambda_i)$. Therefore, the uniqueness theorem implies that this is the distribution of $Y$.
\end{solution}

\begin{exercise}
\label{exer:3.4.22}
Suppose that $X_i \sim \text{Negative-Binomial}(r_i, \theta)$ and $X_1, \ldots, X_n$ are independent. Using moment-generating functions, determine the distribution of $Y = \sum_{i=1}^{n} X_i$.
\end{exercise}

\begin{solution}
The mgf of the $\text{Geometric}(\theta)$ distribution is given by $\theta/(1 - e^s(1 - \theta))$. Therefore, the $\text{Negative Binomial}(r, \theta)$ distribution has mgf given by $m(s) = \theta^r/(1 - e^s(1 - \theta))^r$ since it can be obtained as the sum of $r$ independent $\text{Geometric}(\theta)$ random variables and we use Theorem \ref{thm:3.4.5}. Then $X_i$ has mgf given by $m_i(s) = \theta^{r_i}/(1 - e^s(1 - \theta))^{r_i}$ and, using Theorem \ref{thm:3.4.5} again, we have that $Y$ has mgf
\[
    m_Y(s) = \prod_{i=1}^{n} m_i(s) = \prod_{i=1}^{n} \theta^{r_i}/(1 - e^s(1 - \theta))^{r_i} = \theta^{\sum_{i=1}^{n} r_i}/(1 - e^s(1 - \theta))^{\sum_{i=1}^{n} r_i}
\]
and we recognize this as the mgf of the $\text{Negative Binomial}(\sum_{i=1}^{n} r_i, \theta)$ distribution. Therefore, by the uniqueness theorem this is the distribution of $Y$.
\end{solution}

\begin{exercise}
\label{exer:3.4.23}
Suppose that $X_i \sim \text{Gamma}(\alpha_i, \lambda)$ and $X_1, \ldots, X_n$ are independent. Using moment-generating functions, determine the distribution of $Y = \sum_{i=1}^{n} X_i$.
\end{exercise}

\begin{solution}
The $\text{Gamma}(\alpha, \lambda)$ distribution has mgf $\lambda^{\alpha}/(\lambda - t)^{\alpha}$ for $t < \lambda$ by Problem \ref{exer:3.4.20}. Therefore, by Theorem \ref{thm:3.4.5}, $Y$ has mgf
\[
    m_Y(s) = \prod_{i=1}^{n} m_i(s) = \prod_{i=1}^{n} \lambda^{\alpha_i}/(\lambda - t)^{\alpha_i} = \lambda^{\sum_{i=1}^{n} \alpha_i}/(\lambda - t)^{\sum_{i=1}^{n} \alpha_i}
\]
and we recognize this as the mgf of the $\text{Gamma}(\sum_{i=1}^{n} \alpha_i, \lambda)$ distribution so, by the uniqueness theorem this must be the distribution of $Y$.
\end{solution}

\begin{exercise}
\label{exer:3.4.24}
Suppose $X_1, X_2, \ldots$ is i.i.d.\ Exponential$(\lambda)$ and $N \sim \text{Poisson}(\mu)$ independent of the $X_i$. Determine the moment-generating function of $S_N$. Determine the first moment of this distribution by differentiating this function.
\end{exercise}

\begin{solution}
By Theorem \ref{thm:3.4.7} the mgf is given by (using $m_{X_i}(s) = \lambda/(\lambda - s)$ and $r_N(t) = \exp\{\lambda(t - 1)\}$)
\[
    m_{S_N}(s) = r_N(m_{X_1}(s)) = \exp\left\{\lambda\left(\frac{\lambda}{\lambda - s} - 1\right)\right\} = \exp\left\{\frac{\lambda s}{\lambda - s}\right\}.
\]
Then $m_{S_N}'(s) = \exp\left\{\frac{\lambda s}{\lambda - s}\right\} \left(\frac{\lambda s}{\lambda - s}\right)' = \exp\left\{\frac{\lambda s}{\lambda - s}\right\} \lambda^2(\lambda - s)^{-2}$ and $m_{S_N}'(0) = 1$ is the mean.
\end{solution}

\begin{exercise}
\label{exer:3.4.25}
Suppose $X_1, X_2, \ldots$ are i.i.d.\ Exponential$(\lambda)$ random variables and $N \sim \text{Geometric}(\theta)$ independent of the $X_i$. Determine the moment-generating function of $S_N$. Determine the first moment of this distribution by differentiating this function.
\end{exercise}

\begin{solution}
By Theorem \ref{thm:3.4.7} the mgf is given by (using $m_{X_i}(s) = \lambda/(\lambda - s)$ and $r_N(t) = \theta(1 - t(1 - \theta))^{-1}$)
\[
    m_{S_N}(s) = r_N(m_{X_1}(s)) = \theta/\left(1 - \frac{\lambda}{\lambda - s}(1 - \theta)\right).
\]
Then
\[
    m_{S_N}'(s) = \frac{\theta}{\left(1 - \frac{\lambda}{\lambda - s}(1 - \theta)\right)^2} (1 - \theta) \frac{\lambda}{(\lambda - s)^2},
\]
so $m_{S_N}'(0) = (1 - \theta)/(\lambda\theta)$.
\end{solution}

\begin{exercise}
\label{exer:3.4.26}
Let $X \sim \text{Bernoulli}(\theta)$. Use $c_X(s)$ to compute the mean of $X$.
\end{exercise}

\begin{solution}
Here $c_X(s) = 1 - p + p\cos s + ip\sin s$, so $c_X'(s) = -p\sin s + ip\cos s$, $i\expc(X) = c_X'(0) = ip$, $\expc(X) = p$.
\end{solution}

\begin{exercise}
\label{exer:3.4.27}
Let $Y \sim \text{Binomial}(n, \theta)$.
\begin{enumerate}[(a)]
\item Compute the characteristic function $c_Y(s)$. (Hint: Make use of $c_X(s)$ in Problem~\ref{exer:3.4.26}.)
\item Use $c_Y(s)$ to compute the mean of $Y$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item We can write $Y = X_1 + X_2 + \cdots + X_n$, where the $\{X_i\}$ are i.i.d.\ $\sim \text{Bernoulli}(p)$. Hence, $c_Y(s) = c_{X_1}(s) c_{X_2}(s) \cdots c_{X_n}(s) = (1 - p + p\cos s + ip\sin s)^n$.
    \item $c_Y'(s) = n(1 - p + p\cos s + ip\sin s)^{n-1}(-p\sin s + ip\cos s)$. Hence, $i\expc(Y) = c_Y'(0) = n \cdot 1^{n-1}(ip) = inp$, so $\expc(Y) = np$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.4.28}
The characteristic function of the Cauchy distribution (see Problem~\ref{exer:2.4.21}) is given by $c(t) = e^{-|t|}$. Use this to determine the characteristic function of the sample mean
\[
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
\]
based on a sample of $n$ from the Cauchy distribution. Explain why this implies that the sample mean is also Cauchy distributed. What do you find surprising about this result?
\end{exercise}

\begin{solution}
The sample mean has characteristic function given by
\[
    c_{\bar{X}}(s) = \expc(\exp\{is\bar{X}\}) = \expc\left(\exp\left\{i\frac{s}{n} \sum_{i=1}^{n} X_i\right\}\right) = \prod_{i=1}^{n} c_{X_1}\left(\frac{s}{n}\right) = \left(c_{X_1}\left(\frac{s}{n}\right)\right)^n = \left(\exp\left\{-\frac{|t|}{n}\right\}\right)^n = \exp\{-|t|\}
\]
and we recognize this as the cf of the Cauchy distribution. Then by the uniqueness theorem we must have that $\bar{X}$ is also distributed Cauchy. This implies that the sample mean in this case is just as variable as a single observation. So increasing the sample size does not make the distribution of $\bar{X}$ more concentrated. In fact, it does not change at all!
\end{solution}

\begin{exercise}
\label{exer:3.4.29}
The $k$-th cumulant (when it exists) of a random variable $X$ is obtained by calculating the $k$-th derivative of $\ln c_X(s)$ with respect to $s$, evaluating this at $s = 0$, and dividing by $i^k$. Evaluate $c_X(s)$ and all the cumulants of the $N(\mu, \sigma^2)$ distribution.
\end{exercise}

\begin{solution}
The cf of the $N(0, 1)$ distribution is given by
\[
    c(t) = \int_{-\infty}^{\infty} e^{itx} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \, \mathrm{d}x = e^{-t^2/2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}(x - it)^2\right) \mathrm{d}x = e^{-t^2/2}.
\]
Therefore, if $X = \mu + \sigma Z$, where $Z \sim N(0, 1)$, then $X \sim N(\mu, \sigma^2)$ and $X$ has mgf
\[
    c_X(t) = \expc(e^{itX}) = \expc(e^{it(\mu + \sigma Z)}) = e^{it\mu} \expc(e^{it\sigma Z}) = e^{it\mu} c(t\sigma) = \exp\left\{it\mu - \frac{\sigma^2 t^2}{2}\right\}.
\]
Then we have that $\ln c_X(t) = it\mu - \sigma^2 t^2/2$ so $(\ln c_X(t))' = i\mu - \sigma^2 t$, $(\ln c_X(0))'/i = \mu$ and the first cumulant is $\mu$. Also, $(\ln c_X(t))'' = -\sigma^2$ and so $(\ln c_X(0))''/i^2 = \sigma^2$ and the second cumulant is $\sigma^2$. Also, all higher order derivatives of $\ln c_X(t)$ are $0$, so all higher order cumulants are $0$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional Expectation}
\label{sec:3.5}

We have seen in Sections~\ref{sec:1.5} and \ref{sec:2.8} that conditioning on some event, or some random variable, can change various probabilities. Now, because expectations are defined in terms of probabilities, it seems reasonable that expectations should also change when conditioning on some event or random variable. Such modified expectations are called conditional expectations, as we now discuss.

\subsection{Discrete Case}
\label{ssec:3.5.1}

The simplest case is when $X$ is a discrete random variable, and $A$ is some event of positive probability. We have the following.

\begin{definition}
\label{def:3.5.1}
Let $X$ be a discrete random variable, and let $A$ be some event with $\prb(A) > 0$. Then the \emph{conditional expectation of $X$ given $A$}, is equal to
\[
\expc[X \mid A] = \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid A) = \sum_{x \in \mathbf{R}^1} x \, \frac{\prb(X = x, A)}{\prb(A)}.
\]
\end{definition}

\begin{example}
\label{ex:3.5.1}
Consider rolling a fair six-sided die, so that $S = \{1, 2, 3, 4, 5, 6\}$. Let $X$ be the number showing, so that $X(s) = s$ for $s \in S$. Let $A = \{3, 5, 6\}$ be the event that the die shows $3$, $5$, or $6$. What is $\expc[X \mid A]$?

Here we know that
\[
\prb(X = 3 \mid A) = \prb(X = 3 \mid X \in \{3, 5, 6\}) = 1/3
\]
and that, similarly, $\prb(X = 5 \mid A) = \prb(X = 6 \mid A) = 1/3$. Hence,
\begin{align*}
\expc[X \mid A] &= \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid A) \\
&= 3 \, \prb(X = 3 \mid A) + 5 \, \prb(X = 5 \mid A) + 6 \, \prb(X = 6 \mid A) \\
&= 3 \cdot (1/3) + 5 \cdot (1/3) + 6 \cdot (1/3) = 14/3.
\end{align*}
\end{example}

Often we wish to condition on the value of some other random variable. If the other random variable is also discrete, and if the conditioned value has positive probability, then this works as above.

\begin{definition}
\label{def:3.5.2}
Let $X$ and $Y$ be discrete random variables, with $\prb(Y = y) > 0$. Then the \emph{conditional expectation of $X$ given $Y = y$}, is equal to
\[
\expc[X \mid Y = y] = \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid Y = y) = \sum_{x \in \mathbf{R}^1} x \, \frac{p_{X,Y}(x, y)}{p_Y(y)}.
\]
\end{definition}

\begin{example}
\label{ex:3.5.2}
Suppose the joint probability function of $X$ and $Y$ is given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/7 & x = 5, y = 0 \\
1/7 & x = 5, y = 3 \\
1/7 & x = 5, y = 4 \\
3/7 & x = 8, y = 0 \\
1/7 & x = 8, y = 4 \\
0 & \text{otherwise}.
\end{cases}
\]
Then
\begin{align*}
\expc[X \mid Y = 0] &= \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid Y = 0) \\
&= 5 \, \prb(X = 5 \mid Y = 0) + 8 \, \prb(X = 8 \mid Y = 0) \\
&= 5 \cdot \frac{\prb(X = 5, Y = 0)}{\prb(Y = 0)} + 8 \cdot \frac{\prb(X = 8, Y = 0)}{\prb(Y = 0)} \\
&= 5 \cdot \frac{1/7}{1/7 + 3/7} + 8 \cdot \frac{3/7}{1/7 + 3/7} = \frac{29}{4}.
\end{align*}
Similarly,
\begin{align*}
\expc[X \mid Y = 4] &= \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid Y = 4) \\
&= 5 \, \prb(X = 5 \mid Y = 4) + 8 \, \prb(X = 8 \mid Y = 4) \\
&= 5 \cdot \frac{1/7}{1/7 + 1/7} + 8 \cdot \frac{1/7}{1/7 + 1/7} = 13/2.
\end{align*}
Also,
\begin{align*}
\expc[X \mid Y = 3] &= \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid Y = 3) = 5 \, \prb(X = 5 \mid Y = 3) \\
&= 5 \cdot \frac{1/7}{1/7} = 5.
\end{align*}
\end{example}

Sometimes we wish to condition on a random variable $Y$, without specifying in advance on what value of $Y$ we are conditioning. In this case, the conditional expectation $\expc[X \mid Y]$ is itself a random variable --- namely, it depends on the (random) value of $Y$ that occurs.

\begin{definition}
\label{def:3.5.3}
Let $X$ and $Y$ be discrete random variables. Then the \emph{conditional expectation of $X$ given $Y$}, is the random variable $\expc[X \mid Y]$ which is equal to $\expc[X \mid Y = y]$ when $Y = y$. In particular, $\expc[X \mid Y]$ is a random variable that depends on the random value of $Y$.
\end{definition}

\begin{example}
\label{ex:3.5.3}
Suppose again that the joint probability function of $X$ and $Y$ is given by
\[
p_{X,Y}(x, y) = \begin{cases}
1/7 & x = 5, y = 0 \\
1/7 & x = 5, y = 3 \\
1/7 & x = 5, y = 4 \\
3/7 & x = 8, y = 0 \\
1/7 & x = 8, y = 4 \\
0 & \text{otherwise}.
\end{cases}
\]
We have already computed that $\expc[X \mid Y = 0] = 29/4$, $\expc[X \mid Y = 4] = 13/2$, and $\expc[X \mid Y = 3] = 5$. We can express these results together by saying that
\[
\expc[X \mid Y] = \begin{cases}
29/4 & Y = 0 \\
5 & Y = 3 \\
13/2 & Y = 4.
\end{cases}
\]
That is, $\expc[X \mid Y]$ is a random variable, which depends on the value of $Y$. Note that, because $\prb(Y = y) = 0$ for $y \notin \{0, 3, 4\}$, the random variable $\expc[X \mid Y]$ is undefined in that case; but this is not a problem because that case will never occur.
\end{example}

Finally, we note that just like for regular expectation, conditional expectation is linear.

\begin{theorem}
\label{thm:3.5.1}
Let $X_1$, $X_2$, and $Y$ be random variables; let $A$ be an event; let $a$, $b$, and $y$ be real numbers; and let $Z = aX_1 + bX_2$. Then
\begin{enumerate}[(a)]
\item $\expc[Z \mid A] = a\expc[X_1 \mid A] + b\expc[X_2 \mid A]$.
\item $\expc[Z \mid Y = y] = a\expc[X_1 \mid Y = y] + b\expc[X_2 \mid Y = y]$.
\item $\expc[Z \mid Y] = a\expc[X_1 \mid Y] + b\expc[X_2 \mid Y]$.
\end{enumerate}
\end{theorem}
\subsection{Absolutely Continuous Case}
\label{ssec:3.5.2}

Suppose now that $X$ and $Y$ are jointly absolutely continuous. Then conditioning on $Y = y$, for some particular value of $y$, seems problematic, because $\prb(Y = y) = 0$. However, we have already seen in Section~\ref{ssec:2.8.2} that we can define a conditional density $f_{X|Y}(x \mid y)$ that gives us a density function for $X$, conditional on $Y = y$. And because density functions give rise to expectations, similarly conditional density functions give rise to conditional expectations, as follows.

\begin{definition}
\label{def:3.5.4}
Let $X$ and $Y$ be jointly absolutely continuous random variables, with joint density function $f_{X,Y}(x, y)$. Then the \emph{conditional expectation of $X$ given $Y = y$}, is equal to
\[
\expc[X \mid Y = y] = \int_{x \in \mathbf{R}^1} x \, f_{X|Y}(x \mid y) \, \mathrm{d}x = \int_{x \in \mathbf{R}^1} x \, \frac{f_{X,Y}(x, y)}{f_Y(y)} \, \mathrm{d}x.
\]
\end{definition}

\begin{example}
\label{ex:3.5.4}
Let $X$ and $Y$ be jointly absolutely continuous, with joint density function $f_{X,Y}$ given by
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise}.
\end{cases}
\]
Then for $0 \leqslant y \leqslant 1$,
\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}x = \int_0^1 (4x^2 y + 2y^5) \, \mathrm{d}x = (4y/3) + 2y^5.
\]
Hence,
\begin{align*}
\expc[X \mid Y = y] &= \int_{x \in \mathbf{R}^1} x \, \frac{f_{X,Y}(x, y)}{f_Y(y)} \, \mathrm{d}x = \int_0^1 x \cdot \frac{4x^2 y + 2y^5}{(4y/3) + 2y^5} \, \mathrm{d}x \\
&= \frac{y + y^5}{(4y/3) + 2y^5} = \frac{1 + y^4}{(4/3) + 2y^4}.
\end{align*}
\end{example}

As in the discrete case, we often wish to condition on a random variable without specifying in advance the value of that variable. Thus, $\expc[X \mid Y]$ is again a random variable, depending on the random value of $Y$.

\begin{definition}
\label{def:3.5.5}
Let $X$ and $Y$ be jointly absolutely continuous random variables. Then the \emph{conditional expectation of $X$ given $Y$}, is the random variable $\expc[X \mid Y]$ which is equal to $\expc[X \mid Y = y]$ when $Y = y$. Thus, $\expc[X \mid Y]$ is a random variable that depends on the random value of $Y$.
\end{definition}

\begin{example}
\label{ex:3.5.5}
Let $X$ and $Y$ again have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise}.
\end{cases}
\]
We already know that $\expc[X \mid Y = y] = (1 + y^4)/((4/3) + 2y^4)$. This formula is valid for any $y$ between $0$ and $1$, so we conclude that $\expc[X \mid Y] = (1 + Y^4)/((4/3) + 2Y^4)$. Note that in this last formula, $Y$ is a random variable, so $\expc[X \mid Y]$ is also a random variable.
\end{example}

Finally, we note that in the absolutely continuous case, conditional expectation is still linear, i.e., Theorem~\ref{thm:3.5.1} continues to hold.

\subsection{Double Expectations}
\label{ssec:3.5.3}

Because the conditional expectation $\expc[X \mid Y]$ is itself a random variable (as a function of $Y$), it makes sense to take its expectation, $\expc[\expc[X \mid Y]]$. This is a double expectation. One of the key results about conditional expectation is that it is always equal to $\expc[X]$.

\begin{theorem}[Theorem of total expectation]
\label{thm:3.5.2}
If $X$ and $Y$ are random variables, then $\expc[\expc[X \mid Y]] = \expc[X]$.
\end{theorem}

This theorem follows as a special case of Theorem~\ref{thm:3.5.3} on the next page. But it also makes sense intuitively. Indeed, conditioning on $Y$ will change the conditional value of $X$ in various ways, sometimes making it smaller and sometimes larger, depending on the value of $Y$. However, if we then average over all possible values of $Y$, these various effects will cancel out, and we will be left with just $\expc[X]$.

\begin{example}
\label{ex:3.5.6}
Suppose again that $X$ and $Y$ have joint probability function
\[
p_{X,Y}(x, y) = \begin{cases}
1/7 & x = 5, y = 0 \\
1/7 & x = 5, y = 3 \\
1/7 & x = 5, y = 4 \\
3/7 & x = 8, y = 0 \\
1/7 & x = 8, y = 4 \\
0 & \text{otherwise}.
\end{cases}
\]
Then we know that
\[
\expc[X \mid Y = y] = \begin{cases}
29/4 & y = 0 \\
5 & y = 3 \\
13/2 & y = 4.
\end{cases}
\]
Also, $\prb(Y = 0) = 1/7 + 3/7 = 4/7$, $\prb(Y = 3) = 1/7$, and $\prb(Y = 4) = 1/7 + 1/7 = 2/7$. Hence,
\begin{align*}
\expc[\expc[X \mid Y]] &= \sum_{y \in \mathbf{R}^1} \expc[X \mid Y = y] \, \prb(Y = y) \\
&= \expc[X \mid Y = 0] \, \prb(Y = 0) + \expc[X \mid Y = 3] \, \prb(Y = 3) + \expc[X \mid Y = 4] \, \prb(Y = 4) \\
&= (29/4) \cdot (4/7) + 5 \cdot (1/7) + (13/2) \cdot (2/7) = 47/7.
\end{align*}
On the other hand, we compute directly that $\expc[X] = 5 \, \prb(X = 5) + 8 \, \prb(X = 8) = 5 \cdot (3/7) + 8 \cdot (4/7) = 47/7$. Hence, $\expc[\expc[X \mid Y]] = \expc[X]$, as claimed.
\end{example}

\begin{example}
\label{ex:3.5.7}
Let $X$ and $Y$ again have joint density
\[
f_{X,Y}(x, y) = \begin{cases}
4x^2 y + 2y^5 & 0 \leqslant x \leqslant 1, \, 0 \leqslant y \leqslant 1 \\
0 & \text{otherwise}.
\end{cases}
\]
We already know that
\[
\expc[X \mid Y] = \frac{1 + Y^4}{(4/3) + 2Y^4}
\]
and that $f_Y(y) = (4y/3) + 2y^5$ for $0 \leqslant y \leqslant 1$. Hence,
\begin{align*}
\expc[\expc[X \mid Y]] &= \expc\left[ \frac{1 + Y^4}{(4/3) + 2Y^4} \right] = \int_{-\infty}^{\infty} \expc[X \mid Y = y] \, f_Y(y) \, \mathrm{d}y \\
&= \int_0^1 \frac{1 + y^4}{(4/3) + 2y^4} \cdot \left( \frac{4y}{3} + 2y^5 \right) \mathrm{d}y \\
&= \int_0^1 (y + y^5) \, \mathrm{d}y = 1/2 + 1/6 = 2/3.
\end{align*}
On the other hand,
\begin{align*}
\expc[X] &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \, f_{X,Y}(x, y) \, \mathrm{d}y \, \mathrm{d}x = \int_0^1 \int_0^1 x(4x^2 y + 2y^5) \, \mathrm{d}y \, \mathrm{d}x \\
&= \int_0^1 x(2x^2 + 2/6) \, \mathrm{d}x = \int_0^1 (2x^3 + x/3) \, \mathrm{d}x = 2/4 + 1/6 = 2/3.
\end{align*}
Hence, $\expc[\expc[X \mid Y]] = \expc[X]$, as claimed.
\end{example}

Theorem~\ref{thm:3.5.2} is a special case (with $g(y) = 1$) of the following more general result, which in fact characterizes conditional expectation.

\begin{theorem}
\label{thm:3.5.3}
Let $X$ and $Y$ be random variables, and let $g : \mathbf{R}^1 \to \mathbf{R}^1$ be any function. Then $\expc[g(Y) \expc[X \mid Y]] = \expc[g(Y) X]$.
\end{theorem}

\begin{proof}
See Section~\ref{sec:3.8} for the proof of this result.
\end{proof}

We also note the following related result. It says that, when conditioning on $Y$, any function of $Y$ can be factored out since it is effectively a constant.

\begin{theorem}
\label{thm:3.5.4}
Let $X$ and $Y$ be random variables, and let $g : \mathbf{R}^1 \to \mathbf{R}^1$ be any function. Then $\expc[g(Y) X \mid Y] = g(Y) \expc[X \mid Y]$.
\end{theorem}

\begin{proof}
See Section~\ref{sec:3.8} for the proof of this result.
\end{proof}

Finally, because conditioning twice on $Y$ is the same as conditioning just once on $Y$, we immediately have the following.

\begin{theorem}
\label{thm:3.5.5}
Let $X$ and $Y$ be random variables. Then $\expc[\expc[X \mid Y] \mid Y] = \expc[X \mid Y]$.
\end{theorem}

\subsection{Conditional Variance (Advanced)}
\label{ssec:3.5.4}

In addition to defining conditional expectation, we can define conditional variance. As usual, this involves the expected squared distance of a random variable to its mean. However, in this case, the expectation is a conditional expectation. In addition, the mean is a conditional mean.

\begin{definition}
\label{def:3.5.6}
If $X$ is a random variable, and $A$ is an event with $\prb(A) > 0$, then the \emph{conditional variance of $X$ given $A$}, is equal to
\[
\var(X \mid A) = \expc[(X - \expc[X \mid A])^2 \mid A] = \expc[X^2 \mid A] - (\expc[X \mid A])^2.
\]
Similarly, if $Y$ is another random variable, then
\[
\var(X \mid Y = y) = \expc[(X - \expc[X \mid Y = y])^2 \mid Y = y] = \expc[X^2 \mid Y = y] - (\expc[X \mid Y = y])^2
\]
and
\[
\var(X \mid Y) = \expc[(X - \expc[X \mid Y])^2 \mid Y] = \expc[X^2 \mid Y] - (\expc[X \mid Y])^2.
\]
\end{definition}

\begin{example}
\label{ex:3.5.8}
Consider again rolling a fair six-sided die, so that $S = \{1, 2, 3, 4, 5, 6\}$, with $\prb(\{s\}) = 1/6$ and $X(s) = s$ for $s \in S$, and with $A = \{3, 5, 6\}$. We have already computed that $\prb(X = s \mid A) = 1/3$ for $s \in A$, and that $\expc[X \mid A] = 14/3$. Hence,
\begin{align*}
\var(X \mid A) &= \expc[(X - \expc[X \mid A])^2 \mid A] = \expc[(X - 14/3)^2 \mid A] \\
&= \sum_{s \in S} (s - 14/3)^2 \, \prb(X = s \mid A) \\
&= (3 - 14/3)^2 \cdot (1/3) + (5 - 14/3)^2 \cdot (1/3) + (6 - 14/3)^2 \cdot (1/3) = 14/9 \approx 1.56.
\end{align*}
By contrast, because $\expc[X] = 7/2$, we have
\[
\var(X) = \expc[(X - \expc[X])^2] = \sum_{x=1}^{6} (x - 7/2)^2 \cdot (1/6) = 35/12 \approx 2.92.
\]
Hence, we see that the conditional variance $\var(X \mid A)$ is much smaller than the unconditional variance $\var(X)$. This indicates that, in this example, once we know that event $A$ has occurred, we know more about the value of $X$ than we did originally.
\end{example}

\begin{example}
\label{ex:3.5.9}
Suppose $X$ and $Y$ have joint density function
\[
f_{X,Y}(x, y) = \begin{cases}
8xy & 0 \leqslant x \leqslant y \leqslant 1 \\
0 & \text{otherwise}.
\end{cases}
\]
We have $f_Y(y) = 4y^3$, $f_{X|Y}(x \mid y) = 8xy / 4y^3 = 2x/y^2$ for $0 \leqslant x \leqslant y$, and so
\[
\expc[X \mid Y = y] = \int_0^y x \cdot \frac{2x}{y^2} \, \mathrm{d}x = \int_0^y \frac{2x^2}{y^2} \, \mathrm{d}x = \frac{2y^3}{3y^2} = \frac{2y}{3}.
\]
Therefore,
\begin{align*}
\var(X \mid Y = y) &= \expc[(X - \expc[X \mid Y = y])^2 \mid Y = y] \\
&= \int_0^y \left( x - \frac{2y}{3} \right)^2 \cdot \frac{2x}{y^2} \, \mathrm{d}x = \frac{1}{2} y^2 - \frac{8}{9} y^4 = \frac{y^2}{18}.
\end{align*}
\end{example}

Finally, we note that conditional expectation and conditional variance satisfy the following useful identity.

\begin{theorem}
\label{thm:3.5.6}
For random variables $X$ and $Y$,
\[
\var(X) = \var(\expc[X \mid Y]) + \expc[\var(X \mid Y)].
\]
\end{theorem}

\begin{proof}
See Section~\ref{sec:3.8} for the proof of this result.
\end{proof}

\subsection*{Summary of Section~\ref{sec:3.5}}

\begin{itemize}
\item If $X$ is discrete, then the conditional expectation of $X$ given an event $A$ is equal to $\expc[X \mid A] = \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid A)$.
\item If $X$ and $Y$ are discrete random variables, then $\expc[X \mid Y]$ is itself a random variable, with $\expc[X \mid Y]$ equal to $\expc[X \mid Y = y]$ when $Y = y$.
\item If $X$ and $Y$ are jointly absolutely continuous, then $\expc[X \mid Y]$ is itself a random variable, with $\expc[X \mid Y]$ equal to $\expc[X \mid Y = y]$ when $Y = y$, where $\expc[X \mid Y = y] = \int x \, f_{X|Y}(x \mid y) \, \mathrm{d}x$.
\item Conditional expectation is linear.
\item We always have that $\expc[g(Y) \expc[X \mid Y]] = \expc[g(Y) X]$, and $\expc[\expc[X \mid Y] \mid Y] = \expc[X \mid Y]$.
\item Conditional variance is given by $\var(X \mid Y) = \expc[X^2 \mid Y] - (\expc[X \mid Y])^2$.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:3.5.1}
Suppose $X$ and $Y$ are discrete, with
\[
p_{X,Y}(x, y) = \begin{cases}
1/5 & x = 2, y = 3 \\
1/5 & x = 3, y = 2 \\
1/5 & x = 3, y = 3 \\
1/5 & x = 2, y = 2 \\
1/5 & x = 3, y = 17 \\
0 & \text{otherwise}.
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $\expc[X \mid Y = 3]$.
\item Compute $\expc[Y \mid X = 3]$.
\item Compute $\expc[X \mid Y]$.
\item Compute $\expc[Y \mid X]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X \mid Y = 3) = \sum_x x \, \prb(X = x \mid Y = 3) = (2)((1/5)/(1/5 + 1/5)) + (3)((1/5)/(1/5 + 1/5)) = 5/2$.
    \item $\expc(Y \mid X = 3) = \sum_y y \, \prb(Y = y \mid X = 3) = (2)((1/5)/(1/5 + 1/5)) + (3)((1/5)/(1/5 + 1/5)) + (17)((1/5)/(1/5 + 1/5 + 1/5)) = 22/3$.
    \item $\expc(X \mid Y = 2) = \sum_x x \, \prb(X = x \mid Y = 2) = (2)((1/5)/(1/5 + 1/5)) + (3)((1/5)/(1/5 + 1/5)) = 5/2$. Also $\expc(X \mid Y = 17) = \sum_x x \, \prb(X = x \mid Y = 17) = (3)(1/1) = 3$. Hence,
    \[
        \expc(X \mid Y) = \begin{cases} 5/2 & Y = 2 \\ 5/2 & Y = 3 \\ 3 & Y = 17 \end{cases}
    \]
    \item $\expc(Y \mid X = 2) = \sum_y y \, \prb(Y = y \mid X = 2) = (2)((1/5)/(1/5 + 1/5)) + (3)((1/5)/(1/5 + 1/5)) = 5/2$. Hence,
    \[
        \expc(Y \mid X) = \begin{cases} 5/2 & X = 2 \\ 22/3 & X = 3 \end{cases}
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.2}
Suppose $X$ and $Y$ are jointly absolutely continuous, with
\[
f_{X,Y}(x, y) = \begin{cases}
9(x + y + x^5 + y^5)/16{,}000{,}900 & 0 \leqslant x \leqslant 4, \, 0 \leqslant y \leqslant 5 \\
0 & \text{otherwise}.
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $f_X(x)$.
\item Compute $f_Y(y)$.
\item Compute $\expc[X \mid Y]$.
\item Compute $\expc[Y \mid X]$.
\item Compute $\expc[\expc[X \mid Y]]$, and verify that it is equal to $\expc[X]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}y = \int_0^5 [9(xy + x^5 y^5)/16000900] \, \mathrm{d}y = (9x + 1875x^5)/1280072$.
    \item $f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, \mathrm{d}x = \int_0^4 [9(xy + x^5 y^5)/16000900] \, \mathrm{d}x = (18y + 1536y^5)/4000225$.
    \item For $0 \leqslant y \leqslant 5$,
    \begin{align*}
        \expc(X \mid Y = y) &= \int_{-\infty}^{\infty} x \, f_{X|Y}(x|y) \, \mathrm{d}x = \int_{-\infty}^{\infty} x (f_{X,Y}(x, y)/f_Y(y)) \, \mathrm{d}x \\
        &= \int_0^4 x (9(xy + x^5 y^5)/16000900)/((18y + 1536y^5)/4000225) \, \mathrm{d}x \\
        &= [8(7 + 768y^4)]/[7(3 + 256y^4)].
    \end{align*}
    Hence, $\expc(X \mid Y) = [8(7 + 768Y^4)]/[7(3 + 256Y^4)]$ for $0 \leqslant Y \leqslant 5$.
    \item For $0 \leqslant x \leqslant 4$,
    \begin{align*}
        \expc(Y \mid X = x) &= \int_{-\infty}^{\infty} y \, f_{Y|X}(y|x) \, \mathrm{d}y = \int_{-\infty}^{\infty} y (f_{X,Y}(x, y)/f_X(x)) \, \mathrm{d}y \\
        &= \int_0^5 y (9(xy + x^5 y^5)/16000900)/((9x + 1875x^5)/1280072) \, \mathrm{d}x \\
        &= [70 + 18750x^4]/[21 + 4375x^4].
    \end{align*}
    Hence, $\expc(Y \mid X) = [70 + 18750X^4]/[21 + 4375X^4]$.
    \item 
    \begin{align*}
        \expc(\expc(X \mid Y)) &= \expc([8(7 + 768Y^4)]/[7(3 + 256Y^4)]) \\
        &= \int_{-\infty}^{\infty} [8(7 + 768y^4)]/[7(3 + 256y^4)] f_Y(y) \, \mathrm{d}y \\
        &= \int_0^5 [8(7 + 768y^4)]/[7(3 + 256y^4)] [(18y + 1536y^5)/4000225] \, \mathrm{d}y \\
        &= 3840168/1120063 = 3.42852857
    \end{align*}
    On the other hand, $\expc(X) = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x = \int_0^4 x [(9x + 1875x^5)/1280072] \, \mathrm{d}x = 3840168/1120063 = \expc(\expc(X \mid Y))$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.3}
Suppose $X$ and $Y$ are discrete, with
\[
p_{X,Y}(x, y) = \begin{cases}
1/11 & x = 4, y = 2 \\
2/11 & x = 4, y = 3 \\
4/11 & x = 4, y = 7 \\
1/11 & x = 6, y = 2 \\
1/11 & x = 6, y = 3 \\
1/11 & x = 6, y = 7 \\
1/11 & x = 6, y = 13 \\
0 & \text{otherwise}.
\end{cases}
\]
\begin{enumerate}[(a)]
\item Compute $\expc[Y \mid X = 6]$.
\item Compute $\expc[Y \mid X = 4]$.
\item Compute $\expc[Y \mid X]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(Y \mid X = 6) = \sum_y y \, \prb(Y = y \mid X = 6) = \sum_y y \, \prb(Y = y, X = 6)/\prb(X = 6) = (2)((1/11)/(4/11)) + (3)((1/11)/(4/11)) + (7)((1/11)/(4/11)) + (13)((1/11)/(4/11)) = 25/4 = 6.24$.
    \item $\expc(Y \mid X = -4) = \sum_y y \, \prb(Y = y, X = -4)/\prb(X = -4) = (2)((1/11)/(7/11)) + (3)((2/11)/(7/11)) + (7)((4/11)/(7/11)) = 36/7 = 5.14$.
    \item $\expc(Y \mid X) = 25/4$ whenever $X = 6$ and $\expc(Y \mid X) = 36/7$ whenever $X = -4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.4}
Let $p_{X,Y}$ be as in the previous exercise.
\begin{enumerate}[(a)]
\item Compute $\expc[X \mid Y = 2]$.
\item Compute $\expc[X \mid Y = 3]$.
\item Compute $\expc[X \mid Y = 7]$.
\item Compute $\expc[X \mid Y = 13]$.
\item Compute $\expc[X \mid Y]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X \mid Y = 2) = \sum_x x \, \prb(X = x \mid Y = 2) = \sum_x x \, \prb(X = x, Y = 2)/\prb(Y = 2) = (-4)((1/11)/(2/11)) + (6)((1/11)/(2/11)) = 1$.
    \item $\expc(X \mid Y = 3) = \sum_x x \, \prb(X = x, Y = 3)/\prb(Y = 3) = (-4)((2/11)/(3/11)) + (6)((1/11)/(3/11)) = -2/3$.
    \item $\expc(X \mid Y = 7) = \sum_x x \, \prb(X = x, Y = 7)/\prb(Y = 7) = (-4)((4/11)/(5/11)) + (6)((1/11)/(5/11)) = -2$.
    \item $\expc(X \mid Y = 13) = \sum_x x \, \prb(X = x, Y = 13)/\prb(Y = 13) = (6)((1/11)/(1/11)) = 6$.
    \item $\expc(X \mid Y) = 1$ whenever $Y = 2$; $\expc(X \mid Y) = -2/3$ whenever $Y = 3$, $\expc(X \mid Y) = -2$ whenever $Y = 7$, and $\expc(X \mid Y) = 6$ whenever $Y = 13$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.5}
Suppose that a student must choose one of two summer job offers. If it is not necessary to take a summer course, then a job as a waiter will produce earnings (rounded to the nearest \$1000) with the following probability distribution.
\begin{center}
\begin{tabular}{cccc}
\$1000 & \$2000 & \$3000 & \$4000 \\
\hline
$0.1$ & $0.3$ & $0.4$ & $0.2$
\end{tabular}
\end{center}
If it is necessary to take a summer course, then a part-time job at a hotel will produce earnings (rounded to the nearest \$1000) with the following probability distribution.
\begin{center}
\begin{tabular}{cccc}
\$1000 & \$2000 & \$3000 & \$4000 \\
\hline
$0.3$ & $0.4$ & $0.2$ & $0.1$
\end{tabular}
\end{center}
If the probability that the student will have to take the summer course is $0.6$, then determine the student's expected summer earnings.
\end{exercise}

\begin{solution}
We have that $\expc(\text{earnings} \mid Y = \text{``takes course''}) = \$(1000(0.1) + 2000(0.3) + 3000(0.4) + 4000(0.2)) = \$2700$, while $\expc(X \mid Y = \text{``doesn't take course''}) = \$(1000(0.3) + 2000(0.4) + 3000(0.2) + 4000(0.1)) = \$2100$. Therefore, by TTE we have that $\expc(\text{earnings}) = \$(2700(0.4) + 2100(0.6)) = \$2340$.
\end{solution}

\begin{exercise}
\label{exer:3.5.6}
Suppose you roll two fair six-sided dice. Let $X$ be the number showing on the first die, and let $Z$ be the sum of the two numbers showing.
\begin{enumerate}[(a)]
\item Compute $\expc[X]$.
\item Compute $\expc[Z \mid X = 1]$.
\item Compute $\expc[Z \mid X = 6]$.
\item Compute $\expc[X \mid Z = 2]$.
\item Compute $\expc[X \mid Z = 4]$.
\item Compute $\expc[X \mid Z = 6]$.
\item Compute $\expc[X \mid Z = 7]$.
\item Compute $\expc[X \mid Z = 11]$.
\end{enumerate}
\end{exercise}

\begin{solution}
Let $Y$ be the number showing on the second die. Then, $X$ and $Y$ are independent and have the same distribution. Also $Z = X + Y$.
\begin{enumerate}[(a)]
    \item $\expc(X) = \sum_{x=1}^{6} x(1/6) = 7/2$ as well as $\expc(Y) = 7/2$.
    \item $\expc(Z \mid X = 1) = \expc(X + Y \mid X = 1) = 1 + \expc(Y \mid X = 1) = 1 + \expc(Y) = 1 + (7/2) = 9/2$. In the third equality, Theorem \ref{thm:2.8.4}(a) is used.
    \item $\expc(Z \mid X = 6) = \expc(X + Y \mid X = 6) = 6 + \expc(Y \mid X = 6) = 6 + \expc(Y) = 6 + (7/2) = 19/2$. For (d)--(h), note that $\prb(Z = z) = (6 - |7 - z|)/36$ for $z = 2, \ldots, 12$. The conditional probability is given by $\prb(X = x \mid Z = z) = \prb(X = x, Z = z)/\prb(Z = z) = \prb(X = x, Y = z - x)/\prb(Z = z) = 1/[36\prb(Z = z)] = 1/(6 - |7 - z|)$ for $x = \max(1, z - 6), \ldots, \min(6, z - 1)$ and $z = 2, \ldots, 12$. Hence,
    \[
        \expc(X \mid Z = z) = \sum_{x=\max(1, z-6)}^{\min(6, z-1)} \frac{x}{6 - |7 - z|} = \frac{(\max(1, z - 6) + \min(6, z - 1))(\min(6, z - 1) - \max(1, z - 6) + 1)}{2(6 - |7 - z|)} = \frac{z}{2}.
    \]
    \item The event $Z = 2$ implies $X = 1$ and $Y = 1$. Hence, $\expc(X \mid Z = 2) = 1$. It is the same to $z/2 = 2/2 = 1$.
    \item When $Z = 4$, $\prb(X = x \mid Z = 4) = 1/3$ for $x = 1, 2, 3$, otherwise $0$. Hence, $\expc(X \mid Z = 4) = (1 + 2 + 3)/3 = 2 = 4/2$.
    \item When $Z = 6$, $\prb(X = x \mid Z = 6) = 1/5$ for $x = 1, 2, 3, 4, 5$, otherwise $0$. Hence, $\expc(X \mid Z = 6) = (1 + \cdots + 5)/5 = 3 = 6/2$.
    \item When $Z = 7$, $\prb(X = x \mid Z = 7) = 1/6$ for $x = 1, \ldots, 6$, otherwise $0$. Hence, $\expc(X \mid Z = 7) = (1 + \cdots + 6)/6 = 7/2 = 7/2$.
    \item When $Z = 11$, $\prb(X = x \mid Z = 11) = 1/2$ for $x = 5, 6$, otherwise $0$. Hence, $\expc(X \mid Z = 11) = (5 + 6)/2 = 11/2 = 11/2$. Hence, the theoretic result and the real computation coincide.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.7}
Suppose you roll two fair six-sided dice. Let $Z$ be the sum of the two numbers showing, and let $W$ be the product of the two numbers showing.
\begin{enumerate}[(a)]
\item Compute $\expc[Z \mid W = 4]$.
\item Compute $\expc[W \mid Z = 4]$.
\end{enumerate}
\end{exercise}

\begin{solution}
Let $X$ and $Y$ be the numbers showing on the first and the second dice.
\begin{enumerate}[(a)]
    \item The event $(W = 4)$ occurs only when $(X = 1, Y = 4)$, $(X = 2, Y = 2)$, $(X = 4, Y = 1)$. Hence,
    \[
        \expc(Z \mid W = 4) = (1 + 4)(1/3) + (2 + 2)(1/3) + (4 + 1)(1/3) = 14/3.
    \]
    \item The event $(Z = 4)$ occurs only when $(X = 1, Y = 3)$, $(X = 2, Y = 2)$, $(X = 3, Y = 1)$. Hence,
    \[
        \expc(W \mid Z = 4) = (1 \cdot 3)(1/3) + (2 \cdot 2)(1/3) + (3 \cdot 1)(1/3) = 10/3.
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.8}
Suppose you roll one fair six-sided die and then flip as many coins as the number showing on the die. (For example, if the die shows $4$, then you flip four coins.) Let $X$ be the number showing on the die, and $Y$ be the number of heads obtained.
\begin{enumerate}[(a)]
\item Compute $\expc[Y \mid X = 5]$.
\item Compute $\expc[X \mid Y = 0]$.
\item Compute $\expc[X \mid Y = 2]$.
\end{enumerate}
\end{exercise}

\begin{solution}
The joint probability is given by $\prb(X = x, Y = y) = (1/6) \binom{x}{y} 2^{-x}$ for $x = 1, \ldots, 6$, $y = 0, \ldots, x$, otherwise $0$.
\begin{enumerate}[(a)]
    \item The marginal probability of $X$ is $\prb(X = x) = \sum_{y=0}^{x} (1/6) \binom{x}{y} 2^{-x} = 1/6$. Hence, $\prb(Y = y \mid X = x) = \prb(X = x, Y = y)/\prb(X = x) = \binom{x}{y} 2^{-x} \sim \text{Binomial}(x, 1/2)$. Thus, we get $\expc(Y \mid X = 5) = 5 \cdot (1/2) = 5/2$.
    \item $\prb(Y = 0) = \sum_{x=1}^{6} (1/6) \binom{x}{0} 2^{-x} = 21/128$. Hence,
    \[
        \expc(X \mid Y = 0) = \sum_{x=1}^{6} x \frac{2^{-x}/6}{21/128} = 40/21.
    \]
    \item $\prb(Y = 2) = \sum_{x=1}^{6} (1/6) \binom{x}{2} 2^{-x} = 33/128$. Hence,
    \[
        \expc(X \mid Y = 2) = \sum_{x=1}^{6} x \frac{\binom{x}{2} 2^{-x}/6}{33/128} = 130/33.
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.9}
Suppose you flip three fair coins. Let $X$ be the number of heads obtained, and let $Y = 1$ if the first coin shows heads, otherwise $Y = 0$.
\begin{enumerate}[(a)]
\item Compute $\expc[X \mid Y = 0]$.
\item Compute $\expc[X \mid Y = 1]$.
\item Compute $\expc[Y \mid X = 0]$.
\item Compute $\expc[Y \mid X = 1]$.
\item Compute $\expc[Y \mid X = 2]$.
\item Compute $\expc[Y \mid X = 3]$.
\item Compute $\expc[Y \mid X]$.
\item Verify directly that $\expc[\expc[Y \mid X]] = \expc[Y]$.
\end{enumerate}
\end{exercise}

\begin{solution}
Let $X_1, X_2, X_3$ be the random variables showing the status of $i$th coin. $X_1 = 1$ means that the first coin shows head. Then, $X = X_1 + X_2 + X_3$ and $Y = X_1$. It is easy to check $\expc(X_i) = 1/2$ for $i = 1, 2, 3$.
\begin{enumerate}[(a)]
    \item The event $Y = 0$ implies $X_1 = 0$. $\expc(X \mid Y = 0) = \expc(X_1 + X_2 + X_3 \mid X_1 = 0) = \expc(X_2 + X_3 \mid X_1 = 0) = (1/2) + (1/2) = 1$.
    \item The event $Y = 1$ implies $X_1 = 1$. $\expc(X \mid Y = 1) = \expc(X_1 + X_2 + X_3 \mid X_1 = 1) = 1 + \expc(X_2 + X_3 \mid X_1 = 1) = 1 + (1/2) + (1/2) = 2$.
    \item The event $(X = 0)$ implies $X_1 = X_2 = X_3 = 0$. Hence, $\expc(Y \mid X = 0) = \expc(X_1 \mid X = 0) = 0$.
    \item The event $(X = 1)$ implies only one $X_i = 1$ and the others are $0$. Hence $\prb(X_1 = 1 \mid X = 1) = 1/3$ and $\expc(Y \mid X = 1) = \expc(X_1 \mid X = 1) = (1)(1/3) + (0)(2/3) = 1/3$.
    \item The event $(X = 2)$ implies only one $X_i = 0$ and the others are $1$. Hence $\prb(X_1 = 1 \mid X = 1) = 2/3$ and $\expc(Y \mid X = 2) = \expc(X_1 \mid X = 1) = (1)(2/3) + (0)(1/3) = 2/3$.
    \item The event $(X = 3)$ implies $X_1 = X_2 = X_3 = 1$. Hence, $\expc(Y \mid X = 3) = \expc(X_1 \mid X = 3) = 1$.
    \item From (c)--(f), $\expc(Y \mid X) = X/3$ is obtained.
    \item It is known that $\expc(Y) = \expc(X_1) = 1/2$. From (g), $\expc(\expc(Y \mid X)) = \expc(X)/3 = (3/2)/3 = 1/2$. Hence, we get $\expc[\expc(Y \mid X)] = \expc(Y)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.10}
Suppose you flip one fair coin and roll one fair six-sided die. Let $X$ be the number showing on the die, and let $Y = 1$ if the coin is heads with $Y = 0$ if the coin is tails. Let $Z = XY$.
\begin{enumerate}[(a)]
\item Compute $\expc[Z]$.
\item Compute $\expc[Z \mid X = 4]$.
\item Compute $\expc[Y \mid X = 4]$.
\item Compute $\expc[Y \mid Z = 4]$.
\item Compute $\expc[X \mid Z = 4]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item By Theorem \ref{thm:3.2.3}, $\expc(Z) = \expc(XY) = \expc(X)\expc(Y) = (7/2)(1/2) = 7/4$.
  \item By Theorem \ref{thm:3.5.4}, $\expc(Z \mid X = 4) = \expc(XY \mid X = 4) = 4\expc(Y \mid X = 4) = 4\expc(Y) = 4(1/2) = 2$.
  \item By Theorem \ref{thm:2.8.4}(a), $\expc(Y \mid X = 4) = \expc(Y) = 1/2$.
  \item The event $(Z = 4)$ occurs only when $X = 4$ and $Y = 1$. Hence, $\expc(Y \mid Z = 4) = 1$.
  \item The event $(Z = 4)$ occurs only when $X = 4$ and $Y = 1$. Hence, $\expc(X \mid Z = 4) = 4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.11}
Suppose $X$ and $Y$ are jointly absolutely continuous, with joint density function $f_{X,Y}(x, y) = (6/19)(x^2 + y^3)$ for $0 \leqslant x \leqslant 2$ and $0 \leqslant y \leqslant 1$, otherwise $f_{X,Y}(x, y) = 0$.
\begin{enumerate}[(a)]
\item Compute $\expc[X]$.
\item Compute $\expc[Y]$.
\item Compute $\expc[X \mid Y]$.
\item Compute $\expc[Y \mid X]$.
\item Verify directly that $\expc[\expc[X \mid Y]] = \expc[X]$.
\item Verify directly that $\expc[\expc[Y \mid X]] = \expc[Y]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The marginal density of $X$ is
    \[
        f_X(x) = \int_{\mathbb{R}^1} f_{X,Y}(x, y) \, \mathrm{d}y = \int_0^1 \frac{6}{19}(x^2 + y^3) \, \mathrm{d}y = \frac{6}{19}\left(x^2 + \frac{1}{4}\right)
    \]
    for $0 < x < 2$, otherwise $f_X(x) = 0$. Hence,
    \[
        \expc(X) = \int_0^2 x \cdot \frac{6}{19}\left(x^2 + \frac{1}{4}\right) \mathrm{d}x = \frac{6}{19}\left(\frac{x^4}{4} + \frac{x^2}{8}\right)\Big|_{x=0}^{x=2} = \frac{27}{19}.
    \]
    \item The marginal density of $Y$ is
    \[
        f_Y(y) = \int_{\mathbb{R}^1} f_{X,Y}(x, y) \, \mathrm{d}x = \int_0^2 \frac{6}{19}(x^2 + y^3) \, \mathrm{d}x = \frac{4}{19}(4 + 3y^3).
    \]
    for $0 < y < 1$, otherwise $f_Y(y) = 0$. Hence,
    \[
        \expc(Y) = \int_0^1 y \cdot \frac{4}{19}(4 + 3y^3) \, \mathrm{d}y = \frac{4}{19}\left(2y^2 + \frac{3y^5}{5}\right)\Big|_{y=0}^{y=1} = \frac{52}{95}.
    \]
    \item The conditional density $f_{X|Y}(x|y) = f_{X,Y}(x, y)/f_Y(y) = 3(x^2 + y^3)/(8 + 6y^3)$. Hence,
    \[
        \expc(X \mid Y) = \int_0^2 \frac{x \cdot 3(x^2 + y^3)}{2(4 + 3y^3)} \, \mathrm{d}x = \frac{3(x^4/4 + y^3 x^2/2)}{2(4 + 3y^3)}\Big|_{x=0}^{x=2} = \frac{3(2 + y^3)}{4 + 3y^3}.
    \]
    \item The conditional density $f_{Y|X}(y|x) = f_{X,Y}(x, y)/f_X(x) = (x^2 + y^3)/(x^2 + 1/4)$. Hence,
    \[
        \expc(Y \mid X) = \int_0^1 \frac{y(x^2 + y^3)}{x^2 + 1/4} \, \mathrm{d}x = \frac{x^2 y^2/2 + y^5/5}{x^2 + 1/4}\Big|_{y=0}^{y=1} = \frac{x^2/2 + 1/5}{x^2 + 1/4}.
    \]
    \item The expectation of $\expc(X \mid Y)$ is
    \begin{align*}
        \expc[\expc(X \mid Y)] &= \int_0^1 \frac{3(2 + y^3)}{4 + 3y^3} \cdot \frac{4}{19}(4 + 3y^3) \, \mathrm{d}y = \int_0^1 \frac{12}{19}(2 + y^3) \, \mathrm{d}y \\
        &= \frac{12}{19}\left(2y + \frac{y^4}{4}\right)\Big|_{y=0}^{y=1} = \frac{27}{19}.
    \end{align*}
    Hence, we get $\expc[\expc(X \mid Y)] = \expc(X)$.
    \item The expectation of $\expc(Y \mid X)$ is
    \begin{align*}
        \expc[\expc(Y \mid X)] &= \int_0^2 \frac{x^2/2 + 1/5}{x^2 + 1/4} \cdot \frac{6}{19}\left(x^2 + \frac{1}{4}\right) \mathrm{d}x = \frac{6}{19}\left(\frac{x^2}{2} + \frac{1}{5}\right) \mathrm{d}x \\
        &= \frac{6}{19}\left(\frac{x^3}{6} + \frac{x}{5}\right)\Big|_{x=0}^{x=2} = \frac{52}{95}.
    \end{align*}
    Hence, we get $\expc[\expc(Y \mid X)] = \expc(Y)$.
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:3.5.12}
Suppose there are two urns. Urn I contains 100 chips: 30 are labelled $1$, 40 are labelled $2$, and 30 are labelled $3$. Urn 2 contains 100 chips: 20 are labelled $1$, 50 are labelled $2$, and 30 are labelled $3$. A coin is tossed and if a head is observed, then a chip is randomly drawn from urn 1, otherwise a chip is randomly drawn from urn 2. The value $Y$ on the chip is recorded. If an occurrence of a head on the coin is denoted by $X = 1$, a tail by $X = 0$ and $X \sim \text{Bernoulli}(3/4)$, then determine $\expc[X \mid Y]$, $\expc[Y \mid X]$, $\expc[Y]$ and $\expc[X]$.
\end{exercise}

\begin{solution}
We have that $\expc(Y \mid X)$ is given by $\expc(Y \mid X = 1) = 1(0.3) + 2(0.4) + 3(0.3) = 2.0$, $\expc(Y \mid X = 0) = 1(0.2) + 2(0.5) + 3(0.3) = 2.1$.

So $\expc(Y) = \expc(\expc(Y \mid X)) = 2(0.75) + 2.1(0.25) = 2.025$ and, of course, $\expc(X) = 0.75$.

The conditional distributions of $X$ given $Y$ are (using Bayes' theorem):
$X \mid Y = 1 \sim \text{Bernoulli}(0.3(0.75)/(0.3(0.75) + 0.2(0.25))) = \text{Bernoulli}(0.81818)$,
$X \mid Y = 2 \sim \text{Bernoulli}(0.4(0.75)/(0.4(0.75) + 0.5(0.25))) = \text{Bernoulli}(0.70588)$, and
$X \mid Y = 3 \sim \text{Bernoulli}(0.3(0.75)/(0.3(0.75) + 0.3(0.25))) = \text{Bernoulli}(0.75)$.

Therefore, $\expc(X \mid Y)$ is given by $\expc(X \mid Y = 1) = 0.81818$, $\expc(X \mid Y = 2) = 0.70588$ and $\expc(X \mid Y = 3) = 0.75$.
\end{solution}

\begin{exercise}
\label{exer:3.5.13}
Suppose that five coins are each tossed until the first head is obtained on each coin and where each coin has probability $\theta$ of producing a head. If you are told that the total number of tails observed is $Y = 10$, then determine the expected number of tails observed on the first coin.
\end{exercise}

\begin{solution}
We have that $Y = X_1 + \cdots + X_5 \sim \text{Negative Binomial}(5, \theta)$, and by symmetry, each of the conditional distributions, $X_i$ given $Y = 10$, are the same. Then $\expc(X_1 \mid Y = 10) = \expc(Y - X_2 - \cdots - X_5 \mid Y = 10) = 10 - 4\expc(X_1 \mid Y = 10)$, so $5\expc(X_1 \mid Y = 10) = 10$ and $\expc(X_1 \mid Y = 10) = 2$. Note that this does not depend on $\theta$.
\end{solution}

\begin{exercise}
\label{exer:3.5.14}
(Simpson's paradox) Suppose that the conditional distributions of $Y$ given $X$ are shown in the following table. For example, $p_{Y|X}(1 \mid i)$ could correspond to the probability that a randomly selected heart patient at hospital $i$ has a successful treatment.
\begin{center}
\begin{tabular}{cc}
$p_{Y|X}(0 \mid 1)$ & $p_{Y|X}(1 \mid 1)$ \\
\hline
$0.030$ & $0.970$
\end{tabular}
\qquad
\begin{tabular}{cc}
$p_{Y|X}(0 \mid 2)$ & $p_{Y|X}(1 \mid 2)$ \\
\hline
$0.020$ & $0.980$
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Compute $\expc[Y \mid X]$.
\item Now suppose that patients are additionally classified as being seriously ill ($Z = 1$), or not seriously ill ($Z = 0$). The conditional distributions of $Y$ given $X, Z$, are shown in the following tables. Compute $\expc[Y \mid X, Z]$.
\begin{center}
\begin{tabular}{cc}
$p_{Y|X,Z}(0 \mid 1, 0)$ & $p_{Y|X,Z}(1 \mid 1, 0)$ \\
\hline
$0.010$ & $0.990$
\end{tabular}
\qquad
\begin{tabular}{cc}
$p_{Y|X,Z}(0 \mid 2, 0)$ & $p_{Y|X,Z}(1 \mid 2, 0)$ \\
\hline
$0.013$ & $0.987$
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{cc}
$p_{Y|X,Z}(0 \mid 1, 1)$ & $p_{Y|X,Z}(1 \mid 1, 1)$ \\
\hline
$0.038$ & $0.962$
\end{tabular}
\qquad
\begin{tabular}{cc}
$p_{Y|X,Z}(0 \mid 2, 1)$ & $p_{Y|X,Z}(1 \mid 2, 1)$ \\
\hline
$0.040$ & $0.960$
\end{tabular}
\end{center}
\item Explain why the conditional distributions in part (a) indicate that hospital 2 is the better hospital for a patient who needs to undergo this treatment, but all the conditional distributions in part (b) indicate that hospital 1 is the better hospital. This phenomenon is known as Simpson's paradox.
\item Prove that, in general, $p_{Y|X}(y \mid x) = \sum_z p_{Y|X,Z}(y \mid x, z) \, p_{Z|X}(z \mid x)$ and $\expc[Y \mid X] = \expc[\expc[Y \mid X, Z] \mid X]$.
\item If the conditional distributions $p_{Z|X}(\cdot \mid x)$ corresponding to the example discussed in parts (a) through (c) are given in the following table, verify the result in part (d) numerically and explain how this resolves Simpson's paradox.
\begin{center}
\begin{tabular}{cc}
$p_{Z|X}(0 \mid 1)$ & $p_{Z|X}(1 \mid 1)$ \\
\hline
$0.286$ & $0.714$
\end{tabular}
\qquad
\begin{tabular}{cc}
$p_{Z|X}(0 \mid 2)$ & $p_{Z|X}(1 \mid 2)$ \\
\hline
$0.750$ & $0.250$
\end{tabular}
\end{center}
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(Y \mid X)$ is given by $\expc(Y \mid X = 1) = 0.97$ and $\expc(Y \mid X = 2) = 0.98$.
    \item $\expc(Y \mid X, Z)$ is given by $\expc(Y \mid X = 1, Z = 0) = 0.99$, $\expc(Y \mid X = 2, Z = 0) = 0.987$, $\expc(Y \mid X = 1, Z = 1) = 0.962$, and $\expc(Y \mid X = 2, Z = 1) = 0.960$.
    \item The conditional expectations all correspond to the conditional probabilities of having a successful treatment, so the higher this probability is the better. The conditional expectations $\expc(Y \mid X)$ indicate that hospital 2 is better than hospital 1, while the conditional expectations $\expc(Y \mid X, Z)$ uniformly indicate that hospital 1 is better than hospital 2.
    \item We have that
    \begin{align*}
        \sum_z p_{Y|X,Z}(y \mid x, z) p_{Z|X}(z \mid x) &= \sum_z \frac{p_{X,Y,Z}(x, y, z)}{p_{X,Z}(x, z)} \frac{p_{X,Z}(x, z)}{p_X(x)} \\
        &= \sum_z \frac{p_{X,Y,Z}(x, y, z)}{p_X(x)} = \frac{p_{X,Y}(x, y)}{p_X(x)} = p_{Y|X}(y \mid x)
    \end{align*}
    and
    \begin{align*}
        \expc(\expc(Y \mid X, Z) \mid X) &= \sum_z \expc(Y \mid X, Z) p_Z(z) = \sum_z \sum_y y \, p_{Y|X,Z}(y \mid x, z) p_{Z|X}(z \mid x) \\
        &= \sum_z \sum_y y \frac{p_{X,Y,Z}(x, y, z)}{p_X(x)} = \sum_y \sum_z y \frac{p_{X,Y,Z}(x, y, z)}{p_X(x)} \\
        &= \sum_y y \frac{p_{X,Y}(x, y)}{p_X(x)} = \sum_y y \, p_{Y|X}(y \mid x) = \expc(Y \mid X).
    \end{align*}
    \item We have that $\expc(Y \mid X = 1, Z = 0) = 0.99$, $\expc(Y \mid X = 2, Z = 0) = 0.987$, $\expc(Y \mid X = 1, Z = 1) = 0.962$, and $\expc(Y \mid X = 2, Z = 1) = 0.960$.
    \begin{align*}
        \expc(Y \mid X = 1) &= \expc(Y \mid X = 1, Z = 0) p_{Z|X}(0 \mid 1) + \expc(Y \mid X = 1, Z = 1) p_{Z|X}(1 \mid 1) \\
        &= 0.99(0.286) + 0.962(0.714) = 0.97 \\
        \expc(Y \mid X = 2) &= \expc(Y \mid X = 2, Z = 0) p_{Z|X}(0 \mid 2) + \expc(Y \mid X = 2, Z = 1) p_{Z|X}(1 \mid 2) \\
        &= 0.987(0.75) + 0.960(0.25) = 0.98,
    \end{align*}
    so the result is verified numerically.
    
    The paradox is resolved by noting that the conditional distributions of $Z$ given $X$ indicate that hospital 1 has a far greater proportion of seriously ill patients than does hospital 2.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.15}
Present an example of a random variable $X$, and an event $A$ with $\prb(A) > 0$, such that $\var(X \mid A) > \var(X)$. (Hint: Suppose $S = \{1, 2, 3\}$ with $X(s) = s$, and $A = \{1, 3\}$.)
\end{exercise}

\begin{solution}
Let $S = \{1, 2, 3\}$, $\prb(s) = 1/3$ and $X(s) = s$ for $s \in S$, and $A = \{1, 3\}$. Then $\prb(A) > 0$. Also, $\expc(X) = (1)(1/3) + (2)(1/3) + (3)(1/3) = 2$, and $\expc(X^2) = (1)^2(1/3) + (2)^2(1/3) + (3)^2(1/3) = 14/3$, so $\var(X) = (14/3) - (2)^2 = 2/3$. On the other hand, $\expc(X \mid A) = (1)(1/2) + (3)(1/2) = 2$, and $\expc(X^2 \mid A) = (1)^2(1/2) + (3)^2(1/2) = 5$, so $\var(X \mid A) = 5 - (2)^2 = 1 > 2/3$.
\end{solution}

\begin{exercise}
\label{exer:3.5.16}
Suppose that $X$, given $Y = y$, is distributed $\text{Gamma}(y, \lambda)$ and that the marginal distribution of $Y$ is given by $1/Y \sim \text{Exponential}(\mu)$. Determine $\expc[X]$.
\end{exercise}

\begin{solution}
$\expc(X) = \expc(\expc(X \mid Y)) = \expc(\alpha/Y) = \alpha \expc(1/Y) = \alpha/\lambda$.
\end{solution}

\begin{exercise}
\label{exer:3.5.17}
Suppose that $(X, Y) \sim \text{Bivariate Normal}(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$. Use \eqref{eq:2.7.1} (when given $Y = y$) and its analog (when given $X = x$) to determine $\expc[X \mid Y]$, $\expc[Y \mid X]$, $\var(X \mid Y)$ and $\var(Y \mid X)$.
\end{exercise}

\begin{solution}
Using the analog of (2.7.1) we have that $X = \mu_1 + \sigma_1 Z_1$, $Y = \mu_2 + \sigma_2(\rho Z_1 + \sqrt{1 - \rho^2} Z_2)$, where $Z_1, Z_2$ are i.i.d.\ $N(0, 1)$. Then $X = x$ is equivalent to $Z_1 = (x - \mu_1)/\sigma_1$ and $Z_2$ is independent of $Z_1$ (and so of $X$), so
\begin{align*}
    \expc(Y \mid X = x) &= \expc\left(\mu_2 + \sigma_2\left(\rho Z_1 + \sqrt{1 - \rho^2} Z_2\right) \mid X = x\right) \\
    &= \expc\left(\mu_2 + \sigma_2\left(\rho\left(\frac{X - \mu_1}{\sigma_1}\right) + \sqrt{1 - \rho^2} Z_2\right) \mid X = x\right) \\
    &= \mu_2 + \sigma_2\left(\rho\left(\frac{x - \mu_1}{\sigma_1}\right) + \sqrt{1 - \rho^2} \expc(Z_2)\right) = \mu_2 + \rho\sigma_2\left(\frac{x - \mu_1}{\sigma_1}\right)
\end{align*}
and
\begin{align*}
    \var(Y \mid X = x) &= \var(\sigma_2 \sqrt{1 - \rho^2} Z_2 \mid X = x) = \sigma_2^2(1 - \rho^2) \var(Z_2 \mid X = x) \\
    &= \sigma_2^2(1 - \rho^2) \var(Z_2) = \sigma_2^2(1 - \rho^2).
\end{align*}
Using (2.7.1) we have that $\expc(X \mid Y = y) = \mu_1 + \rho\sigma_1(y - \mu_2)/\sigma_2$ and $\var(X \mid Y = y) = \sigma_1^2(1 - \rho^2)$.
\end{solution}

\begin{exercise}
\label{exer:3.5.18}
Suppose that $(X_1, X_2, X_3) \sim \text{Multinomial}(n; \theta_1, \theta_2, \theta_3)$. Determine $\expc[X_1 \mid X_2]$ and $\var(X_1 \mid X_2)$. (Hint: Show that $X_1$ given $X_2 = x_2$ has a binomial distribution.)
\end{exercise}

\begin{solution}
We have that $X_2 \sim \text{Binomial}(n, \theta_2)$, so the conditional probability function of $X_1$ given $X_2 = x_2$ is given by
\begin{align*}
    p_{X_1|X_2}(x_1 \mid x_2) &= \frac{\binom{n}{x_1 \; x_2 \; n - x_1 - x_2} \theta_1^{x_1} \theta_2^{x_2} (1 - \theta_1 - \theta_2)^{n - x_1 - x_2}}{\binom{n}{x_2} \theta_2^{x_2} (1 - \theta_2)^{n - x_2}} \\
    &= \frac{(n - x_2)! \theta_1^{x_1} (1 - \theta_1 - \theta_2)^{n - x_1 - x_2}}{x_1!(n - x_1 - x_2)!(1 - \theta_2)^{n - x_2}} \\
    &= \binom{n - x_2}{x_2} \left(\frac{\theta_1}{1 - \theta_2}\right)^{x_1} \left(1 - \frac{\theta_1}{1 - \theta_2}\right)^{n - x_1 - x_2}
\end{align*}
and this is the $\text{Binomial}(n - x_2, \theta_1/(1 - \theta_2))$ probability function. Therefore, $\expc(X_1 \mid X_2 = x_2) = (n - x_2)\theta_1/(1 - \theta_2)$ and
\[
    \var(X_1 \mid X_2 = x_2) = (n - x_2) \frac{\theta_1}{1 - \theta_2} \left(1 - \frac{\theta_1}{1 - \theta_2}\right).
\]
\end{solution}

\begin{exercise}
\label{exer:3.5.19}
Suppose that $(X_1, X_2) \sim \text{Dirichlet}(\alpha_1, \alpha_2, \alpha_3)$. Determine $\expc[X_1 \mid X_2]$ and $\var(X_1 \mid X_2)$. (Hint: First show that $X_1/(1 - x_2)$ given $X_2 = x_2$ has a beta distribution and then use Problem~\ref{exer:3.3.24}.)
\end{exercise}

\begin{solution}
We have that the conditional density of $X_1$ given $X_2 = x_2$ is given by (using Problem \ref{exer:2.7.17})
\begin{align*}
    f_{X_1|X_2}(x_1 \mid x_2) &= \frac{\frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)} x_1^{\alpha_1-1} x_2^{\alpha_2-1} (1 - x_1 - x_2)^{\alpha_3-1}}{\frac{\Gamma(\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma(\alpha_2)\Gamma(\alpha_1 + \alpha_3)} x_2^{\alpha_2-1} (1 - x_2)^{\alpha_1 + \alpha_3 - 1}} \\
    &= \frac{\Gamma(\alpha_1 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_3)} \frac{x_1^{\alpha_1-1} (1 - x_1 - x_2)^{\alpha_3-1}}{(1 - x_2)^{\alpha_1 + \alpha_3 - 1}} \\
    &= \frac{\Gamma(\alpha_1 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_3)} \left(\frac{x_1}{1 - x_2}\right)^{\alpha_1-1} \left(1 - \frac{x_1}{1 - x_2}\right)^{\alpha_3-1} \frac{1}{1 - x_2}
\end{align*}
and we see that $X_1/(1 - x_2)$ given $X_2 = x_2$ is distributed $\text{Beta}(\alpha_1, \alpha_3)$, so (Problem \ref{exer:3.2.22}) $\expc(X_1 \mid X_2 = x_2) = (1 - x_2)\alpha_1/(\alpha_1 + \alpha_3)$, $\var(X_1 \mid X_2 = x_2) = (1 - x_2)^2 \alpha_1 \alpha_3/(\alpha_1 + \alpha_3)^2(\alpha_1 + \alpha_3 + 1)$.
\end{solution}

\begin{exercise}
\label{exer:3.5.20}
Let $f_{X,Y}$ be as in Exercise~\ref{exer:3.5.2}.
\begin{enumerate}[(a)]
\item Compute $\var(X)$.
\item Compute $\var(\expc[X \mid Y])$.
\item Compute $\var(X \mid Y)$.
\item Verify that $\var(X) = \var(\expc[X \mid Y]) + \expc[\var(X \mid Y)]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(X^2) = \int_{-\infty}^{\infty} x^2 f_X(x) \, \mathrm{d}x = \int_0^4 x^2 [(9x + 1875x^5)/1280072] \, \mathrm{d}x = 1920072/160009$. Hence, $\var(X) = \expc(X^2) - \expc(X)^2 = (1920072/160009) - (3840168/1120063)^2 = 307320963528/1254541123969 = 0.244967$.
    \item 
    \begin{align*}
        \expc(\expc(X \mid Y)^2) &= \expc(([8(7 + 768Y^4)]/[7(3 + 256Y^4)])^2) \\
        &= \int_{-\infty}^{\infty} (8(7 + 768y^4))^2/(7(3 + 256y^4))^2 f_Y(y) \, \mathrm{d}y \\
        &= \int_0^5 (8(7 + 768y^4))^2/(7(3 + 256y^4))^2 ((18y + 1536y^5)/4000225) \, \mathrm{d}y \\
        &= 11.754808401
    \end{align*}
    Hence,
    \[
        \var(\expc(X \mid Y)) = \expc(\expc(X \mid Y)^2) - \expc(\expc(X \mid Y))^2 = \expc(\expc(X \mid Y)^2) - \expc(X)^2 = 11.754808401 - (3.42852857)^2 = 0.0000002196
    \]
    which is extremely small.
    \item For $0 \leqslant y \leqslant 5$,
    \begin{align*}
        \expc(X^2 \mid Y = y) &= \int_{-\infty}^{\infty} x^2 f_{X|Y}(x|y) \, \mathrm{d}x = \int_{-\infty}^{\infty} x^2 (f_{X,Y}(x, y)/f_Y(y)) \, \mathrm{d}x \\
        &= \int_0^4 x^2 (9(xy + x^5 y^5)/16000900)/((18y + 1536y^5)/4000225) \, \mathrm{d}x \\
        &= (24 + 3072y^4)/(3 + 256y^4).
    \end{align*}
    Hence, $\expc(X^2 \mid Y) = (24 + 3072Y^4)/(3 + 256Y^4)$, for $0 \leqslant Y \leqslant 5$. Then
    \begin{align*}
        \var(X \mid Y) &= \expc(X^2 \mid Y) - \expc(X \mid Y)^2 \\
        &= [(24 + 3072Y^4)/(3 + 256Y^4)] - [(8(7 + 768Y^4))/(7(3 + 256Y^4))]^2 \\
        &= (8/49)(49 + 8064Y^4 + 98304Y^8)/(3 + 256Y^4)^2.
    \end{align*}
    \item From part (c) we have that
    \begin{align*}
        \expc(\var(X \mid Y)) &= \int_{-\infty}^{\infty} [(8/49)(49 + 8064y^4 + 98304y^8)/(3 + 256y^4)^2] f_Y(y) \, \mathrm{d}y \\
        &= \int_0^5 [(8/49)(49 + 8064y^4 + 98304y^8)/(3 + 256y^4)^2] [(18y + 1536y^5)/4000225] \, \mathrm{d}y \\
        &= 0.244967.
    \end{align*}
    Then $\var(\expc(X \mid Y)) + \expc(\var(X \mid Y)) = 0.0000002196 + 0.244967 = 0.244967 = \var(X)$, as it should.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.5.21}
Suppose we have three discrete random variables $X$, $Y$, and $Z$. We say that $X$ and $Y$ are \emph{conditionally independent, given $Z$} if
\[
p_{X,Y|Z}(x, y \mid z) = p_{X|Z}(x \mid z) \, p_{Y|Z}(y \mid z)
\]
for every $x$, $y$, and $z$ such that $\prb(Z = z) > 0$. Prove that when $X$ and $Y$ are conditionally independent, given $Z$, then
\[
\expc[g(X) h(Y) \mid Z] = \expc[g(X) \mid Z] \, \expc[h(Y) \mid Z].
\]
\end{exercise}

\begin{solution}
We have that $\expc(g(X)h(Y) \mid Z) = \sum_{x,y} g(x)h(y) p_{X,Y|Z}(x, y \mid z) = \sum_{x,y} g(x)h(y) p_{X|Z}(x \mid z) p_{Y|Z}(y \mid z) = \sum_x g(x) p_{X|Z}(x \mid z) \sum_y h(y) p_{Y|Z}(y \mid z) = \expc(g(X) \mid Z) \expc(h(Y) \mid Z)$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inequalities}
\label{sec:3.6}

Expectation and variance are closely related to the underlying distributions of random variables. This relationship allows us to prove certain inequalities that are often very useful. We begin with a classic result, Markov's inequality, which is very simple but also very useful and powerful.

\begin{theorem}[Markov's inequality]
\label{thm:3.6.1}
If $X$ is a nonnegative random variable, then for all $a > 0$,
\[
\prb(X \geqslant a) \leqslant \frac{\expc[X]}{a}.
\]
That is, the probability that $X$ exceeds any given value $a$ is no more than the mean of $X$ divided by $a$.
\end{theorem}

\begin{proof}
Define a new random variable $Z$ by
\[
Z = \begin{cases}
a & X \geqslant a \\
0 & X < a.
\end{cases}
\]
Then clearly $Z \leqslant X$, so that $\expc[Z] \leqslant \expc[X]$ by monotonicity. On the other hand,
\[
\expc[Z] = a \, \prb(Z = a) + 0 \cdot \prb(Z = 0) = a \, \prb(Z = a) = a \, \prb(X \geqslant a).
\]
So, $\expc[X] \geqslant \expc[Z] = a \, \prb(X \geqslant a)$. Rearranging, $\prb(X \geqslant a) \leqslant \expc[X]/a$, as claimed.
\end{proof}

Intuitively, Markov's inequality says that if the expected value of $X$ is small, then it is unlikely that $X$ will be too large. We now consider some applications of Theorem~\ref{thm:3.6.1}.

\begin{example}
\label{ex:3.6.1}
Suppose $\prb(X = 3) = 1/2$, $\prb(X = 4) = 1/3$, and $\prb(X = 7) = 1/6$. Then $\expc[X] = 3 \cdot (1/2) + 4 \cdot (1/3) + 7 \cdot (1/6) = 4$. Hence, setting $a = 6$, Markov's inequality says that $\prb(X \geqslant 6) \leqslant 4/6 = 2/3$. In fact, $\prb(X \geqslant 6) = 1/6 \leqslant 2/3$.
\end{example}

\begin{example}
\label{ex:3.6.2}
Suppose $\prb(X = 2) = \prb(X = 8) = 1/2$. Then $\expc[X] = 2 \cdot (1/2) + 8 \cdot (1/2) = 5$. Hence, setting $a = 8$, Markov's inequality says that $\prb(X \geqslant 8) \leqslant 5/8$. In fact, $\prb(X \geqslant 8) = 1/2 \leqslant 5/8$.
\end{example}

\begin{example}
\label{ex:3.6.3}
Suppose $\prb(X = 0) = \prb(X = 2) = 1/2$. Then $\expc[X] = 0 \cdot (1/2) + 2 \cdot (1/2) = 1$. Hence, setting $a = 2$, Markov's inequality says that $\prb(X \geqslant 2) \leqslant 1/2$. In fact, $\prb(X \geqslant 2) = 1/2$, so Markov's inequality is an equality in this case.
\end{example}

Markov's inequality is also used to prove Chebychev's inequality, perhaps the most important inequality in all of probability theory.

\begin{theorem}[Chebychev's inequality]
\label{thm:3.6.2}
Let $Y$ be an arbitrary random variable, with finite mean $\mu_Y$. Then for all $a > 0$,
\[
\prb(|Y - \mu_Y| \geqslant a) \leqslant \frac{\var(Y)}{a^2}.
\]
\end{theorem}

\begin{proof}
Set $X = (Y - \mu_Y)^2$. Then $X$ is a nonnegative random variable. Thus, using Theorem~\ref{thm:3.6.1}, we have $\prb(|Y - \mu_Y| \geqslant a) = \prb(X \geqslant a^2) \leqslant \expc[X]/a^2 = \var(Y)/a^2$, and this establishes the result.
\end{proof}

Intuitively, Chebychev's inequality says that if the variance of $Y$ is small, then it is unlikely that $Y$ will be too far from its mean value $\mu_Y$. We now consider some examples.

\begin{example}
\label{ex:3.6.4}
Suppose again that $\prb(X = 3) = 1/2$, $\prb(X = 4) = 1/3$, and $\prb(X = 7) = 1/6$. Then $\expc[X] = 4$ as above. Also, $\expc[X^2] = 9 \cdot (1/2) + 16 \cdot (1/3) + 49 \cdot (1/6) = 18$, so that $\var(X) = 18 - 4^2 = 2$. Hence, setting $a = 1$, Chebychev's inequality says that $\prb(|X - 4| \geqslant 1) \leqslant 2/1^2 = 2$, which tells us nothing because we always have $\prb(|X - 4| \geqslant 1) \leqslant 1$. On the other hand, setting $a = 3$, we get $\prb(|X - 4| \geqslant 3) \leqslant 2/3^2 = 2/9$, which is true because in fact $\prb(|X - 4| \geqslant 3) = \prb(X = 7) = 1/6 \leqslant 2/9$.
\end{example}

\begin{example}
\label{ex:3.6.5}
Let $X \sim \text{Exponential}(3)$, and let $a = 5$. Then $\expc[X] = 1/3$ and $\var(X) = 1/9$. Hence, by Chebychev's inequality with $a = 1/2$, $\prb(|X - 1/3| \geqslant 1/2) \leqslant (1/9)/(1/2)^2 = 4/9$. On the other hand, because $X \geqslant 0$, $\prb(|X - 1/3| \geqslant 1/2) \leqslant \prb(X \geqslant 5/6)$, and by Markov's inequality, $\prb(X \geqslant 5/6) \leqslant (1/3)/(5/6) = 2/5$. Because $2/5 < 4/9$, we actually get a better bound from Markov's inequality than from Chebychev's inequality in this case.
\end{example}

\begin{example}
\label{ex:3.6.6}
Let $Z \sim N(0, 1)$, and $a = 5$. Then by Chebychev's inequality, $\prb(|Z| \geqslant 5) \leqslant 1/5$.
\end{example}

\begin{example}
\label{ex:3.6.7}
Let $X$ be a random variable having very small variance. Then Chebychev's inequality says that $\prb(|X - \mu_X| \geqslant a)$ is small whenever $a$ is not too small. In other words, usually $|X - \mu_X|$ is very small, i.e., $X \approx \mu_X$. This makes sense, because if the variance of $X$ is very small, then usually $X$ is very close to its mean value $\mu_X$.
\end{example}

Inequalities are also useful for covariances, as follows.

\begin{theorem}[Cauchy--Schwartz inequality]
\label{thm:3.6.3}
Let $X$ and $Y$ be arbitrary random variables, each having finite, nonzero variance. Then
\[
|\cov(X, Y)| \leqslant \sqrt{\var(X) \var(Y)}.
\]
Furthermore, if $\var(Y) \neq 0$, then equality is attained if and only if $X - \mu_X = \beta(Y - \mu_Y)$ where $\beta = \cov(X, Y)/\var(Y)$.
\end{theorem}

\begin{proof}
See Section~\ref{sec:3.8} for the proof.
\end{proof}

The Cauchy--Schwartz inequality says that if the variance of $X$ or $Y$ is small, then the covariance of $X$ and $Y$ must also be small.

\begin{example}
\label{ex:3.6.8}
Suppose $X = C$ is a constant. Then $\var(X) = 0$. It follows from the Cauchy--Schwartz inequality that, for any random variable $Y$, we must have $|\cov(X, Y)| \leqslant (\var(X) \var(Y))^{1/2} = 0 \cdot (\var(Y))^{1/2} = 0$, so that $\cov(X, Y) = 0$.
\end{example}

Recalling that the correlation of $X$ and $Y$ is defined by
\[
\cor(X, Y) = \frac{\cov(X, Y)}{\sqrt{\var(X) \var(Y)}},
\]
we immediately obtain the following important result (which has already been referred to, back when correlation was first introduced).

\begin{corollary}
\label{cor:3.6.1}
Let $X$ and $Y$ be arbitrary random variables, having finite means and finite, nonzero variances. Then $|\cor(X, Y)| \leqslant 1$. Furthermore, $|\cor(X, Y)| = 1$ if and only if
\[
X - \mu_X = \frac{\cov(X, Y)}{\var(Y)} (Y - \mu_Y).
\]
\end{corollary}

So the correlation between two random variables is always between $-1$ and $1$. We also see that $X$ and $Y$ are linearly related if and only if $|\cor(X, Y)| = 1$, and that this relationship is increasing (positive slope) when $\cor(X, Y) = 1$ and decreasing (negative slope) when $\cor(X, Y) = -1$.

\subsection{Jensen's Inequality (Advanced)}
\label{ssec:3.6.1}

Finally, we develop a more advanced inequality that is sometimes very useful. A function $f$ is called \emph{convex} if for every $x, y$, the line segment from $(x, f(x))$ to $(y, f(y))$ lies entirely above the graph of $f$ as depicted in Figure~\ref{fig:3.6.1}.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_6_1.pdf}
  \caption{Plot of the convex function $f(x) = x^4$ and the line segment joining $(2, f(2))$ to $(4, f(4))$.}
  \label{fig:3.6.1}
\end{figure}

In symbols, we require that for every $x, y$ and every $0 \leqslant \alpha \leqslant 1$, we have $\alpha f(x) + (1 - \alpha) f(y) \geqslant f(\alpha x + (1 - \alpha) y)$. Examples of convex functions include $f(x) = x^2$, $f(x) = x^4$, and $f(x) = \max(x, C)$ for any real number $C$. We have the following.

\begin{theorem}[Jensen's inequality]
\label{thm:3.6.4}
Let $X$ be an arbitrary random variable, and let $f : \mathbf{R}^1 \to \mathbf{R}^1$ be a convex function such that $\expc[f(X)]$ is finite. Then $f(\expc[X]) \leqslant \expc[f(X)]$. Equality occurs if and only if $f(X) = a + bX$ for some $a$ and $b$.
\end{theorem}

\begin{proof}
Because $f$ is convex, we can find a linear function $g(x) = ax + b$ such that $g(\expc[X]) = f(\expc[X])$ and $g(x) \leqslant f(x)$ for all $x \in \mathbf{R}^1$ (see, for example, Figure~\ref{fig:3.6.2}).

But then using monotonicity and linearity, we have $\expc[f(X)] \geqslant \expc[g(X)] = \expc[aX + b] = a\expc[X] + b = g(\expc[X]) = f(\expc[X])$, as claimed.

We have equality if and only if $0 = \expc[f(X) - g(X)]$. Because $f(X) - g(X) \geqslant 0$, this occurs (using Challenge~\ref{exer:3.3.29}) if and only if $f(X) = g(X) = aX + b$ with probability $1$.
\end{proof}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig3_6_2.pdf}
  \caption{Plot of the convex function $f(x) = x^4$ and the function $g(x) = 81 + 108(x - 3)$ satisfying $g(x) \leqslant f(x)$ on the interval $[2, 4]$.}
  \label{fig:3.6.2}
\end{figure}

\begin{example}
\label{ex:3.6.9}
Let $X$ be a random variable with finite variance. Then setting $f(x) = x^2$, Jensen's inequality says that $(\expc[X])^2 \leqslant \expc[X^2]$. Of course, we already knew this because $\expc[X^2] - (\expc[X])^2 = \var(X) \geqslant 0$.
\end{example}

\begin{example}
\label{ex:3.6.10}
Let $X$ be a random variable with finite fourth moment. Then setting $f(x) = x^4$, Jensen's inequality says that $(\expc[X])^4 \leqslant \expc[X^4]$.
\end{example}

\begin{example}
\label{ex:3.6.11}
Let $X$ be a random variable with finite mean, and let $M \in \mathbf{R}^1$. Then setting $f(x) = \max(x, M)$, we have that $\expc[\max(X, M)] \geqslant \max(\expc[X], M)$ by Jensen's inequality. In fact, we could also have deduced this from the monotonicity property of expectation, using the two inequalities $\max(X, M) \geqslant X$ and $\max(X, M) \geqslant M$.
\end{example}

\subsection*{Summary of Section~\ref{sec:3.6}}

\begin{itemize}
\item For nonnegative $X$, Markov's inequality says $\prb(X \geqslant a) \leqslant \expc[X]/a$.
\item Chebychev's inequality says $\prb(|Y - \mu_Y| \geqslant a) \leqslant \var(Y)/a^2$.
\item The Cauchy--Schwartz inequality says $|\cov(X, Y)| \leqslant (\var(X) \var(Y))^{1/2}$, so that $|\cor(X, Y)| \leqslant 1$.
\item Jensen's inequality says $f(\expc[X]) \leqslant \expc[f(X)]$ whenever $f$ is convex.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:3.6.1}
Let $Z \sim \text{Poisson}(3)$. Use Markov's inequality to get an upper bound on $\prb(Z \geqslant 7)$.
\end{exercise}

\begin{solution}
Since $Z \geqslant 0$, $\prb(Z \geqslant 7) \leqslant \expc(Z)/7 = 3/7$.
\end{solution}

\begin{exercise}
\label{exer:3.6.2}
Let $X \sim \text{Exponential}(5)$. Use Markov's inequality to get an upper bound on $\prb(X \geqslant 3)$ and compare it with the precise value.
\end{exercise}

\begin{solution}
Since $X \geqslant 0$, $\prb(X \geqslant 3) \leqslant \expc(X)/3 = (1/5)/3 = 1/15$.
\end{solution}

\begin{exercise}
\label{exer:3.6.3}
Let $X \sim \text{Geometric}(1/2)$.
\begin{enumerate}[(a)]
\item Use Markov's inequality to get an upper bound on $\prb(X \geqslant 9)$.
\item Use Markov's inequality to get an upper bound on $\prb(X \geqslant 2)$.
\item Use Chebychev's inequality to get an upper bound on $\prb(|X - 1| \geqslant 1)$.
\item Compare the answers obtained in parts (b) and (c).
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Since $X \geqslant 0$, $\prb(X \geqslant 9) \leqslant \expc(X)/9 = (1 - 1/2)/(1/2)/9 = 1/9$.
    \item Since $X \geqslant 0$, $\prb(X \geqslant 2) \leqslant \expc(X)/2 = (1 - 1/2)/(1/2)/2 = 1/2$.
    \item Since $\expc(X) = (1 - 1/2)/(1/2) = 1$, $\prb(|X - 1| \geqslant 1) \leqslant \var(X)/1^2 = (1 - 1/2)/(1/2)^2/1^2 = 2$.
    \item The upper bound in (b) is smaller and more useful than that in (c).
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.4}
Let $Z \sim N(5, 9)$. Use Chebychev's inequality to get an upper bound on $\prb(|Z - 5| \geqslant 30)$.
\end{exercise}

\begin{solution}
Since $\expc(Z) = 5$, $\prb(|Z - 5| \geqslant 30) \leqslant \var(Z)/30^2 = 9/30^2 = 1/100$.
\end{solution}

\begin{exercise}
\label{exer:3.6.5}
Let $W \sim \text{Binomial}(100, 1/2)$, as in the number of heads when flipping 100 fair coins. Use Chebychev's inequality to get an upper bound on $\prb(|W - 50| \geqslant 10)$.
\end{exercise}

\begin{solution}
Since $\expc(W) = 50$, $\prb(|W - 50| \geqslant 10) \leqslant \var(W)/10^2 = 100(1/2)(1/2)/10^2 = 1/4$.
\end{solution}

\begin{exercise}
\label{exer:3.6.6}
Let $Y \sim N(0, 100)$, and let $Z \sim \text{Binomial}(80, 1/4)$. Determine (with explanation) the largest and smallest possible values of $\cov(Y, Z)$.
\end{exercise}

\begin{solution}
We have $\cov(Y, Z) = \cor(Y, Z) \sqrt{\var(Y) \var(Z)} = \cor(Y, Z) \sqrt{(100)(80 \cdot 1/4 \cdot 3/4)} = \cor(Y, Z) \sqrt{1500}$. This is largest when $\cor(Y, Z) = +1$, where $\cov(Y, Z) = \sqrt{1500} = 38.73$. This is smallest when $\cor(Y, Z) = -1$, where $\cov(Y, Z) = -\sqrt{1500} = -38.73$.
\end{solution}

\begin{exercise}
\label{exer:3.6.7}
Let $X \sim \text{Geometric}(1/11)$. Use Jensen's inequality to determine a lower bound on $\expc[X^4]$, in two different ways.
\begin{enumerate}[(a)]
\item Apply Jensen's inequality to $X$ with $f(x) = x^4$.
\item Apply Jensen's inequality to $X^2$ with $f(x) = x^2$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item By Jensen's inequality, $\expc(X^4) \geqslant \expc(X)^4 = [(1 - 1/11)/(1/11)]^4 = 10^4 = 10000$.
    \item By Jensen's inequality, $\expc(X^4) \geqslant \expc(X^2)^2 = [(1 - 1/11)/(1/11)^2]^2 = (10 \cdot 11)^2 = 12100$, which is larger and hence a better lower bound.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.8}
Let $X$ be the number showing on a fair six-sided die. What bound does Chebychev's inequality give for $\prb(X \geqslant 5 \text{ or } X \leqslant 2)$?
\end{exercise}

\begin{solution}
It is known that $\expc(X) = 7/2$ and $\var(X) = 35/12$. Hence, $\prb(X \geqslant 5 \text{ or } X \leqslant 2) = \prb(|X - \expc(X)| \geqslant 3/2) \leqslant \var(X)/(3/2)^2 = 35/27$. Since $35/27 > 1$, the Chebyshev's inequality bound is meaningless for this problem.
\end{solution}

\begin{exercise}
\label{exer:3.6.9}
Suppose you flip four fair coins. Let $Y$ be the number of heads obtained.
\begin{enumerate}[(a)]
\item What bound does Chebychev's inequality give for $\prb(Y \geqslant 3 \text{ or } Y \leqslant 1)$?
\item What bound does Chebychev's inequality give for $\prb(Y = 4 \text{ or } Y = 0)$?
\end{enumerate}
\end{exercise}

\begin{solution}
Note that $\expc(Y) = 4(1/2) = 2$ and $\var(Y) = 4(1/2)(1/2) = 1$.
\begin{enumerate}[(a)]
    \item $\prb(Y \geqslant 3 \text{ or } Y \leqslant 1) = \prb(|Y - \expc(Y)| \geqslant 1) \leqslant \var(Y)/1^2 = 1$. Hence, Chebyshev's inequality bound gives no improvement.
    \item $\prb(Y \geqslant 4 \text{ or } Y \leqslant 0) = \prb(|Y - \expc(Y)| \geqslant 2) \leqslant \var(Y)/2^2 = 1/4$. Hence, Chebyshev's inequality bound is $1/4$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.10}
Suppose $W$ has density function $f(\omega) = 3\omega^2/2$ for $-1 \leqslant \omega \leqslant 1$, otherwise $f(\omega) = 0$.
\begin{enumerate}[(a)]
\item Compute $\expc[W]$.
\item What bound does Chebychev's inequality give for $\prb(|W - \expc[W]| \geqslant 1/4)$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(W) = \int_{\mathbb{R}^1} w f(w) \, \mathrm{d}w = \int_0^1 w(3w^2) \, \mathrm{d}w = 3w^4/4 \big|_{w=0}^{w=1} = 3/4$.
    \item $\expc(W^2) = \int_0^1 w^2(3w^2) = 3w^5/5 \big|_{w=0}^{w=1} = 3/5$. Thus, $\var(W) = 3/5 - (3/4)^2 = 3/80$. Hence, the Chebyshev's inequality bound is $3/5$ because $\prb(|W - \expc(W)| \geqslant 1/4) \leqslant \var(W)/(1/4)^2 = (3/80)/(1/16) = 3/5$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.11}
Suppose $Z$ has density function $f(z) = z^3/4$ for $0 \leqslant z \leqslant 2$, otherwise $f(z) = 0$.
\begin{enumerate}[(a)]
\item Compute $\expc[Z]$.
\item What bound does Chebychev's inequality give for $\prb(|Z - \expc[Z]| \geqslant 1/2)$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $\expc(Z) = \int_{\mathbb{R}^1} z f(z) \, \mathrm{d}z = \int_0^2 z \cdot z^3/4 \, \mathrm{d}z = (z^5/20) \big|_{z=0}^{z=2} = 8/5$.
    \item For Chebyshev's inequality, we need the variance of $Z$. $\expc(Z^2) = \int_0^2 z^2 \cdot z^3/4 \, \mathrm{d}z = (z^6/24) \big|_{z=0}^{z=2} = 8/3$. Thus, $\var(Z) = \expc(Z^2) - (\expc(Z))^2 = 8/3 - (8/5)^2 = 8/75$. By Chebyshev's inequality,
    \[
        \prb(|Z - \expc(Z)| \geqslant 1/2) \leqslant \frac{\var(Z)}{(1/2)^2} = \frac{32}{75}.
    \]
    Hence, the Chebyshev's inequality bound is $32/75$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.12}
Suppose $\var(X) = 4$ and $\var(Y) = 9$.
\begin{enumerate}[(a)]
\item What is the largest possible value of $\cov(X, Y)$?
\item What is the smallest possible value of $\cov(X, Y)$?
\item Suppose $Z = 3X + 2$. Compute $\var(Z)$ and $\cov(X, Z)$, and compare your answer with part (a).
\item Suppose $W = -3X + 2$. Compute $\var(W)$ and $\cov(X, W)$, and compare your answer with part (b).
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item By Cauchy--Schwarz inequality, $|\cov(X, Y)| \leqslant \sqrt{\var(X)\var(Y)} = 6$. Hence, the largest possible value of $\cov(X, Y)$ is $6$.
    \item By Cauchy--Schwarz inequality, $|\cov(X, Y)| \leqslant \sqrt{\var(X)\var(Y)} = 6$. Hence, the smallest possible value of $\cov(X, Y)$ is $-6$.
    \item The variance of $Z$ is $\var(Z) = (3/2)^2 \var(X) = 9$. $\cov(X, Z) = \cov(X, 3X/2) = (3/2)\var(X) = 6$. Hence, the maximum covariance of $X$ and $Y$ is attained when $Y = 3X/2$ in part (a).
    \item The variance of $W$ is $\var(W) = (-3/2)^2 \var(X) = 9$. $\cov(X, W) = \cov(X, -3X/2) = (-3/2)\var(X) = -6$. Hence, the smallest covariance of $X$ and $Y$ is attained when $Y = -3X/2$ in part (b).
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.13}
Suppose a species of beetle has length 35 millimeters on average. Find an upper bound on the probability that a randomly chosen beetle of this species will be over 80 millimeters long.
\end{exercise}

\begin{solution}
Let $X$ be the length of a randomly-chosen beetle. We know $X > 0$ and $\expc(X) = 35$. By Markov's inequality,
\[
    \prb(X \geqslant 80) \leqslant \frac{\expc(X)}{80} = \frac{35}{80} = \frac{7}{16} = 0.4375.
\]
Hence, $0.4375$ is an upper bound of the probability $\prb(X \geqslant 80)$.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:3.6.14}
Prove that for any $\epsilon > 0$ and $\delta > 0$, there is a positive integer $M$, such that if $X$ is the number of heads when flipping $M$ fair coins, then $\prb(|X/M - 1/2| \geqslant \epsilon) \leqslant \delta$.
\end{exercise}

\begin{solution}
Here $X \sim \text{Binomial}(M, 1/2)$, so $\expc(X) = M/2$ and $\var(X) = M(1/2)(1 - 1/2) = M/4$. Hence, by Chebyshev's inequality, since $\expc(X/M) = 1/2$, $\prb(|(X/M) - (1/2)| \geqslant \delta) \leqslant \var(X/M)/\delta^2 = \var(X)/M^2 \delta^2 = (M/4)/M^2 \delta^2 = 1/4M\delta^2$. This is $\leqslant c$ provided $M \geqslant 1/4\delta^2 c$.
\end{solution}

\begin{exercise}
\label{exer:3.6.15}
Prove that for any $\mu$ and $\sigma^2 > 0$, there is a $\delta > 0$ and a random variable $X$ with $\expc[X] = \mu$ and $\var(X) = \sigma^2$, such that Chebychev's inequality holds with equality, i.e., such that $\prb(|X - \mu| \geqslant a) = \sigma^2/a^2$.
\end{exercise}

\begin{solution}
Let $a = \sigma$ and let $\prb(X = \mu - a) = \prb(X = \mu + a) = 1/2$. Then $\expc(X) = \mu$, $\var(X) = \sigma^2$, and $\prb(|X - \mu| \geqslant a) = 1 = \sigma^2/a^2$.
\end{solution}

\begin{exercise}
\label{exer:3.6.16}
Suppose that $(X, Y)$ is uniform on the set $\{(x_1, y_1), \ldots, (x_n, y_n)\}$ where the $x_1, \ldots, x_n$ are distinct values and the $y_1, \ldots, y_n$ are distinct values.
\begin{enumerate}[(a)]
\item Prove that $X$ is uniformly distributed on $\{x_1, \ldots, x_n\}$ with mean given by $\bar{x} = n^{-1} \sum_{i=1}^{n} x_i$ and variance given by $s_X^2 = n^{-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$.
\item Prove that the correlation coefficient between $X$ and $Y$ is given by
\[
r_{XY} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i - \bar{y})^2}} = \frac{s_{XY}}{s_X s_Y}
\]
where $s_{XY} = n^{-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$. The value $s_{XY}$ is referred to as the \emph{sample covariance} and $r_{XY}$ is referred to as the \emph{sample correlation coefficient} when the values $(x_1, y_1), \ldots, (x_n, y_n)$ are an observed sample from some bivariate distribution.
\item Argue that $r_{XY}$ is also the correlation coefficient between $X$ and $Y$ when we drop the assumption of distinctness for the $x_i$ and $y_i$.
\item Prove that $-1 \leqslant r_{XY} \leqslant 1$ and state the conditions under which $|r_{XY}| = 1$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item and (b)
    \begin{align*}
        \expc(X) &= \sum_{i=1}^{n} x_i \prb(X = x_i) = \sum_{i=1}^{n} x_i \frac{1}{n} = \bar{x}, \\
        \var(X) &= \sum_{i=1}^{n} (x_i - \bar{x})^2 \prb(X = x_i) = \sum_{i=1}^{n} (x_i - \bar{x})^2 \frac{1}{n} = \hat{s}_X^2, \\
        \cov(X, Y) &= \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) \prb(X = x_i, Y = y_i) = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) \frac{1}{n} = \hat{s}_{XY}
    \end{align*}
    Therefore $r_{XY}$ is as stated.
    \item Let $x_1^*, \ldots, x_{n^*}^*$ be the distinct values in $x_1, \ldots, x_n$ and let $f_i$ denote the frequency of $x_i^*$ in $x_1, \ldots, x_n$. Then
    \begin{align*}
        \expc(X) &= \sum_{i=1}^{n^*} x_i^* \prb(X = x_i) = \sum_{i=1}^{n^*} x_i^* \frac{f_i}{n} = \frac{1}{n} \sum_{i=1}^{n} x_i = \bar{x}, \\
        \var(X) &= \sum_{i=1}^{n^*} (x_i^* - \bar{x})^2 \prb(X = x_i) = \sum_{i=1}^{n^*} (x_i^* - \bar{x})^2 \frac{f_i}{n} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 = \hat{s}_X^2
    \end{align*}
    and, similarly, all the other expectations remain the same.
    \item Since $r_{XY}$ is a correlation coefficient we immediately have, from the correlation inequality, that $-1 \leqslant r_{XY} \leqslant 1$ and $r_{XY} = \pm 1$ if and only if $x_i - \bar{x} = \hat{s}_{XY}(y_i - \bar{y})/\hat{s}_X^2$ for $i = 1, \ldots, n$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.17}
Suppose that $X$ is uniformly distributed on $\{x_1, \ldots, x_n\}$ and so has mean $\bar{x} = n^{-1} \sum_{i=1}^{n} x_i$ and variance $s_X^2 = n^{-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$ (see Problem~\ref{exer:3.6.16}(a)). What is the largest proportion of the values $x_i$ that can lie outside $(\bar{x} - 2s_X, \bar{x} + 2s_X)$?
\end{exercise}

\begin{solution}
From Chebyshev's inequality we have that
\[
    \prb(X \notin (\bar{x} - 2\hat{s}, \bar{x} + 2\hat{s})) = \prb(|X - \bar{x}| \geqslant 2\hat{s}_X) \leqslant \frac{\hat{s}_X^2}{(2\hat{s}_X)^2} = \frac{1}{4},
\]
so the largest possible proportion is $1/4$.
\end{solution}

\begin{exercise}
\label{exer:3.6.18}
Suppose that $X$ is distributed with density given by $f_X(x) = 2/x^3$ for $x \geqslant 1$ and is $0$ otherwise.
\begin{enumerate}[(a)]
\item Prove that $f_X$ is a density.
\item Calculate the mean of $X$.
\item Compute $\prb(X \geqslant k)$ and compare this with the upper bound on this quantity given by Markov's inequality.
\item What does Chebyshev's inequality say in this case?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item We have that $\int_1^{\infty} (2/x^3) \, \mathrm{d}x = -x^{-2} \big|_1^{\infty} = 1$, so $f_X$ is a density.
    \item $\expc(X) = \int_1^{\infty} x(2/x^3) \, \mathrm{d}x = \int_1^{\infty} (2/x^2) \, \mathrm{d}x = -2x^{-1} \big|_1^{\infty} = 2$.
    \item Markov's inequality says that $\prb(X \geqslant k) \leqslant \expc(X)/k = 2/k$, while the precise value is $\prb(X \geqslant k) = \int_k^{\infty} (2/x^3) \, \mathrm{d}x = -x^{-2} \big|_k^{\infty} = 1/k^2$, and we see that the tail probability declines quadratically, while Markov's inequality only declines linearly.
    \item We have that $\expc(X^2) = \int_1^{\infty} x^2(2/x^3) \, \mathrm{d}x = \int_1^{\infty} (2/x) \, \mathrm{d}x = -2\ln x \big|_1^{\infty} = \infty$. Therefore, $\var(X) = \infty$ and Chebyshev's inequality does not provide a useful bound in this case.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.19}
Let $g(x) = \max(x, 10)$.
\begin{enumerate}[(a)]
\item Verify that $g$ is a convex function.
\item Suppose $Z \sim \text{Exponential}(5)$. Use Jensen's inequality to obtain a lower bound on $\expc[g(Z)]$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item For $0 < \lambda < 1$ and $x < y$, $g(\lambda x + (1 - \lambda)y) = \max(-\lambda x - (1 - \lambda)y, -10)$. If $x, y \geqslant 10$, then $g(x) = g(y) = g(\lambda x + (1 - \lambda)y) = -10$. If $x, y \leqslant 10$, then $g(x) = -x$, $g(y) = -y$, and $g(\lambda x + (1 - \lambda)y) = -(\lambda x + (1 - \lambda)y) = \lambda g(x) - (1 - \lambda)g(y)$. Finally, if $x < 10 < y$, then $g(x) = x$ and $g(y) = -10$, so $\lambda g(x) + (1 - \lambda)g(y) = \lambda(-x) - (1 - \lambda)(-10) = \lambda(-x) + (1 - \lambda)(-y) + (1 - \lambda)(y - 10)$, while $g(\lambda x + (1 - \lambda)y) \leqslant \lambda(-x) + (1 - \lambda)(-y) + (\lambda x + (1 - \lambda)y - 10) \leqslant \lambda(-x) + (1 - \lambda)(-y) + (1 - \lambda)(y - 10)$.
    \item $\expc(g(Z)) = \expc(\max(-Z, -10)) \geqslant g(\expc(Z)) = g(1/5) = \max(-1/5, -10) = -1/5$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:3.6.20}
It can be shown that a function $f$ with continuous second derivative, is convex on $(a, b)$ if $f''(x) \geqslant 0$ for all $x \in (a, b)$.
\begin{enumerate}[(a)]
\item Use the above fact to show that $f(x) = x^p$ is convex on $(0, \infty)$ whenever $p \geqslant 1$.
\item Use part (a) to prove that $(\expc[|X|^p])^{1/p} \geqslant \expc[|X|]$ whenever $p \geqslant 1$.
\item Prove that $\var(X) = 0$ if and only if $X$ is degenerate at a constant $c$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $f'(x) = px^{p-1}$, $f''(x) = p(p - 1)x^{p-2} \geqslant 0$ for all $x \geqslant 0$ since $p \geqslant 1$. Therefore, $f$ is convex on $(0, \infty)$.
    \item By Jensen's inequality we have that $\expc(f(X)) \geqslant f(\expc(X))$, so $\expc(|X|^p) \geqslant (|\expc(X)|)^p$ and $(\expc(|X|^p))^{1/p} \geqslant |\expc(X)|$.
    \item We have that $\var(X) = \expc(X^2) - (\expc(X))^2$ and $\expc(X^2) - (\expc(X))^2 = 0$ if and only if $\expc(X^2) = (\expc(X))^2$ and this true (by Jensen's inequality) if and only if $X^2 = a + bX$ for some constants $a$ and $b$. The only way this can happen is if $X$ is degenerate at a point, say $c$, $a = 0$ and $b = c$.
\end{enumerate}
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:3.6.21}
Determine (with proof) all functions that are convex and whose negatives are also convex. (That is, find all functions $f$ such that $f$ is convex, and also $-f$ is convex.)
\end{exercise}

\begin{solution}
If $f$ and $-f$ are convex, then for all $x < y$ and $0 < \lambda < 1$, $f(\lambda x + (1 - \lambda)y) \geqslant \lambda f(x) + (1 - \lambda)f(y)$ and $-f(\lambda x + (1 - \lambda)y) \geqslant -\lambda f(x) - (1 - \lambda)f(y)$, so $f(\lambda x + (1 - \lambda)y) \leqslant \lambda f(x) + (1 - \lambda)f(y)$ and $f(\lambda x + (1 - \lambda)y) = \lambda f(x) + (1 - \lambda)f(y)$. Hence, the graph of $f$ from $x$ to $y$ is a straight line, so $f$ must be a linear function, i.e., $f(x) = ax + b$ for some $a$ and $b$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{General Expectations (Advanced)}
\label{sec:3.7}

So far we have considered expected values separately for discrete and absolutely continuous random variables only. However, this separation into two different ``cases'' may seem unnatural. Furthermore, we know that some random variables are neither discrete nor continuous --- for example, mixtures of discrete and continuous distributions.

Hence, it seems desirable to have a more general definition of expected value. Such generality is normally considered in the context of general measure theory, an advanced mathematical subject. However, it is also possible to give a general definition in elementary terms, as follows.

\begin{definition}
\label{def:3.7.1}
Let $X$ be an arbitrary random variable (perhaps neither discrete nor continuous). Then the expected value of $X$ is given by
\[
\expc[X] = \int_0^{\infty} \prb(X > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t
\]
provided either $\int_0^{\infty} \prb(X > t) \, \mathrm{d}t < \infty$ or $\int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t < \infty$.
\end{definition}

This definition appears to contradict our previous definitions of $\expc[X]$. However, in fact, there is no contradiction, as the following theorem shows.

\begin{theorem}
\label{thm:3.7.1}
\begin{enumerate}[(a)]
\item Let $X$ be a discrete random variable with distinct possible values $x_1, x_2, \ldots$, and put $p_i = \prb(X = x_i)$. Then Definition~\ref{def:3.7.1} agrees with the previous definition of $\expc[X]$. That is,
\[
\int_0^{\infty} \prb(X > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t = \sum_i x_i p_i.
\]
\item Let $X$ be an absolutely continuous random variable with density $f_X$. Then Definition~\ref{def:3.7.1} agrees with the previous definition of $\expc[X]$. That is,
\[
\int_0^{\infty} \prb(X > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t = \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x.
\]
\end{enumerate}
\end{theorem}

\begin{proof}
The key to the proof is switching the order of the integration/summation.

\begin{enumerate}
  \item We have
\[
\int_0^{\infty} \prb(X > t) \, \mathrm{d}t = \int_0^{\infty} \sum_{i: x_i > t} p_i \, \mathrm{d}t = \sum_i p_i \int_0^{x_i} \mathrm{d}t = \sum_i p_i x_i
\]
as claimed.

  \item  We have
\[
\int_0^{\infty} \prb(X > t) \, \mathrm{d}t = \int_0^{\infty} \int_t^{\infty} f_X(x) \, \mathrm{d}x \, \mathrm{d}t = \int_0^{\infty} \int_0^{x} f_X(x) \, \mathrm{d}t \, \mathrm{d}x = \int_0^{\infty} x \, f_X(x) \, \mathrm{d}x.
\]
Similarly,
\[
\int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t = \int_{-\infty}^{0} \int_{-\infty}^{t} f_X(x) \, \mathrm{d}x \, \mathrm{d}t = \int_{-\infty}^{0} \int_x^{0} f_X(x) \, \mathrm{d}t \, \mathrm{d}x = -\int_{-\infty}^{0} x \, f_X(x) \, \mathrm{d}x.
\]
Hence,
\begin{align*}
\int_0^{\infty} \prb(X > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t &= \int_0^{\infty} x \, f_X(x) \, \mathrm{d}x + \int_{-\infty}^{0} x \, f_X(x) \, \mathrm{d}x \\
&= \int_{-\infty}^{\infty} x \, f_X(x) \, \mathrm{d}x
\end{align*}
as claimed.
\end{enumerate}
\end{proof}

In other words, Theorem~\ref{thm:3.7.1} says that Definition~\ref{def:3.7.1} includes our previous definitions of expected value, for both discrete and absolutely continuous random variables, while working for any random variable at all. (Note that to apply Definition~\ref{def:3.7.1} we take an integral, not a sum, regardless of whether $X$ is discrete or continuous!)

Furthermore, Definition~\ref{def:3.7.1} preserves the key properties of expected value, as the following theorem shows. (We omit the proof here, but see Challenge~\ref{exer:3.7.10} for a proof of part (c).)

\begin{theorem}
\label{thm:3.7.2}
Let $X$ and $Y$ be arbitrary random variables, perhaps neither discrete nor continuous, with expected values defined by Definition~\ref{def:3.7.1}.
\begin{enumerate}[(a)]
\item (Linearity) If $a$ and $b$ are any real numbers, then $\expc[aX + bY] = a\expc[X] + b\expc[Y]$.
\item If $X$ and $Y$ are independent, then $\expc[XY] = \expc[X]\expc[Y]$.
\item (Monotonicity) If $X \leqslant Y$, then $\expc[X] \leqslant \expc[Y]$.
\end{enumerate}
\end{theorem}

Definition~\ref{def:3.7.1} also tells us about expected values of mixture distributions, as follows.

\begin{theorem}
\label{thm:3.7.3}
For $1 \leqslant i \leqslant k$, let $Y_i$ be a random variable with cdf $F_i$. Let $X$ be a random variable whose cdf corresponds to a finite mixture (as in Section~\ref{ssec:2.5.4}) of the cdfs of the $Y_i$, so that $F_X(x) = \sum_i p_i F_i(x)$ where $p_i \geqslant 0$ and $\sum_i p_i = 1$. Then $\expc[X] = \sum_i p_i \expc[Y_i]$.
\end{theorem}

\begin{proof}
We compute that
\[
\prb(X > t) = 1 - F_X(t) = 1 - \sum_i p_i F_i(t) = \sum_i p_i (1 - F_i(t)) = \sum_i p_i \prb(Y_i > t).
\]
Similarly,
\[
\prb(X < t) = F_X(t^-) = \sum_i p_i F_i(t^-) = \sum_i p_i \prb(Y_i < t).
\]
Hence, from Definition~\ref{def:3.7.1},
\begin{align*}
\expc[X] &= \int_0^{\infty} \prb(X > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t \\
&= \int_0^{\infty} \sum_i p_i \prb(Y_i > t) \, \mathrm{d}t - \int_{-\infty}^{0} \sum_i p_i \prb(Y_i < t) \, \mathrm{d}t \\
&= \sum_i p_i \left( \int_0^{\infty} \prb(Y_i > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(Y_i < t) \, \mathrm{d}t \right) \\
&= \sum_i p_i \expc[Y_i]
\end{align*}
as claimed.
\end{proof}

\subsection*{Summary of Section~\ref{sec:3.7}}

\begin{itemize}
\item For general random variables, we can define a general expected value by $\expc[X] = \int_0^{\infty} \prb(X > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t$.
\item This definition agrees with our previous one, for discrete or absolutely continuous random variables.
\item General expectation is still linear and monotone.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:3.7.1}
Let $X_1$, $X_2$, and $Y$ be as in Example~\ref{ex:2.5.6}, so that $Y$ is a mixture of $X_1$ and $X_2$. Compute $\expc[X_1]$, $\expc[X_2]$, and $\expc[Y]$.
\end{exercise}

\begin{solution}
$\expc(X_1) = 3$, $\expc(X_2) = 0$, and $\expc(Y) = (1/5)\expc(X_1) + (4/5)\expc(X_2) = 3/5$.
\end{solution}

\begin{exercise}
\label{exer:3.7.2}
Suppose we roll a fair six-sided die. If it comes up $1$, then we roll the same die again and let $X$ be the value showing. If it comes up anything other than $1$, then we instead roll a fair eight-sided die (with the sides numbered $1$ through $8$), and let $X$ be the value showing on the eight-sided die. Compute the expected value of $X$.
\end{exercise}

\begin{solution}
$\expc(X) = (1/6)(7/2) + (5/6)(9/2) = 13/3$.
\end{solution}

\begin{exercise}
\label{exer:3.7.3}
Let $X$ be a positive constant random variable, so that $X = C$ for some constant $C > 0$. Prove directly from Definition~\ref{def:3.7.1} that $\expc[X] = C$.
\end{exercise}

\begin{solution}
Here $\prb(X < t) = 0$ for $t < 0$, while $\prb(X > t) = 1$ for $0 < t < C$ and $\prb(X > t) = 0$ for $t > C$. Hence, $\expc(X) = \int_0^{\infty} \prb(X > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t = \int_0^C 1 \, \mathrm{d}t + \int_C^{\infty} 0 \, \mathrm{d}t - \int_{-\infty}^{0} 0 \, \mathrm{d}t = C + 0 - 0 = C$.
\end{solution}

\begin{exercise}
\label{exer:3.7.4}
Let $Z$ be a general random variable (perhaps neither discrete nor continuous), and suppose that $\prb(Z \leqslant 100) = 1$. Prove directly from Definition~\ref{def:3.7.1} that $\expc[Z] \leqslant 100$.
\end{exercise}

\begin{solution}
Here $\prb(Z > t) = 0$ for $t > 100$. Hence, $\expc(Z) = \int_0^{\infty} \prb(Z > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(Z < t) \, \mathrm{d}t \leqslant \int_0^{\infty} \prb(Z > t) \, \mathrm{d}t = \int_0^{100} \prb(Z > t) \, \mathrm{d}t \leqslant \int_0^{100} 1 \, \mathrm{d}t = 100$.
\end{solution}

\begin{exercise}
\label{exer:3.7.5}
Suppose we are told only that $\prb(X > x) = 1/x^2$ for $x \geqslant 1$, and $\prb(X > x) = 1$ for $x < 1$, but we are not told if $X$ is discrete or continuous or neither. Compute $\expc[X]$.
\end{exercise}

\begin{solution}
For $x \leqslant 0$, $\prb(X < x) \leqslant \prb(X \leqslant x) = 1 - \prb(X > x) = 1 - 1 = 0$. From Definition 3.7.1,
\[
    \expc(X) = \int_0^{\infty} \prb(X > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}t = \int_0^1 \prb(X > t) \, \mathrm{d}t + \int_1^{\infty} \prb(X > t) \, \mathrm{d}t = 1 + \int_1^{\infty} \frac{1}{t^2} \, \mathrm{d}t = 1 + \left[-\frac{1}{t}\right]_{t=1}^{t=\infty} = 1 + 1 = 2.
\]
\end{solution}

\begin{exercise}
\label{exer:3.7.6}
Suppose $\prb(Z > z) = 1$ for $z \leqslant 5$, $\prb(Z > z) = (8 - z)/3$ for $5 < z < 8$, and $\prb(Z > z) = 0$ for $z \geqslant 8$. Compute $\expc[Z]$.
\end{exercise}

\begin{solution}
For $z \leqslant 0$, $\prb(Z < z) \leqslant \prb(Z \leqslant z) = 1 - \prb(Z > z) = 1 - 1 = 0$. From Definition 3.7.1,
\begin{align*}
    \expc(Z) &= \int_0^{\infty} \prb(Z > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(Z < t) \, \mathrm{d}t \\
    &= \int_0^5 \prb(Z > t) \, \mathrm{d}t + \int_5^8 \prb(Z > t) \, \mathrm{d}t + \int_8^{\infty} \prb(Z > t) \, \mathrm{d}t - 0 \\
    &= \int_0^5 1 \, \mathrm{d}t + \int_5^8 \frac{8 - t}{3} \, \mathrm{d}t + \int_8^{\infty} 0 \, \mathrm{d}t = 5 + \left[\frac{8t - t^2/2}{3}\right]_{t=5}^{t=8} + 0 = 5 + 3/2 = 13/2.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:3.7.7}
Suppose $\prb(W > \omega) = e^{-5\omega}$ for $\omega \geqslant 0$ and $\prb(W > \omega) = 1$ for $\omega < 0$. Compute $\expc[W]$.
\end{exercise}

\begin{solution}
For $w \leqslant 0$, $\prb(W < w) \leqslant \prb(W \leqslant w) = 1 - \prb(W > w) = 1 - 1 = 0$. By Definition 3.7.1, $\expc(W) = \int_0^{\infty} \prb(W > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(W < t) \, \mathrm{d}t = \int_0^{\infty} e^{-5t} \, \mathrm{d}t - \int_{-\infty}^{0} 0 \, \mathrm{d}t = -\frac{e^{-5t}}{5} \big|_{t=0}^{t=\infty} = \frac{1}{5}$. The density of $W$ at $w$ is $f_W(w) = \frac{\mathrm{d}}{\mathrm{d}w} \prb(W \leqslant w) = \frac{\mathrm{d}}{\mathrm{d}w}(1 - \prb(W > w)) = \frac{\mathrm{d}}{\mathrm{d}w}(1 - e^{-5w}) = 5e^{-5w}$ for $w > 0$, otherwise $f_W(w) = 0$. Hence, $W \sim \text{Exponential}(5)$. We know the expectation of $\text{Exponential}(\lambda)$ is $1/\lambda$. That coincides with the computation result.
\end{solution}

\begin{exercise}
\label{exer:3.7.8}
Suppose $\prb(Y > y) = e^{-y^2/2}$ for $y \geqslant 0$ and $\prb(Y > y) = 1$ for $y < 0$. Compute $\expc[Y]$. (Hint: The density of a standard normal might help you solve the integral.)
\end{exercise}

\begin{solution}
For $y \leqslant 0$, $\prb(Y < y) \leqslant \prb(Y \leqslant y) = 1 - \prb(Y > y) = 1 - 1 = 0$. By Definition 3.7.1,
\[
    \expc(Y) = \int_0^{\infty} \prb(Y > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(Y < t) \, \mathrm{d}t = \int_0^{\infty} e^{-t^2/2} \, \mathrm{d}t - \int_{-\infty}^{0} 0 \, \mathrm{d}t = (2\pi)^{1/2} \int_0^{\infty} (2\pi)^{-1/2} e^{-t^2/2} \, \mathrm{d}t = (2\pi)^{1/2}(1/2) = (\pi/2)^{1/2} = 1.2533.
\]
\end{solution}

\begin{exercise}
\label{exer:3.7.9}
Suppose the cdf of $W$ is given by $F_W(\omega) = 0$ for $\omega < 10$, $F_W(\omega) = (\omega - 10)$ for $10 \leqslant \omega < 11$, and by $F_W(\omega) = 1$ for $\omega \geqslant 11$. Compute $\expc[W]$. (Hint: Remember that $F_W(\omega^-) = \prb(W < \omega) = 1 - \prb(W > \omega)$.)
\end{exercise}

\begin{solution}
For $w \leqslant 0$, $\prb(W < w) \leqslant \prb(W \leqslant w) = F_W(w) = 0$. For $0 < w < 10$, $\prb(W > w) = 1 - \prb(W \leqslant w) = 1 - F_W(w) = 1 - 0 = 1$. For $10 \leqslant w \leqslant 11$, $\prb(W > w) = 1 - \prb(W \leqslant w) = 1 - F_W(w) = 1 - (w - 10) = 11 - w$. For $w > 11$, $\prb(W > w) = 1 - \prb(W \leqslant w) = 1 - F_W(w) = 1 - 1 = 0$. By Definition 3.7.1,
\begin{align*}
    \expc(W) &= \int_0^{\infty} \prb(W > t) \, \mathrm{d}t - \int_{-\infty}^{0} \prb(W < t) \, \mathrm{d}t \\
    &= \int_0^{10} 1 \, \mathrm{d}t + \int_{10}^{11} (11 - t) \, \mathrm{d}t + \int_{11}^{\infty} 0 \, \mathrm{d}t - \int_{-\infty}^{0} 0 \, \mathrm{d}t \\
    &= 10 + \left[11t - t^2/2\right]_{t=10}^{t=11} = 10 + 1/2 = 21/2.
\end{align*}
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:3.7.10}
Prove part (c) of Theorem~\ref{thm:3.7.2}. (Hint: If $X \leqslant Y$, then how does the event $\{X > t\}$ compare to the event $\{Y > t\}$? Hence, how does $\prb(X > t)$ compare to $\prb(Y > t)$? And what about $\{X < t\}$ and $\{Y < t\}$?)
\end{exercise}

\begin{solution}
If $X > t$, then since $Y \geqslant X$, we also have $Y \geqslant X > t$. Hence, $\{X > t\} \subseteq \{Y > t\}$, so, by monotonicity, $\prb(X > t) \leqslant \prb(Y > t)$. Similarly, $\prb(X < t) \geqslant \prb(Y < t)$. Then $\expc(X) = \int_0^{\infty} \prb(X > t) \, \mathrm{d}x - \int_{-\infty}^{0} \prb(X < t) \, \mathrm{d}x \leqslant \int_0^{\infty} \prb(Y > t) \, \mathrm{d}x - \int_{-\infty}^{0} \prb(Y < t) \, \mathrm{d}x = \expc(Y)$, as claimed.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Further Proofs (Advanced)}
\label{sec:3.8}

\subsection*{Proof of Theorem~\ref{thm:3.4.7}}

We want to prove that if $S$ has a compound distribution as in \eqref{eq:3.4.2}, then (a) $\expc[S] = \expc[X_1]\expc[N]$ and (b) $m_S(s) = r_N(m_{X_1}(s))$.

Because the $X_i$ are i.i.d., we have $\expc[X_i] = \expc[X_1]$ for all $i$. Define $\indc_i$ by $\indc_i = \indc_{\{1 \leqslant N \leqslant i\}}$. Then we can write $S = \sum_{i=1}^{\infty} X_i \indc_i$. Also note that $\sum_{i=1}^{\infty} \indc_i = N$. Because $N$ is independent of $X_i$, so is $\indc_i$, and we have
\begin{align*}
\expc[S] &= \expc\left[ \sum_{i=1}^{\infty} X_i \indc_i \right] = \sum_{i=1}^{\infty} \expc[X_i \indc_i] = \sum_{i=1}^{\infty} \expc[X_i] \expc[\indc_i] \\
&= \sum_{i=1}^{\infty} \expc[X_1] \expc[\indc_i] = \expc[X_1] \sum_{i=1}^{\infty} \expc[\indc_i] = \expc[X_1] \expc\left[ \sum_{i=1}^{\infty} \indc_i \right] = \expc[X_1] \expc[N].
\end{align*}
This proves part (a).

Now, using an expectation version of the law of total probability (see Theorem~\ref{thm:3.5.3}), and recalling that $\expc[\exp(\sum_{i=1}^{n} sX_i)] = (m_{X_1}(s))^n$ because the $X_i$ are i.i.d., we compute that
\begin{align*}
m_S(s) &= \expc\left[ \exp\left( \sum_{i=1}^{n} sX_i \right) \right] = \sum_{n=0}^{\infty} \prb(N = n) \expc\left[ \exp\left( \sum_{i=1}^{n} sX_i \right) \,\bigg|\, N = n \right] \\
&= \sum_{n=0}^{\infty} \prb(N = n) \expc\left[ \exp\left( \sum_{i=1}^{n} sX_i \right) \right] = \sum_{n=0}^{\infty} \prb(N = n) (m_{X_1}(s))^n \\
&= \expc[(m_{X_1}(s))^N] = r_N(m_{X_1}(s))
\end{align*}
thus proving part (b).

\subsection*{Proof of Theorem~\ref{thm:3.5.3}}

We want to show that when $X$ and $Y$ are random variables, and $g : \mathbf{R}^1 \to \mathbf{R}^1$ is any function, then $\expc[g(Y) \expc[X \mid Y]] = \expc[g(Y) X]$.

If $X$ and $Y$ are discrete, then
\begin{align*}
\expc[g(Y) \expc[X \mid Y]] &= \sum_{y \in \mathbf{R}^1} g(y) \expc[X \mid Y = y] \, \prb(Y = y) \\
&= \sum_{y \in \mathbf{R}^1} g(y) \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid Y = y) \, \prb(Y = y) \\
&= \sum_{y \in \mathbf{R}^1} g(y) \sum_{x \in \mathbf{R}^1} x \, \frac{\prb(X = x, Y = y)}{\prb(Y = y)} \, \prb(Y = y) \\
&= \sum_{x \in \mathbf{R}^1} \sum_{y \in \mathbf{R}^1} g(y) x \, \prb(X = x, Y = y) = \expc[g(Y) X],
\end{align*}
as claimed.

Similarly, if $X$ and $Y$ are jointly absolutely continuous, then
\begin{align*}
\expc[g(Y) \expc[X \mid Y]] &= \int_{-\infty}^{\infty} g(y) \expc[X \mid Y = y] \, f_Y(y) \, \mathrm{d}y \\
&= \int_{-\infty}^{\infty} g(y) \int_{-\infty}^{\infty} x \, f_{X|Y}(x \mid y) \, \mathrm{d}x \, f_Y(y) \, \mathrm{d}y \\
&= \int_{-\infty}^{\infty} g(y) \int_{-\infty}^{\infty} x \, \frac{f_{X,Y}(x, y)}{f_Y(y)} \, \mathrm{d}x \, f_Y(y) \, \mathrm{d}y \\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(y) x \, f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = \expc[g(Y) X]
\end{align*}
as claimed.

\subsection*{Proof of Theorem~\ref{thm:3.5.4}}

We want to prove that, when $X$ and $Y$ are random variables, and $g : \mathbf{R}^1 \to \mathbf{R}^1$ is any function, then $\expc[g(Y) X \mid Y] = g(Y) \expc[X \mid Y]$.

For simplicity, we assume $X$ and $Y$ are discrete; the jointly absolutely continuous case is similar. Then for any $y$ with $\prb(Y = y) > 0$,
\begin{align*}
\expc[g(Y) X \mid Y = y] &= \sum_{x \in \mathbf{R}^1} \sum_{z \in \mathbf{R}^1} g(z) x \, \prb(X = x, Y = z \mid Y = y) \\
&= \sum_{x \in \mathbf{R}^1} g(y) x \, \prb(X = x \mid Y = y) \\
&= g(y) \sum_{x \in \mathbf{R}^1} x \, \prb(X = x \mid Y = y) = g(y) \expc[X \mid Y = y].
\end{align*}
Because this is true for any $y$, we must have $\expc[g(Y) X \mid Y] = g(Y) \expc[X \mid Y]$, as claimed.

\subsection*{Proof of Theorem~\ref{thm:3.5.6}}

We need to show that for random variables $X$ and $Y$, $\var(X) = \var(\expc[X \mid Y]) + \expc[\var(X \mid Y)]$.

Using Theorem~\ref{thm:3.5.2}, we have that
\begin{equation}
\label{eq:3.8.1}
\var(X) = \expc[(X - \mu_X)^2] = \expc[\expc[(X - \mu_X)^2 \mid Y]].
\end{equation}
Now,
\begin{equation}
\label{eq:3.8.2}
(X - \mu_X)^2 = (X - \expc[X \mid Y] + \expc[X \mid Y] - \mu_X)^2 = (X - \expc[X \mid Y])^2 + (\expc[X \mid Y] - \mu_X)^2 + 2(X - \expc[X \mid Y])(\expc[X \mid Y] - \mu_X).
\end{equation}
But $\expc[(X - \expc[X \mid Y])^2 \mid Y] = \var(X \mid Y)$.

Also, again using Theorem~\ref{thm:3.5.2},
\[
\expc[\expc[(\expc[X \mid Y] - \mu_X)^2 \mid Y]] = \expc[(\expc[X \mid Y] - \mu_X)^2] = \var(\expc[X \mid Y]).
\]

Finally, using Theorem~\ref{thm:3.5.4} and linearity (Theorem~\ref{thm:3.5.1}), we see that
\begin{align*}
\expc[(X - \expc[X \mid Y])(\expc[X \mid Y] - \mu_X) \mid Y] &= (\expc[X \mid Y] - \mu_X) \expc[(X - \expc[X \mid Y]) \mid Y] \\
&= (\expc[X \mid Y] - \mu_X)(\expc[X \mid Y] - \expc[\expc[X \mid Y] \mid Y]) \\
&= (\expc[X \mid Y] - \mu_X)(\expc[X \mid Y] - \expc[X \mid Y]) = 0.
\end{align*}

From \eqref{eq:3.8.1}, \eqref{eq:3.8.2}, and linearity, we have that $\var(X) = \expc[\var(X \mid Y)] + \var(\expc[X \mid Y]) + 0$, which completes the proof.

\subsection*{Proof of Theorem~\ref{thm:3.6.3} (Cauchy--Schwartz inequality)}

We will prove that whenever $X$ and $Y$ are arbitrary random variables, each having finite, nonzero variance, then
\[
|\cov(X, Y)| \leqslant \sqrt{\var(X) \var(Y)}.
\]
Furthermore, if $\var(Y) \neq 0$, then equality is attained if and only if $X - \mu_X = \beta(Y - \mu_Y)$ where $\beta = \cov(X, Y)/\var(Y)$.

If $\var(Y) = 0$, then Challenge~\ref{exer:3.3.30} implies that $Y = \mu_Y$ with probability $1$ (because $\var(Y) = \expc[(Y - \mu_Y)^2] = 0$). This implies that
\[
|\cov(X, Y)| = |\expc[(X - \mu_X)(Y - \mu_Y)]| = 0 \leqslant \sqrt{\var(X) \var(Y)},
\]
and the Cauchy--Schwartz inequality holds.

If $\var(Y) \neq 0$, let $Z = X - \mu_X$ and $W = Y - \mu_Y$. Then for any real number $\beta$, we compute, using linearity, that
\[
\expc[(Z - \beta W)^2] = \expc[Z^2] - 2\beta \expc[ZW] + \beta^2 \expc[W^2] = \var(X) - 2\beta \, \cov(X, Y) + \beta^2 \var(Y) = a\beta^2 + b\beta + c
\]
where $a = \var(Y) > 0$, $b = -2\cov(X, Y)$, and $c = \var(X)$. On the other hand, clearly $\expc[(Z - \beta W)^2] \geqslant 0$ for all $\beta$. Hence, we have a quadratic equation that is always nonnegative, and so has at most one real root.

By the quadratic formula, any quadratic equation has two real roots provided that the discriminant $b^2 - 4ac \geqslant 0$. Because that is not the case here, we must have $b^2 - 4ac \leqslant 0$, i.e.,
\[
4(\cov(X, Y))^2 - 4 \var(Y) \var(X) \leqslant 0.
\]
Dividing by $4$, rearranging, and taking square roots, we see that
\[
|\cov(X, Y)| \leqslant (\var(X) \var(Y))^{1/2}
\]
as claimed.

Finally, $|\cov(X, Y)| = (\var(X) \var(Y))^{1/2}$ if and only if $b^2 - 4ac = 0$, which means the quadratic has one real root. Thus, there is some real number $\beta$ such that $\expc[(Z - \beta W)^2] = 0$. Since $(Z - \beta W)^2 \geqslant 0$, it follows from Challenge~\ref{exer:3.3.29} that this happens if and only if $Z = \beta W = 0$ with probability $1$, as claimed. When this is the case, then
\[
\cov(X, Y) = \expc[ZW] = \expc[\beta W^2] = \beta \expc[W^2] = \beta \var(Y)
\]
and so $\beta = \cov(X, Y)/\var(Y)$ when $\var(Y) \neq 0$.


