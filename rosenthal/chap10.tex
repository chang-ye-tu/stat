\chapter{Relationships Among Variables}
\label{ch:10}

\noindent\textbf{CHAPTER OUTLINE}
\begin{itemize}
\item Section 1 \quad Related Variables
\item Section 2 \quad Categorical Response and Predictors
\item Section 3 \quad Quantitative Response and Predictors
\item Section 4 \quad Quantitative Response and Categorical Predictors
\item Section 5 \quad Categorical Response and Quantitative Predictors
\item Section 6 \quad Further Proofs (Advanced)
\end{itemize}

In this chapter, we are concerned with perhaps the most important application of statistical inference: the problem of analyzing whether or not a relationship exists among variables and what form the relationship takes. As a particular instance of this, recall the example and discussion in Section~\ref{sec:5.1}.

Many of the most important problems in science and society are concerned with relationships among variables. For example, what is the relationship between the amount of carbon dioxide placed into the atmosphere and global temperatures? What is the relationship between class size and scholastic achievement by students? What is the relationship between weight and carbohydrate intake in humans? What is the relationship between lifelength and the dosage of a certain drug for cancer patients? These are all examples of questions whose answers involve relationships among variables. We will see that statistics plays a key role in answering such questions.

In Section~\ref{sec:10.1}, we provide a precise definition of what it means for variables to be related, and we distinguish between two broad categories of relationship, namely, association and cause--effect. Also, we discuss some of the key ideas involved in collecting data when we want to determine whether a cause--effect relationship exists. In the remaining sections, we examine the various statistical methodologies that are used to analyze data when we are concerned with relationships.

We emphasize the use of frequentist methodologies in this chapter. We give some examples of the Bayesian approach, but there are some complexities involved with the distributional problems associated with Bayesian methods that are best avoided at this stage. Sampling algorithms for the Bayesian approach have been developed, along the lines of those discussed in Chapter~\ref{ch:7} (see also Chapter~\ref{ch:11}), but their full discussion would take us beyond the scope of this text. It is worth noting, however, that Bayesian analyses with diffuse priors will often yield results very similar to those obtained via the frequentist approach.

As discussed in Chapter~\ref{ch:9}, model checking is an important feature of any statistical analysis. For the models used in this chapter, a full discussion of the more rigorous P-value approach to model checking requires more development than we can accomplish in this text. As such, we emphasize the informal approach to model checking, via residual and probability plots. This should not be interpreted as a recommendation that these are the preferred methods for such models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Variables}
\label{sec:10.1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Consider a population $\Omega$ with two variables $X, Y : \Omega \to R^1$ defined on it. What does it mean to say that the variables $X$ and $Y$ are related? Perhaps our first inclination is to say that there must be a formula relating the two variables, such as $Y = a + bX^2$ for some choice of constants $a$ and $b$, or $Y = \exp(X)$, etc. But consider a population of humans and suppose $X(\omega)$ is the weight of $\omega$ in kilograms and $Y(\omega)$ is the height of individual $\omega$ in centimeters. From our experience, we know that taller people tend to be heavier, so we believe that there is some kind of relationship between height and weight. We know, too, that there cannot be an exact formula that describes this relationship, because people with the same weight will often have different heights, and people with the same height will often have different weights.

\subsection{The Definition of Relationship}
\label{ssec:10.1.1}

If we think of all the people with a given weight $x$, then there will be a distribution of heights for all those individuals that have weight $x$. We call this distribution the \emph{conditional distribution of $Y$ given that $X = x$}.

We can now express what we mean by our intuitive idea that $X$ and $Y$ are related, for, as we change the value of the weight that we condition on, we expect the conditional distribution to change. In particular, as $x$ increases, we expect that the location of the conditional distribution will increase, although other features of the distribution may change as well. For example, in Figure~\ref{fig:10.1.1} we provide a possible plot of two approximating densities for the conditional distributions of $Y$ given $X = 70$ kg and the conditional distribution of $Y$ given $X = 90$ kg.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_1_1.pdf}
  \caption{Plot of two approximating densities for the conditional distribution of $Y$ given $X = 70$ kg (dashed line) and the conditional distribution of $Y$ given $X = 90$ kg (solid line).}
  \label{fig:10.1.1}
\end{figure}

We see that the conditional distribution has shifted up when $X$ goes from 70 to 90 kg but also that the shape of the distribution has changed somewhat as well. So we can say that a relationship definitely exists between $X$ and $Y$, at least in this population. Notice that, as defined so far, $X$ and $Y$ are not random variables, but they become so when we randomly select $\omega$ from the population. In that case, the conditional distributions referred to become the conditional probability distributions of the random variable $Y$ given that we observe $X = 70$ and $X = 90$, respectively.

We will adopt the following definition to precisely specify what we mean when we say that variables are related.

\begin{definition}
\label{def:10.1.1}
Variables $X$ and $Y$ are \emph{related variables} if there is any change in the conditional distribution of $Y$ given $X = x$, as $x$ changes.
\end{definition}

We could instead define what it means for variables to be unrelated. We say that variables $X$ and $Y$ are \emph{unrelated} if they are independent. This is equivalent to Definition~\ref{def:10.1.1}, because two variables are independent if and only if the conditional distribution of one given the other does not depend on the condition (Exercise~\ref{exer:10.1.1}).

There is an apparent asymmetry in Definition~\ref{def:10.1.1}, because the definition considers only the conditional distribution of $Y$ given $X$ and not the conditional distribution of $X$ given $Y$. But, if there is a change in the conditional distribution of $Y$ given $X = x$, as we change $x$, then by the above comment, $X$ and $Y$ are not independent; thus there must be a change in the conditional distribution of $X$ given $Y = y$, as we change $y$ (also see Problem~\ref{exer:10.1.23}). 

Notice that the definition is applicable no matter what kind of variables we are dealing with. So both could be quantitative variables, or both categorical variables, or one could be a quantitative variable while the other is a categorical variable.

Definition~\ref{def:10.1.1} says that $X$ and $Y$ are related if any change is observed in the conditional distribution. In reality, this would mean that there is practically always a relationship between variables $X$ and $Y$. It seems likely that we will always detect some difference if we carry out a census and calculate all the relevant conditional distributions. This is where the idea of the \emph{strength} of a relationship among variables becomes relevant, for if we see large changes in the conditional distributions, then we can say a strong relationship exists. If we see only very small changes, then we can say a very weak relationship exists that is perhaps of no practical importance.

\paragraph{The Role of Statistical Models}

If a relationship exists between two variables, then its form is completely described by the set of conditional distributions of $Y$ given $X$. Sometimes it may be necessary to describe the relationship using all these conditional distributions. In many problems, however, we look for a simpler presentation. In fact, we often assume a statistical model that prescribes a simple form for how the conditional distributions change as we change $X$.

Consider the following example.

\begin{example}[Simple Normal Linear Regression Model]
\label{ex:10.1.1}
In Section~\ref{ssec:10.3.2}, we will discuss the simple normal linear regression model, where the conditional distribution of quantitative variable $Y$, given the quantitative variable $X = x$, is assumed to be distributed
\begin{equation*}
N(\beta_1 + \beta_2 x, \sigma^2),
\end{equation*}
where $\beta_1$, $\beta_2$, and $\sigma^2$ are unknown. For example, $Y$ could be the blood pressure of an individual and $X$ the amount of salt the person consumed each day.

In this case, the conditional distributions have constant shape and change, as $x$ changes, only through the conditional mean. The mean moves along the line given by $\beta_1 + \beta_2 x$ for some intercept $\beta_1$ and slope $\beta_2$. If this model is correct, then the variables are unrelated if and only if $\beta_2 = 0$, as this is the only situation in which the conditional distributions can remain constant as we change $x$.
\end{example}

Statistical models, like that described in Example~\ref{ex:10.1.1}, can be wrong. There is nothing requiring that two quantitative variables must be related in that way. For example, the conditional variance of $Y$ can vary with $x$, and the very shape of the conditional distribution can vary with $x$, too. The model of Example~\ref{ex:10.1.1} is an instance of a simplifying assumption that is appropriate in many practical contexts. However, methods such as those discussed in Chapter~\ref{ch:9} must be employed to check model assumptions before accepting statistical inferences based on such a model. We will always consider model checking as part of our discussion of the various models used to examine the relationship among variables.

\paragraph{Response and Predictor Variables}

Often, we think of $Y$ as a dependent variable (depending on $X$) and of $X$ as an independent variable (free to vary). Our goal, then, is to predict the value of $Y$ given the value of $X$. In this situation, we call $Y$ the \emph{response variable} and $X$ the \emph{predictor variable}. Sometimes, though, there is really nothing to distinguish the roles of $X$ and $Y$. For example, suppose that $X$ is the weight of an individual in kilograms and $Y$ is the height in centimeters. We could then think of predicting weight from height or conversely. It is then immaterial which we choose to condition on.

In many applications, there is more than one response variable and more than one predictor variable $X$. We will not consider the situation in which we have more than one response variable, but we will consider the case in which $X = (X_1, \ldots, X_k)$ is $k$-dimensional. Here, the various predictors that make up $X$ could be all categorical, all quantitative, or some mixture of categorical and quantitative variables.

The definition of a relationship existing between response variable $Y$ and the set of predictors $(X_1, \ldots, X_k)$ is exactly as in Definition~\ref{def:10.1.1}. In particular, a relationship exists between $Y$ and $(X_1, \ldots, X_k)$ if there is any change in the conditional distribution of $Y$ given $(X_1, \ldots, X_k) = (x_1, \ldots, x_k)$, when $(x_1, \ldots, x_k)$ is varied. If such a relationship exists, then the form of the relationship is specified by the full set of conditional distributions. Again, statistical models are often used where simplifying assumptions are made about the form of the relationship. Consider the following example.

\begin{example}[The Normal Linear Model with $k$ Predictors]
\label{ex:10.1.2}
In Section~\ref{ssec:10.3.4}, we will discuss the normal multiple linear regression model. For this, the conditional distribution of quantitative variable $Y$, given that the quantitative predictors $(X_1, \ldots, X_k) = (x_1, \ldots, x_k)$, is assumed to be the
\begin{equation*}
N(\beta_1 + \beta_2 x_1 + \cdots + \beta_{k+1} x_k, \sigma^2)
\end{equation*}
distribution, where $\beta_1, \ldots, \beta_{k+1}$ and $\sigma^2$ are unknown. For example, $Y$ could be blood pressure, $X_1$ the amount of daily salt intake, $X_2$ the age of the individual, $X_3$ the weight of the individual, etc.

In this case, the conditional distributions have constant shape and change, as the values of the predictors $(x_1, \ldots, x_k)$ change, only through the conditional mean, which changes according to the function $\beta_1 + \beta_2 x_1 + \cdots + \beta_{k+1} x_k$. Notice that, if this model is correct, then the variables are unrelated if and only if $\beta_2 = \cdots = \beta_{k+1} = 0$, as this is the only situation in which the conditional distributions can remain constant as we change $(x_1, \ldots, x_k)$.
\end{example}

When we split a set of variables $(Y, X_1, \ldots, X_k)$ into response $Y$ and predictors $(X_1, \ldots, X_k)$, we are implicitly saying that we are directly interested only in the conditional distributions of $Y$ given $(X_1, \ldots, X_k)$. There may be relationships among the predictors $(X_1, \ldots, X_k)$, however, and these can be of interest.

For example, suppose we have two predictors $X_1$ and $X_2$, and the conditional distribution of $X_1$ given $X_2$ is virtually degenerate at a value $a + cX_2$ for some constants $a$ and $c$. Then it is not a good idea to include both $X_1$ and $X_2$ in a model, such as that discussed in Example~\ref{ex:10.1.2}, as this can make the analysis very sensitive to small changes in the data. This is known as the problem of \emph{multicollinearity}. The effect of multicollinearity, and how to avoid it, will not be discussed any further in this text. This is, however, a topic of considerable practical importance.

\paragraph{Regression Models}

Suppose that the response $Y$ is quantitative and we have $k$ predictors $(X_1, \ldots, X_k)$. One of the most important simplifying assumptions used in practice is the \emph{regression assumption}, namely, we assume that, as we change $(X_1, \ldots, X_k)$, the only thing that can possibly change about the conditional distribution of $Y$ given $(X_1, \ldots, X_k)$ is the conditional mean $\expc(Y \mid X_1, \ldots, X_k)$. The importance of this assumption is that, to analyze the relationship between $Y$ and $(X_1, \ldots, X_k)$, we now need only consider how $\expc(Y \mid X_1, \ldots, X_k)$ changes as $(X_1, \ldots, X_k)$ changes. Indeed, if $\expc(Y \mid X_1, \ldots, X_k)$ does not change as $(X_1, \ldots, X_k)$ changes, then there is no relationship between $Y$ and the predictors. Of course, this kind of an analysis is dependent on the regression assumption holding, and the methods of Section~\ref{sec:9.1} must be used to check this. Regression models --- namely, statistical models where we make the regression assumption --- are among the most important statistical models used in practice. Sections~\ref{sec:10.3} and~\ref{sec:10.4} discuss several instances of regression models.

Regression models are often presented in the form
\begin{equation}
\label{eq:10.1.1}
Y = \expc(Y \mid X_1, \ldots, X_k) + Z,
\end{equation}
where $Z = Y - \expc(Y \mid X_1, \ldots, X_k)$ is known as the \emph{error term}. We see immediately that, if the regression assumption applies, then the conditional distribution of $Z$ given $(X_1, \ldots, X_k)$ is fixed as we change $(X_1, \ldots, X_k)$, and, conversely, if the conditional distribution of $Z$ given $(X_1, \ldots, X_k)$ is fixed as we change $(X_1, \ldots, X_k)$, then the regression assumption holds. So when the regression assumption applies, \eqref{eq:10.1.1} provides a decomposition of $Y$ into two parts: (1) a part possibly dependent on $(X_1, \ldots, X_k)$, namely, $\expc(Y \mid X_1, \ldots, X_k)$, and (2) a part that is always independent of $(X_1, \ldots, X_k)$, namely, the error $Z$. Note that Examples~\ref{ex:10.1.1} and~\ref{ex:10.1.2} can be written in the form~\eqref{eq:10.1.1}, where $Z \sim N(0, \sigma^2)$.

\subsection{Cause--Effect Relationships and Experiments}
\label{ssec:10.1.2}

Suppose now that we have variables $X$ and $Y$ defined on a population $\Omega$ and have concluded that a relationship exists according to Definition~\ref{def:10.1.1}. This may be based on having conducted a full census of $\Omega$, or, more typically, we will have drawn a simple random sample from $\Omega$ and then used the methods of the remaining sections of this chapter to conclude that such a relationship exists. If $Y$ is playing the role of the response and if $X$ is the predictor, then we often want to be able to assert that changes in $X$ are \emph{causing} the observed changes in the conditional distributions of $Y$. Of course, if there are no changes in the conditional distributions, then there is no relationship between $X$ and $Y$ and hence no cause--effect relationship, either.

For example, suppose that the amount of carbon dioxide gas being released in the atmosphere is increasing, and we observe that mean global temperatures are rising. If we have reason to believe that the amount of carbon dioxide released can have an effect on temperature, then perhaps it is sensible to believe that the increase in carbon dioxide emissions is causing the observed increase in mean global temperatures. As another example, for many years it has been observed that smokers suffer from respiratory diseases much more frequently than do nonsmokers. It seems reasonable, then, to conclude that smoking causes an increased risk for respiratory disease. On the other hand, suppose we consider the relationship between weight and height. It seems clear that a relationship exists, but it does not make any sense to say that changes in one of the variables is causing the changes in the conditional distributions of the other.

\paragraph{Confounding Variables}

When can we say that an observed relationship between $X$ and $Y$ is a cause--effect relationship? If a relationship exists between $X$ and $Y$, then we know that there are at least two values $x_1$ and $x_2$ such that $f_{Y|X=x_1} \neq f_{Y|X=x_2}$, i.e., these two conditional distributions are not equal. If we wish to say that this difference is \emph{caused} by the change in $X$, then we have to know categorically that there is no other variable $Z$ defined on $\Omega$ that \emph{confounds} with $X$. The following example illustrates the idea of two variables confounding.

\begin{example}
\label{ex:10.1.3}
Suppose that $\Omega$ is a population of students such that most females hold a part-time job and most males do not. A researcher is interested in the distribution of grades, as measured by grade point average (GPA), and is looking to see if there is a relationship between GPA and gender. On the basis of the data collected, the researcher observes a difference in the conditional distribution of GPA given gender and concludes that a relationship exists between these variables. It seems clear, however, that an assertion of a cause--effect relationship existing between GPA and gender is not warranted, as the difference in the conditional distributions could also be attributed to the difference in part-time work status rather than gender. In this example, part-time work status and gender are \emph{confounded}.
\end{example}

A more careful analysis might rescue the situation described in Example~\ref{ex:10.1.3}, for if $X$ and $Z$ denote the confounding variables, then we could collect data on $Z$ as well and examine the conditional distributions $f_{Y|X=x,Z=z}$. In Example~\ref{ex:10.1.3}, these will be the conditional distributions of GPA, given gender and part-time work status. If these conditional distributions change as we change $x$ for some fixed value of $z$, then we could assert that a cause--effect relationship exists between $X$ and $Y$, provided there are no further confounding variables. Of course, there are probably still more confounding variables, and we really should be conditioning on all of them. This brings up the point that, in any practical application, we almost certainly will never even know all the potential confounding variables.

\paragraph{Controlling Predictor Variable Assignments}

Fortunately, there is sometimes a way around the difficulties raised by confounding variables. Suppose we can control the value of the variable $X(\omega)$ for any $\omega \in \Omega$, i.e., we can assign the value $x$ to $\omega$ so that $X(\omega) = x$ for any of the possible values of $x$. In Example~\ref{ex:10.1.3}, this would mean that we could assign a part-time work status to any student in the population. Now consider the following idealized situation. Imagine assigning every element $\omega \in \Omega$ the value $X(\omega) = x_1$ and then carrying out a census to obtain the conditional distribution $f_{Y|X=x_1}$. Now imagine assigning every $\omega \in \Omega$ the value $X(\omega) = x_2$ and then carrying out a census to obtain the conditional distribution $f_{Y|X=x_2}$. If there is any difference in $f_{Y|X=x_1}$ and $f_{Y|X=x_2}$, then the only possible reason is that the value of $X$ differs. Therefore, if $f_{Y|X=x_1} \neq f_{Y|X=x_2}$, we can assert that a cause--effect relationship exists.

A difficulty with the above argument is that typically we can never exactly determine $f_{Y|X=x_1}$ and $f_{Y|X=x_2}$. But in fact, we may be able to sample from them; then the methods of statistical inference become available to us to infer whether or not there is any difference. Suppose we take a random sample $\omega_1, \ldots, \omega_{n_1+n_2}$ from $\Omega$ and randomly assign $n_1$ of these the value $X(\omega) = x_1$, with the remaining $\omega$'s assigned the value $x_2$. We obtain the $Y$ values $y_{11}, \ldots, y_{1n_1}$ for those $\omega$'s assigned the value $x_1$ and obtain the $Y$ values $y_{21}, \ldots, y_{2n_2}$ for those $\omega$'s assigned the value $x_2$. Then it is apparent that $y_{11}, \ldots, y_{1n_1}$ is a sample from $f_{Y|X=x_1}$ and $y_{21}, \ldots, y_{2n_2}$ is a sample from $f_{Y|X=x_2}$. In fact, provided that $n_1 + n_2$ is small relative to the population size, then we can consider these as i.i.d.\ samples from these conditional distributions.

So we see that in certain circumstances, it is possible to collect data in such a way that we can make inferences about whether or not a cause--effect relationship exists. We now specify the characteristics of the relevant data collection technique.

\paragraph{Conditions for Cause--Effect Relationships}

First, if our inferences are to apply to a population $\Omega$, then we must have a random sample from that population. This is just the characteristic of what we called a \emph{sampling study} in Section~\ref{sec:5.4}, and we must do this to avoid any selection effects. So if the purpose of a study is to examine the relationship between the duration of migraine headaches and the dosage of a certain drug, the investigator must have a random sample from the population of migraine headache sufferers.

Second, we must be able to assign any possible value of the predictor variable $X$ to any selected $\omega$. If we cannot do this, or do not do this, then there may be hidden confounding variables (sometimes called \emph{lurking variables}) that are influencing the conditional distributions of $Y$. So in a study of the effects of the dosage of a drug on migraine headaches, the investigator must be able to impose the dosage on each participant in the study.

Third, after deciding what values of $X$ we will use in our study, we must randomly allocate these values to members of the sample. This is done to avoid the possibility of selection effects. So, after deciding what dosages to use in the study of the effects of the dosage of a drug on migraine headaches, and how many participants will receive each dosage, the investigator must randomly select the individuals who will receive each dosage. This will (hopefully) avoid selection effects, such as only the healthiest individuals getting the lowest dosage, etc.

When these requirements are met, we refer to the data collection process as an \emph{experiment}. Statistical inference based on data collected via an experiment has the capability of inferring that cause--effect relationships exist, so this represents an important and powerful scientific tool.

\paragraph{A Hierarchy of Studies}

Combining this discussion with Section~\ref{sec:5.4}, we see a hierarchy of data collection methods. Observational studies reside at the bottom of the hierarchy. Inferences drawn from observational studies must be taken with a degree of caution, for selection effects could mean that the results do not apply to the population intended, and the existence of confounding variables means that we cannot make inferences about cause--effect relationships. For sampling studies, we know that any inferences drawn will be about the appropriate population; but the existence of confounding variables again causes difficulties for any statements about the existence of cause--effect relationships, e.g., just taking random samples of males and females from the population of Example~\ref{ex:10.1.3} will not avoid the confounding variables. At the top of the hierarchy reside experiments.

It is probably apparent that it is often impossible to conduct an experiment. In Example~\ref{ex:10.1.3}, we cannot assign the value of gender, so nothing can be said about the existence of a cause--effect relationship between GPA and gender.

There are many notorious examples in which assertions are made about the existence of cause--effect relationships but for which no experiment is possible. For example, there have been a number of studies conducted where differences have been noted among the IQ distributions of various racial groups. It is impossible, however, to control the variable racial origin, so it is impossible to assert that the observed differences in the conditional distributions of IQ, given race, are caused by changes in race.

Another example concerns smoking and lung cancer in humans. It has been pointed out that it is impossible to conduct an experiment, as we cannot assign values of the predictor variable (perhaps different amounts of smoking) to humans at birth and then observe the response, namely, whether someone contracts lung cancer or not. This raises an important point. We do not simply reject the results of analyses based on observational studies or sampling studies because the data did not arise from an experiment. Rather, we treat these as evidence --- potentially flawed evidence, but still evidence.

Think of eyewitness evidence in a court of law suggesting that a crime was committed by a certain individual. Eyewitness evidence may be unreliable, but if two or three unconnected eyewitnesses give similar reports, then our confidence grows in the reliability of the evidence. Similarly, if many observational and sampling studies seem to indicate that smoking leads to an increased risk for contracting lung cancer, then our confidence grows that a cause--effect relationship does indeed exist. Furthermore, if we can identify potentially confounding variables, then observational or sampling studies can be conducted taking these into account, increasing our confidence still more. Ultimately, we may not be able to definitively settle the issue via an experiment, but it is still possible to build overwhelming evidence that smoking and lung cancer do have a cause--effect relationship.

\subsection{Design of Experiments}
\label{ssec:10.1.3}

Suppose we have a response $Y$ and a predictor $X$ (sometimes called a \emph{factor} in experimental contexts) defined on a population $\Omega$, and we want to collect data to determine whether a cause--effect relationship exists between them. Following the discussion in Section~\ref{ssec:10.1.1}, we will conduct an experiment. There are now a number of decisions to be made, and our choices constitute what we call the \emph{design of the experiment}.

For example, we are going to assign values of $X$ to the sampled elements, now called \emph{experimental units}, $\omega_1, \ldots, \omega_n$ from $\Omega$. Which of the possible values of $X$ should we use? When $X$ can take only a small finite number of values, then it is natural to use these values. On the other hand, when the number of possible values of $X$ is very large or even infinite, as with quantitative predictors, then we have to choose values of $X$ to use in the experiment.

Suppose we have chosen the values $x_1, \ldots, x_k$ for $X$. We refer to $x_1, \ldots, x_k$ as the \emph{levels} of $X$; any particular assignment $x_i$ to a $\omega_j$ in the sample will be called a \emph{treatment}. Typically, we will choose the levels so that they span the possible range of $X$ fairly uniformly. For example, if $X$ is temperature in degrees Celsius, and we want to examine the relationship between $Y$ and $X$ for $X$ in the range $[0, 100]$, then, using $k = 5$ levels, we might take $x_1 = 0$, $x_2 = 25$, $x_3 = 50$, $x_4 = 75$, and $x_5 = 100$.

Having chosen the levels of $X$, we then have to choose how many treatments of each level we are going to use in the experiment, i.e., decide how many response values $n_i$ we are going to observe at level $x_i$ for $i = 1, \ldots, k$.

In any experiment, we will have a finite amount of resources (money, time, etc.)\ at our disposal, which determines the sample size $n$ from $\Omega$. The question then is how should we choose the $n_i$ so that $n_1 + \cdots + n_k = n$? If we know nothing about the conditional distributions $f_{Y|X=x_i}$, then it makes sense to use \emph{balance}, namely, choose $n_1 = \cdots = n_k$.

On the other hand, suppose we know that some of the $f_{Y|X=x_i}$ will exhibit greater variability than others. For example, we might measure variability by the variance of $f_{Y|X=x_i}$. Then it makes sense to allocate more treatments to the levels of $X$ where the response is more variable. This is because it will take more observations to make accurate inferences about characteristics of such an $f_{Y|X=x_i}$ than for the less variable conditional distributions.

As discussed in Sections~\ref{ssec:6.3.4} and~\ref{ssec:6.3.5}, we also want to choose the $n_i$ so that any inferences we make have desired accuracy. Methods for choosing the sample sizes $n_i$, similar to those discussed in Chapter~\ref{ch:7}, have been developed for these more complicated designs, but we will not discuss these any further here.

Suppose, then, that we have determined $(x_1, n_1), \ldots, (x_k, n_k)$. We refer to this set of ordered pairs as the \emph{experimental design}.

Consider some examples.

\begin{example}
\label{ex:10.1.4}
Suppose that $\Omega$ is a population of students at a given university. The administration is concerned with determining the value of each student being assigned an academic advisor. The response variable $Y$ will be a rating that a student assigns on a scale of 1 to 10 (completely dissatisfied to completely satisfied with their university experience) at the end of a given semester. We treat $Y$ as a quantitative variable. A random sample of $n = 100$ students is selected from $\Omega$, and 50 of these are randomly selected to receive advisers while the remaining 50 are not assigned advisers.

Here, the predictor $X$ is a categorical variable that indicates whether or not the student has an advisor. There are only $k = 2$ levels, and both are used in the experiment. If $x_1 = 0$ denotes no advisor and $x_2 = 1$ denotes having an advisor, then $n_1 = n_2 = 50$, and we have a balanced experiment. The experimental design is given by
\begin{equation*}
\{(0, 50), (1, 50)\}.
\end{equation*}

At the end of the experiment, we want to use the data to make inferences about the conditional distributions $f_{Y|X=0}$ and $f_{Y|X=1}$ to determine whether a cause--effect relationship exists. The methods of Section~\ref{sec:10.4} will be relevant for this.
\end{example}

\begin{example}
\label{ex:10.1.5}
Suppose that $\Omega$ is a population of dairy cows. A feed company is concerned with the relationship between weight gain, measured in kilograms, over a specific time period and the amount of a supplement, measured in grams/liter, of an additive put into the cows' feed. Here, the response $Y$ is the weight gain --- a quantitative variable. The predictor $X$ is the concentration of the additive. Suppose $X$ can plausibly range between 0 and 2, so it is also a quantitative variable.

The experimenter decides to use $k = 4$ levels with $x_1 = 0.00$, $x_2 = 0.66$, $x_3 = 1.32$, and $x_4 = 2.00$. Further, the sample sizes $n_1 = n_2 = n_3 = n_4 = 10$ were determined to be appropriate. So the balanced experimental design is given by
\begin{equation*}
\{(0.00, 10), (0.66, 10), (1.32, 10), (2.00, 10)\}.
\end{equation*}

At the end of the experiment, we want to make inferences about the conditional distributions $f_{Y|X=0.00}$, $f_{Y|X=0.66}$, $f_{Y|X=1.32}$, and $f_{Y|X=2.00}$. The methods of Section~\ref{sec:10.3} are relevant for this.
\end{example}

\paragraph{Control Treatment, the Placebo Effect, and Blinding}

Notice that in Example~\ref{ex:10.1.5}, we included the level $X = 0$, which corresponds to no application of the additive. This is called a \emph{control treatment}, as it gives a baseline against which we can assess the effect of the predictor. In many experiments, it is important to include a control treatment.

In medical experiments, there is often a \emph{placebo effect} --- that is, a disease sufferer given any treatment will often record an improvement in symptoms. The placebo effect is believed to be due to the fact that a sufferer will start to feel better simply because someone is paying attention to the condition. Accordingly, in any experiment to determine the efficacy of a drug in alleviating disease symptoms, it is important that a control treatment be used as well. For example, if we want to investigate whether or not a given drug alleviates migraine headaches, then among the dosages we select for the experiment, we should make sure that we include a pill containing none of the drug (the so-called sugar pill); that way we can assess the extent of the placebo effect. Of course, the recipients should not know whether they are receiving the sugar pill or the drug. This is called a \emph{blind experiment}. If we also conceal the identity of the treatment from the experimenters, so as to avoid any biasing of the results on their part, then this is known as a \emph{double-blind experiment}.

In Example~\ref{ex:10.1.5}, we assumed that it is possible to take a sample from the population of all dairy cows. Strictly speaking, this is necessary if we want to avoid selection effects and make sure that our inferences apply to the population of interest. In practice, however, taking a sample of experimental units from the full population of interest is often not feasible. For example, many medical experiments are conducted on animals, and these are definitely not random samples from the population of the particular animal in question, e.g., rats.

In such cases, however, we simply recognize the possibility that selection effects or lurking variables could render invalid the conclusions drawn from such analyses when they are to be applied to the population of interest. But we still regard the results as evidence concerning the phenomenon under study. It is the job of the experimenter to come as close as possible to the idealized situation specified by a valid experiment; for example, randomization is still employed when assigning treatments to experimental units so that selection effects are avoided as much as possible.

\paragraph{Interactions}

In the experiments we have discussed so far, there has been one predictor. In many practical contexts, there is more than one predictor. Suppose, then, that there are two predictors $X$ and $W$, and that we have decided on the levels $x_1, \ldots, x_k$ for $X$ and the levels $\omega_1, \ldots, \omega_l$ for $W$. One possibility is to look at the conditional distributions $f_{Y|X=x_i}$ for $i = 1, \ldots, k$ and $f_{Y|W=\omega_j}$ for $j = 1, \ldots, l$ to determine whether $X$ and $W$ individually have a relationship with the response $Y$. Such an approach, however, ignores the effect of the two predictors together. In particular, the way the conditional distributions $f_{Y|X=x,W=\omega}$ change as we change $x$ may depend on $\omega$; when this is the case, we say that there is an \emph{interaction} between the predictors.

To investigate the possibility of an interaction existing between $X$ and $W$, we must sample from each of the $kl$ distributions $f_{Y|X=x_i,W=\omega_j}$ for $i = 1, \ldots, k$ and $j = 1, \ldots, l$. The experimental design then takes the form
\begin{equation*}
\{((x_1, \omega_1), n_{11}), ((x_2, \omega_1), n_{21}), \ldots, ((x_k, \omega_l), n_{kl})\},
\end{equation*}
where $n_{ij}$ gives the number of applications of the treatment $(x_i, \omega_j)$. We say that the two predictors $X$ and $W$ are \emph{completely crossed} in such a design because each value of $X$ used in the experiment occurs with each value of $W$ used in the experiment.

Of course, we can extend this discussion to the case where there are more than two predictors. We will discuss in Section~\ref{ssec:10.4.3} how to analyze data to determine whether there are any interactions between predictors.

\begin{example}
\label{ex:10.1.6}
Suppose we have a population of students at a particular university and are investigating the relationship between the response $Y$ given by a student's grade in calculus, and the predictors $W$ and $X$. The predictor $W$ is the number of hours of academic advising given monthly to a student; it can take the values 0, 1, or 2. The predictor $X$ indicates class size, where $X = 0$ indicates small class size and $X = 1$ indicates large class size. So we have a quantitative response $Y$, a quantitative predictor $W$ taking three values, and a categorical predictor $X$ taking two values. The crossed values of the predictors $(W, X)$ are given by the set
\begin{equation*}
\{(0, 0), (1, 0), (2, 0), (0, 1), (1, 1), (2, 1)\},
\end{equation*}
so there are six treatments. To conduct the experiment, the university then takes a random sample of $6n$ students and randomly assigns $n$ students to each treatment.
\end{example}

Sometimes we include additional predictors in an experimental design even when we are not primarily interested in their effects on the response $Y$. We do this because we know that such a variable has a relationship with $Y$. Including such predictors allows us to condition on their values and so investigate more precisely the relationship $Y$ has with the remaining predictors. We refer to such a variable as a \emph{blocking variable}.

\begin{example}
\label{ex:10.1.7}
Suppose the response variable $Y$ is yield of wheat in bushels per acre, and the predictor variable $X$ is an indicator variable for which of three types of wheat is being planted in an agricultural study. Each type of wheat is going to be planted on a plot of land, where all the plots are of the same size, but it is known that the plots used in the experiment will vary considerably with respect to their fertility. Note that such an experiment is another example of a situation in which it is impossible to randomly sample the experimental units (the plots) from the full population of experimental units.

Suppose the experimenter can group the available experimental units into plots of low fertility and high fertility. We call these two classes of fields \emph{blocks}. Let $W$ indicate the type of plot. So $W$ is a categorical variable taking two values. It then seems clear that the conditional distributions $f_{Y|X=x,W=\omega}$ will be much less variable than the conditional distributions $f_{Y|X=x}$.

In this case, $W$ is serving as a blocking variable. The experimental units in a particular block, the one of low fertility or the one of high fertility, are more homogeneous than the full set of plots, so variability will be reduced and inferences will be more accurate.
\end{example}

\paragraph{Summary of Section~\ref{sec:10.1}}
\begin{itemize}
\item We say two variables are related if the conditional distribution of one given the other changes at all, as we change the value of the conditioning variable.
\item To conclude that a relationship between two variables is a cause--effect relationship, we must make sure that (through conditioning) we have taken account of all confounding variables.
\item Statistics provides a practical way of avoiding the effects of confounding variables via conducting an experiment. For this, we must be able to assign the values of the predictor variable to experimental units sampled from the population of interest.
\item The design of experiments is concerned with determining methods of collecting the data so that the analysis of the data will lead to accurate inferences concerning questions of interest.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:10.1.1}
Prove that discrete random variables $X$ and $Y$ are unrelated if and only if $X$ and $Y$ are independent.
\end{exercise}

\begin{solution}
From the definitions we know that if the conditional distribution of $Y$ given $X$ does not change as we change $X$, then $X$ and $Y$ are unrelated and then for any $x_1, x_2$ (that occur with positive probability) and $y$ we have $\prb(Y = y \mid X = x_1) = \prb(Y = y \mid X = x_2)$. Hence,
\[
    \frac{\prb(X = x_1, Y = y)}{\prb(X = x_1)} = \frac{\prb(X = x_2, Y = y)}{\prb(X = x_2)}
\]
so $\prb(X = x_1, Y = y) = \prb(X = x_2, Y = y)\prb(X = x_1)/\prb(X = x_2)$. Summing this over $x_1$ leads to $\prb(X = x_2, Y = y) = \prb(X = x_2)\prb(Y = y)$, and this implies that $X$ and $Y$ are statistically independent. Conversely, if $X$ and $Y$ are statistically independent, then for all $x$ and $y$ we have
\[
    \prb(Y = y \mid X = x) = \frac{\prb(X = x, Y = y)}{\prb(X = x)} = \frac{\prb(X = x)\prb(Y = y)}{\prb(X = x)} = \prb(Y = y),
\]
so the conditional distribution of $Y$ given $X$ does not change as we change $X$, and therefore $X$ and $Y$ are unrelated.
\end{solution}

\begin{exercise}
\label{exer:10.1.2}
Suppose that two variables $X$ and $Y$ defined on a finite population $\Omega$ are functionally related as $Y = g(X)$ for some unknown nonconstant function $g$. Explain how this situation is covered by Definition~\ref{def:10.1.1}, i.e., the definition will lead us to conclude that $X$ and $Y$ are related. What about the situation in which $g(x) = c$ for some value $c$ for every $x$? (Hint: Use the relative frequency functions of the variables.)
\end{exercise}

\begin{solution}
Suppose there exists $x_1 \neq x_2$ such that $g(x_1) \neq g(x_2)$ and $f_X(x_1) \neq 0$, $f_X(x_2) \neq 0$, where $f_X$ is the relative frequency function for $X$. Then we must have
\[
    f_Y(y \mid X = x_i) = \frac{f_{X,Y}(x_i, y)}{f_X(x_i)} = \begin{cases} 0 & y \neq g(x_i) \\ 1 & y = g(x_i) \end{cases}
\]
for $i = 1, 2$. Since $g(x_1) \neq g(x_2)$, this implies that the conditional distribution of $Y$ changes as we change $X$, and therefore $X$ and $Y$ are related.

If $g(x) = c$ for all $x$, we have $g(x_1) = g(x_2) = c$, so $f_Y(y \mid X = x_1) = f_Y(y \mid X = x_2)$ for all $y$, i.e., the conditional distribution of $Y$ given $X$ does not change as we change $x$ and so they are not related.
\end{solution}

\begin{exercise}
\label{exer:10.1.3}
Suppose that a census is conducted on a population $\Omega$ and the joint distribution of $(X, Y)$ is obtained as in the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = 1$ & $Y = 2$ & $Y = 3$ \\ \hline
$X = 1$ & 0.15 & 0.18 & 0.40 \\
$X = 2$ & 0.12 & 0.09 & 0.06
\end{tabular}
\end{center}
Determine whether or not a relationship exists between $Y$ and $X$.
\end{exercise}

\begin{solution}
To check whether or not a relationship exists between $Y$ and $X$ we calculate the conditional distributions of $Y$ given $X$. These are given in the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = 1$ & $Y = 2$ & $Y = 3$ \\
\hline
$X = 1$ & $0.15/0.73 = .20548$ & $0.18/0.73 = .24658$ & $0.40/0.73 = .54795$ \\
$X = 2$ & $0.12/0.27 = .44444$ & $0.09/0.27 = .33333$ & $0.06/0.27 = .22222$
\end{tabular}
\end{center}
The conditional distribution of $Y$ given $X = x$ does change as we change $x$, so we conclude that $X$ and $Y$ are related.
\end{solution}

\begin{exercise}
\label{exer:10.1.4}
Suppose that a census is conducted on a population $\Omega$ and the joint distribution of $(X, Y)$ is obtained as in the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = 1$ & $Y = 2$ & $Y = 3$ \\ \hline
$X = 1$ & $1/6$ & $1/6$ & $1/3$ \\
$X = 2$ & $1/12$ & $1/12$ & $1/6$
\end{tabular}
\end{center}
Determine whether or not a relationship exists between $Y$ and $X$.
\end{exercise}

\begin{solution}
To check whether or not a relationship exists between $Y$ and $X$ we calculate the conditional distribution of $Y$ given $X$. These are given in the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = 1$ & $Y = 2$ & $Y = 3$ \\
\hline
$X = 1$ & $\frac{1/6}{2/3} = \frac{1}{4}$ & $\frac{1/6}{2/3} = \frac{1}{4}$ & $\frac{1/3}{2/3} = \frac{1}{2}$ \\[6pt]
$X = 2$ & $\frac{1/12}{1/3} = \frac{1}{4}$ & $\frac{1/12}{1/3} = \frac{1}{4}$ & $\frac{1/6}{1/3} = \frac{1}{2}$
\end{tabular}
\end{center}
As we can see, the conditional distribution of $Y$ given $X = x$ does not change at all as we change $x$, so we conclude that $X$ and $Y$ are unrelated.
\end{solution}

\begin{exercise}
\label{exer:10.1.5}
Suppose that $X$ is a random variable and $Y = X^2$. Determine whether or not $X$ and $Y$ are related. What happens when $X$ has a degenerate distribution?
\end{exercise}

\begin{solution}
Suppose that $\prb(X = x) > 0$. We have that
\[
    \prb(Y = y \mid X = x) = \frac{\prb(X = x, X^2 = y)}{\prb(X = x)} = \begin{cases} 0 & y \neq x^2 \\ 1 & y = x^2 \end{cases}
\]
and so the conditional distributions will change with $x$ whenever $X$ is not degenerate.
\end{solution}

\begin{exercise}
\label{exer:10.1.6}
Suppose a researcher wants to investigate the relationship between birth weight and performance on a standardized test administered to children at two years of age. If a relationship is found, can this be claimed to be a cause--effect relationship? Explain why or why not?
\end{exercise}

\begin{solution}
This cannot be claimed to be a cause-effect relationship because we cannot assign the value birth-weight at birth.
\end{solution}

\begin{exercise}
\label{exer:10.1.7}
Suppose a large study of all doctors in Canada was undertaken to determine the relationship between various lifestyle choices and lifelength. If the conditional distribution of lifelength given various smoking habits changes, then discuss what can be concluded from this study.
\end{exercise}

\begin{solution}
If the conditional distribution of life-length given various smoking habits changes, then we can conclude that these two variables are related. However, we cannot assign the value of smoking habit (perhaps different amount of smoking), and there might be many other confounding variables that should be taken into account, e.g., exercise habits, eating habits, sleeping habits, etc. So we cannot conclude that this relationship is a cause-effect relationship.
\end{solution}

\begin{exercise}
\label{exer:10.1.8}
Suppose a teacher wanted to determine whether an open- or closed-book exam was a more appropriate way to test students on a particular topic. The response variable is the grade obtained on the exam out of 100. Discuss how the teacher could go about answering this question.
\end{exercise}

\begin{solution}
The teacher should conduct an experiment in which a random sample is drawn from the population of students. Then half of this sample should be randomly selected to write the exam with open book, while the other half writes it with closed book. Then a comparison should be made of the conditional distributions of the response variable $Y$ (the grade obtained) given the predictor $X$ (closed or open book) using the samples to make inference about these distributions.
\end{solution}

\begin{exercise}
\label{exer:10.1.9}
Suppose a researcher wanted to determine whether or not there is a cause--effect relationship between the type of political ad (negative or positive) seen by a voter from a particular population and the way the voter votes. Discuss your advice to the researcher about how best to conduct the study.
\end{exercise}

\begin{solution}
The researcher should draw a random sample from the population of voters and ask them to measure their attitude towards a particular political party on a scale from favorably disposed to unfavorably disposed. Then the researcher should randomly select half of this sample to be exposed to a negative ad (an ad that points out various negative attributes about the opponents), while the other half is exposed to a positive ad (one that points out various positive attributes of the party). They should all then be asked to measure their attitude towards the particular political party on the same scale. Then compare the conditional distribution of the response variable $Y$ (the change in attitude from before seeing the ad to after) given the predictor $X$ (type of ad exposed to) using the samples to make inference about these distributions.
\end{solution}

\begin{exercise}
\label{exer:10.1.10}
If two random variables have a nonzero correlation, are they necessarily related? Explain why or why not.
\end{exercise}

\begin{solution}
Recall that the correlation of any two random variables is non-zero only if the covariance of them is non-zero. This immediately implies that the two variables are not independent, else $\cov(X, Y) = 0$. Therefore, the two variables are related.
\end{solution}

\begin{exercise}
\label{exer:10.1.11}
An experimenter wants to determine the relationship between weight change $Y$ over a specified period and the use of a specially designed diet. The predictor variable $X$ is a categorical variable indicating whether or not a person is on the diet. A total of 200 volunteers signed on for the study; a random selection of 100 of these were given the diet and the remaining 100 continued their usual diet.
\begin{enumerate}[(a)]
\item Record the experimental design.
\item If the results of the study are to be applied to the population of all humans, what concerns do you have about how the study was conducted?
\item It is felt that the amount of weight lost or gained also is dependent on the initial weight $W$ of a participant. How would you propose that the experiment be altered to take this into account?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First, let $x_1 = 0$ denote usual diet and $x_2 = 1$ denote new diet. The experimental design is given by $\{(0, 100), (1, 100)\}$.
    \item There are several concerns about the conduct of this study. First, we have not taken a sample from the population of interest. The individuals involved in the study have volunteered and, as a group, they might be very different from the full population, e.g., in their ability to stick to the diet. Second, the sample size might be too small relative to the population size, so inference may be inconclusive.
    \item We should group the individuals according to their initial weight $W$ into homogeneous groups (blocks) and then randomly apply the treatments to the individuals in each block and compare the conditional distribution of the response given the two predictors, type of diet and initial weight. This will make the comparisons more accurate by reducing variability.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.1.12}
A study will be conducted, involving the population of people aged 15 to 19 in a particular country, to determine whether a relationship exists between the response $Y$ (amount spent in dollars in a week on music downloads) and the predictors $W$ (gender) and $X$ (age in years).
\begin{enumerate}[(a)]
\item If observations are to be taken from every possible conditional distribution of $Y$ given the two factors, then how many such conditional distributions are there?
\item Identify the types of each variable involved in the study.
\item Suppose there are enough funds available to monitor 2000 members of the population. How would you recommend that these resources be allocated among the various combinations of factors?
\item If a relationship is found between the response and the predictors, can this be claimed to be a cause--effect relationship? Explain why or why not.
\item Suppose that in addition, it was believed that family income would likely have an effect on $Y$ and that families could be classified into low and high income. Indicate how you would modify the study to take this into account.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item There are 10 conditional distributions since the factor $W$ has 2 levels and the factor $X$ has 5 levels and so there $5(2) = 10$ combinations.
    \item The predictor variable $W$ (gender) is a categorical variable, while both the response variable $Y$ and the predictor variable $X$ (age in years) are quantitative variables.
    \item To have a balanced design we should allocate 200 individuals to each combination of the factors.
    \item A relationship between the response and the predictors cannot be claimed to be a cause-effect relationship since we cannot assign the values of the predictor variables.
    \item We should use family income as a blocking variable having, say, two levels, namely low and high. Then look at the conditional distributions of the response given the blocking variable and the two predictors.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.1.13}
A random sample of 100 households, from the set of all households containing two or more members in a given geographical area, is selected and their television viewing habits are monitored for six months. A random selection of 50 of the households is sent a brochure each week advertising a certain program. The purpose of the study is to determine whether there is any relationship between exposure to the brochure and whether or not this program is watched.
\begin{enumerate}[(a)]
\item Identify suitable response and predictor variables.
\item If a relationship is found, can this be claimed to be a cause--effect relationship? Explain why or why not.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The response variable could be the number of times an individual has watched the program. A suitable predictor variable is whether or not they received the brochure.
    \item Yes as we have controlled the assignment of the predictor variable.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.1.14}
Suppose we have a quantitative response variable $Y$ and two categorical predictor variables $W$ and $X$, both taking values in $\{0, 1\}$. Suppose the conditional distributions of $Y$ are given by
\begin{align*}
(Y \mid W = 0, X = 0) &\sim N(3, 5), \\
(Y \mid W = 1, X = 0) &\sim N(3, 5), \\
(Y \mid W = 0, X = 1) &\sim N(4, 5), \\
(Y \mid W = 1, X = 1) &\sim N(4, 5).
\end{align*}
Does $W$ have a relationship with $Y$? Does $X$ have a relationship with $Y$? Explain your answers.
\end{exercise}

\begin{solution}
Given a fixed value of $X$, the conditional distribution of $Y$ given $W$ and $X$ does not change as $W$ changes and is given by the $N(3, 5)$ distribution for $X = 0$ and the $N(4, 5)$ distribution for $X = 1$. Therefore, we can conclude that $W$ does not have a relationship with $Y$. However, for a fixed value of $W$, the conditional distribution of $Y$ given $W$ and $X$ changes as $X$ changes from the $N(3, 5)$ distribution for $X = 0$ to the $N(4, 5)$ distribution for $X = 1$. Therefore, we can conclude that $X$ does have a relationship with $Y$.
\end{solution}

\begin{exercise}
\label{exer:10.1.15}
Suppose we have a quantitative response variable $Y$ and two categorical predictor variables $W$ and $X$, both taking values in $\{0, 1\}$. Suppose the conditional distributions of $Y$ are given by
\begin{align*}
(Y \mid W = 0, X = 0) &\sim N(2, 5), \\
(Y \mid W = 1, X = 0) &\sim N(3, 5), \\
(Y \mid W = 0, X = 1) &\sim N(4, 5), \\
(Y \mid W = 1, X = 1) &\sim N(4, 5).
\end{align*}
Does $W$ have a relationship with $Y$? Does $X$ have a relationship with $Y$? Explain your answers.
\end{exercise}

\begin{solution}
Given the value $X = 1$, the conditional distribution of $Y$ given $W$ and $X$ does not change as $W$ changes and is given by the $N(4, 5)$ distribution. While given the value $X = 0$, the conditional distribution of $Y$ given $W$ and $X$ changes as $W$ changes from the $N(2, 5)$ distribution for $W = 0$ to the $N(3, 5)$ distribution for $W = 1$. Therefore, we conclude that $W$ does have a relationship with $Y$. Now, the conditional distribution of $Y$ given $W$ and $X$ changes as $X$ changes, for a fixed value of $W$, and we can conclude that $X$ does have a relationship with $Y$.
\end{solution}

\begin{exercise}
\label{exer:10.1.16}
Do the predictors interact in Exercise~\ref{exer:10.1.14}? Do the predictors interact in Exercise~\ref{exer:10.1.15}? Explain your answers.
\end{exercise}

\begin{solution}
In Exercise~10.1.14 the predictors do not interact since the changes in the conditional distribution of $Y$ given $W$ and $X$, as we change $X$, does not depend on the value of $W$. While in Exercise~10.1.15 the changes in the conditional distribution of $Y$ given $W$ and $X$, as we change $W$, depend on the value of $X$, so the predictors interact.
\end{solution}

\begin{exercise}
\label{exer:10.1.17}
Suppose we have variables $X$ and $Y$ defined on the population $\Omega = \{1, 2, \ldots, 10\}$, where $X(i) = 1$ when $i$ is odd and $X(i) = 0$ when $i$ is even, $Y(i) = 1$ when $i$ is divisible by 3 and $Y(i) = 0$ otherwise.
\begin{enumerate}[(a)]
\item Determine the relative frequency function of $X$.
\item Determine the relative frequency function of $Y$.
\item Determine the joint relative frequency function of $(X, Y)$.
\item Determine all the conditional distributions of $Y$ given $X$.
\item Are $X$ and $Y$ related? Justify your answer.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $X(i) = 1$ for $i \in \{1, 3, 5, 7, 9\}$ and $X(i) = 0$ for $i \in \{2, 4, 6, 8, 10\}$. Hence, the relative frequencies are
    \begin{center}
    \begin{tabular}{c|cc|c}
     & $X = 0$ & $X = 1$ & sum \\
    \hline
    Rel.\ Freq. & 0.5 & 0.5 & 1.0
    \end{tabular}
    \end{center}
    \item $Y(i) = 1$ for $i \in \{3, 6, 9\}$ and $Y(i) = 0$ for $i \in \{1, 2, 4, 5, 7, 8, 10\}$. Hence,
    \begin{center}
    \begin{tabular}{c|cc|c}
     & $Y = 0$ & $Y = 1$ & sum \\
    \hline
    Rel.\ Freq. & 0.7 & 0.3 & 1.0
    \end{tabular}
    \end{center}
    \item There are four possible pairs $(X, Y)$. The relative frequency table is given by
    \begin{center}
    \begin{tabular}{c|cc|c}
    Rel.\ Freq. & $X = 0$ & $X = 1$ & sum \\
    \hline
    $Y = 0$ & 0.3 & 0.4 & 0.7 \\
    $Y = 1$ & 0.2 & 0.1 & 0.3 \\
    \hline
    sum & 0.5 & 0.5 & 1.0
    \end{tabular}
    \end{center}
    \item The conditional probability table is as follows.
    \begin{center}
    \begin{tabular}{c|cc|c}
    $\prb(Y = y \mid X = x)$ & $y = 0$ & $y = 1$ & sum \\
    \hline
    $x = 0$ & 0.6 & 0.4 & 1.0 \\
    $x = 1$ & 0.8 & 0.2 & 1.0
    \end{tabular}
    \end{center}
    \item The conditional distribution of $Y$ given $X$ varies as $X$ varies. For example, $\prb(Y = 0 \mid X = 0) = 0.6 \neq 0.8 = \prb(Y = 0 \mid X = 1)$. Thus, $X$ and $Y$ are related.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.1.18}
A mathematical approach to examining the relationship between variables $X$ and $Y$ is to see whether there is a function $g$ such that $Y = g(X)$. Explain why this approach does not work for many practical applications where we are examining the relationship between variables. Explain how statistics treats this problem.
\end{exercise}

\begin{solution}
If there is exact relationship between $X$ and $Y$, then finding a function $g$ such that $Y = g(X)$ may not be a bad idea. However, there is no such $g$ in most practical problems. In most cases, the responses, $Y$, are not unique even though the predictor values, $X$, are the same because of variation. For example, a study on the relationship between blood pressure and age. Blood pressures of the same aged people are not the same. Even though there is a certain relationship between responses and predictors, responses may not be determined by only predictors in most practical problems. So, we must take into account this variability of responses when looking for relationships among variables.
\end{solution}

\begin{exercise}
\label{exer:10.1.19}
Suppose a variable $X$ takes the values 1 and 2 on a population $\Omega$ and the conditional distributions of $Y$ given $X$ are $N(0, 5)$ when $X = 1$ and $N(0, 7)$ when $X = 2$. Determine whether $X$ and $Y$ are related and if so, describe their relationship.
\end{exercise}

\begin{solution}
The distribution of $Y$ given $X = x$ is not the same when $x$ changes from 1 to 2. Thus, $X$ and $Y$ are related. We see that only the variance of the conditional distribution changes as we change $X$.
\end{solution}

\begin{exercise}
\label{exer:10.1.20}
A variable $Y$ has conditional distribution given $X$ specified by $N(1 + 2x, x)$ when $X = x$. Determine if $X$ and $Y$ are related and if so, describe what their relationship is.
\end{exercise}

\begin{solution}
The conditional distribution of $Y$ given $X = x$ changes as $x$ changes. Thus, $X$ and $Y$ are related. Both the mean and variance of the conditional distributions change as we change $X$ but the distribution is always normal.
\end{solution}

\begin{exercise}
\label{exer:10.1.21}
Suppose that $X \sim \text{Uniform}[-1, 1]$ and $Y = X^2$. Determine the correlation between $Y$ and $X$. Are $X$ and $Y$ related?
\end{exercise}

\begin{solution}
The correlation is given by $\cov(X, Y) = \expc(XY) - \expc(X)\expc(Y) = \expc(X^3) - \expc(X)\expc(X^2) = 0$ since $\expc(X^k) = 0$ for positive odd integer $k$ because $X$ is symmetric. Even though the correlation between $Y$ and $X$ is 0, there is a definite relationship, namely, $Y = X^2$. Note that the conditional distribution of $Y$ given $X = x$ puts $1/2$ the probability at $x$ and $1/2$ the probability at $-x$ and so the conditional distributions change with $X$. Thus, $X$ and $Y$ are related.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:10.1.22}
If there is more than one predictor involved in an experiment, do you think it is preferable for the predictors to interact or not? Explain your answer. Can the experimenter control whether or not predictors interact?
\end{exercise}

\begin{solution}
The situation is somewhat simpler when the predictors do not interact because we can ignore the other predictor when studying the effects of changing just one predictor as the change is the same no matter what value the other predictor takes. Typically, the experimenter cannot control whether or not the predictors interact.
\end{solution}

\begin{exercise}
\label{exer:10.1.23}
Prove directly, using Definition~\ref{def:10.1.1}, that when $X$ and $Y$ are related variables defined on a finite population $\Omega$, then $Y$ and $X$ are also related.
\end{exercise}

\begin{solution}
If $X$ and $Y$ are related, then there exist $x_1, x_2, y$ such that $f_{Y|X}(y \mid x_1) \neq f_{Y|X}(y \mid x_2)$. Now suppose that $f_{X|Y}(x \mid y) = f_{X|Y}(x \mid y')$ for all $x, y, y'$, i.e., that the conditional distribution of $X$ given $Y$ does not change as we change $Y$. Then $f_{X|Y}(x \mid y) = f_{X,Y}(x, y)/f_Y(y) = f_{X,Y}(x, y')/f_Y(y') = f_{X|Y}(x \mid y')$ which implies $f_{X,Y}(x, y) = (f_{X,Y}(x, y')/f_Y(y'))f_Y(y)$ for all $x, y, y'$, which in turn implies
\[
    f_{X,Y}(x, y) = \sum_{y'} f_{X,Y}(x, y) f_Y(y') = \sum_{y'} \frac{f_{X,Y}(x, y')}{f_Y(y')} f_Y(y) f_Y(y') = \left(\sum_{y'} f_{X,Y}(x, y')\right) f_Y(y) = f_X(x) f_Y(y)
\]
for every $x, y$. But this implies $f_{Y|X}(y \mid x_1) = f_X(x_1) f_Y(y)/f_X(x_1) = f_Y(y) = f_{Y|X}(y \mid x_2)$, which is a contradiction. Therefore we must have that $f_{X|Y}(x \mid y) \neq f_{X|Y}(x \mid y')$ for all $x, y, y'$, i.e., that the conditional distribution of $X$ given $Y$ changes as we change $Y$, which implies that $Y$ and $X$ are related variables (by Definition 10.1).
\end{solution}

\begin{exercise}
\label{exer:10.1.24}
Suppose that $X$, $Y$, $Z$ are independent $N(0, 1)$ random variables and that $U = X + Z$, $V = Y + Z$. Determine whether or not the variables $U$ and $V$ are related. (Hint: Calculate $\cov(U, V)$.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \cov(U, V) &= \expc(UV) - \expc(U)\expc(V) \\
    &= \expc((X + Z)(Y + Z)) - \expc(X + Z)\expc(Y + Z) \\
    &= \expc(XY + XZ + YZ + Z^2) - (\expc(X) + \expc(Z))(\expc(Y) + \expc(Z)) \\
    &= \expc(XY) + \expc(XZ) + \expc(YZ) + \expc(Z^2) - 0 \\
    &= \expc(X)\expc(Y) + \expc(X)\expc(Z) + \expc(Y)\expc(Z) + \expc(Z^2) \\
    &= 1,
\end{align*}
so $U$ and $V$ are not independent and so must be related.
\end{solution}

\begin{exercise}
\label{exer:10.1.25}
Suppose that $(X, Y, Z) \sim \text{Multinomial}(n, 1/3, 1/3, 1/3)$. Are $X$ and $Y$ related?
\end{exercise}

\begin{solution}
First note that the joint probability distribution function of $X$ and $Y$ is given by $\prb(X = x, Y = y) = \binom{n}{x}\binom{n-x}{y} (1/3)^x (1/3)^y (1/3)^{n-x-y}$. Since $X \sim \text{Binomial}(n, 1/3)$, the marginal probability function of $X$ is given by $\prb(X = x) = \binom{n}{x} (1/3)^x (2/3)^{n-x}$. Therefore, the conditional distribution of $Y$ given $X = x$ has probability function
\[
    \frac{\binom{n}{x}\binom{n-x}{y} (1/3)^x (1/3)^y (1/3)^{n-x-y}}{\binom{n}{x} (1/3)^x (2/3)^{n-x}} = \binom{n-x}{y} \left(\frac{1/3}{2/3}\right)^y \left(\frac{1/3}{2/3}\right)^{n-x-y} = \binom{n-x}{y} \left(\frac{1}{2}\right)^y \left(\frac{1}{2}\right)^{n-x-y}
\]
and this is the $\text{Binomial}(n - x, 1/2)$ distribution and this changes with $x$. Therefore, $X$ and $Y$ are related.
\end{solution}

\begin{exercise}
\label{exer:10.1.26}
Suppose that $(X, Y) \sim \text{Bivariate-Normal}(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$. Show that $X$ and $Y$ are unrelated if and only if $\cor(X, Y) = 0$.
\end{exercise}

\begin{solution}
By Problem \ref{exer:2.8.27} $X$ and $Y$ are independent if and only if $\rho = 0$ and $\cor(X, Y) = \rho$.
\end{solution}

\begin{exercise}
\label{exer:10.1.27}
Suppose that $(X, Y, Z)$ have probability function $p_{X,Y,Z}$. If $Y$ is related to $X$ but not to $Z$, then prove that $p_{X,Y,Z}(x, y, z) = p_{Y|X}(y \mid x) \, p_{X|Z}(x \mid z) \, p_Z(z)$.
\end{exercise}

\begin{solution}
If the conditional distribution of $Y$ given $X = x$ and $Z = z$ changes as we change $x$ for some value $z$ then $X$ and $Y$ are related. If the conditional distribution of $Y$ given $X = x$ and $Z = z$ never changes as we change $z$ for each fixed value of $x$, then $Z$ and $Y$ are not related.

Now $p_{X,Y,Z}(x, y, z) = p_{Y|X,Z}(y \mid x, z) p_{X,Z}(x, z) = p_{Y|X,Z}(y \mid x, z) p_{X|Z}(x \mid z) p_Z(z)$ and, because $p_{Y|X,Z}(y \mid x, z)$ is constant in $z$, we must have that
\[
    p_{Y|X}(y \mid x) = \frac{p_{X,Y}(x, y)}{p_X(x)} = \frac{\sum_z p_{Y|X,Z}(y \mid x, z) p_{X|Z}(x \mid z) p_Z(z)}{p_X(x)} = \frac{\sum_z p_{X|Z}(x \mid z) p_Z(z)}{p_X(x)} = p_{Y|X,Z}(y \mid x, z).
\]
Therefore, $p_{X,Y,Z}(x, y, z) = p_{Y|X}(y \mid x) p_{X|Z}(x \mid z) p_Z(z)$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Categorical Response and Predictors}
\label{sec:10.2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There are two possible situations when we have a single categorical response $Y$ and a single categorical predictor $X$. The categorical predictor is either random or deterministic, depending on how we sample. We examine these two situations separately.

\subsection{Random Predictor}
\label{ssec:10.2.1}

We consider the situation in which $X$ is categorical, taking values in $\{1, \ldots, a\}$, and $Y$ is categorical, taking values in $\{1, \ldots, b\}$. If we take a sample $\omega_1, \ldots, \omega_n$ from the population, then the values $X(\omega_i) = x_i$ are random, as are the values $Y(\omega_i) = y_j$.

Suppose the sample size $n$ is very small relative to the population size (so we can assume that i.i.d.\ sampling is applicable). Then, letting $\theta_{ij} = \prb(X = i, Y = j)$, we obtain the likelihood function (see Problem~\ref{exer:10.2.15})
\begin{equation}
\label{eq:10.2.1}
L(\theta_{11}, \ldots, \theta_{ab} \mid x_1, y_1, \ldots, x_n, y_n) = \prod_{i=1}^{a} \prod_{j=1}^{b} \theta_{ij}^{f_{ij}},
\end{equation}
where $f_{ij}$ is the number of sample values with $(X, Y) = (i, j)$. An easy computation (see Problem~\ref{exer:10.2.16}) shows that the MLE of $(\theta_{11}, \ldots, \theta_{kl})$ is given by $\hat{\theta}_{ij} = f_{ij}/n$, and that the standard error of this estimate (because the incidence of a sample member falling in the $(i, j)$-th cell is distributed $\text{Bernoulli}(\theta_{ij})$ and using Example~\ref{ex:6.3.2}) is given by
\begin{equation*}
\sqrt{\frac{\hat{\theta}_{ij}(1 - \hat{\theta}_{ij})}{n}}.
\end{equation*}

We are interested in whether or not there is a relationship between $X$ and $Y$. To answer this, we look at the conditional distributions of $Y$ given $X$. The conditional distributions of $Y$ given $X$, using $\theta_{i \cdot} = \theta_{i1} + \cdots + \theta_{ib} = \prb(X = i)$, are given in the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = 1$ & $\cdots$ & $Y = b$ \\ \hline
$X = 1$ & $\theta_{11}/\theta_{1 \cdot}$ & $\cdots$ & $\theta_{1b}/\theta_{1 \cdot}$ \\
$\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
$X = a$ & $\theta_{a1}/\theta_{a \cdot}$ & $\cdots$ & $\theta_{ab}/\theta_{a \cdot}$
\end{tabular}
\end{center}

Then estimating $\theta_{ij}/\theta_{i \cdot}$ by $\hat{\theta}_{ij}/\hat{\theta}_{i \cdot} = f_{ij}/f_{i \cdot}$, where $f_{i \cdot} = f_{i1} + \cdots + f_{ib}$, the estimated conditional distributions are as in the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = 1$ & $\cdots$ & $Y = b$ \\ \hline
$X = 1$ & $f_{11}/f_{1 \cdot}$ & $\cdots$ & $f_{1b}/f_{1 \cdot}$ \\
$\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
$X = a$ & $f_{a1}/f_{a \cdot}$ & $\cdots$ & $f_{ab}/f_{a \cdot}$
\end{tabular}
\end{center}

If we conclude that there is a relationship between $X$ and $Y$, then we look at the table of estimated conditional distributions to determine the form of the relationship, i.e., how the conditional distributions change as we change the value of $X$ we are conditioning on.

How, then, do we infer whether or not a relationship exists between $X$ and $Y$? No relationship exists between $Y$ and $X$ if and only if the conditional distributions of $Y$ given $X = x$ do not change with $x$. This is the case if and only if $X$ and $Y$ are independent, and this is true if and only if
\begin{equation*}
\theta_{ij} = \prb(X = i, Y = j) = \prb(X = i) \prb(Y = j) = \theta_{i \cdot} \theta_{\cdot j}
\end{equation*}
for every $i$ and $j$, where $\theta_{\cdot j} = \theta_{1j} + \cdots + \theta_{aj} = \prb(Y = j)$. Therefore, to assess whether or not there is a relationship between $X$ and $Y$, it is equivalent to assess the null hypothesis $H_0 : \theta_{ij} = \theta_{i \cdot} \theta_{\cdot j}$ for every $i$ and $j$.

How should we assess whether or not the observed data are surprising when $H_0$ holds? The methods of Section~\ref{ssec:9.1.2}, and in particular Theorem~\ref{thm:9.1.2}, can be applied here, as we have that
\begin{equation*}
(F_{11}, F_{12}, \ldots, F_{ab}) \sim \text{Multinomial}(n, \theta_{1 \cdot} \theta_{\cdot 1}, \theta_{1 \cdot} \theta_{\cdot 2}, \ldots, \theta_{a \cdot} \theta_{\cdot b})
\end{equation*}
when $H_0$ holds, where $F_{ij}$ is the count in the $(i, j)$-th cell.

To apply Theorem~\ref{thm:9.1.2}, we need the MLE of the parameters of the model under $H_0$. The likelihood, when $H_0$ holds, is
\begin{equation}
\label{eq:10.2.2}
L(\theta_{1 \cdot}, \ldots, \theta_{a \cdot}, \theta_{\cdot 1}, \ldots, \theta_{\cdot b} \mid x_1, y_1, \ldots, x_n, y_n) = \prod_{i=1}^{a} \prod_{j=1}^{b} (\theta_{i \cdot} \theta_{\cdot j})^{f_{ij}}.
\end{equation}
From this, we deduce (see Problem~\ref{exer:10.2.17}) that the MLE's of the $\theta_{i \cdot}$ and $\theta_{\cdot j}$ are given by $\hat{\theta}_{i \cdot} = f_{i \cdot}/n$ and $\hat{\theta}_{\cdot j} = f_{\cdot j}/n$. Therefore, the relevant chi-squared statistic is
\begin{equation*}
X^2 = \sum_{i=1}^{a} \sum_{j=1}^{b} \frac{(f_{ij} - n \hat{\theta}_{i \cdot} \hat{\theta}_{\cdot j})^2}{n \hat{\theta}_{i \cdot} \hat{\theta}_{\cdot j}}.
\end{equation*}

Under $H_0$, the parameter space has dimension $(a - 1) + (b - 1) = a + b - 2$, so we compare the observed value of $X^2$ with the $\chi^2((a-1)(b-1))$ distribution because $ab - 1 - (a + b - 2) = (a-1)(b-1)$.

Consider an example.

\begin{example}[Piston Ring Data]
\label{ex:10.2.1}
The following table gives the counts of piston ring failures, where variable $Y$ is the compressor number and variable $X$ is the leg position based on a sample of $n = 166$. These data were taken from \emph{Statistical Methods in Research and Production}, by O.~L.\ Davies (Hafner Publishers, New York, 1961).

Here, $Y$ takes four values and $X$ takes three values (N = North, C = Central, and S = South).
\begin{center}
\begin{tabular}{c|cccc}
 & $Y = 1$ & $Y = 2$ & $Y = 3$ & $Y = 4$ \\ \hline
$X = \text{N}$ & 17 & 11 & 11 & 14 \\
$X = \text{C}$ & 17 & 9 & 8 & 7 \\
$X = \text{S}$ & 12 & 13 & 19 & 28
\end{tabular}
\end{center}

The question of interest is whether or not there is any relation between compressor and leg position. Because $f_{1 \cdot} = 53$, $f_{2 \cdot} = 41$, and $f_{3 \cdot} = 72$, the conditional distributions of $Y$ given $X$ are estimated as in the rows of the following table.
\begin{center}
\begin{tabular}{c|cccc}
 & $Y = 1$ & $Y = 2$ & $Y = 3$ & $Y = 4$ \\ \hline
$X = \text{N}$ & $17/53 = 0.321$ & $11/53 = 0.208$ & $11/53 = 0.208$ & $14/53 = 0.264$ \\
$X = \text{C}$ & $17/41 = 0.415$ & $9/41 = 0.222$ & $8/41 = 0.195$ & $7/41 = 0.171$ \\
$X = \text{S}$ & $12/72 = 0.167$ & $13/72 = 0.181$ & $19/72 = 0.264$ & $28/72 = 0.389$
\end{tabular}
\end{center}

Comparing the rows, it certainly looks as if there is a difference in the conditional distributions, but we must assess whether or not the observed differences can be explained as due to sampling error. To see if the observed differences are real, we carry out the chi-squared test.

Under the null hypothesis of independence, the MLE's are given by
\begin{equation*}
\hat{\theta}_{\cdot 1} = \frac{46}{166}, \quad \hat{\theta}_{\cdot 2} = \frac{33}{166}, \quad \hat{\theta}_{\cdot 3} = \frac{38}{166}, \quad \hat{\theta}_{\cdot 4} = \frac{49}{166}
\end{equation*}
for the $Y$ probabilities, and by
\begin{equation*}
\hat{\theta}_{1 \cdot} = \frac{53}{166}, \quad \hat{\theta}_{2 \cdot} = \frac{41}{166}, \quad \hat{\theta}_{3 \cdot} = \frac{72}{166}
\end{equation*}
for the $X$ probabilities. Then the estimated expected counts $n \hat{\theta}_{i \cdot} \hat{\theta}_{\cdot j}$ are given by the following table.
\begin{center}
\begin{tabular}{c|cccc}
 & $Y = 1$ & $Y = 2$ & $Y = 3$ & $Y = 4$ \\ \hline
$X = \text{N}$ & 14.6867 & 10.5361 & 12.1325 & 15.6446 \\
$X = \text{C}$ & 11.3614 & 8.1506 & 9.3855 & 12.1024 \\
$X = \text{S}$ & 19.9518 & 14.3133 & 16.4819 & 21.2530
\end{tabular}
\end{center}

The standardized residuals (using (9.1.6))
\begin{equation*}
\frac{f_{ij} - n \hat{\theta}_{i \cdot} \hat{\theta}_{\cdot j}}{\sqrt{n \hat{\theta}_{i \cdot} \hat{\theta}_{\cdot j} (1 - \hat{\theta}_{i \cdot})(1 - \hat{\theta}_{\cdot j})}}
\end{equation*}
are as in the following table.
\begin{center}
\begin{tabular}{c|rrrr}
 & $Y = 1$ & $Y = 2$ & $Y = 3$ & $Y = 4$ \\ \hline
$X = \text{N}$ & $0.6322$ & $0.1477$ & $-0.3377$ & $-0.4369$ \\
$X = \text{C}$ & $1.7332$ & $0.3051$ & $-0.4656$ & $-1.5233$ \\
$X = \text{S}$ & $-1.8979$ & $-0.3631$ & $0.6536$ & $1.5673$
\end{tabular}
\end{center}

All of the standardized residuals seem reasonable, and we have that $X^2 = 11.7223$ with $\prb(\chi^2(6) \geqslant 11.7223) = 0.0685$, which is not unreasonably small.

So, while there may be some indication that the null hypothesis of no relationship is false, this evidence is not overwhelming. Accordingly, in this case, we may assume that $Y$ and $X$ are independent and use the estimates of cell probabilities obtained under this assumption.
\end{example}

We must also be concerned with model checking, i.e., is the model that we have assumed for the data $(x_1, y_1), \ldots, (x_n, y_n)$ correct? If these observations are i.i.d., then indeed the model is correct, as that is all that is being effectively assumed. So we need to check that the observations are a plausible i.i.d.\ sample. Because the minimal sufficient statistic is given by $(f_{11}, \ldots, f_{ab})$, such a test could be based on the conditional distribution of the sample $(x_1, y_1), \ldots, (x_n, y_n)$ given $(f_{11}, \ldots, f_{ab})$. The distribution theory for such tests is computationally difficult to implement, however, and we do not pursue this topic further in this text.

\subsection{Deterministic Predictor}
\label{ssec:10.2.2}

Consider again the situation in which $X$ is categorical, taking values in $\{1, \ldots, a\}$, and $Y$ is categorical, taking values in $\{1, \ldots, b\}$. But now suppose that we take a sample $\omega_1, \ldots, \omega_n$ from the population, where we have specified that $n_i$ sample members have the value $X(\omega) = i$, etc. This could be by assignment, when we are trying to determine whether a cause--effect relationship exists; or we might have $a$ populations $\Omega_1, \ldots, \Omega_a$ and want to see whether there is any difference in the distribution of $Y$ between populations. Note that $n_1 + \cdots + n_a = n$.

In both cases, we again want to make inferences about the conditional distributions of $Y$ given $X$, as represented by the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = 1$ & $\cdots$ & $Y = b$ \\ \hline
$X = 1$ & $\theta_{1|X=1}$ & $\cdots$ & $\theta_{b|X=1}$ \\
$\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
$X = a$ & $\theta_{1|X=a}$ & $\cdots$ & $\theta_{b|X=a}$
\end{tabular}
\end{center}

A difference in the conditional distributions means there is a relationship between $Y$ and $X$. If we denote the number of observations in the $i$th sample that have $Y = j$ by $f_{ij}$, then assuming the sample sizes are small relative to the population sizes, the likelihood function is given by
\begin{equation}
\label{eq:10.2.3}
L(\theta_{1|X=1}, \ldots, \theta_{b|X=a} \mid x_1, y_1, \ldots, x_n, y_n) = \prod_{i=1}^{a} \prod_{j=1}^{b} \theta_{j|X=i}^{f_{ij}},
\end{equation}
and the MLE is given by $\hat{\theta}_{j|X=i} = f_{ij}/n_i$ (Problem~\ref{exer:10.2.18}).

There is no relationship between $Y$ and $X$ if and only if the conditional distributions do not vary as we vary $X$, or if and only if
\begin{equation*}
H_0 : \theta_{j|X=1} = \cdots = \theta_{j|X=a} = \theta_j
\end{equation*}
for all $j = 1, \ldots, b$, for some probability distribution $(\theta_1, \ldots, \theta_b)$. Under $H_0$, the likelihood function is given by
\begin{equation}
\label{eq:10.2.4}
L(\theta_1, \ldots, \theta_b \mid x_1, y_1, \ldots, x_n, y_n) = \prod_{j=1}^{b} \theta_j^{f_{\cdot j}},
\end{equation}
and the MLE of $\theta_j$ is given by $\hat{\theta}_j = f_{\cdot j}/n$ (see Problem~\ref{exer:10.2.19}). Then, applying Theorem~\ref{thm:9.1.2}, we have that the statistic
\begin{equation*}
X^2 = \sum_{i=1}^{a} \sum_{j=1}^{b} \frac{(f_{ij} - n_i \hat{\theta}_j)^2}{n_i \hat{\theta}_j}
\end{equation*}
has an approximate $\chi^2((a-1)(b-1))$ distribution under $H_0$, because there are $a(b-1)$ free parameters in the full model, $b - 1$ parameters in the independence model, and $a(b-1) - (b-1) = (a-1)(b-1)$.

Consider an example.

\begin{example}
\label{ex:10.2.2}
This example is taken from a famous applied statistics book, \emph{Statistical Methods}, 6th ed., by G.~Snedecor and W.~Cochran (Iowa State University Press, Ames, 1967). Individuals were classified according to their blood type $Y$ (O, A, B, and AB, although the AB individuals were eliminated, as they were small in number) and also classified according to $X$, their disease status (peptic ulcer = P, gastric cancer = G, or control = C). So we have three populations; namely, those suffering from a peptic ulcer, those suffering from gastric cancer, and those suffering from neither. We suppose further that the individuals involved in the study can be considered as random samples from the respective populations.

The data are given in the following table.
\begin{center}
\begin{tabular}{c|ccc|c}
 & $Y = \text{O}$ & $Y = \text{A}$ & $Y = \text{B}$ & Total \\ \hline
$X = \text{P}$ & 983 & 679 & 134 & 1796 \\
$X = \text{G}$ & 383 & 416 & 84 & 883 \\
$X = \text{C}$ & 2892 & 2625 & 570 & 6087
\end{tabular}
\end{center}

The estimated conditional distributions of $Y$ given $X$ are then as follows.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = \text{O}$ & $Y = \text{A}$ & $Y = \text{B}$ \\ \hline
$X = \text{P}$ & $983/1796 = 0.547$ & $679/1796 = 0.378$ & $134/1796 = 0.075$ \\
$X = \text{G}$ & $383/883 = 0.434$ & $416/883 = 0.471$ & $84/883 = 0.095$ \\
$X = \text{C}$ & $2892/6087 = 0.475$ & $2625/6087 = 0.431$ & $570/6087 = 0.093$
\end{tabular}
\end{center}

We now want to assess whether or not there is any evidence for concluding that a difference exists among these conditional distributions. Under the null hypothesis that no difference exists, the MLE's of the probabilities $\theta_1 = \prb(Y = \text{O})$, $\theta_2 = \prb(Y = \text{A})$, and $\theta_3 = \prb(Y = \text{B})$ are given by
\begin{align*}
\hat{\theta}_1 &= \frac{983 + 383 + 2892}{1796 + 883 + 6087} = 0.4857, \\
\hat{\theta}_2 &= \frac{679 + 416 + 2625}{1796 + 883 + 6087} = 0.4244, \\
\hat{\theta}_3 &= \frac{134 + 84 + 570}{1796 + 883 + 6087} = 0.0899.
\end{align*}

Then the estimated expected counts $n_i \hat{\theta}_j$ are given by the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $Y = \text{O}$ & $Y = \text{A}$ & $Y = \text{B}$ \\ \hline
$X = \text{P}$ & 872.3172 & 762.2224 & 161.4604 \\
$X = \text{G}$ & 428.8731 & 374.7452 & 79.3817 \\
$X = \text{C}$ & 2956.4559 & 2583.3228 & 547.2213
\end{tabular}
\end{center}

The standardized residuals (using (9.1.6)) $(f_{ij} - n_i \hat{\theta}_j)/\sqrt{n_i(1 - \hat{\theta}_j)}^{1/2}$ are given by the following table.
\begin{center}
\begin{tabular}{c|rrr}
 & $Y = \text{O}$ & $Y = \text{A}$ & $Y = \text{B}$ \\ \hline
$X = \text{P}$ & $5.2219$ & $-3.9705$ & $-2.2643$ \\
$X = \text{G}$ & $-3.0910$ & $2.8111$ & $0.5441$ \\
$X = \text{C}$ & $-1.6592$ & $1.0861$ & $1.0227$
\end{tabular}
\end{center}

We have that $X^2 = 40.5434$ and $\prb(\chi^2(4) \geqslant 40.5434) = 0.0000$, so we have strong evidence against the null hypothesis of no relationship existing between $Y$ and $X$. Observe the large residuals when $X = \text{P}$ and $Y = \text{O}$, $Y = \text{A}$.

We are left with examining the conditional distributions to ascertain what form the relationship between $Y$ and $X$ takes. A useful tool in this regard is to plot the conditional distributions in bar charts, as we have done in Figure~\ref{fig:10.2.1}. From this, we see that the peptic ulcer population has a greater proportion of blood type O than the other populations.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_2_1.pdf}
  \caption{Plot of the conditional distributions of $Y$ given $X$ in Example~\ref{ex:10.2.2}.}
  \label{fig:10.2.1}
\end{figure}

\subsection{Bayesian Formulation}
\label{ssec:10.2.3}

We now add a prior density $\pi$ for the unknown values of the parameters of the models discussed in Sections~\ref{ssec:10.2.1} and~\ref{ssec:10.2.2}. Depending on how we choose $\pi$, and depending on the particular computation we want to carry out, we could be faced with some difficult computational problems. Of course, we have the Monte Carlo methods available in such circumstances, which can often render a computation fairly straightforward.

The most common choice of prior in these circumstances is to choose a conjugate prior. Because the likelihoods discussed in this section are as in Example~\ref{ex:7.1.3}, we see immediately that Dirichlet priors will be conjugate for the full model in Section~\ref{ssec:10.2.1} and that products of independent Dirichlet priors will be conjugate for the full model in Section~\ref{ssec:10.2.2}.

In Section~\ref{ssec:10.2.1}, the general likelihood --- i.e., no restrictions on the $\theta_{ij}$ --- is of the form
\begin{equation*}
L(\theta_{11}, \ldots, \theta_{ab} \mid x_1, y_1, \ldots, x_n, y_n) = \prod_{i=1}^{a} \prod_{j=1}^{b} \theta_{ij}^{f_{ij}}.
\end{equation*}
If we place a $\text{Dirichlet}(\alpha_{11}, \ldots, \alpha_{ab})$ prior on the parameter, then the posterior density is proportional to
\begin{equation*}
\prod_{i=1}^{a} \prod_{j=1}^{b} \theta_{ij}^{f_{ij} + \alpha_{ij} - 1},
\end{equation*}
so the posterior is a $\text{Dirichlet}(f_{11} + \alpha_{11}, \ldots, f_{ab} + \alpha_{ab})$ distribution.

In Section~\ref{ssec:10.2.2}, the general likelihood is of the form
\begin{equation*}
L(\theta_{1|X=1}, \ldots, \theta_{b|X=a} \mid x_1, y_1, \ldots, x_n, y_n) = \prod_{i=1}^{a} \prod_{j=1}^{b} \theta_{j|X=i}^{f_{ij}}.
\end{equation*}
Because $\sum_{j=1}^{b} \theta_{j|X=i} = 1$ for each $i = 1, \ldots, a$, we must place a prior on each distribution $(\theta_{1|X=i}, \ldots, \theta_{b|X=i})$. If we choose the prior on the $i$th distribution to be $\text{Dirichlet}(\alpha_{1i}, \ldots, \alpha_{ai})$, then the posterior density is proportional to
\begin{equation*}
\prod_{i=1}^{a} \prod_{j=1}^{b} \theta_{j|i}^{f_{ij} + \alpha_{ji} - 1}.
\end{equation*}
We recognize this as the product of independent Dirichlet distributions, with the posterior distribution on $(\theta_{1|X=i}, \ldots, \theta_{b|X=i})$ equal to a $\text{Dirichlet}(f_{i1} + \alpha_{1i}, \ldots, f_{ib} + \alpha_{bi})$ distribution.

A special and important case of the Dirichlet priors corresponds to the situation in which we feel that we have no information about the parameter. In such a situation, it makes sense to choose all the parameters of the Dirichlet to be 1, so that the priors are all uniform.

There are many characteristics of a Dirichlet distribution that can be evaluated in closed form, e.g., the expectation of any polynomial (see Problem~\ref{exer:10.2.20}). But still there will be many quantities for which exact computations will not be available. It turns out that we can always easily generate samples from Dirichlet distributions, provided we have access to a generator for beta distributions. This is available with most statistical packages. We now discuss how to do this.

\begin{example}[Generating from a $\text{Dirichlet}(\alpha_1, \ldots, \alpha_k)$ Distribution]
\label{ex:10.2.3}
The technique we discuss here is a commonly used method for generating from multivariate distributions. If we want to generate a value of the random vector $(X_1, \ldots, X_k)$, then we can proceed as follows. First, generate a value $x_1$ from the marginal distribution of $X_1$. Next, generate a value $x_2$ from the conditional distribution of $X_2$ given $X_1 = x_1$. Then generate a value $x_3$ from the conditional distribution of $X_3$ given that $X_1 = x_1$ and $X_2 = x_2$, etc.

If the distribution of $X$ is discrete, then we have that the probability of a particular vector of values $(x_1, x_2, \ldots, x_k)$ arising via this scheme is
\begin{equation*}
\prb(X_1 = x_1) \prb(X_2 = x_2 \mid X_1 = x_1) \cdots \prb(X_k = x_k \mid X_1 = x_1, \ldots, X_{k-1} = x_{k-1}).
\end{equation*}
Expanding each of these conditional probabilities, we obtain
\begin{equation*}
\prb(X_1 = x_1) \cdot \frac{\prb(X_1 = x_1, X_2 = x_2)}{\prb(X_1 = x_1)} \cdots \frac{\prb(X_1 = x_1, \ldots, X_{k-1} = x_{k-1}, X_k = x_k)}{\prb(X_1 = x_1, \ldots, X_{k-1} = x_{k-1})},
\end{equation*}
which equals $\prb(X_1 = x_1, \ldots, X_{k-1} = x_{k-1}, X_k = x_k)$, and so $(x_1, x_2, \ldots, x_k)$ is a value from the joint distribution of $(X_1, \ldots, X_k)$. This approach also works for absolutely continuous distributions, and the proof is the same but uses density functions instead.

In the case of $(X_1, \ldots, X_{k-1}) \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_k)$, we have that (see Challenge~\ref{exer:10.2.23}) $X_1 \sim \text{Beta}(\alpha_1, \alpha_2 + \cdots + \alpha_k)$ and $X_i$ given $X_1 = x_1, \ldots, X_{i-1} = x_{i-1}$ has the same distribution as $(1 - x_1 - \cdots - x_{i-1}) U_i$, where
\begin{equation*}
U_i \sim \text{Beta}(\alpha_i, \alpha_{i+1} + \cdots + \alpha_k)
\end{equation*}
and $U_2, \ldots, U_{k-1}$ are independent. Note that $X_k = 1 - X_1 - \cdots - X_{k-1}$ for any Dirichlet distribution. So we generate $X_1 \sim \text{Beta}(\alpha_1, \alpha_2 + \cdots + \alpha_k)$, generate $U_2 \sim \text{Beta}(\alpha_2, \alpha_3 + \cdots + \alpha_k)$ and put $X_2 = (1 - X_1) U_2$, generate $U_3 \sim \text{Beta}(\alpha_3, \alpha_4 + \cdots + \alpha_k)$ and put $X_3 = (1 - X_1 - X_2) U_3$, etc.

Below, we present a table of a sample of $n = 5$ values from a $\text{Dirichlet}(2, 3, 1, 1.5)$ distribution.
\begin{center}
\begin{tabular}{c|cccc}
 & $X_1$ & $X_2$ & $X_3$ & $X_4$ \\ \hline
1 & 0.116159 & 0.585788 & 0.229019 & 0.069034 \\
2 & 0.166639 & 0.566369 & 0.056627 & 0.210366 \\
3 & 0.411488 & 0.183686 & 0.326451 & 0.078375 \\
4 & 0.483124 & 0.316647 & 0.115544 & 0.084684 \\
5 & 0.117876 & 0.147869 & 0.418013 & 0.316242
\end{tabular}
\end{center}
Appendix~B contains the code used for this. It can be modified to generate from any Dirichlet distribution.
\end{example}

\paragraph{Summary of Section~\ref{sec:10.2}}
\begin{itemize}
\item In this section, we have considered the situation in which we have a categorical response variable and a categorical predictor variable.
\item We distinguished two situations. The first arises when the value of the predictor variable is not assigned, and the second arises when it is.
\item In both cases, the test of the null hypothesis that no relationship exists involved the chi-squared test.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:10.2.1}
The following table gives the counts of accidents for two successive years in a particular city.
\begin{center}
\begin{tabular}{c|ccc}
 & June & July & August \\ \hline
Year 1 & 60 & 100 & 80 \\
Year 2 & 80 & 100 & 60
\end{tabular}
\end{center}
Is there any evidence of a difference in the distribution of accidents for these months between the two years?
\end{exercise}

\begin{solution}
First, note that the predictor variable, $X$-year, is not random. The estimated conditional distribution of $Y$ given $X$ are recorded in the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & June & July & August \\
\hline
Year 1 & $60/240 = .25$ & $100/240 = .41667$ & $80/240 = .33333$ \\
Year 2 & $80/240 = .33333$ & $100/240 = .41667$ & $60/240 = .25$
\end{tabular}
\end{center}
Under the null hypothesis of no difference in the distributions of thunderstorms between the two years, the MLEs are given by
\[
    \hat{\theta}_1 = \frac{140}{480} = .29167, \quad \hat{\theta}_2 = \frac{200}{480} = .41667, \quad \hat{\theta}_3 = \frac{140}{480} = .29167.
\]
Then the estimates of the expected counts $n_i\theta_j$ are given in the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & June & July & August \\
\hline
Year 1 & 70 & 100 & 70 \\
Year 2 & 70 & 100 & 70
\end{tabular}
\end{center}
The Chi-squared statistic is then equal to $X_0^2 = 5.7143$ and, with $X^2 \sim \chi^2(2)$, the P-value equals $\prb(X^2 > 5.7143) = .05743$. Therefore, we do not have evidence against the null hypothesis of no difference in the distributions of thunderstorms between the two years, at least at the $.05$ level.
\end{solution}

\begin{exercise}
\label{exer:10.2.2}
The following data are from a study by Linus Pauling (1971) (``The significance of the evidence about ascorbic acid and the common cold,'' \emph{Proceedings of the National Academy of Sciences}, Vol.~68, p.~2678), concerned with examining the relationship between taking vitamin C and the incidence of colds. Of 279 participants in the study, 140 received a placebo (sugar pill) and 139 received vitamin C.
\begin{center}
\begin{tabular}{c|cc}
 & No Cold & Cold \\ \hline
Placebo & 31 & 109 \\
Vitamin C & 17 & 122
\end{tabular}
\end{center}
Assess the null hypothesis that there is no relationship between taking vitamin C and the incidence of the common cold.
\end{exercise}

\begin{solution}
First note that the predictor variable, $X$ (received vitamin C or not), is deterministic. The estimated conditional distributions of $Y$ given $X$ are recorded in the following table.
\begin{center}
\begin{tabular}{c|cc}
 & No cold & Cold \\
\hline
Placebo & .22143 & .77857 \\
Vitamin C & .12230 & .87770
\end{tabular}
\end{center}
Under the null hypothesis of no relationship between taking vitamin C and the incidence of the common cold, the MLEs are given by
\[
    \hat{\theta}_1 = \frac{48}{279} = .17204, \quad \hat{\theta}_2 = \frac{231}{279} = .82796.
\]
Then the estimates of the expected counts $n_i\theta_j$ are given in the following table.
\begin{center}
\begin{tabular}{c|cc}
 & No cold & Cold \\
\hline
Placebo & 24.086 & 115.91 \\
Vitamin C & 23.914 & 115.09
\end{tabular}
\end{center}
The Chi-squared statistic is equal to $X_0^2 = 4.8105$ and, with $X^2 \sim \chi^2(1)$, the P-value equals $\prb(X^2 > 4.8105) = .02829$. Therefore, we have evidence against the null hypothesis of no relationship between taking vitamin C and the incidence of the common cold.
\end{solution}

\begin{exercise}
\label{exer:10.2.3}
A simulation experiment is carried out to see whether there is any relationship between the first and second digits of a random variable generated from a $\text{Uniform}[0, 1]$ distribution. A total of 1000 uniforms were generated; if the first and second digits were in $\{0, 1, 2, 3, 4\}$, they were recorded as a 0, and as a 1 otherwise. The cross-classified data are given in the following table.
\begin{center}
\begin{tabular}{c|cc}
 & Second digit $= 0$ & Second digit $= 1$ \\ \hline
First digit $= 0$ & 240 & 250 \\
First digit $= 1$ & 255 & 255
\end{tabular}
\end{center}
Assess the null hypothesis that there is no relationship between the digits.
\end{exercise}

\begin{solution}
The estimated conditional distributions of $Y$ (second digit) given $X$ (first digit) are recorded in the following table.
\begin{center}
\begin{tabular}{c|cc}
 & Second digit 0 & Second digit 1 \\
\hline
First digit 0 & 0.489796 & 0.510204 \\
First digit 1 & 0.500000 & 0.500000
\end{tabular}
\end{center}
Under the null hypothesis of no relationship between the digits, the MLEs are given by
\[
    \hat{\theta}_{\cdot 1} = \frac{495}{1000} = .495, \quad \hat{\theta}_{\cdot 2} = \frac{505}{1000} = .505
\]
for the $Y$ probabilities and
\[
    \hat{\theta}_{1\cdot} = \frac{490}{1000} = .49, \quad \hat{\theta}_{2\cdot} = \frac{510}{1000} = .51
\]
for the $X$ probabilities. Then the estimates of the expected counts $n_i\theta_{i\cdot}\theta_{\cdot j}$ are given in the following table.
\begin{center}
\begin{tabular}{c|cc}
 & Second digit 0 & Second digit 1 \\
\hline
First digit 0 & 242.55 & 247.45 \\
First digit 1 & 252.45 & 257.55
\end{tabular}
\end{center}
The Chi-squared statistic is then equal to $X_0^2 = .10409$ and, with $X^2 \sim \chi^2(1)$, the P-value equals $\prb(X^2 > 0.10409) = .74698$. Therefore, we have no evidence against the null hypothesis of no relationship between the two digits.
\end{solution}

\begin{exercise}
\label{exer:10.2.4}
Grades in a first-year calculus course were obtained for randomly selected students at two universities and classified as pass or fail. The following data were obtained.
\begin{center}
\begin{tabular}{c|cc}
 & Fail & Pass \\ \hline
University 1 & 33 & 143 \\
University 2 & 22 & 263
\end{tabular}
\end{center}
Is there any evidence of a relationship between calculus grades and university?
\end{exercise}

\begin{solution}
First, note that the predictor variable, $X$ (university), is not random. The estimated conditional distributions of $Y$ given $X$ are recorded in the following table.
\begin{center}
\begin{tabular}{c|cc}
 & Fail & Pass \\
\hline
University 1 & 0.187500 & 0.812500 \\
University 2 & 0.077193 & 0.922807
\end{tabular}
\end{center}
Under the null hypothesis of no relationship between calculus grades and university, the MLEs are given by
\[
    \hat{\theta}_1 = \frac{55}{461} = .11931, \quad \hat{\theta}_2 = \frac{406}{461} = .88069.
\]
Then the estimates of the expected counts $n_i\theta_j$ are given in the following table.
\begin{center}
\begin{tabular}{c|cc}
 & Fail & Pass \\
\hline
University 1 & 20.999 & 155.0 \\
University 2 & 34.003 & 251.0
\end{tabular}
\end{center}
The Chi-squared statistic is then equal to $X_0^2 = 12.598$ and, with $X^2 \sim \chi^2(1)$, the P-value equals $\prb(X^2 > 12.598) = .00039$. Therefore, we have strong evidence against the null hypothesis of no relationship between the calculus grades and university.
\end{solution}

\begin{exercise}
\label{exer:10.2.5}
The following data are recorded in \emph{Statistical Methods for Research Workers}, by R.~A.\ Fisher (Hafner Press, New York, 1922), and show the classifications of 3883 Scottish children by gender ($X$) and hair color ($Y$).
\begin{center}
\begin{tabular}{c|ccccc}
 & $Y = \text{fair}$ & $Y = \text{red}$ & $Y = \text{medium}$ & $Y = \text{dark}$ & $Y = \text{jet black}$ \\ \hline
$X = \text{m}$ & 592 & 119 & 849 & 504 & 36 \\
$X = \text{f}$ & 544 & 97 & 677 & 451 & 14
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Is there any evidence for a relationship between hair color and gender?
\item Plot the appropriate bar chart(s).
\item Record the residuals and relate these to the results in parts (a) and (b). What do you conclude about the size of any deviation from independence?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First, note that the predictor variable, $X$ (gender), is not random. The estimated conditional distributions of $Y$ given $X$ are given in the following table.
    \begin{center}
    \begin{tabular}{c|ccccc}
     & $Y = \text{fair}$ & $Y = \text{red}$ & $Y = \text{medium}$ & $Y = \text{dark}$ & $Y = \text{jet black}$ \\
    \hline
    $X = m$ & 0.281905 & 0.0566667 & 0.404286 & 0.240000 & 0.0171429 \\
    $X = f$ & 0.305104 & 0.0544027 & 0.379697 & 0.252944 & 0.0078519
    \end{tabular}
    \end{center}
    Under the null hypothesis of no relationship between hair color and gender, the MLEs are given by
    \begin{align*}
        \hat{\theta}_1 &= \frac{1136}{3883} = .292557, \quad \hat{\theta}_2 = \frac{216}{3883} = .055627, \quad \hat{\theta}_3 = \frac{1526}{3883} = .392995, \\
        \hat{\theta}_4 &= \frac{955}{3883} = .245944, \quad \hat{\theta}_5 = \frac{50}{3883} = 0.012877.
    \end{align*}
    Then the estimates of the expected counts $n_i\theta_j$ are given in the following table.
    \begin{center}
    \begin{tabular}{c|ccccc}
     & $Y = \text{fair}$ & $Y = \text{red}$ & $Y = \text{medium}$ & $Y = \text{dark}$ & $Y = \text{jet black}$ \\
    \hline
    $X = m$ & 614.370 & 116.817 & 825.290 & 516.482 & 27.041 \\
    $X = f$ & 521.630 & 99.183 & 700.710 & 438.518 & 22.959
    \end{tabular}
    \end{center}
    The Chi-squared statistic is then equal to $X_0^2 = 10.4674$ and, with $X^2 \sim \chi^2(4)$, the P-value equals $\prb(X^2 > 10.4674) = .03325$. Therefore, we have some evidence against the null hypothesis of no relationship between hair color and gender.
    \item The appropriate bar plots are the two conditional distributions and these are plotted as follows for males and then females.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-2-5-male.pdf}
      \caption{Bar plot of conditional distribution of hair color for males.}
      %\label{fig:hair-color-male}
    \end{figure}
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-2-5-female.pdf}
      \caption{Bar plot of conditional distribution of hair color for females.}
      %\label{fig:hair-color-female}
    \end{figure}
    \item The standardized residuals are given in the following table. They all look reasonable, so nothing stands out as an explanation of why the model of independence doesn't fit. Overall, it looks like a large sample size has detected a small difference.
    \begin{center}
    \begin{tabular}{c|ccccc}
     & $Y = \text{fair}$ & $Y = \text{red}$ & $Y = \text{medium}$ & $Y = \text{dark}$ & $Y = \text{jet black}$ \\
    \hline
    $X = m$ & $-1.07303$ & $0.20785$ & $1.05934$ & $-0.63250$ & $1.73407$ \\
    $X = f$ & $1.16452$ & $-0.22557$ & $-1.14966$ & $0.68642$ & $-1.88191$
    \end{tabular}
    \end{center}
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.2.6}
Suppose we have a controllable predictor $X$ that takes four different values, and we measure a binary-valued response $Y$. A random sample of 100 was taken from the population and the value of $X$ was randomly assigned to each individual in such a way that there are 25 sample members taking each of the possible values of $X$. Suppose that the following data were obtained.
\begin{center}
\begin{tabular}{c|cccc}
 & $X = 1$ & $X = 2$ & $X = 3$ & $X = 4$ \\ \hline
$Y = 0$ & 12 & 10 & 16 & 14 \\
$Y = 1$ & 13 & 15 & 9 & 11
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Assess whether or not there is any evidence against a cause--effect relationship existing between $X$ and $Y$.
\item Explain why it is possible in this example to assert that any evidence found that a relationship exists is evidence that a cause--effect relationship exists.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First, note that the predictor variable $X$, is not random. The estimated conditional distributions of $Y$ given $X$ are given in the following table.
    \begin{center}
    \begin{tabular}{c|cccc}
     & $X = 1$ & $X = 2$ & $X = 3$ & $X = 4$ \\
    \hline
    $Y = 0$ & 0.48 & 0.40 & 0.64 & 0.56 \\
    $Y = 1$ & 0.52 & 0.60 & 0.36 & 0.44
    \end{tabular}
    \end{center}
    Under the null hypothesis of no relationship between $X$ and $Y$, the MLEs are given by
    \[
        \hat{\theta}_1 = \frac{52}{100} = .52, \quad \hat{\theta}_2 = \frac{48}{100} = .48.
    \]
    Then the estimates of the expected counts $n_i\theta_j$ are given in the following table.
    \begin{center}
    \begin{tabular}{c|cccc}
     & $X = 1$ & $X = 2$ & $X = 3$ & $X = 4$ \\
    \hline
    $Y = 0$ & 13 & 13 & 13 & 13 \\
    $Y = 1$ & 12 & 12 & 12 & 12
    \end{tabular}
    \end{center}
    The Chi-squared statistic is then equal to $X^2 = 3.20513$ and, with $X^2 \sim \chi^2(3)$, the P-value equals $\prb(X^2 > 3.20513) = .36107$. Therefore, we do not have any evidence against the null hypothesis of no cause-effect relationship between $X$ and $Y$.
    \item If a relationship had been detected, this would be evidence of a cause-effect relationship because we have assigned the value of $X$ to each sample element.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.2.7}
Write out in full how you would generate a value from a $\text{Dirichlet}(1, 1, 1, 1)$ distribution.
\end{exercise}

\begin{solution}
We should first generate a value for $X_1 \sim \text{Dirichlet}(1, 3)$. Then generate $U_2$ from the $\text{Beta}(1, 2)$ distribution and set $X_2 = (1 - X_1)U_2$. Then generate $U_3$ from the $\text{Beta}(1, 1)$ distribution and set $X_3 = (1 - X_1 - X_2)U_3$. Then set $X_4 = 1 - X_1 - X_2 - X_3$.
\end{solution}

\begin{exercise}
\label{exer:10.2.8}
Suppose we have two categorical variables defined on a population $\Omega$ and we conduct a census. How would you decide whether or not a relationship exists between $X$ and $Y$? If you decided that a relationship existed, how would you distinguish between a strong and a weak relationship?
\end{exercise}

\begin{solution}
The first step is drawing the frequency table of $(X, Y)$, that is, tabulate $f_{x,y}$, the number of items having $X = x$ and $Y = y$. Also let $N$ be the size of the population. Then check whether $X$ and $Y$ are independent or not, i.e., check whether $f_{x,y} = f_{x\cdot}f_{\cdot y}/N$ for all $x$ and $y$ or not. If $X$ and $Y$ are independent, there is no relationship between $X$ and $Y$. And there is a relationship otherwise. If the frequency table is close to that of independent variables, there is a weak relationship. So, if $|f_{x,y} - f_{x\cdot}f_{\cdot y}/N|$ is small there is a weak relationship and if it is big there is a strong relationship.
\end{solution}

\begin{exercise}
\label{exer:10.2.9}
Suppose you simultaneously roll two dice $n$ times and record the outcomes. Based on these values, how would you assess the null hypothesis that the outcome on each die is independent of the outcome on the other?
\end{exercise}

\begin{solution}
Let $X$ and $Y$ be the numbers showing on each die. Then there are 36 possible pairs $(i, j)$ for $i, j = 1, \ldots, 6$. Then, write a $6 \times 6$ frequency table, say $f_{ij}$, and compute chi-squared statistic, $X^2 = \sum_{i=1}^{6} \sum_{j=1}^{6} (f_{ij} - f_{i\cdot}f_{\cdot j}/n)^2/(f_{i\cdot}f_{\cdot j}/n)$. Using $X^2 \to \chi^2((6-1)(6-1)) \sim \chi^2(25)$, we compute $\prb(\chi^2(25) > X^2)$. If this is small, we have evidence against the null hypothesis.
\end{solution}

\begin{exercise}
\label{exer:10.2.10}
Suppose a professor wants to assess whether or not there is any difference in the final grade distributions (A, B, C, D, and F) between males and females in a particular class. To assess the null hypothesis that there is no difference between these distributions, the professor carries out a chi-squared test.
\begin{enumerate}[(a)]
\item Discuss how the professor carried out this test.
\item If the professor obtained evidence against the null hypothesis, discuss what concerns you have over the use of the chi-squared test.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First of all, write a frequency table, say $f_{ij}$ for $i = A, B, C, D, E$ and $F$, and $j = \text{female}, \text{male}$. Then, compute the chi-squared statistic, $X^2 = \sum_i \sum_j (f_{ij} - f_{i\cdot}f_{\cdot j}/n)^2/(f_{i\cdot}f_{\cdot j}/n)$. Based on $X^2 \to \chi^2((6-1)(2-1)) \sim \chi^2(5)$, compute $\prb(\chi^2(5) > X^2)$. If it is small, we have evidence against the null hypothesis of no difference in the final grade distributions between females and males.
    \item As indicated in part (a), the distribution of $X^2$ is asymptotically $\chi^2(5)$-distribution. However, the professor has not sampled from a population. To carry out the test the professor needs to assume that the class is like a random sample from some larger population of interest and this may not be the case.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.2.11}
Suppose that a chi-squared test is carried out, based on a random sample of $n$ from a population, to assess whether or not two categorical variables $X$ and $Y$ are independent. Suppose the P-value equals 0.001 and the investigator concludes that there is evidence against independence. Discuss how you would check to see if the deviation from independence was of practical significance.
\end{exercise}

\begin{solution}
We look at the differences $|f_{ij} - f_{i\cdot}f_{\cdot j}/n|$ to see how big these are. If these are all quite small, then the deviation from independence detected by the test is of no practical importance.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:10.2.12}
In Example~\ref{ex:10.2.1}, place a uniform prior on the parameters (a Dirichlet distribution with all parameters equal to 1) and then determine the posterior distribution of the parameters.
\end{exercise}

\begin{solution}
We place a $\text{Dirichlet}(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)$ prior distribution on $(\theta_{11}, \theta_{21}, \theta_{31}, \theta_{12}, \theta_{22}, \theta_{32}, \theta_{13}, \theta_{23}, \theta_{33}, \theta_{14}, \theta_{24}, \theta_{34})$, so the posterior is proportional to (using $\theta_{34} = 1 - \text{the other parameters}$) $\theta_{11}^{17}\theta_{21}^{17}\theta_{31}^{12}\theta_{12}^{11}\theta_{22}^{9}\theta_{32}^{13}\theta_{13}^{11}\theta_{23}^{8}\theta_{33}^{19}\theta_{14}^{14}\theta_{24}^{7}\theta_{34}^{28}$. Therefore, the posterior distribution is $\text{Dirichlet}(18, 18, 13, 12, 10, 14, 12, 9, 20, 15, 8, 29)$.
\end{solution}

\begin{exercise}
\label{exer:10.2.13}
In Example~\ref{ex:10.2.2}, place a uniform prior on the parameters of each population (a Dirichlet distribution with all parameters equal to 1) and such that the three priors are independent. Then determine the posterior distribution.
\end{exercise}

\begin{solution}
We place a $\text{Dirichlet}(1, 1, 1)$ prior on $(\theta_{O|X=j}, \theta_{A|X=j})$ for $j = P, G, C$, and we assume that these three distributions are independent. Therefore, the posterior is proportional to
\begin{align*}
    &(\theta_{O|X=P})^{983} (\theta_{A|X=P})^{679} (\theta_{B|X=P})^{134} \\
    &\times (\theta_{O|X=G})^{383} (\theta_{A|X=G})^{416} (\theta_{B|X=G})^{84} \\
    &\times (\theta_{O|X=C})^{2892} (\theta_{A|X=C})^{2625} (\theta_{B|X=C})^{570},
\end{align*}
so $(\theta_{O|X=P}, \theta_{A|X=P}) \mid \text{data} \sim \text{Dirichlet}(984, 680, 135)$, $(\theta_{O|X=G}, \theta_{A|X=G}) \mid \text{data} \sim \text{Dirichlet}(384, 417, 85)$, $(\theta_{O|X=C}, \theta_{A|X=C}) \mid \text{data} \sim \text{Dirichlet}(2893, 2626, 571)$ and they are independent.
\end{solution}

\begin{exercise}
\label{exer:10.2.14}
In a $2 \times 2$ table with probabilities $\theta_{ij}$, prove that the row and column variables are independent if and only if
\begin{equation*}
\frac{\theta_{11} \theta_{22}}{\theta_{12} \theta_{21}} = 1,
\end{equation*}
namely, we have independence if and only if the cross-ratio equals 1.
\end{exercise}

\begin{solution}
Consider the following $2 \times 2$ table.
\begin{center}
\begin{tabular}{c|cc|c}
 & $Y = 1$ & $Y = 2$ & $\prb(X = x)$ \\
\hline
$X = 1$ & $\theta_{11}$ & $\theta_{12}$ & $\theta_{11} + \theta_{12}$ \\
$X = 2$ & $\theta_{21}$ & $\theta_{22}$ & $\theta_{21} + \theta_{22}$ \\
\hline
$\prb(Y = y)$ & $\theta_{11} + \theta_{21}$ & $\theta_{12} + \theta_{22}$ & 1
\end{tabular}
\end{center}
Now $X$ and $Y$ independent implies that
\begin{align*}
    \theta_{11} &= (\theta_{11} + \theta_{12})(\theta_{11} + \theta_{21}), \quad \theta_{12} = (\theta_{11} + \theta_{12})(\theta_{12} + \theta_{22}) \\
    \theta_{21} &= (\theta_{21} + \theta_{22})(\theta_{11} + \theta_{21}), \quad \theta_{22} = (\theta_{21} + \theta_{22})(\theta_{12} + \theta_{22})
\end{align*}
and this implies that
\[
    \frac{\theta_{11}\theta_{22}}{\theta_{12}\theta_{21}} = \frac{(\theta_{11} + \theta_{12})(\theta_{11} + \theta_{21})(\theta_{21} + \theta_{22})(\theta_{12} + \theta_{22})}{(\theta_{11} + \theta_{12})(\theta_{12} + \theta_{22})(\theta_{21} + \theta_{22})(\theta_{11} + \theta_{21})} = 1.
\]
Now $\theta_{11}\theta_{22}/\theta_{12}\theta_{21} = 1$ implies that $\theta_{11}\theta_{22} = \theta_{12}\theta_{21}$ and so $(\theta_{11} + \theta_{12})\theta_{22} = \theta_{12}(\theta_{21} + \theta_{22})$ and $(\theta_{11} + \theta_{12})(\theta_{12} + \theta_{22}) = \theta_{12}(\theta_{21} + \theta_{22} + \theta_{11} + \theta_{12}) = \theta_{12}$. Also, $\theta_{11}\theta_{22} = \theta_{12}\theta_{21}$ implies $(\theta_{11} + \theta_{21})\theta_{22} = (\theta_{12} + \theta_{22})\theta_{21}$ and so $(\theta_{11} + \theta_{21})(\theta_{21} + \theta_{22}) = (\theta_{12} + \theta_{22} + \theta_{11} + \theta_{21})\theta_{21} = \theta_{21}$. Similarly, $\theta_{22} = (\theta_{21} + \theta_{22})(\theta_{12} + \theta_{22})$ and $\theta_{11} = (\theta_{11} + \theta_{12})(\theta_{11} + \theta_{21})$, so $X$ and $Y$ are independent.
\end{solution}

\begin{exercise}
\label{exer:10.2.15}
Establish that the likelihood in \eqref{eq:10.2.1} is correct when the population size is infinite (or when we are sampling with replacement from the population).
\end{exercise}

\begin{solution}
When sampling with replacement from the population, we can think of the sample as an i.i.d.\ sample from this population, so each observation has probability $\theta_{ij}$ of falling in the $(i, j)$ category, namely $\theta_{ij}$. Then when $f_{ij}$ sample elements fall in this cell the likelihood takes the form $\prod_{i=1}^{a} \prod_{j=1}^{b} \theta_{ij}^{f_{ij}}$ as claimed.
\end{solution}

\begin{exercise}
\label{exer:10.2.16}
(MV) Prove that the MLE of $(\theta_{11}, \ldots, \theta_{ab})$ in \eqref{eq:10.2.1} is given by $\hat{\theta}_{ij} = f_{ij}/n$. Assume that $f_{ij} > 0$ for every $i$, $j$. (Hint: Use the facts that a continuous function on this parameter space must achieve its maximum at some point in $\Theta$, and that, if the function is continuously differentiable at such a point, then all its first-order partial derivatives are zero there. This will allow you to conclude that the unique solution to the score equations must be the point where the log-likelihood is maximized. Try the case where $a = 2$, $b = 2$ first.)
\end{exercise}

\begin{solution}
First, note that there are only $ab - 1$ free parameters, so we place $\theta_{ab} = 1 - \sum_{(i,j) \neq (a,b)} \theta_{ij}$. The likelihood function is given by $L(\theta_{11}, \ldots, \theta_{ab} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \prod_{i=1}^{a} \prod_{j=1}^{b} \theta_{ij}^{f_{ij}}$. The log-likelihood function is given by $l(\theta_{11}, \ldots, \theta_{ab} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \sum_{i=1}^{a} \sum_{j=1}^{b} f_{ij} \ln \theta_{ij}$.

The score function is then given by
\[
    S(\theta_{11}, \ldots, \theta_{a(b-1)} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \begin{pmatrix} \frac{f_{11}}{\theta_{11}} - \frac{f_{ab}}{\theta_{ab}} \\ \frac{f_{12}}{\theta_{12}} - \frac{f_{ab}}{\theta_{ab}} \\ \vdots \end{pmatrix}.
\]
Setting this equal to 0 and solving leads to $\theta_{ij} = (f_{ij}/f_{ab})\theta_{ab}$. Then summing both sides over all $(i, j) \neq (a, b)$ leads to $1 - \theta_{ab} = (n - f_{ab})\theta_{ab}/f_{ab}$ or $\theta_{ab} = f_{ab}/n$, and this implies that $\theta_{ij} = f_{ij}/n$ gives a unique solution to the score equations.

Now the log-likelihood takes the value $-\infty$ whenever any $\theta_{ij} = 0$, so the log-likelihood does not attain its maximum at such a point. Therefore, the log-likelihood is maximized at some point for which all $\theta_{ij} \neq 0$, and the log-likelihood is continuously differentiable at such a point. Since the unique solution to the score equations is such a point, it must be the MLE.
\end{solution}

\begin{exercise}
\label{exer:10.2.17}
(MV) Prove that the MLE of $(\theta_{1 \cdot}, \ldots, \theta_{a \cdot}, \theta_{\cdot 1}, \ldots, \theta_{\cdot b})$ in \eqref{eq:10.2.2} is given by $\hat{\theta}_{i \cdot} = f_{i \cdot}/n$ and $\hat{\theta}_{\cdot j} = f_{\cdot j}/n$. Assume that $f_{i \cdot} > 0$, $f_{\cdot j} > 0$ for every $i$, $j$. (Hint: Use the hint in Problem~\ref{exer:10.2.16}.)
\end{exercise}

\begin{solution}
We let $\theta_{1\cdot}, \ldots, \theta_{(a-1)\cdot}, \theta_{\cdot 1}, \ldots, \theta_{\cdot(b-1)}$ be the free parameters since $\theta_{a\cdot} = 1 - \sum_{i=1}^{a-1} \theta_{i\cdot}$ and $\theta_{\cdot b} = 1 - \sum_{j=1}^{b-1} \theta_{\cdot j}$. The likelihood function is then given by $L(\theta_{1\cdot}, \ldots, \theta_{(a-1)\cdot}, \theta_{\cdot 1}, \ldots, \theta_{\cdot(b-1)} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \prod_{i=1}^{a} \prod_{j=1}^{b} (\theta_{i\cdot}\theta_{\cdot j})^{f_{ij}} = \prod_{i=1}^{a} \theta_{i\cdot}^{f_{i\cdot}} \prod_{j=1}^{b} \theta_{\cdot j}^{f_{\cdot j}}$. The log-likelihood function is given by $l(\theta_{1\cdot}, \ldots, \theta_{(a-1)\cdot}, \theta_{\cdot 1}, \ldots, \theta_{\cdot(b-1)} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \sum_{i=1}^{a} f_{i\cdot} \ln \theta_{i\cdot} + \sum_{j=1}^{b} f_{\cdot j} \ln \theta_{\cdot j}$. The score function is then given by
\[
    S(\theta_{1\cdot}, \ldots, \theta_{(a-1)\cdot}, \theta_{\cdot 1}, \ldots, \theta_{\cdot(b-1)} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \begin{pmatrix} \frac{f_{1\cdot}}{\theta_{1\cdot}} - \frac{f_{a\cdot}}{\theta_{a\cdot}} \\ \vdots \\ \frac{f_{\cdot 1}}{\theta_{\cdot 1}} - \frac{f_{\cdot b}}{\theta_{\cdot b}} \\ \vdots \end{pmatrix}.
\]
Setting this equal to 0 and solving leads to
\[
    \theta_{i\cdot} = \frac{f_{i\cdot}}{f_{a\cdot}} \theta_{a\cdot}, \quad \theta_{\cdot j} = \frac{f_{\cdot j}}{f_{\cdot b}} \theta_{\cdot b}.
\]
Summing these over $i = 1, \ldots, a - 1$ and $j = 1, \ldots, b - 1$ leads to the equations
\[
    1 - \theta_{a\cdot} = \frac{n - f_{a\cdot}}{f_{a\cdot}} \theta_{a\cdot} \quad \text{and} \quad 1 - \theta_{\cdot b} = \frac{n - f_{\cdot b}}{f_{\cdot b}} \theta_{\cdot b}.
\]
Therefore, $\theta_{a\cdot} = f_{a\cdot}/n$, $\theta_{\cdot b} = f_{\cdot b}/n$, and this implies that $\theta_{i\cdot} = f_{i\cdot}/n$, $\theta_{\cdot j} = f_{\cdot j}/n$ gives a unique solution to the score equations.

Now the log-likelihood takes the value $-\infty$ whenever any $\theta_{i\cdot} = 0$ or $\theta_{\cdot j} = 0$, so the log-likelihood does not attain its maximum at such a point. Therefore, the log-likelihood is maximized at some point for which all $\theta_{i\cdot} \neq 0$ or $\theta_{\cdot j} \neq 0$, and the log-likelihood is continuously differentiable at such a point. Since the unique solution to the score equations is such a point, it must be the MLE.
\end{solution}

\begin{exercise}
\label{exer:10.2.18}
(MV) Prove that the MLE of $(\theta_{1|X=1}, \ldots, \theta_{b|X=a})$ in \eqref{eq:10.2.3} is given by $\hat{\theta}_{j|X=i} = f_{ij}/n_i$. Assume that $f_{ij} > 0$ for every $i$, $j$. (Hint: Use the hint in Problem~\ref{exer:10.2.16}.)
\end{exercise}

\begin{solution}
There are $a(b - 1)$ free parameters because $\theta_{b|X=i} = 1 - \sum_{j=1}^{b-1} \theta_{j|X=i}$ for $i = 1, \ldots, a$. The likelihood function is given by
\[
    L(\theta_{1|X=1}, \ldots, \theta_{b-1|X=1}, \ldots, \theta_{b-1|X=a} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \prod_{i=1}^{a} \prod_{j=1}^{b} (\theta_{j|X=i})^{f_{ij}}.
\]
The log-likelihood function is given by
\[
    l(\theta_{1|X=1}, \ldots, \theta_{b-1|X=1}, \ldots, \theta_{b-1|X=a} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \sum_{i=1}^{a} \sum_{j=1}^{b} f_{ij} \ln \theta_{j|X=i}.
\]
The score function is then given by
\[
    S(\theta_{1|X=1}, \theta_{1|X=2} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \begin{pmatrix} \frac{f_{11}}{\theta_{1|X=1}} - \frac{f_{1b}}{\theta_{b|X=1}} \\ \vdots \end{pmatrix}.
\]
Setting this equal to 0 and solving leads to $\theta_{j|X=i} = (f_{ij}/f_{ib})\theta_{b|X=i}$. Summing both sides over $j = 1, \ldots, b - 1$ leads to
\[
    1 - \theta_{b|X=i} = \frac{n_{i\cdot} - f_{ib}}{f_{ib}} \theta_{b|X=i}
\]
and this implies that $\theta_{b|X=i} = f_{ib}/n_i$ further implying that $\theta_{j|X=i} = f_{ij}/n_i$ gives a unique solution to the score equations.

Now the log-likelihood takes the value $-\infty$ whenever any $\theta_{j|X=i} = 0$, so the log-likelihood does not attain its maximum at such a point. Therefore, the log-likelihood is maximized at some point for which all $\theta_{j|X=i} \neq 0$, and the log-likelihood is continuously differentiable at such a point. Since the unique solution to the score equations is such a point, it must be the MLE.
\end{solution}

\begin{exercise}
\label{exer:10.2.19}
(MV) Prove that the MLE of $(\theta_1, \ldots, \theta_b)$ in \eqref{eq:10.2.4} is given by $\hat{\theta}_j = f_{\cdot j}/n$. Assume that $f_{\cdot j} > 0$ for every $i$, $j$. (Hint: Use the hint in Problem~\ref{exer:10.2.16}.)
\end{exercise}

\begin{solution}
There are $b - 1$ free parameters because $\theta_b = 1 - \sum_{j=1}^{b-1} \theta_j$. The likelihood function is given by $L(\theta_1, \ldots, \theta_{b-1} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \prod_{i=1}^{a} \prod_{j=1}^{b} \theta_j^{f_{ij}} = \prod_{j=1}^{b} \theta_j^{f_{\cdot j}}$. The log-likelihood function is given by $l(\theta_1, \ldots, \theta_{b-1} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \sum_{j=1}^{b} f_{\cdot j} \ln \theta_j$. The score function is then given by
\[
    S(\theta_1, \ldots, \theta_{b-1} \mid (x_1, y_1), \ldots, (x_n, y_n)) = \begin{pmatrix} \frac{f_{\cdot 1}}{\theta_1} - \frac{f_{\cdot b}}{\theta_b} \\ \vdots \\ \frac{f_{\cdot(b-1)}}{\theta_{b-1}} - \frac{f_{\cdot b}}{\theta_b} \end{pmatrix}.
\]
Setting this equal to 0 gives $\theta_j = (f_{\cdot j}/f_{\cdot b})\theta_b$ and summing this over $j = 1, \ldots, b - 1$ gives $1 - \theta_b = (n - f_{\cdot b})\theta_b/f_{\cdot b}$. This implies that $\theta_b = f_{\cdot b}/n$, further implying that $\theta_j = f_{\cdot j}/n$ gives a unique solution to the score equations.

Now the log-likelihood takes the value $-\infty$ whenever any $\theta_j = 0$, so the log-likelihood does not attain its maximum at such a point. Therefore the log-likelihood is maximized at some point for which all $\theta_j \neq 0$, and the log-likelihood is continuously differentiable at such a point. Since the unique solution to the score equations is such a point, it must be the MLE.
\end{solution}

\begin{exercise}
\label{exer:10.2.20}
Suppose that $X = (X_1, \ldots, X_{k-1}) \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_k)$. Determine
\begin{equation*}
\expc(X_1^{l_1} \cdots X_k^{l_k})
\end{equation*}
in terms of the gamma function, when $l_i \geqslant 0$ for $i = 1, \ldots, k$.
\end{exercise}

\begin{solution}
First, note that the density of $\text{Dirichlet}(\alpha_1, \ldots, \alpha_k)$ density is given by
\[
    \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} x_1^{\alpha_1 - 1} x_2^{\alpha_2 - 1} \cdots x_k^{\alpha_k - 1}.
\]
Therefore,
\begin{align*}
    \expc(X_1^{l_1} \cdots X_k^{l_k}) &= \int_0^1 \cdots \int_0^{1-x_2-\cdots-x_{k-1}} x_1^{l_1} \cdots (1 - x_1 - \cdots - x_{k-1})^{l_k} \\
    &\quad \times \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} x_1^{\alpha_1 - 1} x_2^{\alpha_2 - 1} \cdots (1 - x_1 - \cdots - x_{k-1})^{\alpha_k - 1} \, \mathrm{d}x_1 \cdots \mathrm{d}x_{k-1} \\
    &= \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} \frac{\Gamma(\alpha_1 + l_1) \cdots \Gamma(\alpha_k + l_k)}{\Gamma(\alpha_1 + \cdots + \alpha_k + l_1 + \cdots + l_k)}.
\end{align*}
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:10.2.21}
Suppose that $(\theta_1, \theta_2, \theta_3, \theta_4) \sim \text{Dirichlet}(1, 1, 1, 1)$ as in Exercise~\ref{exer:10.2.7}. Generate a sample of size $N = 10^4$ from this distribution and use this to estimate the expectations of the $\theta_i$. Compare these estimates with their exact values. (Hint: There is some relevant code in Appendix~B for the generation; see Appendix~C for formulas for the exact values of these expectations.)
\end{exercise}

\begin{solution}
The following R code generates a sample from a $\text{Dirichlet}(1, 1, 1, 1)$ distribution and computes the estimates of the expectations.

\begin{listing}[!htbp]
\begin{minted}{R}
# Generate samples from Dirichlet(1, 1, 1, 1) distribution
set.seed(34256734)

# Parameters
k1 <- 1; k2 <- 1; k3 <- 1; k4 <- 1
k5 <- k2 + k3 + k4
k6 <- k3 + k4
n_samples <- 10000

# Storage for samples
c2 <- c3 <- c4 <- c5 <- numeric(n_samples)

for (i in 1:n_samples) {
  # Generate from Beta distributions and construct Dirichlet
  u1 <- rbeta(1, k1, k5)
  c2[i] <- u1
  
  u2 <- rbeta(1, k2, k6)
  c3[i] <- (1 - c2[i]) * u2
  
  u3 <- rbeta(1, k3, k4)
  c4[i] <- (1 - c2[i] - c3[i]) * u3
  
  c5[i] <- 1 - c2[i] - c3[i] - c4[i]
}

# Compute estimates of expectations
k1_est <- mean(c2)
k2_est <- mean(c3)
k3_est <- mean(c4)
k4_est <- mean(c5)

cat("K1:", k1_est, "\n")
cat("K2:", k2_est, "\n")
cat("K3:", k3_est, "\n")
cat("K4:", k4_est, "\n")
\end{minted}
\caption{R code for generating Dirichlet(1,1,1,1) samples and computing expectations (dirichlet\_sample.R).}
\label{lst:dirichlet-sample}
\end{listing}

From Appendix C the exact values of each of these expectations is given by $1/(1 + 1 + 1 + 1) = .25$.
\end{solution}

\begin{exercise}
\label{exer:10.2.22}
For Problem~\ref{exer:10.2.12}, generate a sample of size $N = 10^4$ from the posterior distribution of the parameters and use this to estimate the posterior expectations of the cell probabilities. Compare these estimates with their exact values. (Hint: There is some relevant code in Appendix~B for the generation; see Appendix~C for formulas for the exact values of these expectations.)
\end{exercise}

\begin{solution}
From Problem~10.2.12 we need to generate from a $\text{Dirichlet}(18, 18, 13, 12, 10, 14, 12, 9, 20, 15, 8, 29)$ distribution. The following R code generates the sample and computes the estimates.

\begin{listing}[!htbp]
\begin{minted}{R}
# Generate samples from Dirichlet(18,18,13,12,10,14,12,9,20,15,8,29) distribution
set.seed(34256734)

# Parameters
alpha <- c(18, 18, 13, 12, 10, 14, 12, 9, 20, 15, 8, 29)
k <- length(alpha)
n_samples <- 10000

# Storage for samples
samples <- matrix(0, nrow = n_samples, ncol = k)

for (i in 1:n_samples) {
  remaining <- 1
  for (j in 1:(k-1)) {
    # Sum of remaining alpha parameters
    alpha_sum <- sum(alpha[(j+1):k])
    u <- rbeta(1, alpha[j], alpha_sum)
    samples[i, j] <- remaining * u
    remaining <- remaining - samples[i, j]
  }
  samples[i, k] <- remaining
}

# Compute estimates of posterior expectations
estimates <- colMeans(samples)
for (i in 1:k) {
  cat(sprintf("K%d: %.6f\n", i, estimates[i]))
}

# Exact posterior expected values
s <- sum(alpha)
exact <- alpha / s
cat("\nExact posterior expected values:\n")
for (i in 1:k) {
  cat(sprintf("alpha_%d/s = %.5f\n", i, exact[i]))
}
\end{minted}
\caption{R code for generating Dirichlet(18,18,13,12,10,14,12,9,20,15,8,29) samples (dirichlet\_posterior.R).}
\label{lst:dirichlet-posterior}
\end{listing}

From Appendix C the exact posterior expected values are given by (where $s = 18 + 18 + 13 + 12 + 10 + 14 + 12 + 9 + 20 + 15 + 8 + 29 = 178$) and $(18/s, 18/s, 13/s, 12/s, 10/s, 14/s, 12/s, 9/s, 20/s, 15/s, 8/s, 29/s)$. So the estimates are as recorded in the following table.
\begin{center}
\begin{tabular}{c|c}
$i$ & Estimate of posterior mean of $\alpha_i$ \\
\hline
1 & $1.0112 \times 10^{-1}$ \\
2 & $1.0112 \times 10^{-1}$ \\
3 & $7.3034 \times 10^{-2}$ \\
4 & $6.7416 \times 10^{-2}$ \\
5 & $5.6180 \times 10^{-2}$ \\
6 & $7.8652 \times 10^{-2}$ \\
7 & $6.7416 \times 10^{-2}$ \\
8 & $5.0562 \times 10^{-2}$ \\
9 & $0.11236$ \\
10 & $8.4270 \times 10^{-2}$ \\
11 & $4.4944 \times 10^{-2}$ \\
12 & $0.16292$
\end{tabular}
\end{center}
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:10.2.23}
(MV) Establish the validity of the method discussed in Example~\ref{ex:10.2.3} for generating from a $\text{Dirichlet}(\alpha_1, \ldots, \alpha_k)$ distribution.
\end{exercise}

\begin{solution}
We have that $U_1, U_2, \ldots, U_{k-1}$ are independent, with $U_i \sim \text{Beta}(\alpha_i, \alpha_{i+1} + \cdots + \alpha_k)$ and
\[
    X_1 = U_1, \quad X_2 = (1 - X_1)U_2, \quad \ldots, \quad X_{k-1} = (1 - X_1 - \cdots - X_{k-1})U_{k-1},
\]
so
\[
    U_1 = X_1, \quad U_2 = \frac{X_2}{1 - X_1}, \quad \ldots, \quad U_{k-1} = \frac{X_{k-1}}{1 - X_1 - \cdots - X_{k-1}}.
\]
From this we deduce that the matrix of partial derivatives of this transformation is lower triangular and the $i$th element along the diagonal is $\partial U_i/\partial X_i = 1/(1 - X_1 - \cdots - X_{i-1})$. Therefore, the Jacobian derivative is given by $\prod_{i=2}^{k-1} (1 - X_1 - \cdots - X_{i-1})^{-1}$. Now the joint density of $(U_1, U_2, \ldots, U_{k-1})$ proportional to
\[
    u_1^{\alpha_1 - 1}(1 - u_1)^{\alpha_2 + \cdots + \alpha_k - 1} u_2^{\alpha_2 - 1}(1 - u_2)^{\alpha_3 + \cdots + \alpha_k - 1} \cdots u_{k-1}^{\alpha_{k-1} - 1}(1 - u_{k-1})^{\alpha_k - 1}.
\]
Therefore, the joint density of $(X_1, X_2, \ldots, X_{k-1})$ is proportional to
\begin{align*}
    &\left\{ x_1^{\alpha_1 - 1}(1 - x_1)^{\alpha_2 + \cdots + \alpha_k - 1} \left(\frac{x_2}{1 - x_1}\right)^{\alpha_2 - 1} \left(1 - \frac{x_2}{1 - x_1}\right)^{\alpha_3 + \cdots + \alpha_k - 1} \right. \\
    &\quad \left. \cdots \left(\frac{x_{k-1}}{1 - x_1 - \cdots - x_{k-2}}\right)^{\alpha_{k-1} - 1} \left(1 - \frac{x_{k-1}}{1 - x_1 - \cdots - x_{k-2}}\right)^{\alpha_k - 1} \right\} \\
    &\quad \times \prod_{i=2}^{k-1} (1 - x_1 - \cdots - x_{i-1})^{-1} \\
    &= x_1^{\alpha_1 - 1} x_2^{\alpha_2 - 1} \cdots x_{k-1}^{\alpha_{k-1} - 1} (1 - x_1 - \cdots - x_{k-1})^{\alpha_k - 1} \\
    &\quad \times (1 - x_1)^{\alpha_2 + \cdots + \alpha_k - 1} (1 - x_1)^{1 - \alpha_2 - (\alpha_3 + \cdots + \alpha_k - 1)} (1 - x_1)^{-1} \\
    &\quad \times (1 - x_1 - x_2)^{\alpha_3 + \cdots + \alpha_k - 1} (1 - x_1 - x_2)^{1 - \alpha_3 - (\alpha_4 + \cdots + \alpha_k - 1)} (1 - x_1 - x_2)^{-1} \\
    &\quad \times \cdots \\
    &= x_1^{\alpha_1 - 1} x_2^{\alpha_2 - 1} \cdots x_{k-1}^{\alpha_{k-1} - 1} (1 - x_1 - \cdots - x_{k-1})^{\alpha_k - 1},
\end{align*}
so $(X_1, X_2, \ldots, X_{k-1}) \sim \text{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_k)$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quantitative Response and Predictors}
\label{sec:10.3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When the response and predictor variables are all categorical, it can be difficult to formulate simple models that adequately describe the relationship between the variables. We are left with recording the conditional distributions and plotting these in bar charts. When the response variable is quantitative, however, useful models have been formulated that give a precise mathematical expression for the form of the relationship that may exist. We will study these kinds of models in the next three sections. This section concentrates on the situation in which all the variables are quantitative.

\subsection{The Method of Least Squares}
\label{ssec:10.3.1}

The method of least squares is a general method for obtaining an estimate of a distribution mean. It does not require specific distributional assumptions and so can be thought of as a distribution-free method (see Section~\ref{sec:6.4}).

Suppose we have a random variable $Y$ and we want to estimate $\expc(Y)$ based on a sample $y_1, \ldots, y_n$. The following principle is commonly used to generate estimates.

\begin{quote}
The \emph{least-squares principle} says that we select the point $t(y_1, \ldots, y_n)$ in the set of possible values for $\expc(Y)$ that minimizes the \emph{sum of squared deviations} (hence, ``least squares'') given by $\sum_{i=1}^{n} (y_i - t(y_1, \ldots, y_n))^2$. Such an estimate is called a \emph{least-squares estimate}.
\end{quote}

Note that a least-squares estimate is defined for every sample size, even $n = 1$.

To implement least squares, we must find the minimizing point $t(y_1, \ldots, y_n)$. Perhaps a first guess at this value is the sample average $\bar{y}$. Because $\sum_{i=1}^{n} (y_i - \bar{y})(\bar{y} - t(y_1, \ldots, y_n)) = (\bar{y} - t(y_1, \ldots, y_n)) \sum_{i=1}^{n} (y_i - n\bar{y}) = 0$, we have
\begin{align}
\sum_{i=1}^{n} (y_i - t(y_1, \ldots, y_n))^2 &= \sum_{i=1}^{n} (y_i - \bar{y} + \bar{y} - t(y_1, \ldots, y_n))^2 \notag \\
&= \sum_{i=1}^{n} (y_i - \bar{y})^2 + 2 \sum_{i=1}^{n} (y_i - \bar{y})(\bar{y} - t(y_1, \ldots, y_n)) + \sum_{i=1}^{n} (\bar{y} - t(y_1, \ldots, y_n))^2 \notag \\
&= \sum_{i=1}^{n} (y_i - \bar{y})^2 + n(\bar{y} - t(y_1, \ldots, y_n))^2. \label{eq:10.3.1}
\end{align}
Therefore, the smallest possible value of \eqref{eq:10.3.1} is $\sum_{i=1}^{n} (y_i - \bar{y})^2$, and this is assumed by taking $t(y_1, \ldots, y_n) = \bar{y}$. Note, however, that $\bar{y}$ might not be a possible value for $\expc(Y)$ and that, in such a case, it will not be the least-squares estimate. In general, \eqref{eq:10.3.1} says that the least-squares estimate is the value $t(y_1, \ldots, y_n)$ that is closest to $\bar{y}$ and is a possible value for $\expc(Y)$.

Consider the following example.

\begin{example}
\label{ex:10.3.1}
Suppose that $Y$ has one of the distributions on $S = \{0, 1\}$ given in the following table.
\begin{center}
\begin{tabular}{c|cc}
 & $y = 0$ & $y = 1$ \\ \hline
$p_1(y)$ & $1/2$ & $1/2$ \\
$p_2(y)$ & $1/3$ & $2/3$
\end{tabular}
\end{center}
Then the mean of $Y$ is given by
\begin{equation*}
\expc_1(Y) = 0 \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} = \frac{1}{2} \quad \text{or} \quad \expc_2(Y) = 0 \cdot \frac{1}{3} + 1 \cdot \frac{2}{3} = \frac{2}{3}.
\end{equation*}
Now suppose we observe the sample $(0, 0, 1, 1, 1)$ and so $\bar{y} = 3/5$. Because the possible values for $\expc(Y)$ are in $\{1/2, 2/3\}$, we see that $t(0, 0, 1, 1, 1) = 2/3$ because $(3/5 - 2/3)^2 = 0.004$ while $(3/5 - 1/2)^2 = 0.01$.
\end{example}

Whenever the set of possible values for $\expc(Y)$ is an interval $(a, b)$, however, and $\prb(Y \in (a, b)) = 1$, then $\bar{y} \in (a, b)$. This implies that $\bar{y}$ is the least-squares estimator of $\expc(Y)$. So we see that in quite general circumstances, $\bar{y}$ is the least-squares estimate.

There is an equivalence between least squares and the maximum likelihood method when we are dealing with normal distributions.

\begin{example}[Least Squares with Normal Distributions]
\label{ex:10.3.2}
Suppose that $y_1, \ldots, y_n$ is a sample from an $N(\mu, \sigma_0^2)$ distribution, where $\mu$ is unknown. Then the MLE of $\mu$ is obtained by finding the value of $\mu$ that maximizes
\begin{equation*}
L(\mu \mid y_1, \ldots, y_n) \propto \exp\left\{ -\frac{n}{2\sigma_0^2} (\bar{y} - \mu)^2 \right\}.
\end{equation*}
Equivalently, the MLE maximizes the log-likelihood
\begin{equation*}
l(\mu \mid y_1, \ldots, y_n) = -\frac{n}{2\sigma_0^2} (\bar{y} - \mu)^2.
\end{equation*}
So we need to find the value of $\mu$ that minimizes $(\bar{y} - \mu)^2$, just as with least squares.

In the case of the normal location model, we see that the least-squares estimate and the MLE of $\mu$ agree. This equivalence is true in general for normal models (e.g., the location-scale normal model), at least when we are considering estimates of location parameters.
\end{example}

Some of the most important applications of least squares arise when we have that the response is a random vector $Y = (Y_1, \ldots, Y_n)'$ (the prime indicates that we consider $Y$ as a column), and we observe a single observation $y = (y_1, \ldots, y_n)' \in \mathbb{R}^n$. The expected value of $Y \in \mathbb{R}^n$ is defined to be the vector of expectations of its component random variables, namely,
\begin{equation*}
\expc(Y) = \begin{pmatrix} \expc(Y_1) \\ \vdots \\ \expc(Y_n) \end{pmatrix} \in \mathbb{R}^n.
\end{equation*}

The least-squares principle then says that, based on the single observation $y = (y_1, \ldots, y_n)$, we must find
\begin{equation*}
t(y) = t(y_1, \ldots, y_n) = (t_1(y_1, \ldots, y_n), \ldots, t_n(y_1, \ldots, y_n))
\end{equation*}
in the set of possible values for $\expc(Y)$ (a subset of $\mathbb{R}^n$), that minimizes
\begin{equation}
\label{eq:10.3.2}
\sum_{i=1}^{n} (y_i - t_i(y_1, \ldots, y_n))^2.
\end{equation}
So $t(y)$ is the possible value for $\expc(Y)$ that is closest to $y$, as the squared distance between two points $x, y \in \mathbb{R}^n$ is given by $\sum_{i=1}^{n} (x_i - y_i)^2$.

As is common in statistical applications, suppose that there are predictor variables that may be related to $Y$ and whose values are observed. In this case, we will replace $\expc(Y)$ by its conditional mean, given the observed values of the predictors. The least-squares estimate of the conditional mean is then the value $t(y_1, \ldots, y_n)$ in the set of possible values for the conditional mean of $Y$ that minimizes \eqref{eq:10.3.2}. We will use this definition in the following sections.

Finding the minimizing value of $t(y)$ in \eqref{eq:10.3.2} can be a challenging optimization problem when the set of possible values for the mean is complicated. We will now apply least squares to some important problems where the least-squares solution can be found in closed form.

\subsection{The Simple Linear Regression Model}
\label{ssec:10.3.2}

Suppose we have a single quantitative response variable $Y$ and a single quantitative predictor $X$, e.g., $Y$ could be blood pressure measured in pounds per square inch and $X$ could be age in years. To study the relationship between these variables, we examine the conditional distributions of $Y$ given $X = x$ to see how these change as we change $x$.

We might choose to examine a particular characteristic of these distributions to see how it varies with $x$. Perhaps the most commonly used characteristic is the conditional mean of $Y$ given $X = x$, or $\expc(Y \mid X = x)$ (see Section~\ref{sec:3.5}).

In the regression model (see Section~\ref{sec:10.1}), we assume that the conditional distributions have constant shape and that they change, as we change $x$, at most through the conditional mean. In the \emph{simple linear regression model}, we assume that the only way the conditional mean can change is via the relationship
\begin{equation*}
\expc(Y \mid X = x) = \beta_1 + \beta_2 x
\end{equation*}
for some unknown values of $\beta_1 \in \mathbb{R}^1$ (the \emph{intercept term}) and $\beta_2 \in \mathbb{R}^1$ (the \emph{slope coefficient}). We also refer to $\beta_1$ and $\beta_2$ as the \emph{regression coefficients}.

Suppose we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$. Then, using the simple linear regression model, we have that
\begin{equation}
\label{eq:10.3.3}
\expc \begin{pmatrix} Y_1 \\ \vdots \\ Y_n \end{pmatrix} \Bigg| X_1 = x_1, \ldots, X_n = x_n = \begin{pmatrix} \beta_1 + \beta_2 x_1 \\ \vdots \\ \beta_1 + \beta_2 x_n \end{pmatrix}.
\end{equation}
Equation \eqref{eq:10.3.3} tells us that the conditional expected value of the response $(Y_1, \ldots, Y_n)$ is in a particular subset of $\mathbb{R}^n$. Furthermore, \eqref{eq:10.3.2} becomes
\begin{equation}
\label{eq:10.3.4}
\sum_{i=1}^{n} (y_i - t_i(y))^2 = \sum_{i=1}^{n} (y_i - \beta_1 - \beta_2 x_i)^2,
\end{equation}
and we must find the values of $\beta_1$ and $\beta_2$ that minimize \eqref{eq:10.3.4}. These values are called the \emph{least-squares estimates} of $\beta_1$ and $\beta_2$.

Before we show how to do this, consider an example.

\begin{example}
\label{ex:10.3.3}
Suppose we obtained the following $n = 10$ data points $(x_i, y_i)$.
\begin{center}
\begin{tabular}{cccccccccc}
3.9 & 8.9 & $-2.6$ & 7.1 & 2.4 & 4.6 & 4.1 & 10.7 & 0.2 & $-1.0$ \\
5.4 & 12.6 & 0.6 & 3.3 & 5.6 & 10.4 & 1.1 & 2.3 & 2.1 & 1.6
\end{tabular}
\end{center}
In Figure~\ref{fig:10.3.1}, we have plotted these points together with the line $y = 1 + x$.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_1.pdf}
  \caption{A plot of the data points $(x_i, y_i)$ (+) and the line $y = 1 + x$ in Example~\ref{ex:10.3.3}.}
  \label{fig:10.3.1}
\end{figure}

Notice that with $\beta_1 = 1$ and $\beta_2 = 1$, then
\begin{equation*}
(y_i - \beta_1 - \beta_2 x_i)^2 = (y_i - 1 - x_i)^2
\end{equation*}
is the squared vertical distance between the point $(x_i, y_i)$ and the point on the line with the same $x$ value. So \eqref{eq:10.3.4} is the sum of these squared deviations and in this case equals
\begin{equation*}
(8.9 - 1 - 3.9)^2 + (7.1 - 1 - 2.6)^2 + \cdots + (1.6 - 1 - (-1.2))^2 = 141.15.
\end{equation*}
If $\beta_1 = 1$ and $\beta_2 = 1$ were the least-squares estimates, then 141.15 would be equal to the smallest possible value of \eqref{eq:10.3.4}. In this case, it turns out (see Example~\ref{ex:10.3.4}) that the least-squares estimates are given by the values $b_1 = 1.33$, $b_2 = 2.06$, and the minimized value of \eqref{eq:10.3.4} is given by $8.46$, which is much smaller than $141.15$.
\end{example}

So we see that, in finding the least-squares estimates, we are in essence finding the line $\beta_1 + \beta_2 x$ that best fits the data, in the sense that the sum of squared vertical deviations of the observed points to the line is minimized.

\paragraph{Scatter Plots}

As part of Example~\ref{ex:10.3.3}, we plotted the points $(x_1, y_1), \ldots, (x_n, y_n)$ in a graph. This is called a \emph{scatter plot}, and it is a recommended first step as part of any analysis of the relationship between quantitative variables $X$ and $Y$. A scatter plot can give us a very general idea of whether or not a relationship exists and what form it might take.

It is important to remember, however, that the appearance of such a plot is highly dependent on the scales we choose for the axes. For example, we can make a scatter plot look virtually flat (and so indicate that no relationship exists) by choosing to place too wide a range of tick marks on the $y$-axis. So we must always augment a scatter plot with a statistical analysis based on numbers.

\paragraph{Least-Squares Estimates, Predictions, and Standard Errors}

For the simple linear regression model, we can work out exact formulas for the least-squares estimates of $\beta_1$ and $\beta_2$.

\begin{theorem}
\label{thm:10.3.1}
Suppose that $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$ and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$. Then the least-squares estimates of $\beta_1$ and $\beta_2$ are given by
\begin{equation*}
b_1 = \bar{y} - b_2 \bar{x} \quad \text{and} \quad b_2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2},
\end{equation*}
respectively, whenever $\sum_{i=1}^{n} (x_i - \bar{x})^2 \neq 0$.
\end{theorem}

\begin{proof}
The proof of this result can be found in Section~\ref{sec:10.6}.
\end{proof}

We call the line $y = b_1 + b_2 x$ the \emph{least-squares line}, or \emph{best-fitting line}, and $b_1 + b_2 x$ is the least-squares estimate of $\expc(Y \mid X = x)$. Note that $\sum_{i=1}^{n} (x_i - \bar{x})^2 = 0$ if and only if $x_1 = \cdots = x_n$. In such a case we cannot use least squares to estimate $\beta_1$ and $\beta_2$, although we can still estimate $\expc(Y \mid X = x)$ (see Problem~\ref{exer:10.3.19}).

Now that we have estimates $b_1$, $b_2$ of the regression coefficients, we want to use these for inferences about $\beta_1$ and $\beta_2$. These estimates have the unbiasedness property.

\begin{theorem}
\label{thm:10.3.2}
If $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$ and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then
\begin{enumerate}[(i)]
\item $\expc(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) = \beta_1$,
\item $\expc(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) = \beta_2$.
\end{enumerate}
\end{theorem}

\begin{proof}
The proof of this result can be found in Section~\ref{sec:10.6}.
\end{proof}

Note that Theorem~\ref{thm:10.3.2} and the theorem of total expectation imply that $\expc(B_1) = \beta_1$ and $\expc(B_2) = \beta_2$ unconditionally as well.

Adding the assumption that the conditional variances exist, we have the following theorem.

\begin{theorem}
\label{thm:10.3.3}
If $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$, $\var(Y \mid X = x) = \sigma^2$ for every $x$, and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then
\begin{enumerate}[(i)]
\item $\var(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2 \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)$,
\item $\var(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) = \dfrac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$,
\item $\cov(B_1, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) = \dfrac{-\sigma^2 \bar{x}}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$.
\end{enumerate}
\end{theorem}

\begin{proof}
See Section~\ref{sec:10.6} for the proof of this result.
\end{proof}

For the least-squares estimate $b_1 + b_2 x$ of the mean $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$, we have the following result.

\begin{corollary}
\label{cor:10.3.1}
\begin{equation}
\label{eq:10.3.5}
\var(B_1 + B_2 x \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2 \left( \frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right).
\end{equation}
\end{corollary}

\begin{proof}
See Section~\ref{sec:10.6} for the proof of this result.
\end{proof}

A natural predictor of a future value of $Y$ when $X = x$ is given by the conditional mean $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$. Because we do not know the values of $\beta_1$ and $\beta_2$, we use the estimated mean $b_1 + b_2 x$ as the predictor.

When we are predicting $Y$ at an $x$ value that lies within the range of the observed values of $X$, we refer to this as an \emph{interpolation}. When we want to predict at an $x$ value that lies outside this range, we refer to this as an \emph{extrapolation}. Extrapolations are much less reliable than interpolations. The farther away $x$ is from the observed range of $X$ values, then, intuitively, the less reliable we feel such a prediction will be. Such considerations should always be borne in mind. From \eqref{eq:10.3.5}, we see that the variance of the prediction at the value $X = x$ increases as $x$ moves away from $\bar{x}$. So to a certain extent, the standard error does reflect this increased uncertainty, but note that its form is based on the assumption that the simple linear regression model is correct. Even if we accept the simple linear regression model based on the observed data (we will discuss model checking later in this section), this model may fail to apply for very different values of $x$ and so the predictions would be in error.

We want to use the results of Theorem~\ref{thm:10.3.3} and Corollary~\ref{cor:10.3.1} to calculate standard errors of the least-squares estimates. Because we do not know $\sigma^2$, however, we need an estimate of this quantity as well. The following result shows that
\begin{equation}
\label{eq:10.3.6}
s^2 = \frac{1}{n-2} \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2
\end{equation}
is an unbiased estimate of $\sigma^2$.

\begin{theorem}
\label{thm:10.3.4}
If $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$, $\var(Y \mid X = x) = \sigma^2$ for every $x$, and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then
\begin{equation*}
\expc(S^2 \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2.
\end{equation*}
\end{theorem}

\begin{proof}
See Section~\ref{sec:10.6} for the proof of this result.
\end{proof}

Therefore, the standard error of $b_1$ is then given by
\begin{equation*}
s \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2},
\end{equation*}
and the standard error of $b_2$ is then given by
\begin{equation*}
s \left( \sum_{i=1}^{n} (x_i - \bar{x})^2 \right)^{-1/2}.
\end{equation*}

Under further assumptions, these standard errors can be interpreted just as we interpreted standard errors of estimates of the mean in the location and location-scale normal models.

\begin{example}[Example~\ref{ex:10.3.3} continued]
\label{ex:10.3.4}
Using the data in Example~\ref{ex:10.3.3} and the formulas of Theorem~\ref{thm:10.3.1}, we obtain $b_1 = 1.33$, $b_2 = 2.06$ as the least-squares estimates of the intercept and slope, respectively. So the least-squares line is given by $1.33 + 2.06x$. Using \eqref{eq:10.3.6}, we obtain $s^2 = 1.06$ as the estimate of $\sigma^2$.

Using the formulas of Theorem~\ref{thm:10.3.3}, the standard error of $b_1$ is $0.3408$, while the standard error of $b_2$ is $0.1023$.

The prediction of $Y$ at $X = 2.0$ is given by $1.33 + 2.06 \times 2 = 5.45$. Using Corollary~\ref{cor:10.3.1}, this estimate has standard error $0.341$. This prediction is an interpolation.
\end{example}

\paragraph{The ANOVA Decomposition and the $F$-Statistic}

The following result gives a decomposition of the total sum of squares $\sum_{i=1}^{n} (y_i - \bar{y})^2$.

\begin{lemma}
\label{lem:10.3.1}
If $(x_1, y_1), \ldots, (x_n, y_n)$ are such that $\sum_{i=1}^{n} (x_i - \bar{x})^2 \neq 0$, then
\begin{equation*}
\sum_{i=1}^{n} (y_i - \bar{y})^2 = b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2 + \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2.
\end{equation*}
\end{lemma}

\begin{proof}
The proof of this result can be found in Section~\ref{sec:10.6}.
\end{proof}

We refer to
\begin{equation*}
b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2
\end{equation*}
as the \emph{regression sum of squares} (RSS) and refer to
\begin{equation*}
\sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2
\end{equation*}
as the \emph{error sum of squares} (ESS).

If we think of the total sum of squares as measuring the total observed variation in the response values $y_i$, then Lemma~\ref{lem:10.3.1} provides a decomposition of this variation into the RSS, measuring changes in the response due to changes in the predictor, and the ESS, measuring changes in the response due to the contribution of random error.

It is common to write this decomposition in an \emph{analysis of variance table} (ANOVA).
\begin{center}
\begin{tabular}{l|c|c|c}
Source & Df & Sum of Squares & Mean Square \\ \hline
$X$ & 1 & $b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2$ & $b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2$ \\
Error & $n - 2$ & $\sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2$ & $s^2$ \\ \hline
Total & $n - 1$ & $\sum_{i=1}^{n} (y_i - \bar{y})^2$ &
\end{tabular}
\end{center}

Here, Df stands for \emph{degrees of freedom} (we will discuss how the Df entries are calculated in Section~\ref{ssec:10.3.4}). The entries in the Mean Square column are calculated by dividing the corresponding sum of squares by the Df entry.

To see the significance of the ANOVA table, note that, from Theorem~\ref{thm:10.3.3},
\begin{equation}
\label{eq:10.3.7}
\expc \left( B_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2 \mid X_1 = x_1, \ldots, X_n = x_n \right) = \sigma^2 + \beta_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2,
\end{equation}
which is equal to $\sigma^2$ if and only if $\beta_2 = 0$ (we are always assuming here that the $x_i$ vary). Given that the simple linear regression model is correct, we have that $\beta_2 = 0$ if and only if there is no relationship between the response and the predictor. Therefore, $b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2$ is an unbiased estimator of $\sigma^2$ if and only if $\beta_2 = 0$. Because $s^2$ is always an unbiased estimate of $\sigma^2$ (Theorem~\ref{thm:10.3.4}), a sensible statistic to use in assessing $H_0 : \beta_2 = 0$, is given by
\begin{equation}
\label{eq:10.3.8}
F = \frac{\text{RSS}}{\text{ESS}/(n-2)} = \frac{b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2}{s^2},
\end{equation}
as this is the ratio of two unbiased estimators of $\sigma^2$ when $H_0$ is true. We then conclude that we have evidence against $H_0$ when $F$ is large, as \eqref{eq:10.3.7} also shows that the numerator will tend to be larger than $\sigma^2$ when $H_0$ is false. We refer to \eqref{eq:10.3.8} as the \emph{$F$-statistic}. We will subsequently discuss the sampling distribution of $F$ to see how to determine when the value $F$ is so large as to be evidence against $H_0$.

\begin{example}[Example~\ref{ex:10.3.3} continued]
\label{ex:10.3.5}
Using the data of Example~\ref{ex:10.3.3}, we obtain
\begin{align*}
\sum_{i=1}^{n} (y_i - \bar{y})^2 &= 437.01, \\
b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2 &= 428.55, \\
\sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2 &= 437.01 - 428.55 = 8.46,
\end{align*}
and so
\begin{equation*}
F = \frac{b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2}{s^2} = \frac{428.55}{1.06} = 404.29.
\end{equation*}
Note that $F$ is much bigger than 1, and this seems to indicate a linear effect due to $X$.
\end{example}

\paragraph{The Coefficient of Determination and Correlation}

Lemma~\ref{lem:10.3.1} implies that
\begin{equation*}
R^2 = \frac{b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation*}
satisfies $0 \leqslant R^2 \leqslant 1$. Therefore, the closer $R^2$ is to 1, the more of the observed total variation in the response is accounted for by changes in the predictor. In fact, we interpret $R^2$, called the \emph{coefficient of determination}, as the proportion of the observed variation in the response explained by changes in the predictor via the simple linear regression.

The coefficient of determination is an important descriptive statistic, for, even if we conclude that a relationship does exist, it can happen that most of the observed variation is due to error. If we want to use the model to predict further values of the response, then the coefficient of determination tells us whether we can expect highly accurate predictions or not. A value of $R^2$ near 1 means highly accurate predictions, whereas a value near 0 means that predictions will not be very accurate.

\begin{example}[Example~\ref{ex:10.3.3} continued]
\label{ex:10.3.6}
Using the data of Example~\ref{ex:10.3.3}, we obtain $R^2 = 0.981$. Therefore, 98.1\% of the observed variation in $Y$ can be explained by the changes in $X$ through the linear relation. This indicates that we can expect fairly accurate predictions when using this model, at least when we are predicting within the range of the observed $X$ values.
\end{example}

Recall that in Section~\ref{sec:3.3}, we defined the correlation coefficient between random variables $X$ and $Y$ to be
\begin{equation*}
\rho_{XY} = \cor(X, Y) = \frac{\cov(X, Y)}{\text{Sd}(X) \, \text{Sd}(Y)}.
\end{equation*}
In Corollary~3.6.1, we proved that $-1 \leqslant \rho_{XY} \leqslant 1$ with $|\rho_{XY}| = 1$ if and only if $Y = a + cX$ for some constants $a \in \mathbb{R}^1$ and $c \neq 0$. So $\rho_{XY}$ can be taken as a measure of the extent to which a linear relationship exists between $X$ and $Y$.

If we do not know the joint distribution of $(X, Y)$, then we will have to estimate $\rho_{XY}$. Based on the observations $(x_1, y_1), \ldots, (x_n, y_n)$, the natural estimate to use is the \emph{sample correlation coefficient}
\begin{equation*}
r_{xy} = \frac{s_{xy}}{s_x s_y},
\end{equation*}
where
\begin{equation*}
s_{xy} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
\end{equation*}
is the \emph{sample covariance} estimating $\cov(X, Y)$, and $s_x$, $s_y$ are the sample standard deviations for the $X$ and $Y$ variables, respectively. Then $-1 \leqslant r_{xy} \leqslant 1$ with $|r_{xy}| = 1$ if and only if $y_i = a + cx_i$ for some constants $a \in \mathbb{R}^1$ and $c \neq 0$ for every $i$ (the proof is the same as in Corollary~3.6.1 using the joint distribution that puts probability mass $1/n$ at each point $(x_i, y_i)$ --- see Problem~\ref{exer:3.6.16}).

The following result shows that the coefficient of determination is the square of the correlation between the observed $X$ and $Y$ values.

\begin{theorem}
\label{thm:10.3.5}
If $(x_1, y_1), \ldots, (x_n, y_n)$ are such that $\sum_{i=1}^{n} (x_i - \bar{x})^2 \neq 0$, $\sum_{i=1}^{n} (y_i - \bar{y})^2 \neq 0$, then $R^2 = r_{xy}^2$.
\end{theorem}

\begin{proof}
We have
\begin{equation*}
r_{xy}^2 = \frac{\left( \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) \right)^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i - \bar{y})^2} = \frac{b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} = R^2,
\end{equation*}
where we have used the formula for $b_2$ given in Theorem~\ref{thm:10.3.1}.
\end{proof}

\paragraph{Confidence Intervals and Testing Hypotheses}

We need to make some further assumptions in order to discuss the sampling distributions of the various statistics that we have introduced. We have the following results.

\begin{theorem}
\label{thm:10.3.6}
If $Y$, given $X = x$, is distributed $N(\beta_1 + \beta_2 x, \sigma^2)$ and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then the conditional distributions of $B_1$, $B_2$, and $S^2$, given $X_1 = x_1, \ldots, X_n = x_n$, are as follows.
\begin{enumerate}[(i)]
\item $B_1 \sim N\left( \beta_1, \sigma^2 \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right) \right)$
\item $B_2 \sim N\left( \beta_2, \dfrac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)$
\item $B_1 + B_2 x \sim N\left( \beta_1 + \beta_2 x, \sigma^2 \left( \dfrac{1}{n} + \dfrac{(x - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right) \right)$
\item $(n-2)S^2/\sigma^2 \sim \chi^2(n-2)$ independent of $(B_1, B_2)$
\end{enumerate}
\end{theorem}

\begin{proof}
The proof of this result can be found in Section~\ref{sec:10.6}.
\end{proof}

\begin{corollary}
\label{cor:10.3.2}
\begin{enumerate}[(i)]
\item $\dfrac{B_1 - \beta_1}{S \left( \dfrac{1}{n} + \dfrac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2}} \sim t(n-2)$
\item $\dfrac{(B_2 - \beta_2) \left( \sum_{i=1}^{n} (x_i - \bar{x})^2 \right)^{1/2}}{S} \sim t(n-2)$
\item $\dfrac{B_1 + B_2 x - (\beta_1 + \beta_2 x)}{S \left( \dfrac{1}{n} + \dfrac{(x - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2}} \sim t(n-2)$
\item If $F$ is defined as in \eqref{eq:10.3.8}, then $H_0 : \beta_2 = 0$ is true if and only if $F \sim F(1, n-2)$.
\end{enumerate}
\end{corollary}

\begin{proof}
The proof of this result can be found in Section~\ref{sec:10.6}.
\end{proof}

Using Corollary~\ref{cor:10.3.2}(i), we have that
\begin{equation*}
b_1 \pm s \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2} t_{(1+\gamma)/2}(n-2)
\end{equation*}
is an exact $\gamma$-confidence interval for $\beta_1$. Also, from Corollary~\ref{cor:10.3.2}(ii),
\begin{equation*}
b_2 \pm s \left( \sum_{i=1}^{n} (x_i - \bar{x})^2 \right)^{-1/2} t_{(1+\gamma)/2}(n-2)
\end{equation*}
is an exact $\gamma$-confidence interval for $\beta_2$.

From Corollary~\ref{cor:10.3.2}(iv), we can test $H_0 : \beta_2 = 0$ by computing the P-value
\begin{equation}
\label{eq:10.3.9}
\prb \left( F \geqslant \frac{b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2}{s^2} \right),
\end{equation}
where $F \sim F(1, n-2)$, to see whether or not the observed value \eqref{eq:10.3.8} is surprising. This is sometimes called the \emph{ANOVA test}. Note that Corollary~\ref{cor:10.3.2}(ii) implies that we can also test $H_0 : \beta_2 = 0$ by computing the P-value
\begin{equation}
\label{eq:10.3.10}
\prb \left( |T| \geqslant \frac{|b_2| \left( \sum_{i=1}^{n} (x_i - \bar{x})^2 \right)^{1/2}}{s} \right),
\end{equation}
where $T \sim t(n-2)$. The proof of Corollary~\ref{cor:10.3.2}(iv) reveals that \eqref{eq:10.3.9} and \eqref{eq:10.3.10} are equal.

\begin{example}[Example~\ref{ex:10.3.3} continued]
\label{ex:10.3.7}
Using software or Table~D.4, we obtain $t_{0.975}(8) = 2.306$. Then, using the data of Example~\ref{ex:10.3.3}, we obtain a 0.95-confidence interval for $\beta_1$ as
\begin{align*}
b_1 &\pm s \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2} t_{(1+\gamma)/2}(n-2) \\
&= 1.33 \pm 0.3408 \times 2.306 = [0.544, 2.116],
\end{align*}
and a 0.95-confidence interval for $\beta_2$ as
\begin{align*}
b_2 &\pm s \left( \sum_{i=1}^{n} (x_i - \bar{x})^2 \right)^{-1/2} t_{(1+\gamma)/2}(n-2) \\
&= 2.06 \pm 0.1023 \times 2.306 = [1.824, 2.296].
\end{align*}
The 0.95-confidence interval for $\beta_2$ does not include 0, so we have evidence against the null hypothesis $H_0 : \beta_2 = 0$ and conclude that there is evidence of a relationship between $X$ and $Y$. This is confirmed by the $F$-test of this null hypothesis, as it gives the P-value $\prb(F \geqslant 404.29) = 0.000$ when $F \sim F(1, 8)$.
\end{example}

\paragraph{Analysis of Residuals}

In an application of the simple regression model, we must check to make sure that the assumptions make sense in light of the data we have collected. Model checking is based on the residuals $y_i - b_1 - b_2 x_i$ (after standardization), as discussed in Section~\ref{sec:9.1}. Note that the $i$th residual is just the difference between the observed value $y_i$ at $x_i$ and the predicted value $b_1 + b_2 x_i$ at $x_i$.

From the proof of Theorem~\ref{thm:10.3.4}, we have the following result.

\begin{corollary}
\label{cor:10.3.3}
\begin{enumerate}[(i)]
\item $\expc(Y_i - B_1 - B_2 x_i \mid X_1 = x_1, \ldots, X_n = x_n) = 0$
\item $\var(Y_i - B_1 - B_2 x_i \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2 \left( 1 - \dfrac{1}{n} - \dfrac{(x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)$
\end{enumerate}
\end{corollary}

This leads to the definition of the $i$th \emph{standardized residual} as
\begin{equation}
\label{eq:10.3.11}
\frac{y_i - b_1 - b_2 x_i}{s \left( 1 - \dfrac{1}{n} - \dfrac{(x_i - \bar{x})^2}{\sum_{j=1}^{n} (x_j - \bar{x})^2} \right)^{1/2}}.
\end{equation}

Corollary~\ref{cor:10.3.3} says that \eqref{eq:10.3.11}, with $\sigma$ replacing $s$, is a value from a distribution with conditional mean 0 and conditional variance 1. Furthermore, when the conditional distribution of the response given the predictors is normal, then the conditional distribution of this quantity is $N(0, 1)$ (see Problem~\ref{exer:10.3.21}). These results are approximately true for \eqref{eq:10.3.11} for large $n$. Furthermore, it can be shown (see Problem~\ref{exer:10.3.20}) that
\begin{equation*}
\cov(Y_i - B_1 - B_2 x_i, Y_j - B_1 - B_2 x_j \mid X_1 = x_1, \ldots, X_n = x_n) = -\sigma^2 \left( \frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{\sum_{k=1}^{n} (x_k - \bar{x})^2} \right).
\end{equation*}
Therefore, under the normality assumption, the residuals are approximately independent when $n$ is large and
\begin{equation*}
\frac{(x_i - \bar{x})}{\sum_{k=1}^{n} (x_k - \bar{x})^2} \to 0
\end{equation*}
as $n \to \infty$. This will be the case whenever $\var(X)$ is finite (see Challenge~\ref{exer:10.3.27}) or, in the design context, when the values of the predictor are chosen accordingly. So one approach to model checking here is to see whether the values given by \eqref{eq:10.3.11} look at all like a sample from the $N(0, 1)$ distribution. For this, we can use the plots discussed in Chapter~\ref{ch:9}.

\begin{example}[Example~\ref{ex:10.3.3} continued]
\label{ex:10.3.8}
Using the data of Example~\ref{ex:10.3.3}, we obtain the following standardized residuals.
\begin{center}
\begin{tabular}{ccccc}
$0.49643$ & $-0.43212$ & $-1.73371$ & $1.00487$ & $0.08358$ \\
$0.17348$ & $-0.75281$ & $0.28430$ & $1.43570$ & $-1.51027$
\end{tabular}
\end{center}
These are plotted against the predictor $x$ in Figure~\ref{fig:10.3.2}.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_2.pdf}
  \caption{Plot of the standardized residuals in Example~\ref{ex:10.3.8}.}
  \label{fig:10.3.2}
\end{figure}

It is recommended that we plot the standardized residuals against the predictor, as this may reveal some underlying relationship that has not been captured by the model. This residual plot looks reasonable. In Figure~\ref{fig:10.3.3}, we have a normal probability plot of the standardized residuals. These points lie close to the line through the origin with slope equal to 1, so we conclude that we have no evidence against the model here.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_3.pdf}
  \caption{Normal probability plot of the standardized residuals in Example~\ref{ex:10.3.8}.}
  \label{fig:10.3.3}
\end{figure}

What do we do if model checking leads to a failure of the model? As discussed in Chapter~\ref{ch:9}, perhaps the most common approach is to consider making various transformations of the data to see whether there is a simple modification of the model that will pass. We can make transformations, not only to the response variable $Y$, but to the predictor variable $X$ as well.

\paragraph{An Application of Simple Linear Regression Analysis}

The following data set is taken from \emph{Statistical Methods}, 6th ed., by G.~Snedecor and W.~Cochran (Iowa State University Press, Ames, 1967) and gives the record speed $Y$ in miles per hour at the Indianapolis Memorial Day car races in the years 1911--1941, excepting the years 1917--1918. We have coded the year $X$ starting at 0 in 1911 and incrementing by 1 for each year. There are $n = 29$ data points $(x_i, y_i)$. The goal of the analysis is to obtain the least-squares line and, if warranted, make inferences about the regression coefficients. We take the normal simple linear regression model as our statistical model. Note that this is an observational study.
\begin{center}
\begin{tabular}{cc|cc|cc}
Year & Speed & Year & Speed & Year & Speed \\ \hline
0 & 74.6 & 12 & 91.0 & 22 & 104.2 \\
1 & 78.7 & 13 & 98.2 & 23 & 104.9 \\
2 & 75.9 & 14 & 101.1 & 24 & 106.2 \\
3 & 82.5 & 15 & 95.9 & 25 & 109.1 \\
4 & 89.8 & 16 & 97.5 & 26 & 113.6 \\
5 & 83.3 & 17 & 99.5 & 27 & 117.2 \\
8 & 88.1 & 18 & 97.6 & 28 & 115.0 \\
9 & 88.6 & 19 & 100.4 & 29 & 114.3 \\
10 & 89.6 & 20 & 96.6 & 30 & 115.1 \\
11 & 94.5 & 21 & 104.1 &  & 
\end{tabular}
\end{center}

Using Theorem~\ref{thm:10.3.1}, we obtain the least-squares line as $y = 77.5681 + 1.27793x$. This line, together with a scatter plot of the values $(x_i, y_i)$, is plotted in Figure~\ref{fig:10.3.4}. The fit looks quite good, but this is no guarantee of model correctness, and we must carry out some form of model checking.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_4.pdf}
  \caption{A scatter plot of the data together with a plot of the least-squares line.}
  \label{fig:10.3.4}
\end{figure}

Figure~\ref{fig:10.3.5} is a plot of the standardized residuals against the predictor. This plot looks reasonable, with no particularly unusual pattern apparent. Figure~\ref{fig:10.3.6} is a normal probability plot of the standardized residuals. The curvature in the center might give rise to some doubt about the normality assumption. We generated a few samples of $n = 29$ from an $N(0, 1)$ distribution, however, and looking at the normal probability plots (always recommended) reveals that this is not much cause for concern. Of course, we should also carry out model checking procedures based upon the standardized residuals and using P-values, but we do not pursue this topic further here.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_5.pdf}
  \caption{A plot of the standardized residuals against the predictor.}
  \label{fig:10.3.5}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_6.pdf}
  \caption{A normal probability plot of the standardized residuals.}
  \label{fig:10.3.6}
\end{figure}

Based on the results of our model checking, we decide to proceed to inferences about the regression coefficients. The estimates and their standard errors are given in the following table, where we have used the estimate of $\sigma^2$ given by $s^2 = 2.999^2$, to compute the standard errors. We have also recorded the $t$-statistics appropriate for testing each of the hypotheses $H_0 : \beta_1 = 0$ and $H_0 : \beta_2 = 0$.
\begin{center}
\begin{tabular}{c|ccc}
Coefficient & Estimate & Standard Error & $t$-statistic \\ \hline
$\beta_1$ & 77.568 & 1.118 & 69.39 \\
$\beta_2$ & 1.278 & 0.062 & 20.55
\end{tabular}
\end{center}

From this, we see that the P-value for assessing $H_0 : \beta_2 = 0$ is given by
\begin{equation*}
\prb(|T| \geqslant 20.55) = 0.000
\end{equation*}
when $T \sim t(27)$, and so we have strong evidence against $H_0$. It seems clear that there is a strong positive relationship between $Y$ and $X$. Since the 0.975 point of the $t(27)$ distribution equals 2.0518, a 0.95-confidence interval for $\beta_2$ is given by
\begin{equation*}
1.278 \pm 0.062 \times 2.0518 = [1.1508, 1.4052].
\end{equation*}

The ANOVA decomposition is given in the following table.
\begin{center}
\begin{tabular}{l|c|c|c}
Source & Df & Sum of Squares & Mean Square \\ \hline
Regression & 1 & 3797.0 & 3797.0 \\
Error & 27 & 242.8 & 9.0 \\ \hline
Total & 28 & 4039.8 & 
\end{tabular}
\end{center}

Accordingly, we have that $F = 3797.0/9.0 = 421.888$ and, as $F \sim F(1, 27)$ when $H_0 : \beta_2 = 0$ is true, $\prb(F \geqslant 421.888) = 0.000$, which simply confirms (as it must) what we got from the preceding $t$-test.

The coefficient of determination is given by $R^2 = 3797.0/4039.8 = 0.94$. Therefore, 94\% of the observed variation in the response variable can be explained by the changes in the predictor through the simple linear regression. The value of $R^2$ indicates that the fitted model will be an excellent predictor of future values, provided that the value of $X$ that we want to predict at is in the range (or close to it) of the values of $X$ used to fit the model.

\subsection{Bayesian Simple Linear Model (Advanced)}
\label{ssec:10.3.3}

For the Bayesian formulation of the simple linear regression model with normal error, we need to add a prior distribution for the unknown parameters of the model, namely, $\beta_1$, $\beta_2$, and $\sigma^2$. There are many possible choices for this. A relevant prior is dependent on the application.

To help simplify the calculations, we reparameterize the model as follows. Let
\begin{equation*}
\mu_1 = \beta_1 + \beta_2 \bar{x} \quad \text{and} \quad \mu_2 = \beta_2.
\end{equation*}
It is then easy to show (see Problem~\ref{exer:10.3.24}) that
\begin{align}
\sum_{i=1}^{n} (y_i - \beta_1 - \beta_2 x_i)^2 &= \sum_{i=1}^{n} (y_i - \mu_1 - \mu_2(x_i - \bar{x}))^2 \notag \\
&= \sum_{i=1}^{n} (y_i - \bar{y})^2 - n(\mu_1 - \bar{y})^2 - \mu_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2 - 2\mu_2 \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}). \label{eq:10.3.12}
\end{align}

The likelihood function, using this reparameterization, then equals
\begin{equation*}
(\sigma^2)^{-n/2} \exp \left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - \mu_1 - \mu_2(x_i - \bar{x}))^2 \right\}.
\end{equation*}

From \eqref{eq:10.3.12}, and setting
\begin{equation*}
c_x^2 = \sum_{i=1}^{n} (x_i - \bar{x})^2, \quad c_y^2 = \sum_{i=1}^{n} (y_i - \bar{y})^2, \quad c_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}),
\end{equation*}
we can write this as
\begin{align*}
&(\sigma^2)^{-n/2} \exp \left\{ -\frac{c_y^2}{2\sigma^2} \right\} \exp \left\{ -\frac{n}{2\sigma^2} (\mu_1 - \bar{y})^2 \right\} \exp \left\{ -\frac{1}{2\sigma^2} (\mu_2^2 c_x^2 + 2\mu_2 c_{xy}) \right\} \\
&= (\sigma^2)^{-n/2} \exp \left\{ -\frac{c_y^2 - c_x^2 a^2}{2\sigma^2} \right\} \exp \left\{ -\frac{n}{2\sigma^2} (\mu_1 - \bar{y})^2 \right\} \exp \left\{ -\frac{c_x^2}{2\sigma^2} (\mu_2 - a)^2 \right\},
\end{align*}
where the last equality follows from $\mu_2^2 c_x^2 + 2\mu_2 c_{xy} = c_x^2(\mu_2 - a)^2 - c_x^2 a^2$ with $a = c_{xy}/c_x^2$.

This implies that, whenever the prior distribution on $(\mu_1, \mu_2)$ is such that $\mu_1$ and $\mu_2$ are independent given $\sigma^2$, then the posterior distributions of $\mu_1$ and $\mu_2$ are also independent given $\sigma^2$. Note also that $\bar{y}$ and $a$ are the least-squares estimates (as well as the MLE's) of $\mu_1$ and $\mu_2$, respectively (see Problem~\ref{exer:10.3.24}).

Now suppose we take the prior to be
\begin{align*}
(\mu_1 \mid \sigma^2) &\sim N(\nu_1, \sigma^2/\tau_1), \\
(\mu_2 \mid \sigma^2) &\sim N(\nu_2, \sigma^2/\tau_2), \\
1/\sigma^2 &\sim \text{Gamma}(\alpha, \beta).
\end{align*}
Note that $\mu_1$ and $\mu_2$ are independent given $\sigma^2$.

As it turns out, this prior is conjugate, so we can easily determine an exact form for the posterior distribution (see Problem~\ref{exer:10.3.25}). The joint posterior of $(\mu_1, \mu_2, 1/\sigma^2)$ is given by
\begin{align*}
(\mu_1 \mid \sigma^2) &\sim N \left( \frac{n + \tau_1^{-1} \nu_1 + n\bar{y}}{n + \tau_1^{-1}}, \frac{\sigma^2}{n + \tau_1^{-1}} \right), \\
(\mu_2 \mid \sigma^2) &\sim N \left( \frac{c_x^2 + \tau_2^{-1} \nu_2 + c_x^2 a}{c_x^2 + \tau_2^{-1}}, \frac{\sigma^2}{c_x^2 + \tau_2^{-1}} \right), \\
1/\sigma^2 &\sim \text{Gamma}\left( \alpha + \frac{n}{2}, \beta_{xy} \right),
\end{align*}
where
\begin{align*}
\beta_{xy} &= \beta + \frac{1}{2} (c_y^2 - c_x^2 a^2) + \frac{n\bar{y}^2}{2} + \frac{\tau_1^{-1} \nu_1^2}{2} - \frac{(n + \tau_1^{-1})^{-1} (n\bar{y} + \tau_1^{-1} \nu_1)^2}{2} \\
&\quad + \frac{c_x^2 a^2}{2} + \frac{\tau_2^{-1} \nu_2^2}{2} - \frac{(c_x^2 + \tau_2^{-1})^{-1} (c_x^2 a + \tau_2^{-1} \nu_2)^2}{2}.
\end{align*}

Of course, we must select the values of the hyperparameters $\nu_1$, $\tau_1$, $\nu_2$, $\tau_2$, $\alpha$, and $\beta$ to fully specify the prior.

Now observe that for a diffuse analysis, i.e., when we have little or no prior information about the parameters, we let $\tau_1, \tau_2 \to \infty$ and $\alpha \to 0$, and the posterior converges to
\begin{align*}
(\mu_1 \mid \sigma^2) &\sim N(\bar{y}, \sigma^2/n), \\
(\mu_2 \mid \sigma^2) &\sim N(a, \sigma^2/c_x^2), \\
1/\sigma^2 &\sim \text{Gamma}((n-2)/2, \beta_{xy}),
\end{align*}
where $\beta_{xy} = \frac{1}{2}(c_y^2 - c_x^2 a^2)$. But this still leaves us with the necessity of choosing the hyperparameter $\beta$. We will see, however, that this choice has only a small effect on the analysis when $n$ is not too small.

We can easily work out the marginal posterior distribution of the $\mu_i$. For example, in the diffuse case, the marginal posterior density of $\mu_2$ is proportional to
\begin{align*}
&\int_0^\infty (1/\sigma^2)^{1/2} \exp \left\{ -\frac{c_x^2}{2\sigma^2} (\mu_2 - a)^2 \right\} (1/\sigma^2)^{(n-2)/2 - 1} \exp \left\{ -\frac{\beta_{xy}}{\sigma^2} \right\} \mathrm{d}(1/\sigma^2) \\
&= \int_0^\infty (1/\sigma^2)^{(n-2-1)/2} \exp \left\{ -\frac{\beta_{xy} + \frac{c_x^2}{2}(\mu_2 - a)^2}{\sigma^2} \right\} \mathrm{d}(1/\sigma^2).
\end{align*}
Making the change of variable $(1/\sigma^2) = \lambda$, where
\begin{equation*}
\lambda = \beta_{xy} + \frac{c_x^2}{2} (\mu_2 - a)^2 \cdot \frac{1}{\sigma^2},
\end{equation*}
in the preceding integral, shows that the marginal posterior density of $\mu_2$ is proportional to
\begin{equation*}
\left( 1 + \frac{c_x^2}{2\beta_{xy}} (\mu_2 - a)^2 \right)^{-(n-1)/2} \int_0^\infty \lambda^{(n-2-1)/2} \exp(-\lambda) \, \mathrm{d}\lambda,
\end{equation*}
which is proportional to
\begin{equation*}
\left( 1 + \frac{c_x^2}{2\beta_{xy}} (\mu_2 - a)^2 \right)^{2(-(n-1)/2)}.
\end{equation*}

This establishes (see Problem~\ref{exer:4.6.17}) that the posterior distribution of $\mu_2$ is specified by
\begin{equation*}
\frac{\mu_2 - a}{\sqrt{2\beta_{xy}/c_x^2}} \sim t(2\alpha + n).
\end{equation*}
So a $\gamma$-HPD (highest posterior density) interval for $\mu_2$ is given by
\begin{equation*}
a \pm \sqrt{\frac{2\beta_{xy}}{c_x^2}} \, t_{(1+\gamma)/2}(2\alpha + n).
\end{equation*}
Note that these intervals will not change much as we change $\alpha$, provided that $n$ is not too small.

We consider an application of a Bayesian analysis for such a model.

\begin{example}[Haavelmo's Data on Income and Investment]
\label{ex:10.3.9}
The data for this example were taken from \emph{An Introduction to Bayesian Inference in Econometrics}, by A.~Zellner (Wiley Classics, New York, 1996). The response variable $Y$ is income in U.S.\ dollars per capita (deflated), and the predictor variable $X$ is investment in dollars per capita (deflated) for the United States for the years 1922--1941. The data are provided in the following table.
\begin{center}
\begin{tabular}{c|cc|c|cc}
Year & Income & Investment & Year & Income & Investment \\ \hline
1922 & 433 & 39 & 1932 & 372 & 22 \\
1923 & 483 & 60 & 1933 & 381 & 17 \\
1924 & 479 & 42 & 1934 & 419 & 27 \\
1925 & 486 & 52 & 1935 & 449 & 33 \\
1926 & 494 & 47 & 1936 & 511 & 48 \\
1927 & 498 & 51 & 1937 & 520 & 51 \\
1928 & 511 & 45 & 1938 & 477 & 33 \\
1929 & 534 & 60 & 1939 & 517 & 46 \\
1930 & 478 & 39 & 1940 & 548 & 54 \\
1931 & 440 & 41 & 1941 & 629 & 100
\end{tabular}
\end{center}

In Figure~\ref{fig:10.3.7}, we present a normal probability plot of the standardized residuals, obtained via a least-squares fit. In Figure~\ref{fig:10.3.8}, we present a plot of the standardized residuals against the predictor. Both plots indicate that the model assumptions are reasonable.

Suppose now that we analyze these data using the limiting diffuse prior with $\alpha = 2$. Here, we have that $\bar{y} = 483$, $c_y^2 = 64993$, $c_x^2 = 5710.55$, and $c_{xy} = 17408.3$, so that $a = 17408.3/5710.55 = 3.05$ and $\beta_{xy} = 64993 - 17408.3^2/23792.35$. The posterior is then given by
\begin{align*}
(\mu_1 \mid \sigma^2) &\sim N(483, \sigma^2/20), \\
(\mu_2 \mid \sigma^2) &\sim N(3.05, \sigma^2/5710.55), \\
1/\sigma^2 &\sim \text{Gamma}(12, 23792.35).
\end{align*}

The primary interest here is in the investment multiplier $\mu_2$. By the above results, a 0.95-HPD interval for $\mu_2$, using $t_{0.975}(24) = 2.0639$, is given by
\begin{align*}
a &\pm \sqrt{\frac{2\beta_{xy}}{c_x^2}} \, t_{(1+\gamma)/2}(2\alpha + n)^{-1} \\
&= 3.05 \pm \sqrt{\frac{2 \times 23792.35}{5710.55}} \, t_{0.975}(24) = 3.05 \pm 0.589 \times 2.0639 \\
&= [1.834, 4.266].
\end{align*}
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_7.pdf}
  \caption{Normal probability plot of the standardized residuals in Example~\ref{ex:10.3.9}.}
  \label{fig:10.3.7}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_8.pdf}
  \caption{Plot of the standardized residuals against the predictor in Example~\ref{ex:10.3.9}.}
  \label{fig:10.3.8}
\end{figure}

\subsection{The Multiple Linear Regression Model (Advanced)}
\label{ssec:10.3.4}

We now consider the situation in which we have a quantitative response $Y$ and quantitative predictors $X_1, \ldots, X_k$. For the regression model, we assume that the conditional distributions of $Y$ given the predictors, have constant shape and that they change, as the predictors change, at most through the conditional mean $\expc(Y \mid X_1 = x_1, \ldots, X_k = x_k)$. For the linear regression model, we assume that this conditional mean is of the form
\begin{equation}
\label{eq:10.3.13}
\expc(Y \mid X_1 = x_1, \ldots, X_k = x_k) = \beta_1 x_1 + \cdots + \beta_k x_k.
\end{equation}
This is linear in the unknown $\beta_i \in \mathbb{R}^1$ for $i = 1, \ldots, k$.

We will develop only the broad outline of the analysis of the multiple linear regression model here. All results will be stated without proofs provided. The proofs can be found in more advanced texts. It is important to note, however, that all of these results are just analogs of the results we developed by elementary methods in Section~\ref{ssec:10.3.2}, for the simple linear regression model.

\paragraph{Matrix Formulation of the Least-Squares Problem}

For the analysis of the multiple linear regression model, we need some matrix concepts. We will briefly discuss some of these here, but also see Appendix~A.4.

Let $A \in \mathbb{R}^{m \times n}$ denote a rectangular array of numbers with $m$ rows and $n$ columns, and let $a_{ij}$ denote the entry in the $i$th row and $j$th column (referred to as the $(i, j)$-th entry of $A$). For example,
\begin{equation*}
A = \begin{pmatrix} 1.2 & 1.0 & 0.0 \\ 3.2 & 0.2 & 6.3 \end{pmatrix} \in \mathbb{R}^{2 \times 3}
\end{equation*}
denotes a $2 \times 3$ matrix and, for example, $a_{22} = 0.2$.

We can add two matrices of the same dimensions $m$ and $n$ by simply adding their elements componentwise. So if $A, B \in \mathbb{R}^{m \times n}$ and $C = A + B$, then $c_{ij} = a_{ij} + b_{ij}$. Furthermore, we can multiply a matrix by a real number $c$ by simply multiplying every entry in the matrix by $c$. So if $A \in \mathbb{R}^{m \times n}$, then $B = cA \in \mathbb{R}^{m \times n}$ and $b_{ij} = ca_{ij}$. We will sometimes write a matrix $A \in \mathbb{R}^{m \times n}$ in terms of its columns as $A = (a_1, \ldots, a_n)$, so that here $a_i \in \mathbb{R}^m$. Finally, if $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^n$, then we define the product of $A$ times $b$ as $Ab = b_1 a_1 + \cdots + b_n a_n \in \mathbb{R}^m$.

Suppose now that $Y \in \mathbb{R}^n$ and that $\expc(Y)$ is constrained to lie in a set of the form
\begin{equation*}
S = \{ \beta_1 \psi_1 + \cdots + \beta_k \psi_k : \beta_i \in \mathbb{R}^1, \, i = 1, \ldots, k \},
\end{equation*}
where $\psi_1, \ldots, \psi_k$ are fixed vectors in $\mathbb{R}^n$. A set such as $S$ is called a \emph{linear subspace} of $\mathbb{R}^n$. When $\psi_1, \ldots, \psi_k$ has the \emph{linear independence property}, namely,
\begin{equation*}
\beta_1 \psi_1 + \cdots + \beta_k \psi_k = 0
\end{equation*}
if and only if $\beta_1 = \cdots = \beta_k = 0$, then we say that $S$ has \emph{dimension} $k$ and $\psi_1, \ldots, \psi_k$ is a \emph{basis} for $S$.

If we set
\begin{equation*}
V = (\psi_1, \ldots, \psi_k) = \begin{pmatrix} \psi_{11} & \psi_{12} & \cdots & \psi_{1k} \\ \psi_{21} & \psi_{22} & \cdots & \psi_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ \psi_{n1} & \psi_{n2} & \cdots & \psi_{nk} \end{pmatrix} \in \mathbb{R}^{n \times k},
\end{equation*}
then we can write
\begin{equation*}
\expc(Y) = \beta_1 \psi_1 + \cdots + \beta_k \psi_k = \begin{pmatrix} \beta_1 \psi_{11} + \beta_2 \psi_{12} + \cdots + \beta_k \psi_{1k} \\ \beta_1 \psi_{21} + \beta_2 \psi_{22} + \cdots + \beta_k \psi_{2k} \\ \vdots \\ \beta_1 \psi_{n1} + \beta_2 \psi_{n2} + \cdots + \beta_k \psi_{nk} \end{pmatrix} = V\beta,
\end{equation*}
for some unknown point $\beta = (\beta_1, \beta_2, \ldots, \beta_k)'$. When we observe $y \in \mathbb{R}^n$, then the least-squares estimate of $\expc(Y)$ is obtained by finding the value of $\beta$ that minimizes
\begin{equation*}
\sum_{i=1}^{n} (y_i - \beta_1 \psi_{i1} - \beta_2 \psi_{i2} - \cdots - \beta_k \psi_{ik})^2.
\end{equation*}
It can be proved that a unique minimizing value for $\beta \in \mathbb{R}^k$ exists whenever $\psi_1, \ldots, \psi_k$ is a basis. The minimizing value of $\beta$ will be denoted by $b$ and is called the \emph{least-squares estimate} of $\beta$. The point $b_1 \psi_1 + \cdots + b_k \psi_k = Vb$ is the least-squares estimate of $\expc(Y)$ and is sometimes called the \emph{vector of fitted values}. The point $y - Vb$ is called the \emph{vector of residuals}.

We now consider how to calculate $b$. For this, we need to understand what it means to multiply the matrix $A \in \mathbb{R}^{m \times k}$ on the right by the matrix $B \in \mathbb{R}^{k \times n}$. The matrix product $AB$ is defined to be the $m \times n$ matrix whose $(i, j)$-th entry is given by
\begin{equation*}
\sum_{l=1}^{k} a_{il} b_{lj}.
\end{equation*}
Notice that the array $A$ must have the same number of columns as the number of rows of $B$ for this product to be defined. The \emph{transpose} of a matrix $A \in \mathbb{R}^{m \times k}$ is defined to be
\begin{equation*}
A' = \begin{pmatrix} a_{11} & \cdots & a_{m1} \\ \vdots & \ddots & \vdots \\ a_{1k} & \cdots & a_{mk} \end{pmatrix} \in \mathbb{R}^{k \times m},
\end{equation*}
namely, the $i$th column of $A$ becomes the $i$th row of $A'$. For a matrix $A \in \mathbb{R}^{k \times k}$, the \emph{matrix inverse} of $A$ is defined to be the matrix $A^{-1}$ such that
\begin{equation*}
AA^{-1} = A^{-1}A = I,
\end{equation*}
where $I \in \mathbb{R}^{k \times k}$ has 1's along its diagonal and 0's everywhere else; it is called the $k \times k$ \emph{identity matrix}. It is not always the case that $A \in \mathbb{R}^{k \times k}$ has an inverse, but when it does it can be shown that the inverse is unique. Note that there are many mathematical and statistical software packages that include the facility for computing matrix products, transposes, and inverses.

We have the following fundamental result.

\begin{theorem}
\label{thm:10.3.7}
If $\expc(Y) \in S = \{ \beta_1 \psi_1 + \cdots + \beta_k \psi_k : \beta_i \in \mathbb{R}^1, \, i = 1, \ldots, k \}$ and the columns of $V = (\psi_1, \ldots, \psi_k)$ have the linear independence property, then $(V'V)^{-1}$ exists, the least-squares estimate of $\beta$ is unique, and it is given by
\begin{equation}
\label{eq:10.3.14}
b = \begin{pmatrix} b_1 \\ \vdots \\ b_k \end{pmatrix} = (V'V)^{-1} V' y.
\end{equation}
\end{theorem}

\paragraph{Least-Squares Estimates, Predictions, and Standard Errors}

For the linear regression model \eqref{eq:10.3.13}, we have that (writing $X_{ij}$ for the $j$th value of $X_i$)
\begin{equation*}
\expc \begin{pmatrix} Y_1 \\ \vdots \\ Y_n \end{pmatrix} \Bigg| X_{ij} = x_{ij} \text{ for all } i, j = \begin{pmatrix} \beta_1 x_{11} + \cdots + \beta_k x_{1k} \\ \vdots \\ \beta_1 x_{n1} + \cdots + \beta_k x_{nk} \end{pmatrix} = \beta_1 \psi_1 + \cdots + \beta_k \psi_k = V\beta,
\end{equation*}
where $\psi_1, \ldots, \psi_k$ and
\begin{equation*}
V = (\psi_1, \psi_2, \ldots, \psi_k) = \begin{pmatrix} x_{11} & \cdots & x_{1k} \\ x_{21} & \cdots & x_{2k} \\ \vdots & \ddots & \vdots \\ x_{n1} & \cdots & x_{nk} \end{pmatrix} \in \mathbb{R}^{n \times k}.
\end{equation*}
We will assume, hereafter, that the columns $\psi_1, \ldots, \psi_k$ of $V$ have the linear independence property. Then (replacing expectation by conditional expectation) it is immediate that the least-squares estimate of $\beta$ is given by \eqref{eq:10.3.14}.

As with the simple linear regression model, we have a number of results concerning the least-squares estimates. We state these here without proof.

\begin{theorem}
\label{thm:10.3.8}
If the $(x_{i1}, \ldots, x_{ik}, y_i)$ are independent observations for $i = 1, \ldots, n$ and the linear regression model applies, then
\begin{equation*}
\expc(B_i \mid X_{ij} = x_{ij} \text{ for all } i, j) = \beta_i
\end{equation*}
for $i = 1, \ldots, k$.
\end{theorem}

So Theorem~\ref{thm:10.3.8} states that the least-squares estimates are unbiased estimates of the linear regression coefficients.

If we want to assess the accuracy of these estimates, then we need to be able to compute their standard errors.

\begin{theorem}
\label{thm:10.3.9}
If the $(x_{i1}, \ldots, x_{ik}, y_i)$ are independent observations for $i = 1, \ldots, n$ from the linear regression model, and if $\var(Y \mid X_1 = x_1, \ldots, X_k = x_k) = \sigma^2$ for every $(x_1, \ldots, x_k)$, then
\begin{equation}
\label{eq:10.3.15}
\cov(B_i, B_j \mid X_{ij} = x_{ij} \text{ for all } i, j) = \sigma^2 c_{ij},
\end{equation}
where $c_{ij}$ is the $(i, j)$-th entry in the matrix $(V'V)^{-1}$.
\end{theorem}

We have the following result concerning the estimation of the mean
\begin{equation*}
\expc(Y \mid X_1 = x_1, \ldots, X_k = x_k) = \beta_1 x_1 + \cdots + \beta_k x_k
\end{equation*}
by the estimate $b_1 x_1 + \cdots + b_k x_k$.

\begin{corollary}
\label{cor:10.3.4}
\begin{equation}
\label{eq:10.3.16}
\var(B_1 x_1 + \cdots + B_k x_k \mid X_{ij} = x_{ij} \text{ for all } i, j) = \sigma^2 \left( \sum_{i=1}^{k} x_i^2 c_{ii} + 2 \sum_{i < j} x_i x_j c_{ij} \right) = \sigma^2 x' (V'V)^{-1} x,
\end{equation}
where $x = (x_1, \ldots, x_k)'$.
\end{corollary}

We also use $b_1 x_1 + \cdots + b_k x_k = b'x$ as a prediction of a new response value when $(X_1, \ldots, X_k) = (x_1, \ldots, x_k)$.

We see, from Theorem~\ref{thm:10.3.9} and Corollary~\ref{cor:10.3.4}, that we need an estimate of $\sigma^2$ to compute standard errors. The estimate is given by
\begin{equation}
\label{eq:10.3.17}
s^2 = \frac{1}{n-k} \sum_{i=1}^{n} (y_i - b_1 x_{i1} - \cdots - b_k x_{ik})^2 = \frac{1}{n-k} (y - Xb)'(y - Xb),
\end{equation}
and we have the following result.

\begin{theorem}
\label{thm:10.3.10}
If the $(x_{i1}, \ldots, x_{ik}, y_i)$ are independent observations for $i = 1, \ldots, n$ from the linear regression model, and if $\var(Y \mid X_1 = x_1, \ldots, X_k = x_k) = \sigma^2$, then
\begin{equation*}
\expc(S^2 \mid X_{ij} = x_{ij} \text{ for all } i, j) = \sigma^2.
\end{equation*}
\end{theorem}

Combining \eqref{eq:10.3.15} and \eqref{eq:10.3.17}, we deduce that the standard error of $b_i$ is $s \sqrt{c_{ii}}$. Combining \eqref{eq:10.3.16} and \eqref{eq:10.3.17}, we deduce that the standard error of $b_1 x_1 + \cdots + b_k x_k$ is
\begin{equation*}
s \left( \sum_{i=1}^{k} x_i^2 c_{ii} + 2 \sum_{i < j} x_i x_j c_{ij} \right)^{1/2} = s \sqrt{x'(V'V)^{-1}x}.
\end{equation*}

\paragraph{The ANOVA Decomposition and $F$-Statistics}

When one of the predictors $X_1, \ldots, X_k$ is constant, then we say that the model has an \emph{intercept term}. By convention, we will always take this to be the first predictor. So when we want the model to have an intercept term, we take $X_1 \equiv 1$ and $\beta_1$ is the intercept, e.g., the simple linear regression model. Note that it is common to denote the intercept term by $\beta_0$, so that $X_0 \equiv 1$ and $X_1, \ldots, X_k$ denote the predictors that actually change. We will also adopt this convention when it seems appropriate.

Basically, inclusion of an intercept term is very common, as this says that, when the predictors that actually change have no relationship with the response $Y$, then the intercept is the unknown mean of the response. When we do not include an intercept, then this says we know that the mean response is 0 when there is no relationship between $Y$ and the nonconstant predictors. Unless there is substantive, application-based evidence to support this, we will generally not want to make this assumption.

Denoting the intercept term by $\beta_1$ so that $X_1 \equiv 1$, we have the following ANOVA decomposition for this model that shows how to isolate the observed variation in $Y$ that can be explained by changes in the nonconstant predictors.

\begin{lemma}
\label{lem:10.3.2}
If, for $i = 1, \ldots, n$, the values $(x_{i1}, \ldots, x_{ik}, y_i)$ are such that the matrix $V$ has linearly independent columns, with $\psi_1$ equal to a column of ones, then $b_1 = \bar{y} - b_2 \bar{x}_2 - \cdots - b_k \bar{x}_k$ and
\begin{equation*}
\sum_{i=1}^{n} (y_i - \bar{y})^2 = \sum_{i=1}^{n} (b_2(x_{i2} - \bar{x}_2) + \cdots + b_k(x_{ik} - \bar{x}_k))^2 + \sum_{i=1}^{n} (y_i - b_1 x_{i1} - \cdots - b_k x_{ik})^2.
\end{equation*}
\end{lemma}

We call
\begin{equation*}
\text{RSS}(X_2, \ldots, X_k) = \sum_{i=1}^{n} (b_2(x_{i2} - \bar{x}_2) + \cdots + b_k(x_{ik} - \bar{x}_k))^2
\end{equation*}
the \emph{regression sum of squares} and
\begin{equation*}
\text{ESS} = \sum_{i=1}^{n} (y_i - b_1 x_{i1} - \cdots - b_k x_{ik})^2
\end{equation*}
the \emph{error sum of squares}. This leads to the following ANOVA table.
\begin{center}
\begin{tabular}{l|c|c|c}
Source & Df & Sum of Squares & Mean Square \\ \hline
$X_2, \ldots, X_k$ & $k - 1$ & $\text{RSS}(X_2, \ldots, X_k)$ & $\text{RSS}(X_2, \ldots, X_k)/(k-1)$ \\
Error & $n - k$ & ESS & $s^2$ \\ \hline
Total & $n - 1$ & $\sum_{i=1}^{n} (y_i - \bar{y})^2$ & 
\end{tabular}
\end{center}

When there is an intercept term, the null hypothesis of no relationship between the response and the predictors is equivalent to $H_0 : \beta_2 = \cdots = \beta_k = 0$. As with the simple linear regression model, the mean square for regression can be shown to be an unbiased estimator of $\sigma^2$ if and only if the null hypothesis is true. Therefore, a sensible statistic to use for assessing the null hypothesis is the $F$-statistic
\begin{equation*}
F = \frac{\text{RSS}(X_2, \ldots, X_k)/(k-1)}{s^2},
\end{equation*}
with large values being evidence against the null.

Often, we want to assess the null hypothesis $H_0 : \beta_{l+1} = \cdots = \beta_k = 0$ or, equivalently, the hypothesis that the model is given by
\begin{equation*}
\expc(Y \mid X_1 = x_1, \ldots, X_k = x_k) = \beta_1 x_1 + \cdots + \beta_l x_l,
\end{equation*}
where $l < k$. This hypothesis says that the last $k - l$ predictors $X_{l+1}, \ldots, X_k$ have no relationship with the response.

If we denote the least-squares estimates of $\beta_1, \ldots, \beta_l$, obtained by fitting the smaller model, by $\tilde{b}_1, \ldots, \tilde{b}_l$, then we have the following result.

\begin{lemma}
\label{lem:10.3.3}
If the $(x_{i1}, \ldots, x_{ik}, y_i)$ for $i = 1, \ldots, n$ are values for which the matrix $V$ has linearly independent columns, with $\psi_1$ equal to a column of ones, then
\begin{equation}
\label{eq:10.3.18}
\text{RSS}(X_2, \ldots, X_k) = \sum_{i=1}^{n} (b_2(x_{i2} - \bar{x}_2) + \cdots + b_k(x_{ik} - \bar{x}_k))^2 \geqslant \sum_{i=1}^{n} (\tilde{b}_2(x_{i2} - \bar{x}_2) + \cdots + \tilde{b}_l(x_{il} - \bar{x}_l))^2 = \text{RSS}(X_2, \ldots, X_l).
\end{equation}
\end{lemma}

On the right of the inequality in \eqref{eq:10.3.18}, we have the regression sum of squares obtained by fitting the model based on the first $l$ predictors. Therefore, we can interpret the difference of the left and right sides of \eqref{eq:10.3.18}, namely,
\begin{equation*}
\text{RSS}(X_{l+1}, \ldots, X_k \mid X_2, \ldots, X_l) = \text{RSS}(X_2, \ldots, X_k) - \text{RSS}(X_2, \ldots, X_l),
\end{equation*}
as the contribution of the predictors $X_{l+1}, \ldots, X_k$ to the regression sum of squares when the predictors $X_1, \ldots, X_l$ are in the model. We get the following ANOVA table (actually only the first three columns of the ANOVA table) corresponding to this decomposition of the total sum of squares.
\begin{center}
\begin{tabular}{l|c|c}
Source & Df & Sum of Squares \\ \hline
$X_2, \ldots, X_l$ & $l - 1$ & $\text{RSS}(X_2, \ldots, X_l)$ \\
$X_{l+1}, \ldots, X_k \mid X_2, \ldots, X_l$ & $k - l$ & $\text{RSS}(X_{l+1}, \ldots, X_k \mid X_2, \ldots, X_l)$ \\
Error & $n - k$ & ESS \\ \hline
Total & $n - 1$ & $\sum_{i=1}^{n} (y_i - \bar{y})^2$
\end{tabular}
\end{center}

It can be shown that the null hypothesis $H_0 : \beta_{l+1} = \cdots = \beta_k = 0$ holds if and only if
\begin{equation*}
\frac{\text{RSS}(X_{l+1}, \ldots, X_k \mid X_2, \ldots, X_l)}{k - l}
\end{equation*}
is an unbiased estimator of $\sigma^2$. Therefore, a sensible statistic to use for assessing this null hypothesis is the $F$-statistic
\begin{equation*}
F = \frac{\text{RSS}(X_{l+1}, \ldots, X_k \mid X_2, \ldots, X_l)/(k - l)}{s^2},
\end{equation*}
with large values being evidence against the null.

\paragraph{The Coefficient of Determination}

The coefficient of determination for this model is given by
\begin{equation*}
R^2 = \frac{\text{RSS}(X_2, \ldots, X_k)}{\sum_{i=1}^{n} (y_i - \bar{y})^2},
\end{equation*}
which, by Lemma~\ref{lem:10.3.2}, is always between 0 and 1. The value of $R^2$ gives the proportion of the observed variation in $Y$ that is explained by the inclusion of the nonconstant predictors in the model.

It can be shown that $R^2$ is the square of the \emph{multiple correlation coefficient} between $Y$ and $(X_1, \ldots, X_k)$. However, we do not discuss the multiple correlation coefficient in this text.

\paragraph{Confidence Intervals and Testing Hypotheses}

For inference, we have the following result.

\begin{theorem}
\label{thm:10.3.11}
If the conditional distribution of $Y$ given $(X_1, \ldots, X_k) = (x_1, \ldots, x_k)$ is $N(\beta_1 x_1 + \cdots + \beta_k x_k, \sigma^2)$ and if we observe the independent values $(x_{i1}, \ldots, x_{ik}, y_i)$ for $i = 1, \ldots, n$, then the conditional distributions of the $B_i$ and $S^2$, given $X_{ij} = x_{ij}$ for all $i$, $j$, are as follows.
\begin{enumerate}[(i)]
\item $B_i \sim N(\beta_i, \sigma^2 c_{ii})$
\item $B_1 x_1 + \cdots + B_k x_k$ is distributed
\begin{equation*}
N \left( \beta_1 x_1 + \cdots + \beta_k x_k, \sigma^2 \left( \sum_{i=1}^{k} x_i^2 c_{ii} + 2 \sum_{i < j} x_i x_j c_{ij} \right) \right)
\end{equation*}
\item $(n - k) S^2/\sigma^2 \sim \chi^2(n - k)$ independent of $(B_1, \ldots, B_k)$
\end{enumerate}
\end{theorem}

\begin{corollary}
\label{cor:10.3.5}
\begin{enumerate}[(i)]
\item $\dfrac{B_i - \beta_i}{S \sqrt{c_{ii}}} \sim t(n - k)$
\item $\dfrac{B_1 x_1 + \cdots + B_k x_k - (\beta_1 x_1 + \cdots + \beta_k x_k)}{S \left( \sum_{i=1}^{k} x_i^2 c_{ii} + 2 \sum_{i < j} x_i x_j c_{ij} \right)^{1/2}} \sim t(n - k)$
\item $H_0 : \beta_{l+1} = \cdots = \beta_k = 0$ is true if and only if
\begin{equation*}
F = \frac{(\text{RSS}(X_2, \ldots, X_k) - \text{RSS}(X_2, \ldots, X_l))/(k - l)}{S^2} \sim F(k - l, n - k)
\end{equation*}
\end{enumerate}
\end{corollary}

\paragraph{Analysis of Residuals}

In an application of the multiple regression model, we must check to make sure that the assumptions make sense. Model checking is based on the residuals $y_i - b_1 x_{i1} - \cdots - b_k x_{ik}$ (after standardization), just as discussed in Section~\ref{sec:9.1}. Note that the $i$th residual is simply the difference between the observed value $y_i$ at $(x_{i1}, \ldots, x_{ik})$ and the predicted value $b_1 x_{i1} + \cdots + b_k x_{ik}$ at $(x_{i1}, \ldots, x_{ik})$.

We also have the following result (this can be proved as a Corollary of Theorem~\ref{thm:10.3.10}).

\begin{corollary}
\label{cor:10.3.6}
\begin{enumerate}[(i)]
\item $\expc(Y_i - B_1 x_{i1} - \cdots - B_k x_{ik} \mid V) = 0$
\item $\cov(Y_i - B_1 x_{i1} - \cdots - B_k x_{ik}, Y_j - B_1 x_{j1} - \cdots - B_k x_{jk} \mid V) = \sigma^2 d_{ij}$, where $d_{ij}$ is the $(i, j)$-th entry of the matrix $I - V(V'V)^{-1}V'$.
\end{enumerate}
\end{corollary}

Therefore, the standardized residuals are given by
\begin{equation}
\label{eq:10.3.19}
\frac{y_j - b_1 x_{j1} - \cdots - b_k x_{jk}}{s \sqrt{d_{ii}}}.
\end{equation}
When $s$ is replaced by $\sigma$ in \eqref{eq:10.3.19}, Corollary~\ref{cor:10.3.6} implies that this quantity has conditional mean 0 and conditional variance 1. Furthermore, when the conditional distribution of the response given the predictors is normal, then it can be shown that the conditional distribution of this quantity is $N(0, 1)$. These results are also approximately true for \eqref{eq:10.3.19} for large $n$. Furthermore, it can be shown that the covariances between the standardized residuals go to 0 as $n \to \infty$ under certain reasonable conditions on distribution of the predictor variables. So one approach to model checking here is to see whether the values given by \eqref{eq:10.3.19} look at all like a sample from the $N(0, 1)$ distribution.

What do we do if model checking leads to a failure of the model? As in Chapter~\ref{ch:9}, we can consider making various transformations of the data to see if there is a simple modification of the model that will pass. We can make transformations not only to the response variable $Y$ but to the predictor variables $X_1, \ldots, X_k$ as well.

\paragraph{An Application of Multiple Linear Regression Analysis}

The computations needed to implement a multiple linear regression analysis cannot be carried out by hand. These are much too time-consuming and error-prone. It is therefore important that a statistician have a computer with suitable software available when doing a multiple linear regression analysis.

The data in Table~\ref{tab:10.1} are taken from \emph{Statistical Theory and Methodology in Science and Engineering}, 2nd ed., by K.~A.\ Brownlee (John Wiley \& Sons, New York, 1965). The response variable $Y$ is stack loss (Loss), which represents 10 times the percentage of ammonia lost as unabsorbed nitric oxide. The predictor variables are $X_1 =$ air flow (Air), $X_2 =$ temperature of inlet water (Temp), and $X_3 =$ the concentration of nitric acid (Acid). Also recorded is the day (Day) on which the observation was taken.

\begin{table}[!htbp]
\centering
\caption{Data for Application of Multiple Linear Regression Analysis}
\label{tab:10.1}
\begin{tabular}{c|cccc|c|cccc}
Day & Air & Temp & Acid & Loss & Day & Air & Temp & Acid & Loss \\ \hline
1 & 80 & 27 & 89 & 42 & 12 & 58 & 17 & 88 & 13 \\
2 & 80 & 27 & 88 & 37 & 13 & 58 & 18 & 82 & 11 \\
3 & 75 & 25 & 90 & 37 & 14 & 58 & 19 & 93 & 12 \\
4 & 62 & 24 & 87 & 28 & 15 & 50 & 18 & 89 & 8 \\
5 & 62 & 22 & 87 & 18 & 16 & 50 & 18 & 86 & 7 \\
6 & 62 & 23 & 87 & 18 & 17 & 50 & 19 & 72 & 8 \\
7 & 62 & 24 & 93 & 19 & 18 & 50 & 19 & 79 & 8 \\
8 & 62 & 24 & 93 & 20 & 19 & 50 & 20 & 80 & 9 \\
9 & 58 & 23 & 87 & 15 & 20 & 56 & 20 & 82 & 15 \\
10 & 58 & 18 & 80 & 14 & 21 & 70 & 20 & 91 & 15 \\
11 & 58 & 18 & 89 & 14 &  &  &  &  & 
\end{tabular}
\end{table}

We consider the model $(Y \mid x_1, x_2, x_3) \sim N(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3, \sigma^2)$. Note that we have included an intercept term. Figure~\ref{fig:10.3.9} is a normal probability plot of the standardized residuals. This looks reasonable, except for one residual, $-2.63822$, that diverges quite distinctively from the rest of the values, which lie close to the 45-degree line. Printing out the standardized residuals shows that this residual is associated with the observation on the twenty-first day. Possibly there was something unique about this day's operations, and so it is reasonable to discard this data value and refit the model.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_9.pdf}
  \caption{Normal probability plot of the standardized residuals based on all the data.}
  \label{fig:10.3.9}
\end{figure}

Figure~\ref{fig:10.3.10} is a normal probability plot obtained by fitting the model to the first 20 observations. This looks somewhat better, but still we might be concerned about at least one of the residuals that deviates substantially from the 45-degree line.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_10.pdf}
  \caption{Normal probability plot of the standardized residuals based on the first 20 data values.}
  \label{fig:10.3.10}
\end{figure}

Following the analysis of these data in \emph{Fitting Equations to Data}, by C.~Daniel and F.~S.\ Wood (Wiley-Interscience, New York, 1971), we consider instead the model
\begin{equation}
\label{eq:10.3.20}
(\ln Y \mid x_1, x_2, x_3) \sim N(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3, \sigma^2),
\end{equation}
i.e., we transform the response variable by taking its logarithm and use all of the data. Often, when models do not fit, simple transformations like this can lead to major improvements. In this case, we see a much improved normal probability plot, as provided in Figure~\ref{fig:10.3.11}.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_11.pdf}
  \caption{Normal probability plot of the standardized residuals for all the data using $\ln Y$ as the response.}
  \label{fig:10.3.11}
\end{figure}

We also looked at plots of the standardized residuals against the various predictors, and these looked reasonable. Figure~\ref{fig:10.3.12} is a plot of the standardized residuals against the values of Air.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig10_3_12.pdf}
  \caption{A plot of the standardized residuals for all the data, using $\ln Y$ as the response, against the values of the predictor Air.}
  \label{fig:10.3.12}
\end{figure}

Now that we have accepted the model \eqref{eq:10.3.20}, we can proceed to inferences about the unknowns of the model. The least-squares estimates of the $\beta_i$, their standard errors (Se), the corresponding $t$-statistics for testing the $\beta_i = 0$, and the P-values for this are given in the following table.
\begin{center}
\begin{tabular}{c|cccc}
Coefficient & Estimate & Se & $t$-statistic & P-value \\ \hline
$\beta_0$ & $-0.948700$ & $0.647700$ & $-1.46$ & $0.161$ \\
$\beta_1$ & $0.034565$ & $0.007343$ & $4.71$ & $0.000$ \\
$\beta_2$ & $0.063460$ & $0.020040$ & $3.17$ & $0.006$ \\
$\beta_3$ & $-0.002864$ & $0.008510$ & $-0.34$ & $0.742$
\end{tabular}
\end{center}
The estimate of $\sigma^2$ is given by $s^2 = 0.0312$.

To test the null hypothesis that there is no relationship between the response and the predictors, or that, equivalently, $H_0 : \beta_1 = \beta_2 = \beta_3 = 0$, we have the following ANOVA table.
\begin{center}
\begin{tabular}{l|c|c|c}
Source & Df & Sum of Squares & Mean Square \\ \hline
$X_1, X_2, X_3$ & 3 & 4.9515 & 1.6505 \\
Error & 17 & 0.5302 & 0.0312 \\ \hline
Total & 20 & 5.4817 & 
\end{tabular}
\end{center}

The value of the $F$-statistic is given by $1.6505/0.0312 = 52.900$ and when $F \sim F(3, 17)$, we have that $\prb(F \geqslant 52.900) = 0.000$. So there is substantial evidence against the null hypothesis. To see how well the model explains the variation in the response, we computed the value of $R^2 = 86.9\%$. Therefore, approximately 87\% of the observed variation in $Y$ can be explained by changes in the predictors in the model.

While we have concluded that a relationship exists between the response and the predictors, it may be that some of the predictors have no relationship with the response. For example, the table of $t$-statistics above would seem to indicate that perhaps $X_3$ (acid) is not affecting $Y$. We can assess this via the following ANOVA table, obtained by fitting the model $(\ln Y \mid x_1, x_2, x_3) \sim N(\beta_0 + \beta_1 x_1 + \beta_2 x_2, \sigma^2)$.
\begin{center}
\begin{tabular}{l|c|c|c}
Source & Df & Sum of Squares & Mean Square \\ \hline
$X_1, X_2$ & 2 & 4.9480 & 2.4740 \\
$X_3 \mid X_1, X_2$ & 1 & 0.0035 & 0.0035 \\
Error & 17 & 0.5302 & 0.0312 \\ \hline
Total & 20 & 5.4817 & 
\end{tabular}
\end{center}

Note that $\text{RSS}(X_3 \mid X_1, X_2) = 4.9515 - 4.9480 = 0.0035$. The value of the $F$-statistic for testing $H_0 : \beta_3 = 0$ is $0.0035/0.0312 = 0.112$ and when $F \sim F(1, 17)$, we have that $\prb(F \geqslant 0.112) = 0.742$. So we have no evidence against the null hypothesis and can drop $X_3$ from the model. Actually, this is the same P-value as obtained via the $t$-test of this null hypothesis, as, in general, the $t$-test that a single regression coefficient is 0 is equivalent to the $F$-test. Similar tests of the need to include $X_1$ and $X_2$ do not lead us to drop these variables from the model.

So based on the above results, we decide to drop $X_3$ from the model and use the equation
\begin{equation}
\label{eq:10.3.21}
\expc(Y \mid X_1 = x_1, X_2 = x_2) = -0.7522 + 0.035402 X_1 + 0.06346 X_2
\end{equation}
to describe the relationship between $Y$ and the predictors. Note that the least-squares estimates of $\beta_0$, $\beta_1$, and $\beta_2$ in \eqref{eq:10.3.21} are obtained by refitting the model without $X_3$.

\paragraph{Summary of Section~\ref{sec:10.3}}
\begin{itemize}
\item In this section, we examined the situation in which the response variable and the predictor variables are quantitative.
\item In this situation, the linear regression model provides a possible description of the form of any relationship that may exist between the response and the predictors.
\item Least squares is a standard method for fitting linear regression models to data.
\item The ANOVA is a decomposition of the total variation observed in the response variable into a part attributable to changes in the predictor variables and a part attributable to random error.
\item If we assume a normal linear regression model, then we have inference methods available such as confidence intervals and tests of significance. In particular, we have available the $F$-test to assess whether or not a relationship exists between the response and the predictors.
\item A normal linear regression model is checked by examining the standardized residuals.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:10.3.1}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a $\text{Bernoulli}(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown. What is the least-squares estimate of the mean of this distribution?
\end{exercise}

\begin{solution}
Since $\bar{x} \in [0, 1]$ with probability 1, we have that $\bar{x}$ is the least-squares estimate of the mean $\theta$.
\end{solution}

\begin{exercise}
\label{exer:10.3.2}
Suppose that $(x_1, \ldots, x_n)$ is a sample from the $\text{Uniform}[0, \theta]$, where $\theta > 0$ is unknown. What is the least-squares estimate of the mean of this distribution?
\end{exercise}

\begin{solution}
Since $\bar{x} \in [0, \theta] \subset [0, \infty)$ with probability 1, we have that $\bar{x}$ is the least-squares estimate of the mean $\theta/2 \in [0, \infty)$.
\end{solution}

\begin{exercise}
\label{exer:10.3.3}
Suppose that $(x_1, \ldots, x_n)$ is a sample from the $\text{Exponential}(\lambda)$, where $\lambda > 0$ is unknown. What is the least-squares estimate of the mean of this distribution?
\end{exercise}

\begin{solution}
Since $\bar{x} \in (0, \infty)$ with probability 1, we have that $\bar{x}$ is the least-squares estimate of the mean $1/\theta \in (0, \infty)$.
\end{solution}

\begin{exercise}
\label{exer:10.3.4}
Consider the $n = 11$ data values in the following table.
\begin{center}
\begin{tabular}{c|cc|c|cc}
Observation & $X$ & $Y$ & Observation & $X$ & $Y$ \\ \hline
1 & $-5.00$ & $-10.00$ & 7 & $1.00$ & $3.52$ \\
2 & $-4.00$ & $-8.83$ & 8 & $2.00$ & $5.64$ \\
3 & $-3.00$ & $-9.15$ & 9 & $3.00$ & $7.28$ \\
4 & $-2.00$ & $-4.26$ & 10 & $4.00$ & $7.62$ \\
5 & $-1.00$ & $-0.30$ & 11 & $5.00$ & $8.51$ \\
6 & $0.00$ & $0.04$ &  &  & 
\end{tabular}
\end{center}
Suppose we consider the simple normal linear regression to describe the relationship between the response $Y$ and the predictor $X$.
\begin{enumerate}[(a)]
\item Plot the data in a scatter plot.
\item Calculate the least-squares line and plot this on the scatter plot in part (a).
\item Plot the standardized residuals against $X$.
\item Produce a normal probability plot of the standardized residuals.
\item What are your conclusions based on the plots produced in parts (c) and (d)?
\item If appropriate, calculate 0.95-confidence intervals for the intercept and slope.
\item Construct the ANOVA table to test whether or not there is a relationship between the response and the predictors. What is your conclusion?
\item If the model is correct, what proportion of the observed variation in the response is explained by changes in the predictor?
\item Predict a future $Y$ at $X = 0.0$. Is this prediction an extrapolation or an interpolation? Determine the standard error of this prediction.
\item Predict a future $Y$ at $X = 6.0$. Is this prediction an extrapolation or an interpolation? Determine the standard error of this prediction.
\item Predict a future $Y$ at $X = 20.0$. Is this prediction an extrapolation or an interpolation? Determine the standard error of this prediction. Compare this with the standard errors obtained in parts (i) and (j) and explain the differences.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A scatter plot is given below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-4a.pdf}
      \caption{Scatter plot of $Y$ versus $X$ for Exercise 10.3.4.}
      %\label{fig:scatter-10-3-4a}
    \end{figure}
    \item The least-squares estimates of $\beta_1$ and $\beta_2$ are given by $b_2 = 2.1024$ and $b_1 = \bar{y} = -0.00091$, so the least-squares line is given by $y = -0.00091 + 2.1024x$. A scatter plot of the data together with a plot of the least-squares line follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-4b.pdf}
      \caption{Scatter plot with least-squares regression line $Y = -0.0009091 + 2.10236X$, $S = 1.54276$, $R^2 = 95.8\%$, $R^2(\text{adj}) = 95.3\%$.}
      %\label{fig:regression-10-3-4b}
    \end{figure}
    \item The plot of the standardized residuals against $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-4c.pdf}
      \caption{Standardized residuals versus $X$ (response is $Y$).}
      %\label{fig:residuals-X-10-3-4c}
    \end{figure}
    \item A normal probability plot of the standardized residuals is given below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-4d.pdf}
      \caption{Normal probability plot of the standardized residuals (response is $Y$).}
      %\label{fig:normal-prob-10-3-4d}
    \end{figure}
    \item Both graphs indicate that the normal simple linear regression model is reasonable.
    \item A $.95$-confidence interval for the intercept is given by
    \[
        -0.00091 \pm 0.4652(2.2622) = (-1.0533, 1.0515)
    \]
    and a $.95$-confidence interval for the slope is given by $2.1024 \pm 0.1471 \cdot 2.2622 = (1.7696, 2.4352)$.
    \item The ANOVA table is follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $X$ & 1 & 486.19 & 486.19 \\
    Error & 9 & 21.42 & 2.38 \\
    Total & 10 & 507.61 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0: \beta_2 = 0$ is given by $F = 486.19/2.38 = 204.28$ and, since $F \sim F(1, 9)$ under $H_0$, the P-value is given by $\prb(F > 204.28) = .000$, so we reject the null hypothesis of no effect between $X$ and $Y$.
    \item The proportion of the observed variation in the response that is being explained by changes in the predictor is given by the coefficient of determination $R^2 = 486.19/507.61 = .9578$.
    \item The prediction is given by $\hat{y} = -0.00091 + 2.1024(0) = -0.00091$. This is an interpolation because $0.0$ is in the range of observed $X$ values. The standard error of this prediction is, since $\bar{x} = 0$ (using Corollary 10.3.1), $(2.38/11)^{1/2} = 0.46515$.
    \item The prediction is given by $\hat{y} = -0.00091 + 2.1024(6) = 12.613$. This is an extrapolation because $6$ is not in the range of observed $X$ values. The standard error of this prediction is, since $\bar{x} = 0$ (using Corollary 10.3.1),
    \[
        (2.38)^{1/2} \left(\frac{1}{11} + \frac{(6 - 0)^2}{110}\right)^{1/2} = 0.99763.
    \]
    \item The prediction is given by $\hat{y} = -0.00091 + 2.1024(20) = 42.047$. This is an extrapolation because $12$ is not in the range of observed $X$ values. The standard error of this prediction is, since $\bar{x} = 0$ (using Corollary 10.3.1),
    \[
        (2.38)^{1/2} \left(\frac{1}{11} + \frac{(20 - 0)^2}{110}\right)^{1/2} = 2.9784.
    \]
    The standard errors get larger as we move away from the observed $X$ values.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.5}
Consider the $n = 11$ data values in the following table.
\begin{center}
\begin{tabular}{c|cc|c|cc}
Observation & $X$ & $Y$ & Observation & $X$ & $Y$ \\ \hline
1 & $-5.00$ & $65.00$ & 7 & $1.00$ & $6.52$ \\
2 & $-4.00$ & $39.17$ & 8 & $2.00$ & $17.64$ \\
3 & $-3.00$ & $17.85$ & 9 & $3.00$ & $34.28$ \\
4 & $-2.00$ & $7.74$ & 10 & $4.00$ & $55.62$ \\
5 & $-1.00$ & $2.70$ & 11 & $5.00$ & $83.51$ \\
6 & $0.00$ & $0.04$ &  &  & 
\end{tabular}
\end{center}
Suppose we consider the simple normal linear regression to describe the relationship between the response $Y$ and the predictor $X$.
\begin{enumerate}[(a)]
\item Plot the data in a scatter plot.
\item Calculate the least-squares line and plot this on the scatter plot in part (a).
\item Plot the standardized residuals against $X$.
\item Produce a normal probability plot of the standardized residuals.
\item What are your conclusions based on the plots produced in parts (c) and (d)?
\item If appropriate, calculate 0.95-confidence intervals for the intercept and slope.
\item Do the results of your analysis allow you to conclude that there is a relationship between $Y$ and $X$? Explain why or why not.
\item If the model is correct, what proportion of the observed variation in the response is explained by changes in the predictor?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A scatter plot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-5a.pdf}
      \caption{Scatter plot of $Y$ versus $X$ for Exercise 10.3.5.}
      %\label{fig:scatter-10-3-5a}
    \end{figure}
    \item The least-squares estimates of $\beta_1$ and $\beta_2$ are given by $b_2 = 2.10236$ and $b_1 = 29.9991$. The least-squares line is then given by $y = 29.9991 + 2.10236x$. A scatter plot of the data together with a plot of the least-squares line follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-5b.pdf}
      \caption{Scatter plot with least-squares regression line $Y = 29.9991 + 2.10236X$, $S = 28.5887$, $R^2 = 6.2\%$, $R^2(\text{adj}) = 0.0\%$.}
      %\label{fig:regression-10-3-5b}
    \end{figure}
    \item The plot of the standardized residuals against $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-5c.pdf}
      \caption{Standardized residuals versus $X$ (response is $Y$).}
      %\label{fig:residuals-X-10-3-5c}
    \end{figure}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-5d.pdf}
      \caption{Normal probability plot of the standardized residuals (response is $Y$).}
      %\label{fig:normal-prob-10-3-5d}
    \end{figure}
    \item The plot of the standardized residuals against $X$ indicates very clearly that there is a problem with this model.
    \item Based on (e), it is not appropriate to calculate confidence intervals for the intercept and slope.
    \item Nothing can be concluded about the relationship between $Y$ and $X$ based on this model as we have determined that it is inappropriate.
    \item The proportion of the observed variation in the response that is being explained by changes in the predictor is given by the coefficient of determination $R^2 = 486.193/7842.01 = 0.062$, which is very low.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.6}
Suppose the following data record the densities of an organism in a containment vessel for 10 days. Suppose we consider the simple normal linear regression to describe the relationship between the response $Y$ (density) and the predictor $X$ (day).
\begin{center}
\begin{tabular}{c|c|c|c}
Day & Number/Liter & Day & Number/Liter \\ \hline
1 & 1.6 & 6 & 1341.6 \\
2 & 16.7 & 7 & 2042.9 \\
3 & 65.2 & 8 & 7427.0 \\
4 & 23.6 & 9 & 15571.8 \\
5 & 345.3 & 10 & 33128.5
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Plot the data in a scatter plot.
\item Calculate the least-squares line and plot this on the scatter plot in part (a).
\item Plot the standardized residuals against $X$.
\item Produce a normal probability plot of the standardized residuals.
\item What are your conclusions based on the plots produced in parts (c) and (d)?
\item Can you think of a transformation of the response that might address any problems found? If so, repeat parts (a) through (e) after performing this transformation. (Hint: The scatter plot looks like exponential growth. What transformation is the inverse of exponentiation?)
\item Calculate 0.95-confidence intervals for the appropriate intercept and slope.
\item Construct the appropriate ANOVA table to test whether or not there is a relationship between the response and the predictors. What is your conclusion?
\item Do the results of your analysis allow you to conclude that there is a relationship between $Y$ and $X$? Explain why or why not.
\item Compute the proportion of variation explained by the predictor for the two models you have considered. Compare the results.
\item Predict a future $Y$ at $X = 12$. Is this prediction an extrapolation or an interpolation?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A scatter plot of the data is given below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-6a.pdf}
      \caption{Scatter plot of Density versus Day.}
      %\label{fig:scatter-10-3-6a}
    \end{figure}
    \item The least-squares estimates of $\beta_1$ and $\beta_2$ are given by $b_2 = 2732.67$ and $b_1 = -9033.28$, respectively. The least-squares line is then given by $y = -9033.28 + 2732.67x$. A scatter plot of the data together with a plot of the least-squares line follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-6b.pdf}
      \caption{Scatter plot with least-squares regression line Density $= -9033.28 + 2732.67$ Day, $S = 7293.81$, $R^2 = 59.1\%$, $R^2(\text{adj}) = 54.0\%$.}
      %\label{fig:regression-10-3-6b}
    \end{figure}
    \item A plot of the standardized residuals against $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-6c.pdf}
      \caption{Standardized residuals versus Day (response is Density).}
      %\label{fig:residuals-Day-10-3-6c}
    \end{figure}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-6d.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Density).}
      %\label{fig:normal-prob-10-3-6d}
    \end{figure}
    \item The plot of the standardized residuals against $X$ indicates very clearly that there is a problem with this model.
    \item Taking the logarithm of the response, we obtain the least-squares line given by $\ln(y) = 0.169155 + 1.06500x$. A scatter plot of the data together with the least-squares line follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-6f1.pdf}
      \caption{Scatter plot with least-squares regression line Log-Density $= 0.169155 + 1.06500$ Day, $S = 0.696788$, $R^2 = 96.0\%$, $R^2(\text{adj}) = 95.5\%$.}
      %\label{fig:regression-log-10-3-6f1}
    \end{figure}
    A plot of the standardized residuals against $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-6f2.pdf}
      \caption{Standardized residuals versus Day (response is Log-Density).}
      %\label{fig:residuals-log-10-3-6f2}
    \end{figure}
    A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-6f3.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Log-Density).}
      %\label{fig:normal-prob-log-10-3-6f3}
    \end{figure}
    Both graphs above look reasonable and therefore indicate no evidence against the normal linear model for the transformed response.
    \item As we can see from the scatter plot in part (a), the relationship between $X$ and $Y$ is definitely non-linear, and therefore it is not appropriate to calculate confidence intervals for the intercept and slope. However, after transforming the response, the relationship looks quite linear, so for this model $.95$-confidence intervals for the intercept and the slope are given by $0.169155 \pm 0.4760(2.306) = (-0.9285, 1.2668)$ and $1.065 \pm 0.07671(2.306) = (0.88811, 1.2419)$, respectively.
    \item The ANOVA table based on the transformed data (in part f) is given below.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $X$ & 1 & 93.573 & 93.573 \\
    Error & 8 & 3.884 & 0.486 \\
    Total & 9 & 97.458 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0: \beta_2 = 0$ for this model is then given by $F = 93.573/3.884 = 24.092$ and, since $F \sim F(1, 8)$ under $H_0$, the P-value is $\prb(F > 24.092) = 0.000$. Therefore, we have strong evidence against the null hypothesis of no relationship between $\ln Y$ and $X$.
    \item Yes, we can conclude that there is a relationship. We can then express the relationship between $X$ and $Y$ as $\expc(\ln Y \mid X = x) = 0.169155 + 1.06500x$.
    \item The proportion of the observed variation in the response that is being explained by changes in the predictor is given by the coefficient of determination $R^2 = 616068769/1.042 \times 10^9 = 59.1$ for the first model, which is quite low, and $R^2 = 93.573/97.458 = .96014$ for the second model (as in part f), which is quite high.
    \item The prediction of $\ln Y$ at $X = 12$ is given by $0.169155 + 1.06500(12) = 12.949$. The prediction of $Y$ is then given by $\exp(12.949) = 4.2042 \times 10^5$. This is an extrapolation as $12$ lies outside the range of observed $X$ values.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.7}
A student takes weekly quizzes in a course and receives the following grades over 12 weeks.
\begin{center}
\begin{tabular}{c|c|c|c}
Week & Grade & Week & Grade \\ \hline
1 & 65 & 7 & 74 \\
2 & 55 & 8 & 76 \\
3 & 62 & 9 & 48 \\
4 & 73 & 10 & 80 \\
5 & 68 & 11 & 85 \\
6 & 76 & 12 & 90
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Plot the data in a scatter plot with $X =$ week and $Y =$ grade.
\item Calculate the least-squares line and plot this on the scatter plot in part (a).
\item Plot the standardized residuals against $X$.
\item What are your conclusions based on the plot produced in (c)?
\item Calculate 0.95-confidence intervals for the intercept and slope.
\item Construct the ANOVA table to test whether or not there is a relationship between the response and the predictors. What is your conclusion?
\item What proportion of the observed variation in the response is explained by changes in the predictor?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A scatter plot is given below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-7a.pdf}
      \caption{Scatter plot of grade versus week.}
      %\label{fig:scatter-10-3-7a}
    \end{figure}
    \item For the data analysis, we need to do some computations. We define $S_{AB} = \sum_{i=1}^{n}(a_i - \bar{a})(b_i - \bar{b})$ for two random variables $A$ and $B$. Then, $S_{XY} = \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y}) = \sum_{i=1}^{n} x_i y_i - \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i/n = 5822 - 78 \cdot 852/12 = 284$, $S_{XX} = \sum_{i=1}^{n}(x_i - \bar{x})^2 = \sum_{i=1}^{n} x_i^2 - (\sum_{i=1}^{n} x_i)^2/n = 650 - 78^2/12 = 143$ and $S_{YY} = \sum_{i=1}^{n} y_i^2 - (\sum_{i=1}^{n} y_i)^2/n = 62104 - 852^2/12 = 1612$. The regression coefficients are $b_2 = \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})/\sum_{i=1}^{n}(x_i - \bar{x})^2 = S_{XY}/S_{XX} = 284/143 = 1.9860$ and $b_1 = \bar{y} - b_2\bar{x} = 71 - 1.9860 \times 6.5 = 58.9090$.
    \item A plot of the fitted values and standardized residuals is given below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-7c.pdf}
      \caption{Scatter plot of fitted values with regression line and standardized residuals versus week.}
      %\label{fig:fitted-residuals-10-3-7c}
    \end{figure}
    \item The standardized residual of the ninth week departs from the other residuals in part (c). This provides some evidence that the model is not correct.
    \item From Corollary 10.3.2, the $\gamma$-confidence intervals of $\beta_1$ and $\beta_2$ are $b_1 \pm s(1/n + \bar{x}^2/S_{XX})^{1/2} t_{(1+\gamma)/2}(n - 2)$ and $b_2 \pm s S_{XX}^{-1/2} t_{(1+\gamma)/2}(n - 2)$. Note that $t_{0.975}(10) = 2.228$ from Table D.4. Hence, the required confidence intervals are
    \begin{align*}
        b_1 \pm s(1/n + \bar{x}^2/S_{XX})^{1/2} t_{(1+\gamma)/2}(n - 2) &= 58.0909 \pm (10.2370)(0.6155)(2.228) \\
        &= [44.0545, 72.1283] \\
        b_2 \pm s S_{XX}^{-1/2} t_{(1+\gamma)/2}(n - 2) &= 1.9860 \pm (10.2370)(0.0836)(2.228) \\
        &= [0.0787, 3.8933].
    \end{align*}
    \item For the ANOVA table, we need to compute the total sum of squares and the regression sum of squares. They are $\sum_{i=1}^{n}(y_i - \bar{y})^2 = S_{YY} = 1612$ and RSS $= b_2^2 \sum_{i=1}^{n}(x_i - \bar{x})^2 = (S_{XY}/S_{XX})^2 \cdot S_{XX} = S_{XY}^2/S_{XX} = 284^2/143 = 564.0280$. Hence, ESS $= 1612 - 564.0280 = 1047.9720$.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & Sum of Squares & Mean Square \\
    \hline
    $X$ & 1 & 564.0280 & 564.0280 \\
    Error & 10 & 1047.9720 & 104.7972 \\
    Total & 11 & 1612.0000 &
    \end{tabular}
    \end{center}
    We compute the $F$-statistic
    \[
        F = \frac{\text{RSS}}{\text{ESS}/(n - 2)} = \frac{564.0280}{1047.9720/10} = 5.3821.
    \]
    The probability $\prb(F(1, 10) \geqslant 5.3821) < 0.05$ from Table D.5. Hence, we conclude there is evidence against the null hypothesis of no linear relationship between the response and the predictor.
    \item The coefficient of determination is given by
    \[
        R^2 = \frac{b_2^2 \sum_{i=1}^{n}(x_i - \bar{x})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} = \frac{\text{RSS}}{S_{YY}} = \frac{564.0280}{1612} = 0.3499.
    \]
    Hence, almost 35\% of the observed variation in the response is explained by changes in the predictor.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.8}
Suppose that $Y = \expc(Y \mid X) + Z$, where $X$, $Y$, and $Z$ are random variables.
\begin{enumerate}[(a)]
\item Show that $\expc(Z \mid X) = 0$.
\item Show that $\cov(\expc(Y \mid X), Z) = 0$. (Hint: Write $Z = Y - \expc(Y \mid X)$ and use Theorems~\ref{thm:3.5.2} and~\ref{thm:3.5.4}.)
\item Suppose that $Z$ is independent of $X$. Show that this implies that the conditional distribution of $Y$ given $X$ depends on $X$ only through its conditional mean. (Hint: Evaluate the conditional distribution function of $Y$ given $X = x$.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item From the relationship, $Z = Y - \expc(Y \mid X)$ and
    \[
        \expc(Z \mid X) = \expc(Y - \expc(Y \mid X) \mid X) = \expc(Y \mid X) - \expc(Y \mid X) = 0.
    \]
    \item The covariance can be written as
    \[
        \cov(\expc(Y \mid X), Z) = \expc(\expc(Y \mid X) Z) - \expc(\expc(Y \mid X))\expc(Z).
    \]
    Theorem \ref{thm:3.5.2} implies $\expc(Z) = \expc(\expc(Y \mid X))$ and $\expc(Z \mid X) = 0$ from part (a). So, $\expc(Z) = \expc(\expc(Z \mid X)) = \expc(0) = 0$. In a similar vein, $\expc(\expc(Y \mid X) Z) = \expc(\expc(\expc(Y \mid X) Z \mid X))$ and $\expc(\expc(Y \mid X) Z \mid X) = \expc(Y \mid X)\expc(Z \mid X) = 0$. Therefore, $\cov(\expc(Y \mid X), Z) = 0 - 0 = 0$.
    \item Given $X = x$, $\expc(Y \mid X = x)$ is constant. So, the conditional cdf of $Y$ given $X = x$ is
    \[
        F_{Y|X}(y \mid x) = \prb(Y \leqslant y \mid x) = \prb(Y - \expc(Y \mid X = x) \leqslant y - \expc(Y \mid X = x) \mid x) = \prb(Z \leqslant y - \expc(Y \mid X = x) \mid x) = F_Z(y - \expc(Y \mid X = x)).
    \]
    We see from this that the conditional distribution $Y$ given $X$ depends on $X$ only through its conditional mean $\expc(Y \mid X)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.9}
Suppose that $X$ and $Y$ are random variables such that a regression model describes the relationship between $Y$ and $X$. If $\expc(Y \mid X) = \exp(\beta_1 + \beta_2 X)$, then discuss whether or not this is a simple linear regression model (perhaps involving a predictor other than $X$).
\end{exercise}

\begin{solution}
In general, $\expc(Y \mid X) = \exp(\beta_1 + \beta_2 X)$ is not a simple linear regression model since it cannot be written in the form $\expc(Y \mid X) = \beta_1^* + \beta_2^* V$ where $V$ is an observed variable and the $\beta_i^*$ are unobserved parameter values.
\end{solution}

\begin{exercise}
\label{exer:10.3.10}
Suppose that $X$ and $Y$ are random variables and $\cor(X, Y) = 1$. Does a simple linear regression model hold to describe the relationship between $Y$ and $X$? If so, what is it?
\end{exercise}

\begin{solution}
Corollary 3.6.1 implies that
\[
    Y = \expc(Y) + \frac{\cov(X, Y)}{\var(X)}(X - \expc(X)).
\]
By letting $\beta_2 = \cov(X, Y)/\var(X)$ and $\beta_1 = \expc(Y) - \beta_2\expc(X)$, the model becomes $Y = \beta_1 + \beta_2 X$. Hence, it is a simple linear regression model where $Z \equiv 0$.
\end{solution}

\begin{exercise}
\label{exer:10.3.11}
Suppose that $X$ and $Y$ are random variables such that a regression model describes the relationship between $Y$ and $X$. If $\expc(Y \mid X) = \beta_1 + \beta_2 X^2$, then discuss whether or not this is a simple linear regression model (perhaps involving a predictor other than $X$).
\end{exercise}

\begin{solution}
We can write $\expc(Y \mid X) = \expc(Y \mid X^2)$ in this case and $\expc(Y \mid X^2) = \beta_1 + \beta_2 X^2$ so this is a simple linear regression model but the predictor is $X^2$ not $X$.
\end{solution}

\begin{exercise}
\label{exer:10.3.12}
Suppose that $X \sim N(2, 3)$ independently of $Z \sim N(0, 1)$ and $Y = X + Z$. Does this structure imply that the relationship between $Y$ and $X$ can be summarized by a simple linear regression model? If so, what are $\beta_1$, $\beta_2$, and $\sigma^2$?
\end{exercise}

\begin{solution}
The conditional expectation of $Y$ given $X$ is
\[
    \expc(Y \mid X) = \expc(X + Z \mid X) = X + \expc(Z \mid X) = X + \expc(Z) = X = 0 + 1 \cdot X.
\]
Hence, $\beta_1 = 0$, $\beta_2 = 1$ and $\sigma^2 = \var(Y - \expc(Y \mid X)) = \var(Z) = 1$.
\end{solution}

\begin{exercise}
\label{exer:10.3.13}
Suppose that a simple linear model is fit to data. An analysis of the residuals indicates that there is no reason to doubt that the model is correct; the ANOVA test indicates that there is substantial evidence against the null hypothesis of no relationship between the response and predictor. The value of $R^2$ is found to be 0.05. What is the interpretation of this number and what are the practical consequences?
\end{exercise}

\begin{solution}
The residual analysis shows the model is compatible with the data. Also there is a linear relationship between the response and predictor from the ANOVA test. However, the obtained $R^2 = 0.05$ is very small. That means the linear model only explains 5\% of the response. Hence, the predictor explains only 5\% of the response and 95\% of the variation in the response is due to random error. The model will not have much predictive power.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:10.3.14}
Suppose we consider the simple normal linear regression to describe the relationship between the response $Y$ (income) and the predictor $X$ (investment) for the data in Example~\ref{ex:10.3.9}.
\begin{enumerate}[(a)]
\item Plot the data in a scatter plot.
\item Calculate the least-squares line and plot this on the scatter plot in part (a).
\item Plot the standardized residuals against $X$.
\item Produce a normal probability plot of the standardized residuals.
\item What are your conclusions based on the plots produced in parts (c) and (d)?
\item If appropriate, calculate 0.95-confidence intervals for the intercept and slope.
\item Do the results of your analysis allow you to conclude that there is a relationship between $Y$ and $X$? Explain why or why not.
\item If the model is correct, what proportion of the observed variation in the response is explained by changes in the predictor?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A scatter plot of the data is given below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-14a.pdf}
      \caption{Scatter plot of Income versus Investment.}
      %\label{fig:scatter-10-3-14a}
    \end{figure}
    \item The least-squares estimates of $\beta_1$ and $\beta_2$ are given by $b_2 = 3.04845$ and $b_1 = 344.703$. The least-squares line is then given by $y = 344.703 + 3.04845x$. A scatter plot of the data together with a plot of the least-squares line follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-14b.pdf}
      \caption{Scatter plot with least-squares regression line Income $= 344.703 + 3.04845$ Investment, $S = 25.7384$, $R^2 = 81.7\%$, $R^2(\text{adj}) = 80.6\%$.}
      %\label{fig:regression-10-3-14b}
    \end{figure}
    \item The plot of the standardized residuals against $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-14c.pdf}
      \caption{Standardized residuals versus Investment (response is Income).}
      %\label{fig:residuals-10-3-14c}
    \end{figure}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-14d.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Income).}
      %\label{fig:normal-prob-10-3-14d}
    \end{figure}
    \item Both plots above indicate that the model assumptions are reasonable.
    \item A $.95$-confidence interval for the intercept is given by $344.703 \pm 16.48(2.1009) = (310.08, 379.33)$ and a $.95$-confidence interval for the slope is given by $3.04845 \pm 0.3406(2.1009) = (2.3329, 3.764)$.
    \item The $F$ statistics for testing $H_0: \beta_2 = 0$ is given by $F = 53069/662 = 80.165$ and, since $F \sim F(1, 18)$ under $H_0$, the P-value is $\prb(F > 80.165) = 0.000$, indicating strong evidence against the null hypothesis of no linear relationship. Since we have accepted the model as appropriate, this leads us to conclude that a relationship between $Y$ and $X$ exists.
    \item The proportion of the observed variation in the response that is being explained by changes in the predictor is given by the coefficient of determination $R^2 = 53069/64993 = .81653$, which is reasonably high.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.15}
The following data are measurements of tensile strength (100 lb/in$^2$) and hardness (Rockwell E) on 20 pieces of die-cast aluminum.
\begin{center}
\begin{tabular}{c|cc|c|cc}
Sample & Strength & Hardness & Sample & Strength & Hardness \\ \hline
1 & 293 & 53 & 11 & 298 & 60 \\
2 & 349 & 70 & 12 & 292 & 51 \\
3 & 340 & 78 & 13 & 380 & 95 \\
4 & 340 & 55 & 14 & 345 & 88 \\
5 & 340 & 64 & 15 & 257 & 51 \\
6 & 354 & 71 & 16 & 265 & 54 \\
7 & 322 & 82 & 17 & 246 & 52 \\
8 & 334 & 67 & 18 & 286 & 64 \\
9 & 247 & 56 & 19 & 324 & 83 \\
10 & 348 & 86 & 20 & 282 & 56
\end{tabular}
\end{center}
Suppose we consider the simple normal linear regression to describe the relationship between the response $Y$ (strength) and the predictor $X$ (hardness).
\begin{enumerate}[(a)]
\item Plot the data in a scatter plot.
\item Calculate the least-squares line and plot this on the scatter plot in part (a).
\item Plot the standardized residuals against $X$.
\item Produce a normal probability plot of the standardized residuals.
\item What are your conclusions based on the plots produced in parts (c) and (d)?
\item If appropriate, calculate 0.95-confidence intervals for the intercept and slope.
\item Do the results of your analysis allow you to conclude that there is a relationship between $Y$ and $X$? Explain why or why not.
\item If the model is correct, what proportion of the observed variation in the response is explained by changes in the predictor?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A scatter plot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-15a.pdf}
      \caption{Scatter plot of Strength versus Hardness.}
      %\label{fig:scatter-10-3-15a}
    \end{figure}
    \item The least-squares estimates of $\beta_1$ and $\beta_2$ are given by $b_2 = 2.14440$ and $b_1 = 168.854$ respectively. The least-squares line is then given by $y = 168.854 + 2.14440x$. A scatter plot of the data together with a plot of the least-squares line follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-15b.pdf}
      \caption{Scatter plot with least-squares regression line Strength $= 168.854 + 2.14440$ Hardness, $S = 26.0837$, $R^2 = 58.6\%$, $R^2(\text{adj}) = 56.3\%$.}
      %\label{fig:regression-10-3-15b}
    \end{figure}
    \item A plot of the standardized residuals against $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-15c.pdf}
      \caption{Standardized residuals versus Hardness (response is Strength).}
      %\label{fig:residuals-10-3-15c}
    \end{figure}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-15d.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Strength).}
      %\label{fig:normal-prob-10-3-15d}
    \end{figure}
    \item Both plots look reasonable. The first plot might reveal some trend indicating a possible violation of the assumption of equal variances.
    \item Then, $0.95$-confidence intervals for the intercept and the slope are given by $168.854 \pm 28.98 \cdot (2.1009) = (107.97, 229.74)$ and $2.1444 \pm 0.4250(2.1009) = (1.2515, 3.0373)$, respectively.
    \item The $F$ statistic for testing $H_0: \beta_2 = 0$ is given by $F = 17323/680 = 25.475$ and, since $F \sim F(1, 18)$ under $H_0$, the P-value equals $\prb(F > 25.475) = 0.000$, indicating strong evidence against the null hypothesis of no linear relationship. We conclude that there is a linear relationship between $X$ and $Y$.
    \item The proportion of the observed variation in the response that is being explained by changes in the predictor is given by the coefficient of determination $R^2 = 17323/29570 = .58583$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.16}
Tests were carried out to determine the effect of gas inlet temperature (degrees Fahrenheit) and rotor speed (rpm) on the tar content (grains/cu ft) of a gas stream, producing the following data.
\begin{center}
\begin{tabular}{c|ccc}
Observation & Tar & Speed & Temperature \\ \hline
1 & 60.0 & 2400 & 54.5 \\
2 & 65.0 & 2450 & 58.5 \\
3 & 63.5 & 2500 & 58.0 \\
4 & 44.0 & 2700 & 62.5 \\
5 & 54.5 & 2700 & 68.0 \\
6 & 26.0 & 2775 & 45.5 \\
7 & 54.0 & 2800 & 63.0 \\
8 & 53.5 & 2900 & 64.5 \\
9 & 33.5 & 3075 & 57.0 \\
10 & 44.0 & 3150 & 64.0
\end{tabular}
\end{center}
Suppose we consider the normal linear regression model
\begin{equation*}
(Y \mid W = w, X = x) \sim N(\beta_1 + \beta_2 w + \beta_3 x, \sigma^2)
\end{equation*}
to describe the relationship between $Y$ (tar content) and the predictors $W$ (rotor speed) and $X$ (temperature).
\begin{enumerate}[(a)]
\item Plot the response in scatter plots against each predictor.
\item Calculate the least-squares equation.
\item Plot the standardized residuals against $W$ and $X$.
\item Produce a normal probability plot of the standardized residuals.
\item What are your conclusions based on the plots produced in parts (c) and (d)?
\item If appropriate, calculate 0.95-confidence intervals for the regression coefficients.
\item Construct the ANOVA table to test whether or not there is a relationship between the response and the predictors. What is your conclusion?
\item If the model is correct, what proportion of the observed variation in the response is explained by changes in the predictors?
\item In an ANOVA table, assess the null hypothesis that there is no effect due to $W$ given that $X$ is in the model.
\item Estimate the mean of $Y$ when $W = 2750$ and $X = 50.0$. If we consider this value as a prediction of a future $Y$ at these settings, is this an extrapolation or interpolation?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A scatter plot of the response $Y$ against the predictor $W$ (speed) follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-16a1.pdf}
      \caption{Scatter plot of Tar versus Speed.}
      %\label{fig:scatter-tar-speed}
    \end{figure}
    The scatter plot of the response $Y$ against the predictor $X$ (temperature) follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-16a2.pdf}
      \caption{Scatter plot of Tar versus Temperature.}
      %\label{fig:scatter-tar-temp}
    \end{figure}
    \item The least-squares estimates of $\beta_1$, $\beta_2$, and $\beta_3$ are given by $b_1 = 87.8$, $b_2 = -0.0406$ and $b_3 = 1.23$. The least-squares equation is then given by $Y = 87.8 - 0.0406W + 1.23X$.
    \item A plot of the standardized residuals against $W$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-16c1.pdf}
      \caption{Standardized residuals versus Speed (response is Tar).}
      %\label{fig:residuals-speed}
    \end{figure}
    The plot of the standardized residuals against $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-16c2.pdf}
      \caption{Standardized residuals versus Temperature (response is Tar).}
      %\label{fig:residuals-temp}
    \end{figure}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-16d.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Tar).}
      %\label{fig:normal-prob-10-3-16d}
    \end{figure}
    \item The normal probability plot seems to indicate that the normality assumption is suspect. The other residual plots look reasonable.
    \item The $.95$-confidence intervals for the regression coefficients are given by $87.8 \pm 28.98(2.3646) = (19.274, 156.33)$ for $\beta_1$, $-0.0406 \pm 0.009142(2.3646) = (-0.062217, -0.018983)$ for $\beta_2$, and $1.23 \pm 0.3595(2.3646) = (.37993, 2.0801)$ for $\beta_3$.
    \item The ANOVA table is given below.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $W, X$ & 2 & 1159.29 & 579.65 \\
    Error & 7 & 315.58 & 45.08 \\
    Total & 9 & 1474.87 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0: \beta_2 = \beta_3 = 0$ is given by $F = 579.65/45.08 = 12.858$, and since $F \sim F(2, 7)$ under $H_0$, the P-value equals $\prb(F > 12.858) = 0.0045$. This provides strong evidence against the null hypothesis of no relationship between the response and the predictors.
    \item The proportion of the observed variation in the response that is being explained by changes in the predictor is given by the coefficient of determination $R^2 = 1159.29/1474.87 = .78603$.
    \item The ANOVA table for testing the null hypothesis $H_0: \beta_2 = 0$, given that $X$ is in the model, follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $X$ & 1 & 271.06 & 271.06 \\
    $W \mid X$ & 1 & 888.24 & 888.24 \\
    Error & 7 & 315.58 & 45.08 \\
    Total & 9 & 1474.87 &
    \end{tabular}
    \end{center}
    The $F$ statistic is then $F = 888.24/45.08 = 19.704$, and since $F \sim F(1, 7)$ under $H_0$, the P-value equals $\prb(F > 19.704) = .00301$, so we have some evidence against the null hypothesis. We conclude that $W$ (speed) has an effect on the response $Y$ (tar), given that $X$ is in the model.
    \item The estimate of the mean of $Y$ when $W = 2750$ and $X = 50.0$ is given by $\hat{Y} = 87.8 - 0.0406(2750) + 1.23(50.0) = 37.65$. This is an extrapolation because $50.0$ is not in the range of observed $X$ values.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.17}
Suppose we consider the normal linear regression model
\begin{equation*}
(Y \mid X = x) \sim N(\beta_1 + \beta_2 x + \beta_3 x^2, \sigma^2)
\end{equation*}
for the data of Exercise~\ref{exer:10.3.5}.
\begin{enumerate}[(a)]
\item Plot the response $Y$ in a scatter plot against $X$.
\item Calculate the least-squares equation.
\item Plot the standardized residuals against $X$.
\item Produce a normal probability plot of the standardized residuals.
\item What are your conclusions based on the plots produced in parts (c) and (d)?
\item If appropriate, calculate 0.95-confidence intervals for the regression coefficients.
\item Construct the ANOVA table to test whether or not there is a relationship between the response and the predictor. What is your conclusion?
\item If the model is correct, what proportion of the observed variation in the response is explained by changes in the predictors?
\item In an ANOVA table, assess the null hypothesis that there is no effect due to $X^2$ given that $X$ is in the model.
\item Compare the predictions of $Y$ at $X = 6$ using the simple linear regression model and using the linear model with a linear and quadratic term.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A scatter plot of the response $Y$ against the predictor $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-17a.pdf}
      \caption{Scatter plot of $Y$ versus $X$.}
      %\label{fig:scatter-10-3-17a}
    \end{figure}
    \item The least-squares estimates of $\beta_1$, $\beta_2$ and $\beta_3$ are given by $b_1 = 0.752$ and $b_2 = 2.10$ and $b_3 = 2.92$. The least-squares line is then given by $Y = 0.752 + 2.10X + 2.92X^2$.
    \item A plot of the standardized residuals against $X$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-17c.pdf}
      \caption{Standardized residuals versus $X$ (response is $Y$).}
      %\label{fig:residuals-10-3-17c}
    \end{figure}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-3-17d.pdf}
      \caption{Normal probability plot of the standardized residuals (response is $Y$).}
      %\label{fig:normal-prob-10-3-17d}
    \end{figure}
    \item All plots above, for the most part, look reasonable, so the model assumptions seem reasonable.
    \item Then, $.95$-confidence intervals for the regression coefficients are given by $0.752 \pm 0.6553(2.306) = (-.75912, 2.2631)$ for $\beta_1$, $2.10 \pm 0.1372(2.306) = (1.7836, 2.4164)$ for $\beta_2$, and $2.92 \pm 0.04911(2.306) = (2.8068, 3.0332)$ for $\beta_3$.
    \item The ANOVA table is given below.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $X, X^2$ & 2 & 7825.5 & 3912.7 \\
    Error & 8 & 16.6 & 2.1 \\
    Total & 10 & 7842.0 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0: \beta_2 = \beta_3 = 0$ is given by $F = 3912.7/2.1 = 1890.54$, and since $F \sim F(2, 8)$ under $H_0$, the P-value equals $\prb(F > 1890.54) = 0.000$, indicating strong evidence against the null hypothesis of no relationship between the response and the predictors.
    \item The proportion of the observed variation in the response that is being explained by changes in the predictor is given by the coefficient of determination $R^2 = 7825.5/7842.0 = .9979$, which is very high.
    \item The ANOVA table for testing the null hypothesis $H_0: \beta_3 = 0$ given that $X$ is in the model follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $X$ & 1 & 486.2 & 486.2 \\
    $X^2 \mid X$ & 1 & 7339.3 & 7339.3 \\
    Error & 8 & 16.6 & 2.1 \\
    Total & 10 & 7842.0 &
    \end{tabular}
    \end{center}
    The $F$ statistic is given by $F = 7339.3/2.1 = 3494.9$, and since $F \sim F(1, 8)$ under $H_0$, the P-value equals $\prb(F > 3494.9) = 0.000$, so we have strong evidence against the null hypothesis. We conclude that $X^2$ has an effect on the response, given that $X$ is in the model.
    \item We predict $Y$ at $X = 6$ by $29.9991 + 2.10236(6) = 42.613$ using the simple linear model and by $0.752 + 2.10(6) + 2.92(6^2) = 118.47$ using the linear model containing the linear and quadratic terms. So there is a substantial difference in these predictions.
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:10.3.18}
Suppose that $(x_1, \ldots, x_n)$ is a sample from the mixture distribution
\begin{equation*}
0.5 \, \text{Uniform}[0, 1] + 0.5 \, \text{Uniform}[2, \theta],
\end{equation*}
where $\theta > 2$ is unknown. What is the least-squares estimate of the mean of this distribution?
\end{exercise}

\begin{solution}
First, note that the mean of this distribution is given by $(1/2)2 + (1/2)((\theta - 2)/2) = (\theta - 1)/4$ and that this value is in the interval $(7/4, \infty)$. Therefore, the least-squares estimate is given by $\bar{x}$ whenever $\bar{x} \in (7/4, \infty)$ and is equal to $7/4$ whenever $\bar{x} \leqslant 7/4$.
\end{solution}

\begin{exercise}
\label{exer:10.3.19}
Consider the simple linear regression model and suppose that for the data collected, we have $\sum_{i=1}^{n} (x_i - \bar{x})^2 = 0$. Explain how, and for which value of $x$, you would estimate $\expc(Y \mid X = x)$.
\end{exercise}

\begin{solution}
Since $\sum_{i=1}^{n} (x_i - \bar{x})^2 = 0$, we must have $(x_i - \bar{x})^2 = 0$, so $x_i = \bar{x}$ for every $i$ and all the $x_i$ are equal to the same value, say $x$. Then we need to estimate the conditional mean of $Y$ at $X = x$ based on a sample $(y_1, \ldots, y_n)$ from this distribution. The model says that this conditional mean is of the form $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$, where $\beta_1, \beta_2 \in \mathbb{R}^1$. Therefore, $\expc(Y \mid X = x)$ can be any value in $\mathbb{R}^1$, and the least-squares estimate is given by the sample average $\bar{y}$.
\end{solution}

\begin{exercise}
\label{exer:10.3.20}
For the simple linear regression model, under the assumptions of Theorem~\ref{thm:10.3.3}, establish that
\begin{equation*}
\cov(Y_i - B_1 - B_2 x_i, Y_j - B_1 - B_2 x_j \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2 \delta_{ij} - \sigma^2 \left( \frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{\sum_{k=1}^{n} (x_k - \bar{x})^2} \right),
\end{equation*}
where $\delta_{ij} = 1$ when $i = j$ and is 0 otherwise. (Hint: Use Theorems~\ref{thm:3.3.2} and~\ref{thm:10.3.3}.)
\end{exercise}

\begin{solution}
For convenience we write $\cov(A, B \mid X_1 = x_1, \ldots, X_n = x_n) = \cov(A, B)$. By Theorem \ref{thm:3.3.2} (linearity of covariance) we have
\[
    \cov(Y_i - B_1 - B_2 x_i, Y_j - B_1 - B_2 x_j) = \cov(Y_i, Y_j) - \cov(Y_i, B_1 + B_2 x_j) - \cov(Y_j, B_1 + B_2 x_i) + \cov(B_1 + B_2 x_i, B_1 + B_2 x_j).
\]
Now $\cov(Y_i, Y_j) = \sigma^2 \delta_{ij}$, where $\delta_{ij} = 1$ when $i = j$ and is 0 otherwise. Also,
\begin{align*}
    \cov(Y_i, B_2) &= \cov\left(Y_i, \frac{\sum_{j=1}^{n} (Y_j - \bar{Y})(x_j - \bar{x})}{\sum_{j=1}^{n} (x_j - \bar{x})^2}\right) \\
    &= \frac{1}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \cov\left(Y_i, \sum_{j=1}^{n} x_j Y_j - \bar{x} \sum_{j=1}^{n} Y_j - \bar{Y} \sum_{j=1}^{n} x_j + n\bar{x}\bar{Y}\right) \\
    &= \frac{1}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \cov\left(Y_i, \sum_{j=1}^{n} x_j Y_j - n\bar{x}\bar{Y} - n\bar{x}\bar{Y} \sum_{j=1}^{n} x_j + n\bar{x}\bar{Y}\right) \\
    &= \frac{1}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \cov\left(Y_i, \sum_{j=1}^{n} x_j Y_j - n\bar{x}\bar{Y}\right) \\
    &= \frac{1}{\sum_{i=1}^{n} (x_i - \bar{x})^2} (x_i \sigma^2 - n\bar{x} \cov(Y_i, \bar{Y})) \\
    &= \frac{1}{\sum_{i=1}^{n} (x_i - \bar{x})^2} (x_i \sigma^2 - \bar{x}\sigma^2) = \frac{\sigma^2 (x_i - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}.
\end{align*}
and $\cov(Y_i, B_1) = \cov(Y_i, \bar{Y} - B_2 \bar{x}) = \cov(Y_i, \bar{Y}) - \bar{x} \cov(Y_i, B_2) = \sigma^2/n - \sigma^2 (x_i - \bar{x})\bar{x}/\sum_{i=1}^{n} (x_i - \bar{x})^2$. Therefore,
\begin{align*}
    \cov(Y_i, B_1 + B_2 x_j) &= \frac{\sigma^2}{n} - \frac{\sigma^2 (x_i - \bar{x})\bar{x}}{\sum_{i=1}^{n} (x_i - \bar{x})^2} + \frac{\sigma^2 (x_i - \bar{x}) x_j}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \\
    &= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right) = \cov(Y_j, B_1 + B_2 x_i).
\end{align*}
Also, using Theorem \ref{thm:10.3.3} we have that
\begin{align*}
    &\cov(B_1 + B_2 x_i, B_1 + B_2 x_j) \\
    &= \var(B_1) + x_i x_j \var(B_2) + (x_i + x_j) \cov(B_1, B_2) \\
    &= \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} + \frac{x_i x_j}{\sum_{i=1}^{n} (x_i - \bar{x})^2} - \frac{(x_i + x_j)\bar{x}}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right) \\
    &= \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right).
\end{align*}
All together this implies that
\begin{align*}
    &\cov(Y_i - B_1 - B_2 x_i, Y_j - B_1 - B_2 x_j) \\
    &= \sigma^2 \delta_{ij} - 2\sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right) + \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right) \\
    &= \sigma^2 \delta_{ij} - \sigma^2 \left(\frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right).
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:10.3.21}
Establish that \eqref{eq:10.3.11} is distributed $N(0, 1)$ when $S$ is replaced by $\sigma$ in the denominator. (Hint: Use Theorem~\ref{thm:4.6.1} and Problem~\ref{exer:10.3.20}.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    Y_i - (B_1 + B_2 x_i) &= Y_i - (\bar{Y} - B_2 \bar{x} - B_2 x_i) \\
    &= Y_i - \bar{Y} - B_2 (x_i - \bar{x}) \\
    &= Y_i - \bar{Y} - (x_i - \bar{x}) \frac{\sum_{j=1}^{n} (x_j - \bar{x})(Y_j - \bar{Y})}{\sum_{j=1}^{n} (x_j - \bar{x})^2}
\end{align*}
and we note that this is a linear combination of the independent normals $Y_1, \ldots, Y_n$. Therefore, by Theorem \ref{thm:4.6.1} we have that $Y_i - (B_1 + B_2 x_i)$, given $X_1 = x_1, \ldots, X_n = x_n$, is normally distributed with mean
\begin{align*}
    &\expc(Y_i - (B_1 + B_2 x_i) \mid X_1 = x_1, \ldots, X_n = x_n) \\
    &= \expc(Y_i \mid X_1 = x_1, \ldots, X_n = x_n) - \expc(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) \\
    &\quad - \expc(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) x_i \\
    &= \beta_1 + \beta_2 x_i - \beta_1 - \beta_2 x_i = 0,
\end{align*}
and with variance (using Problem~10.3.20 with $i = j$)
\[
    \var(Y_i - (B_1 + B_2 x_i) \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2 \left(1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right).
\]
Therefore,
\[
    \frac{Y_i - (B_1 + B_2 x_i)}{\sigma \left(1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right)^{1/2}} \sim N(0, 1)
\]
as claimed.
\end{solution}

\begin{exercise}[Prediction intervals]
\label{exer:10.3.22}
Under the assumptions of Theorem~\ref{thm:10.3.6}, prove that the interval
\begin{equation*}
b_1 + b_2 x \pm s \left( 1 + \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{k=1}^{n} (x_k - \bar{x})^2} \right)^{1/2} t_{(1+\gamma)/2}(n-2),
\end{equation*}
based on independent $(x_1, y_1), \ldots, (x_n, y_n)$, will contain $Y$ with probability equal to $\gamma$ for a future independent $(X, Y)$ with $X = x$. (Hint: Theorems~\ref{thm:4.6.1} and~\ref{thm:3.3.2} and Corollary~\ref{cor:10.3.1}.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    Y - (B_1 + B_2 x) &= Y - (\bar{Y} - B_2 \bar{x} - B_2 x) \\
    &= Y - \bar{Y} - B_2 (x - \bar{x}) \\
    &= Y - \bar{Y} - (x_i - \bar{x}) \frac{\sum_{j=1}^{n} (x_j - \bar{x})(Y_j - \bar{Y})}{\sum_{j=1}^{n} (x_j - \bar{x})^2}
\end{align*}
and we note that this is a linear combination of the independent normals $Y, Y_1, \ldots, Y_n$. Therefore, by Theorem \ref{thm:4.6.1} we have that $Y - (B_1 + B_2 x)$, given $X = x, X_1 = x_1, \ldots, X_n = x_n$, is normally distributed with mean
\begin{align*}
    &\expc(Y - (B_1 + B_2 x) \mid X = x, X_1 = x_1, \ldots, X_n = x_n) \\
    &= \expc(Y \mid X = x, X_1 = x_1, \ldots, X_n = x_n) \\
    &\quad - \expc(B_1 \mid X = x, X_1 = x_1, \ldots, X_n = x_n) \\
    &\quad - \expc(B_2 \mid X = x, X_1 = x_1, \ldots, X_n = x_n) x \\
    &= \expc(Y \mid X = x) - \expc(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) \\
    &\quad - \expc(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) x \\
    &= \beta_1 + \beta_2 x - \beta_1 - \beta_2 x = 0,
\end{align*}
and with variance (using Corollary 10.3.1)
\begin{align*}
    &\var(Y - (B_1 + B_2 x) \mid X = x, X_1 = x_1, \ldots, X_n = x_n) \\
    &= \var(Y \mid X = x) + \var(B_1 + B_2 x \mid X_1 = x_1, \ldots, X_n = x_n) \\
    &= \sigma^2 \left(1 + \frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right).
\end{align*}
Also, $Y - (B_1 + B_2 x)$ is independent of $(n - 2)S^2/\sigma^2 \sim \chi^2(n - 2)$, so by Definition 4.6.2
\[
    T = \frac{Y - (B_1 + B_2 x)}{\sigma \left(1 + \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right)^{1/2}} \bigg/ \sqrt{\frac{(n - 2)S^2}{(n - 2)\sigma^2}} = \frac{Y - (B_1 + B_2 x)}{S \left(1 + \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right)^{1/2}} \sim t(n - 2).
\]
Therefore,
\[
    \gamma = \prb\left(-t_{\frac{1+\gamma}{2}}(n - 2) < T < t_{\frac{1+\gamma}{2}}(n - 2) \,\Big|\, X = x, X_1 = x_1, \ldots, X_n = x_n\right),
\]
so the probability that
\[
    Y \in \left[B_1 + B_2 x \pm S \left(1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right)^{1/2} t_{\frac{1+\gamma}{2}}(n - 2)\right]
\]
is equal to $\gamma$.
\end{solution}

\begin{exercise}
\label{exer:10.3.23}
Consider the regression model with no intercept, given by $\expc(Y \mid X = x) = \beta x$, where $\beta \in \mathbb{R}^1$ is unknown. Suppose we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$.
\begin{enumerate}[(a)]
\item Determine the least-squares estimate of $\beta$.
\item Prove that the least-squares estimate $b$ of $\beta$ is unbiased and, when $\var(Y \mid X = x) = \sigma^2$, prove that
\begin{equation*}
\var(B \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{\sigma^2}{\sum_{i=1}^{n} x_i^2}.
\end{equation*}
\item Under the assumptions given in part (b), prove that
\begin{equation*}
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (y_i - bx_i)^2
\end{equation*}
is an unbiased estimator of $\sigma^2$.
\item Record an appropriate ANOVA decomposition for this model and a formula for $R^2$ measuring the proportion of the variation observed in $Y$ due to changes in $X$.
\item When $(Y \mid X = x) \sim N(\beta x, \sigma^2)$ and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$, prove that $b \sim N(\beta, \sigma^2 / \sum_{i=1}^{n} x_i^2)$.
\item Under the assumptions of part (e), and assuming that $(n-1)S^2/\sigma^2 \sim \chi^2(n-1)$ independent of $B$ (this can be proved), indicate how you would test the null hypothesis of no relationship between $Y$ and $X$.
\item How would you define standardized residuals for this model and use them to check model validity?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Putting $b = \sum_{i=1}^{n} x_i y_i / \sum_{i=1}^{n} x_i^2$, we have that
    \begin{align*}
        \sum_{i=1}^{n} (y_i - \beta x_i)^2 &= \sum_{i=1}^{n} (y_i - b x_i + b x_i - \beta x_i)^2 \\
        &= \sum_{i=1}^{n} (y_i - b x_i)^2 + 2(b - \beta) \sum_{i=1}^{n} (y_i - b x_i) x_i + (b - \beta)^2 \sum_{i=1}^{n} x_i^2 \\
        &= \sum_{i=1}^{n} (y_i - b x_i)^2 + (b - \beta)^2 \sum_{i=1}^{n} x_i^2
    \end{align*}
    since $\sum_{i=1}^{n} (y_i - b x_i) x_i = \sum_{i=1}^{n} x_i y_i - b \sum_{i=1}^{n} x_i^2 = 0$, and this is clearly minimized, as a function of $\beta$, by $b$.
    \item We have that
    \[
        \expc(B \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{\sum_{i=1}^{n} x_i \expc(Y_i \mid X_1 = x_1, \ldots, X_n = x_n)}{\sum_{i=1}^{n} x_i^2} = \frac{\sum_{i=1}^{n} x_i (\beta x_i)}{\sum_{i=1}^{n} x_i^2} = \beta \frac{\sum_{i=1}^{n} x_i^2}{\sum_{i=1}^{n} x_i^2} = \beta
    \]
    and
    \[
        \var(B \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{\sum_{i=1}^{n} x_i^2 \var(Y_i \mid X_1 = x_1, \ldots, X_n = x_n)}{(\sum_{i=1}^{n} x_i^2)^2} = \frac{\sigma^2 \sum_{i=1}^{n} x_i^2}{(\sum_{i=1}^{n} x_i^2)^2} = \frac{\sigma^2}{\sum_{i=1}^{n} x_i^2}.
    \]
    \item We have that
    \[
        \expc(S^2 \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{1}{n - 1} \sum_{i=1}^{n} \expc((Y_i - B x_i)^2 \mid X_1 = x_1, \ldots, X_n = x_n)
    \]
    and
    \begin{align*}
        &\expc((Y_i - B x_i)^2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
        &= \expc((Y_i - \beta x_i + \beta x_i - B x_i)^2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
        &= \var(Y_i \mid X_1 = x_1, \ldots, X_n = x_n) + x_i^2 \var(B \mid X_1 = x_1, \ldots, X_n = x_n) \\
        &\quad - 2x_i \cov(Y_i, B \mid X_1 = x_1, \ldots, X_n = x_n) \\
        &= \sigma^2 + \frac{\sigma^2 x_i^2}{\sum_{i=1}^{n} x_i^2} - \frac{2x_i^2}{\sum_{i=1}^{n} x_i^2} \var(Y_i \mid X_1 = x_1, \ldots, X_n = x_n) \\
        &= \sigma^2 - \frac{\sigma^2 x_i^2}{\sum_{i=1}^{n} x_i^2} = \sigma^2 \left(1 - \frac{x_i^2}{\sum_{i=1}^{n} x_i^2}\right).
    \end{align*}
    Combining these, we obtain $\expc(S^2 \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2$.
    \item We have that
    \begin{align*}
        \sum_{i=1}^{n} y_i^2 &= \sum_{i=1}^{n} (y_i - b x_i + b x_i)^2 = \sum_{i=1}^{n} (y_i - b x_i)^2 + 2b \sum_{i=1}^{n} (y_i - b x_i) x_i + b^2 \sum_{i=1}^{n} x_i^2 \\
        &= \sum_{i=1}^{n} (y_i - b x_i)^2 + b^2 \sum_{i=1}^{n} x_i^2.
    \end{align*}
    Here we have that $\sum_{i=1}^{n} (y_i - b x_i)^2$ is the error sum of squares and $b^2 \sum_{i=1}^{n} x_i^2$ is the regression sum of squares. The coefficient of determination is then given by $R^2 = b^2 \sum_{i=1}^{n} x_i^2 / \sum_{i=1}^{n} y_i^2$ and this is the proportion of the total variation observed in $Y$ (as measured by $\sum_{i=1}^{n} y_i^2$) due to changes in $X$.
    \item Since $B$ is a linear combination of independent normal variables we have that $B$ is normally distributed with mean given by (part (b)) $\beta$ and variance (part (b)) given by $\sigma^2 / \sum_{i=1}^{n} x_i^2$.
    \item We have that $(B - \beta)/\sigma (\sum_{i=1}^{n} x_i^2)^{-1/2} \sim N(0, 1)$ independent of $(n - 1)S^2/\sigma^2 \sim \chi^2(n - 1)$, so $(B - \beta)/S (\sum_{i=1}^{n} x_i^2)^{-1/2} \sim t(n - 1)$. Now there is no relationship between $X$ and $Y$ if and only if $\beta = 0$, so we test $H_0: \beta = 0$ by computing the P-value $\prb(|T| > |b/s (\sum_{i=1}^{n} x_i^2)^{-1/2}|)$, where $T \sim t(n - 1)$.
    \item We have that $y_i = b x_i + (y_i - b x_i)$ and when the model is correct $y_i - b x_i$ is a value from a distribution with mean 0 and variance (see part (b)) $\sigma^2 (1 - x_i^2 / \sum_{i=1}^{n} x_i^2)$. Therefore, the $i$th standardized residual is given by $(y_i - b x_i)/s (1 - x_i^2 / \sum_{i=1}^{n} x_i^2)^{1/2}$. We can plot these in residual plots and normal probability plots to see if they look like samples from the $N(0, 1)$ distribution.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.3.24}
For data $(x_1, y_1), \ldots, (x_n, y_n)$, prove that if $\mu_1 = \beta_1 + \beta_2 \bar{x}$ and $\mu_2 = \beta_2$, then $\sum_{i=1}^{n} (y_i - \beta_1 - \beta_2 x_i)^2$ equals
\begin{equation*}
\sum_{i=1}^{n} (y_i - \bar{y})^2 - n(\mu_1 - \bar{y})^2 - \mu_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2 - 2\mu_2 \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}).
\end{equation*}
From this, deduce that $\bar{y}$ and $a = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) / \sum_{i=1}^{n} (x_i - \bar{x})^2$ are the least squares of $\mu_1$ and $\mu_2$, respectively.
\end{exercise}

\begin{solution}
First, we should express the $\beta$'s in terms of the $\alpha$'s as follows $\beta_2 = \alpha_2$ and $\beta_1 = \alpha_1 - \alpha_2 \bar{x}$. Substituting those into the sum of squares, and noting that $\sum_{i=1}^{n} (x_i - \bar{x}) = \sum (y_i - \bar{y}) = 0$, we get
\begin{align*}
    \sum_{i=1}^{n} (y_i - \beta_1 - \beta_2 x_i)^2 &= \sum_{i=1}^{n} (y_i - \alpha_1 - \alpha_2 (x_i - \bar{x}))^2 \\
    &= \sum_{i=1}^{n} (y_i - \bar{y} + \bar{y} - \alpha_1 - \alpha_2 (x_i - \bar{x}))^2 \\
    &= \sum_{i=1}^{n} (y_i - \bar{y})^2 + 2(\bar{y} - \alpha_1) \sum_{i=1}^{n} (y_i - \bar{y}) - 2\alpha_2 \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}) \\
    &\quad + \sum_{i=1}^{n} (\bar{y} - \alpha_1 - \alpha_2 (x_i - \bar{x}))^2 \\
    &= \sum_{i=1}^{n} (y_i - \bar{y})^2 - 2\alpha_2 \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}) + n(\bar{y} - \alpha_1)^2 \\
    &\quad - 2\alpha_2 (\bar{y} - \alpha_1) \sum_{i=1}^{n} (x_i - \bar{x}) + \alpha_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2 \\
    &= \sum_{i=1}^{n} (y_i - \bar{y})^2 - 2\alpha_2 \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}) + n(\bar{y} - \alpha_1)^2 + \alpha_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2
\end{align*}
as claimed.

Clearly, this is minimized for $\alpha_1$, independently of $\alpha_2$, by selecting $\alpha_1 = \bar{y}$. Then we must minimize
\begin{align*}
    &-2\alpha_2 \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}) + \alpha_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2 \\
    &= \sum_{i=1}^{n} (x_i - \bar{x})^2 \left(\alpha_2 - \frac{\sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\right)^2 - \frac{(\sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}))^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{align*}
for $\alpha_2$. Clearly, this is minimized by taking $\alpha_2 = \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}) / \sum_{i=1}^{n} (x_i - \bar{x})^2$ as claimed.
\end{solution}

\begin{exercise}
\label{exer:10.3.25}
For the model discussed in Section~\ref{ssec:10.3.3}, prove that the prior given by $(\mu_1 \mid \sigma^2) \sim N(\nu_1, \sigma^2/\tau_1)$, $(\mu_2 \mid \sigma^2) \sim N(\nu_2, \sigma^2/\tau_2)$, and $1/\sigma^2 \sim \text{Gamma}(\alpha, \beta)$ leads to the posterior distribution stated there. Conclude that this prior is conjugate with the posterior distribution, as specified. (Hint: The development is similar to Example~\ref{ex:7.1.4}, as detailed in Section~\ref{sec:7.5}.)
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{c_y^2 - c_x^2 a^2}{2\sigma^2}\right) \exp\left(-\frac{n}{2\sigma^2} (\alpha_1 - \bar{y})^2\right) \exp\left(-\frac{c_x^2}{2\sigma^2} (\alpha_2 - a)^2\right).
\]
The posterior distribution of $\alpha_1$, given $\sigma^2$, is then proportional to
\begin{align*}
    &\exp\left(-\frac{n}{2\sigma^2} (\alpha_1 - \bar{y})^2 - \frac{1}{2\tau_1^2 \sigma^2} (\alpha_1 - \mu_1)^2\right) \\
    &\propto \exp\left(-\frac{1}{2\sigma^2} \left\{\left(n + \frac{1}{\tau_1^2}\right) \alpha_1^2 - 2\left(n\bar{y} + \frac{\mu_1}{\tau_1^2}\right) \alpha_1\right\}\right) \\
    &\propto \exp\left[-\frac{1}{2\sigma^2} \left(n + \frac{1}{\tau_1^2}\right) \left(\alpha_1 - \left(n + \frac{1}{\tau_1^2}\right)^{-1} \left(n\bar{y} + \frac{\mu_1}{\tau_1^2}\right)\right)^2\right]
\end{align*}
and we recognize this as being proportional to the density of a $N\left(\left(n + 1/\tau_1^2\right)^{-1} \left(n\bar{y} + \mu_1/\tau_1^2\right), \left(n + 1/\tau_1^2\right)^{-1} \sigma^2\right)$ distribution.

Also, the posterior distribution of $\alpha_2$, given $\sigma^2$, is then proportional to
\begin{align*}
    &\exp\left(-\frac{c_x^2}{2\sigma^2} (\alpha_2 - a)^2 - \frac{1}{2\tau_2^2 \sigma^2} (\alpha_2 - \mu_2)^2\right) \\
    &\propto \exp\left(-\frac{1}{2\sigma^2} \left\{\left(c_x^2 + \frac{1}{\tau_2^2}\right) \alpha_2^2 - \left(c_x^2 a + \frac{\mu_2}{\tau_2^2}\right) \alpha_2\right\}\right) \\
    &\propto \exp\left[-\frac{1}{2\sigma^2} \left(c_x^2 + \frac{1}{\tau_2^2}\right) \left(\alpha_2 - \left(c_x^2 + \frac{1}{\tau_2^2}\right)^{-1} \left(c_x^2 a + \frac{\mu_2}{\tau_2^2}\right)\right)^2\right]
\end{align*}
and we recognize this as being proportional to the density of a $N\left(\left(c_x^2 + 1/\tau_2^2\right)^{-1} \left(c_x^2 a + \mu_2/\tau_2^2\right), \left(c_x^2 + 1/\tau_2^2\right)^{-1} \sigma^2\right)$ distribution.

Finally, the posterior density of $1/\sigma^2$ is proportional to $(1/\sigma^2)^{n/2 + \kappa - 1} \exp(-v_{xy}/\sigma^2)$, where
\begin{align*}
    v_{xy} &= \frac{1}{2} \left\{(c_y^2 - a^2 c_x^2) + \left[n\bar{y}^2 + \frac{\mu_1^2}{\tau_1^2} - \left(n + \frac{1}{\tau_1^2}\right)^{-1} \left(n\bar{y} + \frac{\mu_1}{\tau_1^2}\right)^2\right]\right. \\
    &\quad \left. + \left[a^2 c_x^2 + \frac{\mu_2^2}{\tau_2^2} - \left(c_x^2 + \frac{1}{\tau_2^2}\right)^{-1} \left(c_x^2 a + \frac{\mu_2}{\tau_2^2}\right)^2\right]\right\} + v
\end{align*}
and we recognize this as being proportional to the density of a $\text{Gamma}(\kappa + n/2, v_{xy})$ distribution. Therefore, we established that the posterior distributions above are from the same family of distribution as the prior and therefore this prior is conjugate.
\end{solution}

\begin{exercise}
\label{exer:10.3.26}
For the model specified in Section~\ref{ssec:10.3.3}, prove that when $\tau_1, \tau_2 \to \infty$, and $\alpha \to 0$, the posterior distribution of $\mu_1$ is given by the distribution of $\bar{y} + \sqrt{2\beta_{xy}/(n(2\alpha + n))} \, Z$, where $Z \sim t(2\alpha + n)$ and $\beta_{xy} = (c_y^2 - a^2 c_x^2)/2$.
\end{exercise}

\begin{solution}
When $\tau_1 \to \infty$, $\tau_2 \to \infty$ and $\nu \to 0$ and the posterior converges to
\[
    \alpha_1 \mid \alpha_2, \sigma^2 \sim N(\bar{y}, \sigma^2/n), \quad \alpha_2 \mid \sigma^2 \sim N\left(a, \frac{\sigma^2}{c_x^2}\right), \quad 1/\sigma^2 \sim \text{Gamma}\left(\kappa + \frac{n}{2}, \nu_{xy}\right)
\]
where $\nu_{xy} = \{c_y^2 - a^2 c_x^2\}/2$, then the marginal posterior density of $\alpha_1$ is proportional to
\begin{align*}
    &\int_0^{\infty} \left(\frac{1}{\sigma^2}\right)^{1/2} \exp\left\{-\frac{n}{2\sigma^2} (\alpha_1 - \bar{y})^2\right\} \left(\frac{1}{\sigma^2}\right)^{\kappa + n/2 - 1} \exp\left\{-\frac{\nu_{xy}}{\sigma^2}\right\} \mathrm{d}\left(\frac{1}{\sigma^2}\right) \\
    &= \int_0^{\infty} \left(\frac{1}{\sigma^2}\right)^{\kappa + n/2 - 1/2} \exp\left\{-\left(\nu_{xy} + \frac{n}{2} (\alpha_1 - \bar{y})^2\right) \frac{1}{\sigma^2}\right\} \mathrm{d}\left(\frac{1}{\sigma^2}\right).
\end{align*}
Making the change of variable $1/\sigma^2 \to w$, where $w = (\nu_{xy} + \frac{n}{2} (\alpha_1 - \bar{y})^2)/\sigma^2$, in the above integral, shows that the marginal posterior density of $\alpha_1$ is proportional to $\left(1 + \frac{n}{2\nu_{xy}} (\alpha_1 - \bar{y})^2\right)^{-(2\kappa + n + 1)/2}$. This establishes (Problem \ref{exer:4.6.17}) that the posterior distribution of $\alpha_1$ is given by $\sqrt{2\kappa + n} \cdot \frac{\alpha_1 - \bar{y}}{\sqrt{2\nu_{xy}/n}} \sim t(2\kappa + n)$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:10.3.27}
If $(X_1, \ldots, X_n)$ is a sample from a distribution with finite variance, then prove that
\begin{equation*}
\frac{X_i - \bar{X}}{\sum_{k=1}^{n} (X_k - \bar{X})^2} \xrightarrow{\text{a.s.}} 0.
\end{equation*}
\end{exercise}

\begin{solution}
Let $\mu$ be the mean and $\sigma^2$ be the variance of the distribution of $X$. By the SLLN we have that $\bar{X} \xrightarrow{a.s.} \mu$ so, of necessity, $\bar{X}^2 \xrightarrow{a.s.} \mu^2$. Further, $\frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2 - \bar{X}^2 \xrightarrow{a.s.} \sigma^2 + \mu^2 - \mu^2 = \sigma^2$ since (again by the SLLN) $n^{-1} \sum_{i=1}^{n} X_i^2 \xrightarrow{a.s.} \expc(X^2) = \sigma^2 + \mu^2$. Also, for any random variable $Y$, we have that $Y/\sqrt{n} \xrightarrow{a.s.} 0$. Therefore,
\[
    \frac{X_i - \bar{X}}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}} = \frac{(X_i - \bar{X})/\sqrt{n}}{\sqrt{\frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2}} \xrightarrow{a.s.} \frac{0}{\sigma} = 0.
\]
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quantitative Response and Categorical Predictors}
\label{sec:10.4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we consider the situation in which the response is quantitative and the predictors are categorical. There can be many categorical predictors, but we restrict our discussion to at most two, as this gives the most important features of the general case. The general case is left to a further course.

\subsection{One Categorical Predictor (One-Way ANOVA)}
\label{ssec:10.4.1}

Suppose now that the response $Y$ is quantitative and the predictor $X$ is categorical, taking $a$ values or levels denoted $1, \ldots, a$. With the regression model, we assume that the only aspect of the conditional distribution of $Y$, given $X = x$, that changes as $x$ changes, is the mean. We let
\begin{equation*}
\mu_i = \expc(Y \mid X = i)
\end{equation*}
denote the mean response when the predictor $X$ is at level $i$. Note that this is immediately a linear regression model.

We introduce the dummy variables
\begin{equation*}
X_i = \begin{cases}
1 & X = i \\
0 & X \neq i
\end{cases}
\end{equation*}
for $i = 1, \ldots, a$. Notice that, whatever the value is of the response $Y$, only one of the dummy variables takes the value 1, and the rest take the value 0. Accordingly, we can write
\begin{equation*}
\expc(Y \mid X_1 = x_1, \ldots, X_a = x_a) = \mu_1 x_1 + \cdots + \mu_a x_a
\end{equation*}
because one and only one of the $x_i = 1$ whereas the rest are 0. This has exactly the same form as the model discussed in Section~\ref{ssec:10.3.4}, as the $X_i$ are quantitative. As such, all the results of Section~\ref{ssec:10.3.4} immediately apply (we will restate relevant results here).

\paragraph{Inferences About Individual Means}

Now suppose that we observe $n_i$ values $y_{i1}, \ldots, y_{in_i}$ when $X = i$ and all the response values are independent. Note that we have $a$ independent samples. The least-squares estimates of the $\mu_i$ are obtained by minimizing
\begin{equation*}
\sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \mu_i)^2.
\end{equation*}
The least-squares estimates are then equal to (see Problem~\ref{exer:10.4.14})
\begin{equation*}
b_i = \bar{y}_{i \cdot} = \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij}.
\end{equation*}
These can be shown to be unbiased estimators of the $\mu_i$.

Assuming that the conditional distributions of $Y$ given $X = x$ all have variance equal to $\sigma^2$, we have that the conditional variance of $\bar{Y}_{i \cdot}$ is given by $\sigma^2/n_i$ and the conditional covariance between $\bar{Y}_{i \cdot}$ and $\bar{Y}_{j \cdot}$ when $i \neq j$ is 0. Furthermore, under these conditions, an unbiased estimator of $\sigma^2$ is given by
\begin{equation*}
s^2 = \frac{1}{N - a} \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{i \cdot})^2,
\end{equation*}
where $N = n_1 + \cdots + n_k$.

If, in addition, we assume the normal linear regression model, namely,
\begin{equation*}
(Y \mid X = i) \sim N(\mu_i, \sigma^2),
\end{equation*}
then $\bar{Y}_{i \cdot} \sim N(\mu_i, \sigma^2/n_i)$ independent of $(N - a)S^2/\sigma^2 \sim \chi^2(N - a)$. Therefore, by Definition~\ref{def:4.6.2},
\begin{equation*}
T = \frac{\bar{Y}_{i \cdot} - \mu_i}{S/\sqrt{n_i}} \sim t(N - a),
\end{equation*}
which leads to a $\gamma$-confidence interval of the form
\begin{equation*}
\bar{y}_{i \cdot} \pm \frac{s}{\sqrt{n_i}} \, t_{(1+\gamma)/2}(N - a)
\end{equation*}
for $\mu_i$. Also, we can test the null hypothesis $H_0 : \mu_i = \mu_{i0}$ by computing the P-value
\begin{equation*}
\prb\left( |T| \geqslant \frac{|\bar{y}_{i \cdot} - \mu_{i0}|}{s/\sqrt{n_i}} \right) = 2 \left( 1 - G_{N-a}\left( \frac{|\bar{y}_{i \cdot} - \mu_{i0}|}{s/\sqrt{n_i}} \right) \right),
\end{equation*}
where $G_{N-a}$ is the cdf of the $t(N - a)$ distribution. Note that these inferences are just like those derived in Section~\ref{sec:6.3} for the location-scale normal model, except we now use a different estimator of $\sigma^2$ (with more degrees of freedom).

\paragraph{Inferences about Differences of Means and Two Sample Inferences}

Often we want to make inferences about a difference of means $\mu_i - \mu_j$. Note that $\expc(\bar{Y}_{i \cdot} - \bar{Y}_{j \cdot}) = \mu_i - \mu_j$ and
\begin{equation*}
\var(\bar{Y}_{i \cdot} - \bar{Y}_{j \cdot}) = \var(\bar{Y}_{i \cdot}) + \var(\bar{Y}_{j \cdot}) = \sigma^2(1/n_i + 1/n_j)
\end{equation*}
because $\bar{Y}_{i \cdot}$ and $\bar{Y}_{j \cdot}$ are independent. By Theorem~\ref{thm:4.6.1},
\begin{equation*}
\bar{Y}_{i \cdot} - \bar{Y}_{j \cdot} \sim N(\mu_i - \mu_j, \sigma^2(1/n_i + 1/n_j)).
\end{equation*}
Furthermore,
\begin{equation*}
\frac{\bar{Y}_{i \cdot} - \bar{Y}_{j \cdot} - (\mu_i - \mu_j)}{(1/n_i + 1/n_j)^{1/2}} \cdot \frac{1}{\sigma} \sim N(0, 1)
\end{equation*}
independent of $(N - a)S^2/\sigma^2 \sim \chi^2(N - a)$. Therefore, by Definition~\ref{def:4.6.2},
\begin{equation}
\label{eq:10.4.1}
T = \frac{\bar{Y}_{i \cdot} - \bar{Y}_{j \cdot} - (\mu_i - \mu_j)}{(1/n_i + 1/n_j)^{1/2}} \cdot \frac{1}{\sqrt{\frac{(N-a)S^2}{\sigma^2}/(N-a)}} = \frac{\bar{Y}_{i \cdot} - \bar{Y}_{j \cdot} - (\mu_i - \mu_j)}{S(1/n_i + 1/n_j)^{1/2}} \sim t(N - a).
\end{equation}
This leads to the $\gamma$-confidence interval
\begin{equation*}
\bar{y}_{i \cdot} - \bar{y}_{j \cdot} \pm s \sqrt{\frac{1}{n_i} + \frac{1}{n_j}} \, t_{(1+\gamma)/2}(N - a)
\end{equation*}
for the difference of means $\mu_i - \mu_j$. We can test the null hypothesis $H_0 : \mu_i = \mu_j$, i.e., that the difference in the means equals 0, by computing the P-value
\begin{equation*}
\prb\left( |T| \geqslant \frac{|\bar{y}_{i \cdot} - \bar{y}_{j \cdot}|}{s\sqrt{1/n_i + 1/n_j}} \right) = 2 \left( 1 - G_{N-a}\left( \frac{|\bar{y}_{i \cdot} - \bar{y}_{j \cdot}|}{s\sqrt{1/n_i + 1/n_j}} \right) \right).
\end{equation*}

When $a = 2$, i.e., there are just two values for $X$, we refer to \eqref{eq:10.4.1} as the \emph{two-sample $t$-statistic}, and the corresponding inference procedures are called the \emph{two-sample $t$-confidence interval} and the \emph{two-sample $t$-test} for the difference of means. In this case, if we conclude that $\mu_1 \neq \mu_2$, then we are saying that a relationship exists between $Y$ and $X$.

\paragraph{The ANOVA for Assessing a Relationship with the Predictor}

Suppose, in the general case when $a \geqslant 2$, we are interested in assessing whether or not there is a relationship between the response and the predictor. There is no relationship if and only if all the conditional distributions are the same; this is true, under our assumptions, if and only if $\mu_1 = \cdots = \mu_a$, i.e., if and only if all the means are equal. So testing the null hypothesis that there is no relationship between the response and the predictor is equivalent to testing the null hypothesis $H_0 : \mu_1 = \cdots = \mu_a = \mu$ for some unknown $\mu$.

If the null hypothesis is true, the least-squares estimate of $\mu$ is given by $\bar{y}_{\cdot \cdot}$, the overall average response value. In this case, we have that the total variation decomposes as (see Problem~\ref{exer:10.4.15})
\begin{equation*}
\sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{\cdot \cdot})^2 = \sum_{i=1}^{a} n_i (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2 + \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{i \cdot})^2,
\end{equation*}
and so the relevant ANOVA table for testing $H_0$ is given below.
\begin{center}
\begin{tabular}{l|c|c|c}
Source & Df & Sum of Squares & Mean Square \\ \hline
$X$ & $a - 1$ & $\sum_{i=1}^{a} n_i (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2$ & $\sum_{i=1}^{a} n_i (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2 / (a - 1)$ \\
Error & $N - a$ & $\sum_{i} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{i \cdot})^2$ & $s^2$ \\ \hline
Total & $N - 1$ & $\sum_{i} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{\cdot \cdot})^2$ &
\end{tabular}
\end{center}

To assess $H_0$, we use the $F$-statistic
\begin{equation*}
F = \frac{\sum_{i=1}^{a} n_i (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2 / (a - 1)}{s^2}
\end{equation*}
because, under the null hypothesis, both the numerator and the denominator are unbiased estimators of $\sigma^2$. When the null hypothesis is false, the numerator tends to be larger than $\sigma^2$. When we add the normality assumption, we have that $F \sim F(a - 1, N - a)$, and so we compute the P-value
\begin{equation*}
\prb\left( F \geqslant \frac{\sum_{i=1}^{a} n_i (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2 / (a - 1)}{s^2} \right)
\end{equation*}
to assess whether the observed value of $F$ is so large as to be surprising. Note that when $a = 2$, this P-value equals the P-value obtained via the two-sample $t$-test.

\paragraph{Multiple Comparisons}

If we reject the null hypothesis of no differences among the means, then we want to see where the differences exist. For this, we use inference methods based on \eqref{eq:10.4.1}. Of course, we have to worry about the problem of multiple comparisons, as discussed in Section~\ref{sec:9.3}. Recall that this problem arises whenever we are testing many null hypotheses using a specific critical value, such as 5\%, as a cutoff for a P-value, to decide whether or not a difference exists. The cutoff value for an individual P-value is referred to as the \emph{individual error rate}. In effect, even if no differences exist, the probability of concluding that at least one difference exists, the \emph{family error rate}, can be quite high.

There are a number of procedures designed to control the family error rate when making multiple comparisons. The simplest is to lower the individual error rate, as the family error rate is typically an increasing function of this quantity. This is the approach we adopt here, and we rely on statistical software to compute and report the family error rate for us. We refer to this procedure as \emph{Fisher's multiple comparison test}.

\paragraph{Model Checking}

To check the model, we look at the standardized residuals (see Problem~\ref{exer:10.4.17}) given by
\begin{equation}
\label{eq:10.4.2}
\frac{y_{ij} - \bar{y}_{i \cdot}}{s\sqrt{1 - 1/n_i}}.
\end{equation}
We will restrict our attention to various plots of the standardized residuals for model checking.

We now consider an example.

\begin{example}
\label{ex:10.4.1}
A study was undertaken to determine whether or not eight different types of fat are absorbed in different amounts during the cooking of donuts. Results were collected based on cooking six different donuts and then measuring the amount of fat in grams absorbed. We take the variable $X$ to be the type of fat and use the model of this section. The collected data are presented in the following table.
\begin{center}
\begin{tabular}{l|cccccc}
Fat 1 & 164 & 177 & 168 & 156 & 172 & 195 \\
Fat 2 & 172 & 197 & 167 & 161 & 180 & 190 \\
Fat 3 & 177 & 184 & 187 & 169 & 179 & 197 \\
Fat 4 & 178 & 196 & 177 & 181 & 184 & 191 \\
Fat 5 & 163 & 177 & 144 & 165 & 166 & 178 \\
Fat 6 & 163 & 193 & 176 & 172 & 176 & 178 \\
Fat 7 & 150 & 179 & 146 & 141 & 169 & 183 \\
Fat 8 & 164 & 169 & 155 & 149 & 170 & 167
\end{tabular}
\end{center}

A normal probability plot of the standardized residuals is provided in Figure~\ref{fig:10.4.1}. A plot of the standardized residuals against type of fat is provided in Figure~\ref{fig:10.4.2}. Neither plot gives us significant grounds for concern over the validity of the model, although there is some indication of a difference in the variability of the response as the type of fat changes. Another useful plot in this situation is a side-by-side boxplot, as it shows graphically where potential differences may lie. Such a plot is provided in Figure~\ref{fig:10.4.3}.

The following table gives the mean amounts of each fat absorbed.
\begin{center}
\begin{tabular}{cccccccc}
Fat 1 & Fat 2 & Fat 3 & Fat 4 & Fat 5 & Fat 6 & Fat 7 & Fat 8 \\
172.00 & 177.83 & 182.17 & 184.50 & 165.50 & 176.33 & 161.33 & 162.33
\end{tabular}
\end{center}
The grand mean response is given by 172.8.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_4_1.pdf}
\caption{Normal probability plot of the standardized residuals in Example~\ref{ex:10.4.1}.}
\label{fig:10.4.1}
\end{figure}

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_4_2.pdf}
\caption{Standardized residuals versus type of fat in Example~\ref{ex:10.4.1}.}
\label{fig:10.4.2}
\end{figure}

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_4_3.pdf}
\caption{Side-by-side boxplots of the response versus type of fat in Example~\ref{ex:10.4.1}.}
\label{fig:10.4.3}
\end{figure}

To assess the null hypothesis of no differences among the types of fat, we calculate the following ANOVA table.
\begin{center}
\begin{tabular}{l|c|c|c}
Source & Df & Sum of Squares & Mean Square \\ \hline
$X$ & 7 & 3344 & 478 \\
Error & 40 & 5799 & 145 \\ \hline
Total & 47 & 9143 &
\end{tabular}
\end{center}

Then we use the $F$-statistic given by $F = 478/145 = 3.3$. Because $F \sim F(7, 40)$ under $H_0$, we obtain the P-value $\prb(F \geqslant 3.3) = 0.007$. Therefore, we conclude that there is a difference among the fat types at the 0.05 level.

To ascertain where the differences exist, we look at all pairwise differences. There are $8 \times 7/2 = 28$ such comparisons. If we use the 0.05 level to determine whether or not a difference among means exists, then software computes the family error rate as 0.481, which seems uncomfortably high. When we use the 0.01 level, the family error rate falls to 0.151. With the individual error rate at 0.003, the family error rate is 0.0546. Using the individual error rate of 0.003, the only differences detected among the means are those between Fat 4 and Fat 7, and Fat 4 and Fat 8. Note that Fat 4 has the highest absorption whereas Fats 7 and 8 have the lowest absorptions.

Overall, the results are somewhat inconclusive, as we see some evidence of differences existing, but we are left with some anomalies as well. For example, Fats 4 and 5 are not different and neither are Fats 7 and 5, but Fats 4 and 7 are deemed to be different. To resolve such conflicts requires either larger sample sizes or a more refined experiment so that the comparisons are more accurate.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Repeated Measures (Paired Comparisons)}
\label{ssec:10.4.2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Consider $k$ quantitative variables $Y_1, \ldots, Y_k$ defined on a population $\Pi$. Suppose that our purpose is to compare the distributions of these variables. Typically, these will be similar variables, all measured in the same units.

\begin{example}
\label{ex:10.4.2}
Suppose that $\Pi$ is a set of students enrolled in a first-year program requiring students to take both calculus and physics, and we want to compare the marks achieved in these subjects. If we let $Y_1$ denote the calculus grade and $Y_2$ denote the physics grade, then we want to compare the distributions of these variables.
\end{example}

\begin{example}
\label{ex:10.4.3}
Suppose we want to compare the distributions of the duration of headaches for two treatments (A and B) in a population $\Pi$ of migraine headache sufferers. We let $Y_1$ denote the duration of a headache after being administered treatment A and let $Y_2$ denote the duration of a headache after being administered treatment B.
\end{example}

The repeated-measures approach to the problem of comparing the distributions of $Y_1, \ldots, Y_k$ involves taking a random sample $\pi_1, \ldots, \pi_n$ from $\Pi$ and, for each $\pi_i \in \Pi$, obtaining the $k$-dimensional value $(Y_1(\pi_i), \ldots, Y_k(\pi_i)) = (y_{i1}, \ldots, y_{ik})$. This gives a sample of $n$ from a $k$-dimensional distribution. Obviously, this is called repeated measures because we are taking the measurements $(Y_1(\pi_i), \ldots, Y_k(\pi_i))$ on the same $\pi_i$.

An alternative to repeated measures is to take $k$ independent samples from $\Pi$ and, for each of these samples, to obtain the values of one and only one of the variables $Y_i$. There is an important reason why the repeated-measures approach is preferred: We expect less variation in the values of differences, like $Y_i - Y_j$, under repeated-measures sampling, than we do under independent sampling because the values $Y_1, \ldots, Y_k$ are being taken on the same member of the population in repeated measures.

To see this more clearly, suppose all of the variances and covariances exist for the joint distribution of $Y_1, \ldots, Y_k$. This implies that
\begin{equation}
\label{eq:10.4.3}
\var(Y_i - Y_j) = \var(Y_i) + \var(Y_j) - 2\cov(Y_i, Y_j).
\end{equation}
Because $Y_i$ and $Y_j$ are similar variables, being measured on the same individual, we expect them to be positively correlated. Now with independent sampling, we have that $\var(Y_i - Y_j) = \var(Y_i) + \var(Y_i)$, so the variances of differences should be smaller with repeated measures than with independent sampling.

When we assume that the distributions of the $Y_i$ differ at most in their means, then it makes sense to make inferences about the differences of the population means $\mu_i - \mu_j$ using the differences of the sample means $\bar{y}_{\cdot i} - \bar{y}_{\cdot j}$. In the repeated-measures context, we can write
\begin{equation*}
\bar{y}_{\cdot i} - \bar{y}_{\cdot j} = \frac{1}{n} \sum_{l=1}^{n} (y_{li} - y_{lj}).
\end{equation*}
Because the individual components of this sum are independent and so,
\begin{equation*}
\var(\bar{Y}_{\cdot i} - \bar{Y}_{\cdot j}) = \frac{\var(Y_i) + \var(Y_j) - 2\cov(Y_i, Y_j)}{n}.
\end{equation*}
We can consider the differences $d_1 = y_{1i} - y_{1j}, \ldots, d_n = y_{ni} - y_{nj}$ to be a sample of $n$ from a one-dimensional distribution with mean $\mu_i - \mu_j$ and variance $\tau^2$ given by \eqref{eq:10.4.3}. Accordingly, we estimate $\mu_i - \mu_j$ by $\bar{d} = \bar{y}_{\cdot i} - \bar{y}_{\cdot j}$ and estimate $\tau^2$ by
\begin{equation}
\label{eq:10.4.4}
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (d_i - \bar{d})^2.
\end{equation}

If we assume that the joint distribution of $Y_1, \ldots, Y_k$ is multivariate normal (this means that any linear combination of these variables is normally distributed --- see Problem~\ref{exer:9.1.18}), then this forces the distribution of $Y_i - Y_j$ to be $N(\mu_i - \mu_j, \tau^2)$. Accordingly, we have all the univariate techniques discussed in Chapter~\ref{ch:6} for inferences about $\mu_i - \mu_j$.

The discussion so far has been about whether the distributions of variables differed. Assuming these distributions differ at most in their means, this leads to a comparison of the means. We can, however, record an observation as $(X, Y)$ where $X$ takes values in $\{1, \ldots, k\}$ and $X = i$ means that $Y = Y_i$. Then the conditional distribution of $Y$ given $X = i$ is the same as the distribution of $Y_i$. Therefore, if we conclude that the distributions of the $Y_i$ are different, we can conclude that a relationship exists between $Y$ and $X$. In Example~\ref{ex:10.4.2}, this means that a relationship exists between a student's grade and whether or not the grade was in calculus or physics. In Example~\ref{ex:10.4.3}, this means that a relationship exists between length of a headache and the treatment.

When can we assert that such a relationship is in fact a cause--effect relationship? Applying the discussion in Section~\ref{ssec:10.1.2}, we know that we have to be able to assign the value of $X$ to a randomly selected element of the population. In Example~\ref{ex:10.4.2}, we see this is impossible, so we cannot assert that such a relationship is a cause--effect relationship. In Example~\ref{ex:10.4.3}, however, we can indeed do this --- namely, for a randomly selected individual, we randomly assign a treatment to the first headache experienced during the study period and then apply the other treatment to the second headache experienced during the study period.

A full discussion of repeated measures requires more advanced concepts in statistics. We restrict our attention now to the presentation of an example when $k = 2$, which is commonly referred to as \emph{paired comparisons}.

\begin{example}[Blood Pressure Study]
\label{ex:10.4.4}
The following table came from a study of the effect of the drug captopril on blood pressure, as reported in \emph{Applied Statistics, Principles and Examples} by D.~R.~Cox and E.~J.~Snell (Chapman and Hall, London, 1981). Each measurement is the difference in the systolic blood pressure before and after having been administered the drug.
\begin{center}
\begin{tabular}{ccccc}
$-9$ & $-4$ & $-21$ & $-3$ & $-20$ \\
$-31$ & $-17$ & $-26$ & $-26$ & $-10$ \\
$-23$ & $-33$ & $-19$ & $-19$ & $-23$
\end{tabular}
\end{center}

Figure~\ref{fig:10.4.4} is a normal probability plot for these data and, because this looks reasonable, we conclude that the inference methods based on the assumption of normality are acceptable. Note that here we have not standardized the variable first, so we are only looking to see if the plot is reasonably straight.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_4_4.pdf}
\caption{Normal probability plot for the data in Example~\ref{ex:10.4.4}.}
\label{fig:10.4.4}
\end{figure}

The mean difference is given by $\bar{d} = -18.93$ with standard deviation $s = 9.03$. Accordingly, the standard error of the estimate of the difference in the means, using \eqref{eq:10.4.4}, is given by $s/\sqrt{15} = 2.33$. A 0.95-confidence interval for the difference in the mean systolic blood pressure, before and after being administered captopril, is then
\begin{equation*}
\bar{d} \pm \frac{s}{\sqrt{n}} \, t_{0.975}(n - 1) = -18.93 \pm 2.33 \times t_{0.975}(14) = (-23.93, -13.93).
\end{equation*}
Because this does not include 0, we reject the null hypothesis of no difference in the means at the 0.05 level. The actual P-value for the two-sided test is given by
\begin{equation*}
\prb(|T| \geqslant 18.93/2.33) = 0.000
\end{equation*}
because $T \sim t(14)$ under the null hypothesis $H_0$ that the means are equal. Therefore, we have strong evidence against $H_0$. It seems that we have strong evidence that the drug is leading to a drop in blood pressure.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Two Categorical Predictors (Two-Way ANOVA)}
\label{ssec:10.4.3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now suppose that we have a single quantitative response $Y$ and two categorical predictors $A$ and $B$, where $A$ takes $a$ levels and $B$ takes $b$ levels. One possibility is to consider running two one-factor studies. One study will examine the relationship between $Y$ and $A$, and the second study will examine the relationship between $Y$ and $B$. There are several disadvantages to such an approach, however.

First, and perhaps foremost, doing two separate analyses will not allow us to determine the joint relationship $A$ and $B$ have with $Y$. This relates directly to the concept of \emph{interaction} between predictors. We will soon define this concept more precisely, but basically, if $A$ and $B$ interact, then the conditional relationship between $Y$ and $A$, given $B = j$, changes in some substantive way as we change $j$. If the predictors $A$ and $B$ do not interact, then indeed we will be able to examine the relationship between the response and each of the predictors separately. But we almost never know that this is the case beforehand and must assess whether or not an interaction exists based on collected data.

A second reason for including both predictors in the analysis is that this will often lead to a reduction in the contribution of random error to the results. By this, we mean that we will be able to explain some of the observed variation in $Y$ by the inclusion of the second variable in the model. This depends, however, on the additional variable having a relationship with the response. Furthermore, for the inclusion of a second variable to be worthwhile, this relationship must be strong enough to justify the loss in degrees of freedom available for the estimation of the contribution of random error to the experimental results. As we will see, including the second variable in the analysis results in a reduction in the degrees of freedom in the Error row of the ANOVA table. Degrees of freedom are playing the role of sample size here. The fewer the degrees of freedom in the Error row, the less accurate our estimate of $\sigma^2$ will be.

When we include both predictors in our analysis, and we have the opportunity to determine the sampling process, it is important that we \emph{cross} the predictors. By this, we mean that we observe $Y$ at each combination
\begin{equation*}
(A, B) = (i, j), \quad (1, a) \times (1, b).
\end{equation*}
Suppose, then, that we have $n_{ij}$ response values at the $(A, B) = (i, j)$ setting of the predictors. Then, letting
\begin{equation*}
\expc(Y \mid A = i, B = j) = \mu_{ij}
\end{equation*}
be the mean response when $A = i$ and $B = j$, and introducing the dummy variables
\begin{equation*}
X_{ij} = \begin{cases}
1 & A = i \text{ and } B = j \\
0 & A \neq i \text{ or } B \neq j
\end{cases}
\end{equation*}
we can write
\begin{equation*}
\expc(Y \mid X_{ij} = x_{ij} \text{ for all } i, j) = \mu_{11} x_{11} + \mu_{21} x_{21} + \cdots + \mu_{ab} x_{ab} = \sum_{i=1}^{a} \sum_{j=1}^{b} \mu_{ij} x_{ij}.
\end{equation*}
The relationship between $Y$ and the predictors is completely encompassed in the changes in the $\mu_{ij}$ as $i$ and $j$ change. From this, we can see that a regression model for this situation is immediately a linear regression model.

\paragraph{Inferences About Individual Means and Differences of Means}

Now let $y_{ijk}$ denote the $k$th response value when $X_{ij} = 1$. Then, as in Section~\ref{ssec:10.4.1}, the least-squares estimate of $\mu_{ij}$ is given by
\begin{equation*}
b_{ij} = \bar{y}_{ij \cdot} = \frac{1}{n_{ij}} \sum_{k=1}^{n_{ij}} y_{ijk},
\end{equation*}
the mean of the observations when $X_{ij} = 1$. If in addition we assume that the conditional distributions of $Y$ given the predictors all have variance equal to $\sigma^2$, then with $N = n_{11} + n_{21} + \cdots + n_{ab}$, we have that
\begin{equation}
\label{eq:10.4.5}
s^2 = \frac{1}{N - ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{ij \cdot})^2
\end{equation}
is an unbiased estimator of $\sigma^2$. Therefore, using \eqref{eq:10.4.5}, the standard error of $\bar{y}_{ij \cdot}$ is given by $s/\sqrt{n_{ij}}$.

With the normality assumption, we have that $\bar{Y}_{ij \cdot} \sim N(\mu_{ij}, \sigma^2/n_{ij})$ independent of
\begin{equation*}
\frac{(N - ab)S^2}{\sigma^2} \sim \chi^2(N - ab).
\end{equation*}
This leads to the $\gamma$-confidence intervals
\begin{equation*}
\bar{y}_{ij \cdot} \pm \frac{s}{\sqrt{n_{ij}}} \, t_{(1+\gamma)/2}(N - ab)
\end{equation*}
for $\mu_{ij}$ and
\begin{equation*}
\bar{y}_{ij \cdot} - \bar{y}_{kl \cdot} \pm s \sqrt{\frac{1}{n_{ij}} + \frac{1}{n_{kl}}} \, t_{(1+\gamma)/2}(N - ab)
\end{equation*}
for the difference of means $\mu_{ij} - \mu_{kl}$.

\paragraph{The ANOVA for Assessing Interaction and Relationships with the Predictors}

We are interested in whether or not there is any relationship between $Y$ and the predictors. There is no relationship between the response and the predictors if and only if all the $\mu_{ij}$ are equal. Before testing this, however, it is customary to test the null hypothesis that there is no interaction between the predictors. The precise definition of no interaction here is that
\begin{equation*}
\mu_{ij} = \alpha_i + \beta_j
\end{equation*}
for all $i$ and $j$, for some constants $\alpha_i$ and $\beta_j$, i.e., the means can be expressed additively. Note that if we fix $B = j$ and let $A$ vary, then these response curves (a response curve is a plot of the means of one variable while holding the value of the second variable fixed) are all parallel. This is an equivalent way of saying that there is no interaction between the predictors.

In Figure~\ref{fig:10.4.5}, we have depicted response curves in which the factors do not interact, and in Figure~\ref{fig:10.4.6} we have depicted response curves in which they do. Note that the solid lines, for example, joining $\mu_{11}$ and $\mu_{21}$, are there just to make it easier to display the parallelism (or lack thereof) and have no other significance.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_4_5.pdf}
\caption{Response curves for expected response with two predictors, with $A$ taking three levels and $B$ taking two levels. Because they are parallel, the predictors do not interact.}
\label{fig:10.4.5}
\end{figure}

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_4_6.pdf}
\caption{Response curves for expected response with two predictors, with $A$ taking three levels and $B$ taking two levels. They are not parallel, so the predictors interact.}
\label{fig:10.4.6}
\end{figure}

To test the null hypothesis of no interaction, we must first fit the model where $\mu_{ij} = \alpha_i + \beta_j$, i.e., find the least-squares estimates of the $\mu_{ij}$ under these constraints. We will not pursue the mathematics of obtaining these estimates here, but rely on software to do this for us and to compute the sum of squares relevant for testing the null hypothesis of no interaction (from the results of Section~\ref{ssec:10.3.4}, we know that this is obtained by differencing the regression sum of squares obtained from the full model and the regression sums of squares obtained from the model with no interaction).

If we decide that an interaction exists, then it is immediate that both $A$ and $B$ have an effect on $Y$ (if $A$ does not have an effect, then $A$ and $B$ cannot interact --- see Problem~\ref{exer:10.4.16}); we must look at differences among the $\bar{y}_{ij \cdot}$ to determine the form of the relationship. If we decide that no interaction exists, then $A$ has an effect if and only if the $\alpha_i$ vary, and $B$ has an effect if and only if the $\beta_j$ vary. We can test the null hypothesis $H_0 : \alpha_1 = \cdots = \alpha_a$ of no effect due to $A$ and the null hypothesis $H_0 : \beta_1 = \cdots = \beta_b$ of no effect due to $V$ separately, once we have decided that no interaction exists.

The details for deriving the relevant sums of squares for all these hypotheses are not covered here, but many statistical packages will produce an ANOVA table, as given below.
\begin{center}
\begin{tabular}{l|c|c}
Source & Df & Sum of Squares \\ \hline
$A$ & $a - 1$ & RSS($A$) \\
$B$ & $b - 1$ & RSS($B$) \\
$A \times B$ & $(a - 1)(b - 1)$ & RSS($A \times B$) \\
Error & $N - ab$ & $\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{ij \cdot})^2$ \\ \hline
Total & $N - 1$ & $\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{\cdot \cdot \cdot})^2$
\end{tabular}
\end{center}

Note that if we had included only $A$ in the model, then there would be $N - a$ degrees of freedom for the estimation of $\sigma^2$. By including $B$, we lose $N - a - (N - ab) = a(b - 1)$ degrees of freedom for the estimation of $\sigma^2$.

Using this table, we first assess the null hypothesis $H_0$ : no interaction between $A$ and $B$, using $F \sim F((a - 1)(b - 1), N - ab)$ under $H_0$, via the P-value
\begin{equation*}
\prb\left( F \geqslant \frac{\text{RSS}(A \times B)/((a - 1)(b - 1))}{s^2} \right),
\end{equation*}
where $s^2$ is given by \eqref{eq:10.4.5}. If we decide that no interaction exists, then we assess the null hypothesis $H_0$ : no effect due to $A$, using $F \sim F(a - 1, N - ab)$ under $H_0$, via the P-value
\begin{equation*}
\prb\left( F \geqslant \frac{\text{RSS}(A)/(a - 1)}{s^2} \right),
\end{equation*}
and assess $H_0$ : no effect due to $B$, using $F \sim F(b - 1, N - ab)$ under $H_0$, via the P-value
\begin{equation*}
\prb\left( F \geqslant \frac{\text{RSS}(B)/(b - 1)}{s^2} \right).
\end{equation*}

\paragraph{Model Checking}

To check the model, we look at the standardized residuals given by (see Problem~\ref{exer:10.4.18})
\begin{equation}
\label{eq:10.4.6}
\frac{y_{ijk} - \bar{y}_{ij \cdot}}{s\sqrt{1 - 1/n_{ij}}}.
\end{equation}
We will restrict our attention to various plots of the standardized residuals for model checking.

We consider an example of a two-factor analysis.

\begin{example}
\label{ex:10.4.5}
The data in the following table come from G.~E.~P.~Box and D.~R.~Cox, ``An analysis of transformations'' (\emph{Journal of the Royal Statistical Society}, 1964, Series B, p.~211) and represent survival times, in hours, of animals exposed to one of three different types of poisons and allocated four different types of treatments. We let $A$ denote the treatments and $B$ denote the type of poison, so we have $3 \times 4 = 12$ different $(A, B)$ combinations. Each combination was administered to four different animals; i.e., $n_{ij} = 4$ for every $i$ and $j$.
\begin{center}
\begin{tabular}{l|cccc}
& $A_1$ & $A_2$ & $A_3$ & $A_4$ \\ \hline
$B_1$ & 3.1, 4.5, 4.6, 4.3 & 8.2, 11.0, 8.8, 7.2 & 4.3, 4.5, 6.3, 7.5 & 4.5, 7.1, 6.6, 6.2 \\
$B_2$ & 3.6, 2.9, 4.0, 2.3 & 9.2, 6.1, 4.9, 12.4 & 4.4, 3.5, 3.1, 4.0 & 5.6, 10.2, 7.1, 3.8 \\
$B_3$ & 2.2, 2.1, 1.8, 2.3 & 3.0, 3.7, 3.8, 2.9 & 2.3, 2.5, 2.4, 2.2 & 3.0, 3.6, 3.1, 3.3
\end{tabular}
\end{center}

A normal probability plot for these data, using the standardized residuals after fitting the two-factor model, reveals a definite problem. In the above reference, a transformation of the response to the reciprocal $1/Y$ is suggested, based on a more sophisticated analysis, and this indeed leads to much more appropriate standardized residual plots. Figure~\ref{fig:10.4.7} is a normal probability plot for the standardized residuals based on the reciprocal response. This normal probability plot looks reasonable.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_4_7.pdf}
\caption{Normal probability plot of the standardized residuals in Example~\ref{ex:10.4.5} using the reciprocal of the response.}
\label{fig:10.4.7}
\end{figure}

Figure~\ref{fig:10.4.8} is a plot of the standardized residuals against the various $(A, B)$ combinations, where we have coded the combination $(i, j)$ as $b(i - 1) + j$ with $b = 3$, $i = 1, 2, 3, 4$, and $j = 1, 2, 3$. This coding assigns a unique integer to each combination $(i, j)$ and is convenient when comparing scatter plots of the response for each treatment. Again, this residual plot looks reasonable.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_4_8.pdf}
\caption{Scatter plot for the data in Example~\ref{ex:10.4.5} of the standardized residuals against each value of $(A, B)$ using the reciprocal of the response.}
\label{fig:10.4.8}
\end{figure}

Below we provide the least-squares estimates of the $\mu_{ij}$ for the transformed model.
\begin{center}
\begin{tabular}{l|cccc}
& $A_1$ & $A_2$ & $A_3$ & $A_4$ \\ \hline
$B_1$ & 0.24869 & 0.11635 & 0.18627 & 0.16897 \\
$B_2$ & 0.32685 & 0.13934 & 0.27139 & 0.17015 \\
$B_3$ & 0.48027 & 0.30290 & 0.42650 & 0.30918
\end{tabular}
\end{center}

The ANOVA table for the data, as obtained from a standard statistical package, is given below.
\begin{center}
\begin{tabular}{l|c|c|c}
Source & Df & Sum of Squares & Mean Square \\ \hline
$A$ & 3 & 0.20414 & 0.06805 \\
$B$ & 2 & 0.34877 & 0.17439 \\
$A \times B$ & 6 & 0.01571 & 0.00262 \\
Error & 36 & 0.08643 & 0.00240 \\ \hline
Total & 47 & 0.65505 &
\end{tabular}
\end{center}

From this, we determine that $s = \sqrt{0.00240} = 4.89898 \times 10^{-2}$, and so the standard errors of the least-squares estimates are all equal to $s/2 = 0.0244949$.

To test the null hypothesis of no interaction between $A$ and $B$, we have, using $F \sim F(6, 36)$ under $H_0$, the P-value
\begin{equation*}
\prb\left( F \geqslant \frac{0.00262}{0.00240} \right) = \prb(F \geqslant 1.09) = 0.387.
\end{equation*}
We have no evidence against the null hypothesis.

So we can go on to test the null hypothesis of no effect due to $A$, and we have, using $F \sim F(2, 36)$ under $H_0$, the P-value
\begin{equation*}
\prb\left( F \geqslant \frac{0.06805}{0.00240} \right) = \prb(F \geqslant 28.35) = 0.000.
\end{equation*}
We reject this null hypothesis.

Similarly, testing the null hypothesis of no effect due to $B$, we have, using $F \sim F(2, 36)$ under $H_0$, the P-value
\begin{equation*}
\prb\left( F \geqslant \frac{0.17439}{0.00240} \right) = \prb(F \geqslant 72.66) = 0.000.
\end{equation*}
We reject this null hypothesis as well.

Accordingly, we have decided that the appropriate model is the additive model given by $\expc(1/Y \mid A = i, B = j) = \mu_{ij} = \alpha_i + \beta_j$ (we are still using the transformed response $1/Y$). We can also write this as $\expc(1/Y \mid A = i, B = j) = \mu + \alpha_i + \beta_j$ for any choice of $\mu$. Therefore, there is no unique estimate of the additive effects due to $A$ or $B$. However, we still have unique least-squares estimates of the means, which are obtained (using software) by fitting the model with constraints on the $\mu_{ij}$ corresponding to no interaction existing. These are recorded in the following table.
\begin{center}
\begin{tabular}{l|cccc}
& $A_1$ & $A_2$ & $A_3$ & $A_4$ \\ \hline
$B_1$ & 0.26977 & 0.10403 & 0.21255 & 0.13393 \\
$B_2$ & 0.31663 & 0.15089 & 0.25942 & 0.18080 \\
$B_3$ & 0.46941 & 0.30367 & 0.41219 & 0.33357
\end{tabular}
\end{center}

As we have decided that there is no interaction between $A$ and $B$, we can assess single-factor effects by examining the response means for each factor separately. For example, the means for investigating the effect of $A$ are given in the following table.
\begin{center}
\begin{tabular}{cccc}
$A_1$ & $A_2$ & $A_3$ & $A_4$ \\
0.352 & 0.186 & 0.295 & 0.216
\end{tabular}
\end{center}

We can compare these means using procedures based on the $t$-distribution. For example, a 0.95-confidence interval for the difference in the means at levels $A_1$ and $A_2$ is given by
\begin{equation}
\label{eq:10.4.7}
\bar{y}_{1 \cdot} - \bar{y}_{2 \cdot} \pm s \sqrt{\frac{1}{12}} \, t_{0.975}(36) = 0.352 - 0.186 \pm \sqrt{\frac{0.00240}{12}} \times 2.0281 = (0.13732, 0.19468).
\end{equation}
This indicates that we would reject the null hypothesis of no difference between these means at the $0.05$ level.

Notice that we have used the estimate of $\sigma^2$ based on the full model in \eqref{eq:10.4.7}. Logically, it would seem to make more sense to use the estimate based on fitting the additive model because we have decided that it is appropriate. When we do so, this is referred to as \emph{pooling}, as it can be shown that the new error estimate is calculated by adding RSS($A \times B$) to the original ESS and dividing by the sum of the $A \times B$ degrees of freedom and the error degrees of freedom. Not to pool is regarded as a somewhat more conservative procedure.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Randomized Blocks}
\label{ssec:10.4.4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With two-factor models, we generally want to investigate whether or not both of these factors have a relationship with the response $Y$. Suppose, however, that we know that a factor $B$ has a relationship with $Y$, and we are interested in investigating whether or not another factor $A$ has a relationship with $Y$. Should we run a single-factor experiment using the predictor $A$ or run a two-factor experiment including the factor $B$?

The answer is as we have stated at the start of Section~\ref{ssec:10.4.2}. Including the factor $B$ will allow us, if $B$ accounts for a lot of the observed variation, to make more accurate comparisons. Notice, however, that if $B$ does not have a substantial effect on $Y$, then its inclusion will be a waste, as we sacrificed $a(b - 1)$ degrees of freedom that would otherwise go toward the estimation of $\sigma^2$.

So it is important that we do indeed know that $B$ has a substantial effect. In such a case, we refer to $B$ as a \emph{blocking variable}. It is important again that the blocking variable $B$ be crossed with $A$. Then we can test for any effect due to $A$ by first testing for an interaction between $A$ and $B$; if no such interaction is found, then we test for an effect due to $A$ alone, just as we have discussed in Section~\ref{ssec:10.4.3}.

A special case of using a blocking variable arises when we have $n_{ij} = 1$ for all $i$ and $j$. In this case, $N = ab$, so there are no degrees of freedom available for the estimation of error. In fact, we have that (see Problem~\ref{exer:10.4.19}) $s^2 = 0$. Still, such a design has practical value, provided we are willing to assume that there is no interaction between $A$ and $B$. This is called a \emph{randomized block design}.

For a randomized block design, we have that
\begin{equation}
\label{eq:10.4.8}
s^2 = \frac{\text{RSS}(A \times B)}{(a - 1)(b - 1)}
\end{equation}
is an unbiased estimate of $\sigma^2$, and so we have $(a - 1)(b - 1)$ degrees of freedom for the estimation of error. Of course, this will not be correct if $A$ and $B$ do interact, but when they do not, this can be a highly efficient design, as we have removed the effect of the variation due to $B$ and require only $ab$ observations for this. When the randomized block design is appropriate, we test for an effect due to $A$, using $F \sim F(a - 1, (a - 1)(b - 1))$ under $H_0$, via the P-value
\begin{equation*}
\prb\left( F \geqslant \frac{\text{RSS}(A)/(a - 1)}{s^2} \right).
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{One Categorical and One Quantitative Predictor}
\label{ssec:10.4.5}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It is also possible that the response is quantitative while some of the predictors are categorical and some are quantitative. We now consider the situation where we have one categorical predictor $A$ taking $a$ values, and one quantitative predictor $W$. We assume that the regression model applies. Furthermore, we restrict our attention to the situation where we suppose that, within each level of $A$, the mean response varies as
\begin{equation*}
\expc(Y \mid A = i, W = w) = \beta_{i1} + \beta_{i2} w,
\end{equation*}
so that we have a simple linear regression model within each level of $A$.

If we introduce the dummy variables
\begin{equation*}
X_{ij} = \begin{cases}
W^{j-1} & A = i \\
0 & A \neq i
\end{cases}
\end{equation*}
for $i = 1, \ldots, a$ and $j = 1, 2$, then we can write the linear regression model as
\begin{equation*}
\expc(Y \mid X_{ij} = x_{ij}) = \beta_{11} x_{11} + \beta_{12} x_{12} + \cdots + \beta_{a1} x_{a1} + \beta_{a2} x_{a2}.
\end{equation*}
Here, $\beta_{i1}$ is the intercept and $\beta_{i2}$ is the slope specifying the relationship between $Y$ and $W$ when $A = i$. The methods of Section~\ref{ssec:10.3.4} are then available for inference about this model.

We also have a notion of interaction in this context, as we say that the two predictors interact if the slopes of the lines vary across the levels of $A$. So saying that no interaction exists is the same as saying that the response curves are parallel when graphed for each level of $A$. If an interaction exists, then it is definite that both $A$ and $W$ have an effect on $Y$. Thus the null hypothesis that no interaction exists is equivalent to $H_0 : \beta_{12} = \cdots = \beta_{a2}$.

If we decide that no interaction exists, then we can test for no effect due to $W$ by testing the null hypothesis that the common slope is equal to 0, or we can test the null hypothesis that there is no effect due to $A$ by testing $H_0 : \beta_{11} = \cdots = \beta_{a1}$, i.e., that the intercept terms are the same across the levels of $A$.

We do not pursue the analysis of this model further here. Statistical software is available, however, that will calculate the relevant ANOVA table for assessing the various null hypotheses.

\paragraph{Analysis of Covariance}

Suppose we are running an experimental design and for each experimental unit we can measure, but not control, a quantitative variable $W$ that we believe has an effect on the response $Y$. If the effect of this variable is appreciable, then good statistical practice suggests we should include this variable in the model, as we will reduce the contribution of error to our experimental results and thus make more accurate comparisons. Of course, we pay a price when we do this, as we lose degrees of freedom that would otherwise be available for the estimation of error. So we must be sure that $W$ does have a significant effect in such a case. Also, we do not test for an effect of such a variable, as we presumably know it has an effect. This technique is referred to as the \emph{analysis of covariance} and is obviously similar in nature to the use of blocking variables.

\paragraph{Summary of Section~\ref{sec:10.4}}

\begin{itemize}
\item We considered the situation involving a quantitative response and categorical predictor variables.
\item By the introduction of dummy variables for the predictor variables, we can consider this situation as a particular application of the multiple regression model of Section~\ref{ssec:10.3.4}.
\item If we decide that a relationship exists, then we typically try to explain what form this relationship takes by comparing means. To prevent finding too many statistically significant differences, we lower the individual error rate to ensure a sensible family error rate.
\item When we have two predictors, we first check to see if the factors interact. If the two predictors interact, then both have an effect on the response.
\item A special case of a two-way analysis arises when one of the predictors serves as a blocking variable. It is generally important to know that the blocking variable has an effect on the response, so that we do not waste degrees of freedom by including it.
\item Sometimes we can measure variables on individual experimental units that we know have an effect on the response. In such a case, we include these variables in our model, as they will reduce the contribution of random error to the analysis and make our inferences more accurate.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:10.4.1}
The following values of a response $Y$ were obtained for three settings of a categorical predictor $A$.
\begin{center}
\begin{tabular}{l|rrrr}
$A = 1$ & 2.9976 & 0.3606 & 4.7716 & 1.5652 \\
$A = 2$ & 0.7468 & $-1.3308$ & $-2.2167$ & 0.3184 \\
$A = 3$ & 2.1192 & 2.3739 & 0.3335 & 3.3015
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data with one categorical predictor.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data.
\item Plot the standardized residuals against $A$ (if you are using a computer for your calculations, also produce a normal probability plot of the standardized residuals). Does this give you grounds for concern that the model assumptions are incorrect?
\item Carry out a one-way ANOVA to test for any difference among the conditional means of $Y$ given $A$.
\item If warranted, construct 0.95-confidence intervals for the differences between the means and summarize your findings.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-1a.pdf}
      \caption{Side-by-side boxplot of $Y$ versus $A$.}
      %\label{fig:boxplot-10-4-1a}
    \end{figure}
    \item A plot of the standardized residuals against $A$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-1b1.pdf}
      \caption{Standardized residuals versus $A$ (response is $Y$).}
      %\label{fig:residuals-10-4-1b1}
    \end{figure}
    A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-1b2.pdf}
      \caption{Normal probability plot of the standardized residuals (response is $Y$).}
      %\label{fig:normal-prob-10-4-1b2}
    \end{figure}
    Both plots look reasonable, indicating no serious concerns about the correctness of the model assumptions.
    \item The ANOVA table for testing $H_0: \beta_1 = \beta_2 = \beta_3$ is given below.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $A$ & 2 & 4.37 & 2.18 \\
    Error & 9 & 18.85 & 2.09 \\
    Total & 11 & 23.22 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$ is given by $F = 2.18/2.09 = 1.0431$, and since $F \sim F(2, 9)$ under $H_0$, we have P-value $\prb(F > 1.0431) = .39135$. Therefore, we do not have evidence against the null hypothesis of no difference among the conditional means of $Y$ given $X$.
    \item Since we did not find any relationship between $Y$ and $X$, there is no need to calculate these confidence intervals.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.2}
The following values of a response $Y$ were obtained for three settings of a categorical predictor $A$.
\begin{center}
\begin{tabular}{l|rrrr}
$A = 1$ & 0.090 & 0.800 & 33.070 & 1.890 \\
$A = 2$ & 5.120 & 1.580 & 1.760 & 1.740 \\
$A = 3$ & 5.080 & 3.510 & 4.420 & 1.190
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data with one categorical predictor.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data.
\item Plot the standardized residuals against $A$ (if you are using a computer for your calculations, also produce a normal probability plot of the standardized residuals). Does this give you grounds for concern that the model assumptions are incorrect?
\item If concerns arise about the validity of the model, can you ``fix'' the problem?
\item If you have been able to fix any problems encountered with the model, carry out a one-way ANOVA to test for any differences among the conditional means of $Y$ given $A$.
\item If warranted, construct 0.95-confidence intervals for the differences between the means and summarize your findings.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-2a.pdf}
      \caption{Side-by-side boxplot of $Y$ versus $A$.}
      %\label{fig:boxplot-10-4-2a}
    \end{figure}
    \item A plot of the standardized residuals against $A$ follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-2b1.pdf}
      \caption{Standardized residuals versus $A$ (response is $Y$).}
      %\label{fig:residuals-10-4-2b1}
    \end{figure}
    A normal probability plot of the standardized residuals is given below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-2b2.pdf}
      \caption{Normal probability plot of the standardized residuals (response is $Y$).}
      %\label{fig:normal-prob-10-4-2b2}
    \end{figure}
    Both plots indicate a problem with the model assumptions.
    \item A possible way to ``fix'' this problem is to remove the extreme observation in the first category, namely 33.07. After removing this value, we get the following plot of the standardized residuals against $A$ and normal probability plot of the standardized residuals. These look much more reasonable.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-2c1.pdf}
      \caption{Standardized residuals versus $A$ after removing outlier (response is $Y$).}
      %\label{fig:residuals-10-4-2c1}
    \end{figure}
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-2c2.pdf}
      \caption{Normal probability plot of the standardized residuals after removing outlier (response is $Y$).}
      %\label{fig:normal-prob-10-4-2c2}
    \end{figure}
    \item The ANOVA table for testing $H_0: \beta_1 = \beta_2 = \beta_3$, after removing the outlier, is given below.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $A$ & 2 & 14.840 & 7.420 \\
    Error & 8 & 58.904 & 7.363 \\
    Total & 11 & 73.744 &
    \end{tabular}
    \end{center}
    The $F$ statistics for testing $H_0$ is given by $F = 7.41/7.363 = 1.01$, and since $F \sim F(2, 8)$ under $H_0$, the P-value equals $\prb(F > 1.01) = .407$. Therefore, we have no evidence against the null hypothesis of no difference among the conditional means of $Y$ given $X$.
    \item There is no need to compute these confidence intervals as we found no evidence of a relationship between the response and the predictor.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.3}
The following table gives the percentage moisture content of two different types of cheeses determined by randomly sampling batches of cheese from the production process.
\begin{center}
\begin{tabular}{l|cccccc}
Cheese 1 & 39.02 & 38.79 & 35.74 & 35.41 & 37.02 & 36.00 \\
Cheese 2 & 38.96 & 39.01 & 35.58 & 35.52 & 35.70 & 36.04
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data with one categorical predictor.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data.
\item Plot the standardized residuals against Cheese (if you are using a computer for your calculations, also produce a normal probability plot of the standardized residuals). Does this give you grounds for concern that the model assumptions are incorrect?
\item Carry out a one-way ANOVA to test for any differences among the conditional means of $Y$ given Cheese. Note that this is the same as a $t$-test for the difference in the means.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-3a.pdf}
      \caption{Side-by-side boxplot of \% moisture versus Cheese.}
      %\label{fig:boxplot-10-4-3a}
    \end{figure}
    \item A plot of the standardized residuals against cheese follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-3b1.pdf}
      \caption{Standardized residuals versus Cheese (response is \% moisture).}
      %\label{fig:residuals-10-4-3b1}
    \end{figure}
    A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-3b2.pdf}
      \caption{Normal probability plot of the standardized residuals (response is \% moisture).}
      %\label{fig:normal-prob-10-4-3b2}
    \end{figure}
    Both plots indicate a possible problem with the model assumptions.
    \item The ANOVA table for testing $H_0: \beta_1 = \beta_2$ is given below.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Cheese & 1 & 0.114 & 0.114 \\
    Error & 10 & 26.865 & 2.686 \\
    Total & 11 & 26.979 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$ is given by $F = .114/2.686 = .04$ and, since $F \sim F(1, 10)$ under $H_0$, the P-value equals $\prb(F > .04) = .841$. Therefore, we do not have any evidence against the null hypothesis of no difference among the conditional means of $Y$ given Cheese.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.4}
In an experiment, rats were fed a stock ration for 100 days with various amounts of gossypol added. The following weight gains in grams were recorded.
\begin{center}
\begin{tabular}{l|l}
0.00\% Gossypol & 228, 229, 218, 216, 224, 208, 235, 229, \\
& 233, 219, 224, 220, 232, 200, 208, 232 \\
0.04\% Gossypol & 186, 229, 220, 208, 228, 198, 222, 273, \\
& 216, 198, 213 \\
0.07\% Gossypol & 179, 193, 183, 180, 143, 204, 114, 188, \\
& 178, 134, 208, 196 \\
0.10\% Gossypol & 130, 87, 135, 116, 118, 165, 151, 59, \\
& 126, 64, 78, 94, 150, 160, 122, 110, 178 \\
0.13\% Gossypol & 154, 130, 118, 118, 118, 104, 112, 134, \\
& 98, 100, 104
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data and treat gossypol as a categorical predictor taking five levels.
\begin{enumerate}[(a)]
\item Create a side-by-side boxplot graph for the data. Does this give you any reason to be concerned about the assumptions that underlie an analysis based on the normal regression model?
\item Produce a plot of the standardized residuals against the factor gossypol (if you are using a computer for your calculations, also produce a normal probability plot of the standardized residuals). What are your conclusions?
\item Carry out a one-way ANOVA to test for any differences among the mean responses for the different amounts of gossypol.
\item Compute 0.95-confidence intervals for all the pairwise differences of means and summarize your conclusions.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-4a.pdf}
      \caption{Side-by-side boxplot of Weight gained versus Gossypol \%.}
      %\label{fig:boxplot-10-4-4a}
    \end{figure}
    Some of the boxplots don't look very symmetrical, which should be the case for normal samples. So these graphs are some evidence that the normality assumption may not be appropriate.
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-4b1.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Weight gained).}
      %\label{fig:normal-prob-10-4-4b1}
    \end{figure}
    A plot of the standardized residuals against the factor gossypol follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-4b2.pdf}
      \caption{Standardized residuals versus Gossypol (response is Weight gained).}
      %\label{fig:residuals-10-4-4b2}
    \end{figure}
    Again, these plots provide some evidence that the normality assumption may not be appropriate.
    \item The ANOVA table for testing $H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5$ follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Gossypol & 4 & 141334 & 35333 \\
    Error & 62 & 38754 & 625 \\
    Total & 66 & 180087 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$ is given by $F = 35333/625 = 56.533$, and since $F \sim F(4, 62)$ under $H_0$, the P-value equals $\prb(F > 56.533) = 0.000$. Therefore, we have strong evidence against the null hypothesis of no difference amongst the mean level of the response given different amounts of gossypol.
    \item The $0.95$-confidence intervals for the difference between the means are given in the following table.
    \begin{verbatim}
Family error rate = 0.279
Individual error rate = 0.0500
Critical value = 1.999
Intervals for (column level mean) - (row level mean)

        0.00       0.04       0.07       0.10
 0.04  -14.75
        24.40
 0.07   28.10      21.50
        66.27      63.23
 0.10   84.60      77.85      35.98
       119.42     116.53      73.67
 0.13   85.34      78.78      36.87     -16.44
       124.49     121.40      78.59      22.24
    \end{verbatim}
    Only the mean at the 0.00\% level does not differ from the mean at the 0.04\% level and the mean at the 0.10\% level does not differ from the mean at the 0.13\% level at the 5\% significant level, as both intervals include the value 0.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.5}
In an investigation into the effect of deficiencies of trace elements on a variable $Y$ measured on sheep, the data in the following table were obtained.
\begin{center}
\begin{tabular}{l|cccccc}
Control & 13.2 & 13.6 & 11.9 & 13.0 & 14.5 & 13.4 \\
Cobalt & 11.9 & 12.2 & 13.9 & 12.8 & 12.7 & 12.9 \\
Copper & 14.2 & 14.0 & 15.1 & 14.9 & 13.7 & 15.8 \\
Cobalt + Copper & 15.0 & 15.6 & 14.5 & 15.8 & 13.9 & 14.4
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data with one categorical predictor.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data.
\item Plot the standardized residuals against the predictor (if you are using a computer for your calculations, also produce a normal probability plot of the standardized residuals). Does this give you grounds for concern that the model assumptions are incorrect?
\item Carry out a one-way ANOVA to test for any differences among the conditional means of $Y$ given the predictor.
\item If warranted, construct 0.95-confidence intervals for all the pairwise differences between the means and summarize your findings.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-5a.pdf}
      \caption{Side-by-side boxplot of $Y$ versus Group.}
      %\label{fig:boxplot-10-4-5a}
    \end{figure}
    \item A plot of the standardized residuals against the predictor follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-5b1.pdf}
      \caption{Standardized residuals versus Treatment (response is $Y$).}
      %\label{fig:residuals-10-4-5b1}
    \end{figure}
    A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-5b2.pdf}
      \caption{Normal probability plot of the standardized residuals (response is $Y$).}
      %\label{fig:normal-prob-10-4-5b2}
    \end{figure}
    Both plots look reasonable, indicating no concerns about the correctness of the model assumptions.
    \item The ANOVA table for testing $H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4$ follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Treatment & 3 & 19.241 & 6.414 \\
    Error & 20 & 11.788 & 0.589 \\
    Total & 23 & 31.030 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$ is given by $F = 6.414/0.589 = 10.89$ and, since $F \sim F(3, 20)$ under $H_0$, the P-value equals $\prb(F > 10.89) = .00019$. Therefore, we have strong evidence against the null hypothesis of no difference among the conditional means of $Y$ given the predictor.
    \item The $0.95$-confidence intervals for the difference between the means are given in the following table.
    \begin{verbatim}
Family error rate = 0.192
Individual error rate = 0.0500
Critical value = 2.086
Intervals for (column level mean) - (row level mean)

        1          2          3
 2  -0.3913
     1.4580
 3  -2.2746    -2.8080
    -0.4254    -0.9587
 4  -2.5246    -3.0580    -1.1746
    -0.6754    -1.2087     0.6746
    \end{verbatim}
    The mean response for the control treatment does not differ from the mean response given the Cobalt treatment and the mean response for the Copper treatment does not differ from the mean response for the Cobalt+Copper treatment at the 5\% level, since both intervals include the value 0. All other mean differences are judged to be nonzero at the 5\% level.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.6}
Two diets were given to samples of pigs over a period of time, and the following weight gains (in lbs) were recorded.
\begin{center}
\begin{tabular}{l|l}
Diet A & 8, 4, 14, 15, 11, 10, 6, 12, 13, 7 \\
Diet B & 7, 13, 22, 15, 12, 14, 18, 8, 21, 23, 10, 17
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data.
\item Plot the standardized residuals against Diet. Also produce a normal probability plot of the standardized residuals. Does this give you grounds for concern that the model assumptions are incorrect?
\item Carry out a one-way ANOVA to test for a difference between the conditional means of $Y$ given Diet.
\item Construct 0.95-confidence intervals for differences between the means.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-6a.pdf}
      \caption{Side-by-side boxplot of Weight gains versus Diet.}
      %\label{fig:boxplot-10-4-6a}
    \end{figure}
    \item A plot of the standardized residuals against Diet follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-6b1.pdf}
      \caption{Standardized residuals versus Diet (response is Weight gains).}
      %\label{fig:residuals-10-4-6b1}
    \end{figure}
    A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-6b2.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Weight gains).}
      %\label{fig:normal-prob-10-4-6b2}
    \end{figure}
    Both plots look reasonable, indicating no concerns about the correctness of the model assumptions.
    \item The ANOVA table for testing $H_0: \beta_1 = \beta_2$ follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Treatment & 1 & 136.4 & 136.4 \\
    Error & 20 & 434.0 & 21.7 \\
    Total & 21 & 570.4 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$ is given by $F = 136.4/21.7 = 6.2857$ and, since $F \sim F(1, 20)$ under $H_0$, the P-value equals $\prb(F > 6.2857) = .02091$. Therefore, we have evidence against the null hypothesis of no difference among the conditional means of $Y$ given Diet at the 5\% level but not at 1\%.
    \item A $.95$-confidence interval for the difference between the means follows.
    \[
        \beta_1 - \beta_2 \in (10.0 - 15.0) \pm 4.658 \left(\frac{1}{10} + \frac{1}{12}\right)^{1/2} 2.086 = (-9.1604, -.83961).
    \]
    Note that this does not include the value 0 and therefore supports our conclusion from part (c).
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.7}
Ten students were randomly selected from the students in a university who took first-year calculus and first-year statistics. Their grades in these courses are recorded in the following table.
\begin{center}
\begin{tabular}{l|cccccccccc}
Student & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
Calculus & 66 & 61 & 77 & 62 & 66 & 68 & 64 & 75 & 59 & 71 \\
Statistics & 66 & 63 & 79 & 63 & 67 & 70 & 71 & 80 & 63 & 74
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data.
\item Treating the calculus and statistics marks as separate samples, carry out a one-way ANOVA to test for any difference between the mean mark in calculus and the mean mark in statistics. Produce the appropriate plots to check for model assumptions.
\item Now take into account that each student has a calculus mark and a statistics mark and test for any difference between the mean mark in calculus and the mean mark in statistics. Produce the appropriate plots to check for model assumptions. Compare your results with those obtained in part (b).
\item Estimate the correlation between the calculus and statistics marks.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-7a.pdf}
      \caption{Side-by-side boxplot of Marks versus Course.}
      %\label{fig:boxplot-10-4-7a}
    \end{figure}
    \item Treating the marks as separate samples, the ANOVA table for testing any difference between the mean mark in Calculus and the mean mark in Statistics follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Course & 1 & 36.45 & 36.45 \\
    Error & 18 & 685.30 & 38.07 \\
    Total & 19 & 721.75 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0: \beta_1 = \beta_2$ is given by $F = 36.45/38.07 = .95745$ and, since $F \sim F(1, 19)$ under $H_0$, the P-value equals $\prb(F > .95745) = .3408$. Therefore, we do not have any evidence against the null hypothesis of no difference among the conditional means of $Y$ given Course.
    
    A plot of the standardized residuals against Course follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-7b1.pdf}
      \caption{Standardized residuals versus Course (response is Marks).}
      %\label{fig:residuals-10-4-7b1}
    \end{figure}
    A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-7b2.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Marks).}
      %\label{fig:normal-prob-10-4-7b2}
    \end{figure}
    Both plots look reasonable, indicating no concerns about the correctness of the model assumptions.
    \item Treating this data as repeated measures, the mean difference between the mark in Calculus and the mark in Statistics is given by $\bar{d} = -2.7$ with standard deviation $s = 2.00250$. The P-value for testing $H_0: \mu_1 = \mu_2$, since $T \sim t(9)$ under $H_0$, the P-value is given by $\prb(|T| > |-2.7/(2.00250/\sqrt{10})|) = 2\prb(T > 4.2637) = .0021$, so we have strong evidence against the null hypothesis. Hence we conclude that there is a difference between the mean mark in Calculus and the mean mark in Statistics. A normal probability plot of the data follows and this does not indicate any reason to doubt model assumptions.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-7c.pdf}
      \caption{Normal probability plot of mark differences.}
      %\label{fig:normal-prob-10-4-7c}
    \end{figure}
    \item The estimate of the correlation between the Calculus and Statistics marks is given by the sample correlation coefficient $r_{xy} = 0.944155$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.8}
The following data were recorded in \emph{Statistical Methods}, 6th ed., by G.~Snedecor and W.~Cochran (Iowa State University Press, Ames, 1967) and represent the average number of florets observed on plants in seven plots. Each of the plants was planted with either high corms or low corms (a type of underground stem).
\begin{center}
\begin{tabular}{l|ccccccc}
& Plot 1 & Plot 2 & Plot 3 & Plot 4 & Plot 5 & Plot 6 & Plot 7 \\ \hline
Corm High & 11.2 & 13.3 & 12.8 & 13.7 & 12.2 & 11.9 & 12.1 \\
Corm Low & 14.6 & 12.6 & 15.0 & 15.6 & 12.7 & 12.0 & 13.1
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data.
\item Treating the Corm High and Corm Low measurements as separate samples, carry out a one-way ANOVA to test for any difference between the population means. Produce the appropriate plots to check for model assumptions.
\item Now take into account that each plot has a Corm High and Corm Low measurement. Compare your results with those obtained in part (b). Produce the appropriate plots to check for model assumptions.
\item Estimate the correlation between the calculus and statistics marks.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-8a.pdf}
      \caption{Side-by-side boxplot of Number of florets versus Treatment.}
      %\label{fig:boxplot-10-4-8a}
    \end{figure}
    \item Treating the Corm High and Corm Low measurements as separate samples, the ANOVA table for testing any difference between the population means follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Treatment & 1 & 5.040 & 5.040 \\
    Error & 12 & 16.014 & 1.335 \\
    Total & 13 & 21.054 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0: \beta_1 = \beta_2$ is given by $F = 5.040/1.335 = 3.7753$ and, since $F \sim F(1, 19)$ under $H_0$, the P-value equals $\prb(F > 3.7753) = .07585$. Therefore, we do not have substantial evidence against the null hypothesis of no difference amongst the conditional means of $Y$ given Corm level.
    
    A plot of the standardized residuals against Treatment follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-8b1.pdf}
      \caption{Standardized residuals versus Treatment (response is Number of florets).}
      %\label{fig:residuals-10-4-8b1}
    \end{figure}
    A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-8b2.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Number of florets).}
      %\label{fig:normal-prob-10-4-8b2}
    \end{figure}
    Both plots look reasonable, indicating no concerns about the correctness of the model assumptions.
    \item Treating this data as repeated measures, the mean difference between the number of florets in plots with Corm High and the number of florets in plots with Corm Low is given by $\bar{d} = -1.2$ with standard deviation $s = 1.395$. The P-value for testing $H_0: \mu_1 = \mu_2$, since $T \sim t(6)$ under $H_0$, equals $\prb(|T| > |-1.2/(1.395/\sqrt{7})|) = 2\prb(T > 2.2759) = .06316$, so we do not have substantial evidence against the null. Hence, we conclude that there is no difference between the mean number of florets in plots with Corm High and the mean number of florets in plots with Corm Low.
    
    A normal probability plot of the data follows and this reveals no evidence of model incorrectness.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-8c.pdf}
      \caption{Normal probability plot of differences.}
      %\label{fig:normal-prob-10-4-8c}
    \end{figure}
    \item The estimate of the correlation between the Calculus and Statistics marks is given by the sample correlation coefficient $r_{xy} = 0.301949$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.9}
Suppose two measurements, $Y_1$ and $Y_2$, corresponding to different treatments, are taken on the same individual who has been randomly sampled from a population $\Pi$. Suppose that $Y_1$ and $Y_2$ have the same variance and are negatively correlated. Our goal is to compare the treatment means. Explain why it would have been better to have randomly sampled two individuals from $\Pi$ and applied the treatments to these individuals separately. (Hint: Consider $\var(Y_1 - Y_2)$ in these two sampling situations.)
\end{exercise}

\begin{solution}
When $Y_1$ and $Y_2$ are measured on the same individual we have that $\var(Y_1 - Y_2) = \var(Y_1) + \var(Y_2) - 2\cov(Y_1, Y_2) = 2(\var(Y_1) - \cov(Y_1, Y_2)) > 2\var(Y_1)$ since $\cov(Y_1, Y_2) < 0$. If we had measured $Y_1$ and $Y_2$ on independently randomly selected individuals, then we have that $\var(Y_1 - Y_2) = 2\var(Y_1)$ since $\cov(Y_1, Y_2) = 0$ in that case. So we get less variation under independence sampling.
\end{solution}

\begin{exercise}
\label{exer:10.4.10}
List the assumptions that underlie the validity of the one-way ANOVA test discussed in Section~\ref{ssec:10.4.1}.
\end{exercise}

\begin{solution}
The following assumptions are required: (1) we have a regression model relating the response $Y$ to the predictor $X$, i.e., the conditional distribution of $Y$ given $X$, depends on $X$ only through $\expc(Y \mid X)$ and the error $Z = Y - \expc(Y \mid X)$ is independent of $X$, (2) the error $Z = Y - \expc(Y \mid X)$ is normally distributed.
\end{solution}

\begin{exercise}
\label{exer:10.4.11}
List the assumptions that underlie the validity of the paired comparison test discussed in Section~\ref{ssec:10.4.2}.
\end{exercise}

\begin{solution}
The following assumption is required: the difference of the two responses $Y_1$ and $Y_2$ is normally distributed, i.e., $Y_1 - Y_2 \sim N(\mu, \sigma^2)$.
\end{solution}

\begin{exercise}
\label{exer:10.4.12}
List the assumptions that underlie the validity of the two-way ANOVA test discussed in Section~\ref{ssec:10.4.3}.
\end{exercise}

\begin{solution}
The following assumptions are required: (1) we have a regression model relating the response $Y$ to the predictors $X_1$ and $X_2$, i.e., the conditional distribution of $Y$ given $(X_1, X_2)$, depends on $(X_1, X_2)$ only through $\expc(Y \mid X_1, X_2)$ and the error $Z = Y - \expc(Y \mid X_1, X_2)$ is independent of $(X_1, X_2)$, (2) the error $Z = Y - \expc(Y \mid X_1, X_2)$ is normally distributed.
\end{solution}

\begin{exercise}
\label{exer:10.4.13}
List the assumptions that underlie the validity of the test used with the randomized block design, discussed in Section~\ref{ssec:10.4.4}, when $n_{ij} = 1$ for all $i$ and $j$.
\end{exercise}

\begin{solution}
The following assumptions are required: (1) we have a regression model relating the response $Y$ to the predictors $X_1$ and $X_2$, i.e., the conditional distribution of $Y$ given $(X_1, X_2)$, depends on $(X_1, X_2)$ only through $\expc(Y \mid X_1, X_2)$ and the error $Z = Y - \expc(Y \mid X_1, X_2)$ is independent of $(X_1, X_2)$, (2) the error $Z = Y - \expc(Y \mid X_1, X_2)$ is normally distributed, and (3) $X_1$ and $X_2$ do not interact.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:10.4.14}
Prove that $\sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \mu_i)^2$ is minimized as a function of the $\mu_i$ by $\mu_i = \bar{y}_{i \cdot} = (y_{i1} + \cdots + y_{in_i})/n_i$ for $i = 1, \ldots, a$.
\end{exercise}

\begin{solution}
To prove this we express the sum of squares as follows
\[
    \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \beta_i)^2 = \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i + \bar{y}_i - \beta_i)^2 = \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 + 2\sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)(\bar{y}_i - \beta_i) + \sum_{i=1}^{a} n_i (\bar{y}_i - \beta_i)^2.
\]
First, note that the second term is equal to 0 since
\[
    \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)(\bar{y}_i - \beta_i) = (\bar{y}_i - \beta_i) \left(\sum_{j=1}^{n_i} y_{ij} - n_i \bar{y}_i\right) = (\bar{y}_i - \beta_i) \left(\sum_{j=1}^{n_i} y_{ij} - n_i \frac{\sum_{j=1}^{n_i} y_{ij}}{n_i}\right) = 0.
\]
So the above sum of squares is minimized as a function of $\beta_i$ if and only if the second term is equal to 0, and if and only if $\bar{y}_i - \beta_i = 0$, if and only if $\beta_i = \bar{y}_i$.
\end{solution}

\begin{exercise}
\label{exer:10.4.15}
Prove that
\begin{equation*}
\sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{\cdot \cdot})^2 = \sum_{i=1}^{a} n_i (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2 + \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{i \cdot})^2,
\end{equation*}
where $\bar{y}_{i \cdot} = (y_{i1} + \cdots + y_{in_i})/n_i$ and $\bar{y}_{\cdot \cdot}$ is the grand mean.
\end{exercise}

\begin{solution}
To prove this we express the sum of squares as follows.
\[
    \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y})^2 = \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i + \bar{y}_i - \bar{y})^2 = \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 + 2\sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)(\bar{y}_i - \bar{y}) + \sum_{i=1}^{a} n_i (\bar{y}_i - \bar{y})^2.
\]
Note that the second term is equal to 0 since
\[
    \sum_{i=1}^{a} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)(\bar{y}_i - \bar{y}) = \sum_{i=1}^{a} \left((\bar{y}_i - \bar{y}) \left(\sum_{j=1}^{n_i} y_{ij} - n_i \bar{y}_i\right)\right) = \sum_{i=1}^{a} \left((\bar{y}_i - \bar{y}) \left(\sum_{j=1}^{n_i} y_{ij} - n_i \frac{\sum_{j=1}^{n_i} y_{ij}}{n_i}\right)\right) = 0.
\]
\end{solution}

\begin{exercise}
\label{exer:10.4.16}
Argue that if the relationship between a quantitative response $Y$ and two categorical predictors $A$ and $B$ is given by a linear regression model, then $A$ and $B$ both have an effect on $Y$ whenever $A$ and $B$ interact. (Hint: What does it mean in terms of response curves for an interaction to exist, for an effect due to $A$ to exist?)
\end{exercise}

\begin{solution}
If an interaction exists between the two factors, then the $b$ response curves are not parallel and therefore cannot be horizontal, i.e., there must be effect due to both factors.
\end{solution}

\begin{exercise}
\label{exer:10.4.17}
Establish that \eqref{eq:10.4.2} is the appropriate expression for the standardized residual for the linear regression model with one categorical predictor.
\end{exercise}

\begin{solution}
By assumption we have $Y_{ij} \sim N(\beta_i, \sigma^2)$ and these are all independent. Therefore, we have that $\bar{Y}_i \sim N(\beta_i, \sigma^2/n_i)$. Further, $\cov(Y_{ij}, \bar{Y}_i) = \cov\left(Y_{ij}, \frac{\sum_{k=1}^{n_i} Y_{ik}}{n_i}\right) = \sigma^2/n_i$. Therefore, by Theorem \ref{thm:4.6.1} we have that $Y_{ij} - \bar{Y}_i \sim N(0, \sigma^2(1 - 1/n_i))$ as claimed.
\end{solution}

\begin{exercise}
\label{exer:10.4.18}
Establish that \eqref{eq:10.4.6} is the appropriate expression for the standardized residual for the linear regression model with two categorical predictors.
\end{exercise}

\begin{solution}
By assumption we have $Y_{ijk} \sim N(\beta_{ij}, \sigma^2)$ and these are all independent. Then we have that $\bar{Y}_{ij} \sim N(\beta_{ij}, \sigma^2/n_{ij})$. Further, $\cov(Y_{ijk}, \bar{Y}_{ij}) = \cov\left(Y_{ijk}, \frac{\sum_{l=1}^{n_{ij}} Y_{ijl}}{n_{ij}}\right) = \sigma^2/n_{ij}$. Therefore, by Theorem \ref{thm:4.6.1} we have $Y_{ijk} - \bar{Y}_{ij} \sim N(0, \sigma^2(1 - 1/n_{ij}))$.
\end{solution}

\begin{exercise}
\label{exer:10.4.19}
Establish that $s^2 = 0$ for the linear regression model with two categorical predictors when $n_{ij} = 1$ for all $i$ and $j$.
\end{exercise}

\begin{solution}
First, recall that $s^2 = \frac{1}{N - ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{ij})^2$. Now if $n_{ij} = 1$ then $\bar{y}_{ij} = y_{ijk}$ for all $i$ and $j$. Hence, $y_{ijk} - \bar{y}_{ij} = 0$, which establishes that $s^2 = 0$ as claimed.
\end{solution}

\begin{exercise}
\label{exer:10.4.20}
How would you assess whether or not the randomized block design was appropriate after collecting the data?
\end{exercise}

\begin{solution}
By looking at various plots of the residuals, for example, a normal probability plot of the standardized residuals.
\end{solution}

\subsection*{Computer Problems}

\begin{exercise}
\label{exer:10.4.21}
Use appropriate software to carry out Fisher's multiple comparison test on the data in Exercise~\ref{exer:10.4.5} so that the family error rate is between 0.04 and 0.05. What individual error rate is required?
\end{exercise}

\begin{solution}
Controlling a family error rate of 0.0455, the $0.95$-confidence intervals for the difference between the means are given in the following table. It required a 0.01 individual error rate.
\begin{verbatim}
Family error rate = 0.0455
Individual error rate = 0.0100
Critical value = 2.845
Intervals for (column level mean) - (row level mean)

          Coblat   Coblat+C    Control
Coblat+C -3.3944
         -0.8723
Control  -1.7944     0.3389
          0.7277     2.8611
Copper   -3.1444    -1.0111    -2.6111
         -0.6223     1.5111    -0.0889
\end{verbatim}
\end{solution}

\begin{exercise}
\label{exer:10.4.22}
Consider the data in Exercise~\ref{exer:10.4.3}, but now suppose we also take into account that the cheeses were made in lots where each lot corresponded to a production run. Recording the data this way, we obtain the following table.
\begin{center}
\begin{tabular}{l|cc|cc|cc}
& \multicolumn{2}{c|}{Lot 1} & \multicolumn{2}{c|}{Lot 2} & \multicolumn{2}{c}{Lot 3} \\ \hline
Cheese 1 & 39.02 & 38.79 & 35.74 & 35.41 & 37.02 & 36.00 \\
Cheese 2 & 38.96 & 39.01 & 35.58 & 35.52 & 35.70 & 36.04
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data with two categorical predictors.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data for each treatment.
\item Produce a table of cell means.
\item Produce a normal probability plot of the standardized residuals and a plot of the standardized residuals against each treatment combination (code the treatment combinations so there is a unique integer corresponding to each). Comment on the validity of the model.
\item Construct the ANOVA table testing first for no interaction between $A$ and $B$ and, if necessary, an effect due to $A$ and an effect due to $B$.
\item Based on the results of part (d), construct the appropriate table of means, plot the corresponding response curve, and make all pairwise comparisons among the means.
\item Compare your results with those obtained in Exercise~\ref{exer:10.4.4} and comment on the differences.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data by treatment (using the coding $3(i-1)+j$) follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-22a.pdf}
      \caption{Side-by-side boxplot of \% moisture versus treatment.}
      %\label{fig:boxplot-10-4-22a}
    \end{figure}
    \item A table of the cell means is given follows.
    \begin{center}
    \begin{tabular}{c|ccc}
     & Lot 1 & Lot 2 & Lot 3 \\
    \hline
    Cheese 1 & 38.905 & 35.575 & 36.510 \\
    Cheese 2 & 38.985 & 35.550 & 35.870
    \end{tabular}
    \end{center}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-22c1.pdf}
      \caption{Normal probability plot of the standardized residuals (response is \% moisture).}
      %\label{fig:normal-prob-10-4-22c1}
    \end{figure}
    A plot of the standardized residuals against each of the treatment combinations (using the coding $3(i-1)+j$) follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-22c2.pdf}
      \caption{Standardized residuals versus treatment (response is \% moisture).}
      %\label{fig:residuals-10-4-22c2}
    \end{figure}
    Both plots looks reasonable, so indicating no serious concerns about the correctness of the model assumptions.
    \item The ANOVA table for testing all relevant hypotheses follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Cheese & 1 & 0.114 & 0.114 \\
    Lot & 2 & 25.900 & 12.950 \\
    Interaction & 2 & 0.303 & 0.151 \\
    Error & 6 & 0.662 & 0.110 \\
    Total & 11 & 26.979 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$: no interaction between cheese and lot, is given by $F = 0.151/0.110 = 1.3727$ and, since $F \sim F(2, 6)$ under $H_0$, the P-value equals $\prb(F > 1.3727) = .32293$. Therefore, we do not have evidence against the null hypothesis of no interaction effect.
    
    We can then proceed to calculate the P-value for testing $H_0$: no effect due to cheese. This is given by $\prb(F > 0.114/0.110 = 1.0364) = .34794$, since $F \sim F(1, 6)$ under $H_0$. Therefore, we do not have any evidence against the null hypothesis of no effect due to Cheese.
    
    The P-value for testing $H_0$: no effect due to lot, since $F \sim F(2, 6)$ under $H_0$, is given by $\prb(F > 12.950/0.110 = 117.73) = .00002$. Therefore, we have strong evidence against the null hypothesis of no effect due to Lot.
    \item Since we conclude that only the factor Lot has a significant effect on the response, we calculate the following table of means.
    \begin{center}
    \begin{tabular}{c|ccc}
     & Lot 1 & Lot 2 & Lot 3 \\
    \hline
    Mean & 38.945 & 35.563 & 36.190
    \end{tabular}
    \end{center}
    The corresponding response curve follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-22e.pdf}
      \caption{Main Effects Plot -- Data Means for \% moisture.}
      %\label{fig:main-effects-10-4-22e}
    \end{figure}
    The $.95$-confidence intervals for the difference between the means are given in the following table.
    \begin{verbatim}
Family error rate = 0.113
Individual error rate = 0.0500
Critical value = 2.262
Intervals for (column level mean) - (row level mean)

        1          2
 2   2.8288
     3.9362
 3   2.2013    -1.1812
     3.3087    -0.0738
    \end{verbatim}
    As we can see, all the confidence intervals above do not include the value 0, indicating differences between all the means at the 5\% level.
    \item Our result here agrees with the result of Exercise~10.4.3 as in both cases no significant effect due to cheese was found although we do find an effect due to Lot.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.23}
A two-factor experimental design was carried out, with factors $A$ and $B$ both categorical variables taking three values. Each treatment was applied four times and the following response values were obtained.
\begin{center}
\begin{tabular}{l|cc|cc|cc}
& \multicolumn{2}{c|}{$A = 1$} & \multicolumn{2}{c|}{$A = 2$} & \multicolumn{2}{c}{$A = 3$} \\ \hline
$B = 1$ & 19.86, 20.88 & 20.15, 25.44 & 26.37, 24.38 & 24.87, 30.93 & 29.72, 29.64 & 30.06, 35.49 \\
$B = 2$ & 15.35, 15.86 & 21.86, 26.92 & 22.82, 20.98 & 29.38, 34.13 & 27.12, 24.27 & 34.78, 40.72 \\
$B = 3$ & 4.01, 4.48 & 21.66, 25.93 & 10.34, 9.38 & 30.59, 40.04 & 15.64, 14.03 & 36.80, 42.55
\end{tabular}
\end{center}
Suppose we assume the normal regression model for these data with two categorical predictors.
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data for each treatment.
\item Produce a table of cell means.
\item Produce a normal probability plot of the standardized residuals and a plot of the standardized residuals against each treatment combination (code the treatment combinations so there is a unique integer corresponding to each). Comment on the validity of the model.
\item Construct the ANOVA table testing first for no interaction between $A$ and $B$ and, if necessary, an effect due to $A$ and an effect due to $B$.
\item Based on the results of part (d), construct the appropriate table of means, plot the corresponding response curves, and make all pairwise comparisons among the means.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data by treatment (using the coding $3(i-1)+j$ with $A = i$, $B = j$) follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-23a.pdf}
      \caption{Side-by-side boxplot of Response versus treatment.}
      %\label{fig:boxplot-10-4-23a}
    \end{figure}
    \item A table of the cell means is given follows.
    \begin{center}
    \begin{tabular}{c|ccc}
     & $A = 1$ & $A = 2$ & $A = 3$ \\
    \hline
    $B = 1$ & 21.58 & 26.64 & 31.23 \\
    $B = 2$ & 20.00 & 26.83 & 31.72 \\
    $B = 3$ & 14.02 & 22.59 & 27.26
    \end{tabular}
    \end{center}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-23c1.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Response).}
      %\label{fig:normal-prob-10-4-23c1}
    \end{figure}
    A plot of the standardized residuals against each of the treatment combination follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-23c2.pdf}
      \caption{Standardized residuals versus treatment (response is Response).}
      %\label{fig:residuals-10-4-23c2}
    \end{figure}
    Both plots indicate a possible problem with the model assumptions, but nothing severe.
    \item The ANOVA table for testing all relevant hypotheses follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    $A$ & 2 & 807.2 & 403.6 \\
    $B$ & 2 & 204.2 & 102.1 \\
    $A \times B$ & 4 & 17.0 & 4.2 \\
    Error & 27 & 2158.0 & 79.9 \\
    Total & 35 & 3186.3 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$: no interaction between Cheese and Lot, is given by $F = 4.2/79.9 = 0.0526$ and, since $F \sim F(4, 27)$ under $H_0$, the relevant P-value equals $\prb(F > 0.0526) = .99451$. Therefore, we do not have any evidence against the null hypothesis of no interaction effect.
    
    We can then proceed to calculate the P-value for testing $H_0$: no effect due to $A$ and, since $F \sim F(2, 27)$ under $H_0$, this is given by $\prb(F > 403.6/79.9) = .01369$. Therefore, we have some evidence against the null hypothesis of no effect due to $A$.
    
    We can also test $H_0$: no effect due to $B$ and, since $F \sim F(2, 27)$ under $H_0$, the P-value equals $\prb(F(2, 6) > 102.1/79.9) = .29497$. Therefore, we have enough no evidence against the null hypothesis of no effect due to $B$.
    \item Since we conclude that only factor $A$ has a significant effect on the response we calculate the following table of means.
    \begin{center}
    \begin{tabular}{c|ccc}
     & $A = 1$ & $A = 2$ & $A = 3$ \\
    \hline
    Mean & 18.53 & 25.35 & 30.07
    \end{tabular}
    \end{center}
    A plot of the corresponding response curve follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-23e.pdf}
      \caption{Main Effects Plot -- Data Means for Response.}
      %\label{fig:main-effects-10-4-23e}
    \end{figure}
    The $0.95$-confidence intervals for the difference between the means are given in the following table.
    \begin{verbatim}
Family error rate = 0.120
Individual error rate = 0.0500
Critical value = 2.035
Intervals for (column level mean) - (row level mean)

         1          2
 2  -13.872
      0.237
 3  -18.589    -11.772
     -4.481      2.337
    \end{verbatim}
    As we can see, at the 5\% significance level, only the means of the first and third groups do not differ.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.24}
A chemical paste is made in batches and put into casks. Ten delivery batches were randomly selected for testing; then three casks were randomly selected from each delivery and the paste strength was measured twice, based on samples drawn from each sampled cask. The response was expressed as a percentage of fill strength. The collected data are given in the following table. Suppose we assume the normal regression model for these data with two categorical predictors.
\begin{center}
\begin{tabular}{l|cc|cc|cc|cc|cc}
& \multicolumn{2}{c|}{Batch 1} & \multicolumn{2}{c|}{Batch 2} & \multicolumn{2}{c|}{Batch 3} & \multicolumn{2}{c|}{Batch 4} & \multicolumn{2}{c}{Batch 5} \\ \hline
Cask 1 & 62.8 & 62.6 & 60.0 & 61.4 & 58.7 & 57.5 & 57.1 & 56.4 & 55.1 & 55.1 \\
Cask 2 & 60.1 & 62.3 & 57.5 & 56.9 & 63.9 & 63.1 & 56.9 & 58.6 & 54.7 & 54.2 \\
Cask 3 & 62.7 & 63.1 & 61.1 & 58.9 & 65.4 & 63.7 & 64.7 & 64.5 & 58.5 & 57.5
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{l|cc|cc|cc|cc|cc}
& \multicolumn{2}{c|}{Batch 6} & \multicolumn{2}{c|}{Batch 7} & \multicolumn{2}{c|}{Batch 8} & \multicolumn{2}{c|}{Batch 9} & \multicolumn{2}{c}{Batch 10} \\ \hline
Cask 1 & 63.4 & 64.9 & 62.5 & 62.6 & 59.2 & 59.4 & 54.8 & 54.8 & 58.3 & 59.3 \\
Cask 2 & 59.3 & 58.1 & 61.0 & 58.7 & 65.2 & 66.0 & 64.0 & 64.0 & 59.2 & 59.2 \\
Cask 3 & 60.5 & 60.0 & 56.9 & 57.7 & 64.8 & 64.1 & 57.7 & 56.8 & 58.9 & 56.8
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Produce a side-by-side boxplot for the data for each treatment.
\item Produce a table of cell means.
\item Produce a normal probability plot of the standardized residuals and a plot of the standardized residuals against each treatment combination (code the treatment combinations so there is a unique integer corresponding to each). Comment on the validity of the model.
\item Construct the ANOVA table testing first for no interaction between Batch and Cask and, if necessary, no effect due to Batch and no effect due to Cask.
\item Based on the results of part (d), construct the appropriate table of means and plot the corresponding response curves.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item A side-by-side boxplot of the data by treatment (using the coding $3(i-1)+j$ with $A = i$, $B = j$) follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-24a.pdf}
      \caption{Side-by-side boxplot of \% of fill strength versus treatment.}
      %\label{fig:boxplot-10-4-24a}
    \end{figure}
    \item A table of the cell means follows.
    \begin{center}
    \begin{tabular}{c|ccc}
     & Cask 1 & Cask 2 & Cask 3 \\
    \hline
    Batch 1 & 62.70 & 61.20 & 62.90 \\
    Batch 2 & 60.70 & 57.20 & 60.00 \\
    Batch 3 & 58.10 & 63.50 & 64.55 \\
    Batch 4 & 56.75 & 57.75 & 64.60 \\
    Batch 5 & 55.10 & 54.45 & 58.00 \\
    Batch 6 & 64.15 & 58.70 & 60.25 \\
    Batch 7 & 62.55 & 59.85 & 57.30 \\
    Batch 8 & 59.30 & 65.60 & 64.45 \\
    Batch 9 & 54.80 & 64.00 & 57.25 \\
    Batch 10 & 58.80 & 59.20 & 57.85
    \end{tabular}
    \end{center}
    \item A normal probability plot of the standardized residuals follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-24c1.pdf}
      \caption{Normal probability plot of the standardized residuals.}
      %\label{fig:normal-prob-10-4-24c1}
    \end{figure}
    A plot of the standardized residuals against each of the treatment combinations follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-24c2.pdf}
      \caption{Standardized residuals versus treatment (response is \% of fill strength).}
      %\label{fig:residuals-10-4-24c2}
    \end{figure}
    Both plots looks reasonable, indicating no concerns about the correctness of the model assumptions.
    \item The ANOVA table for testing all relevant hypothesis follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Cask & 2 & 20.425 & 10.213 \\
    Batch & 9 & 249.328 & 27.703 \\
    Interaction & 18 & 328.841 & 18.269 \\
    Error & 30 & 19.555 & 0.652 \\
    Total & 59 & 618.150 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$: no interaction between Cask and Batch, is given by $F = 18.269/0.652 = 28.02$ and, since $F \sim F(18, 30)$ under $H_0$, the P-value equals $\prb(F > 28.02) = .0000$. Therefore, we have strong evidence against the null hypothesis of no interaction. It is clear now that both predictors, Cask and Batch, will have a significant effect on the response. There is no need to test for an effect due to either Cask or Batch.
    \item Since we conclude that Cask and Batch interact, the appropriate table of means is given in part (b). The plot of the response curves follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-24e.pdf}
      \caption{Interaction Plot -- Data Means for \% of fill strength.}
      %\label{fig:interaction-10-4-24e}
    \end{figure}
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:10.4.25}
The following data arose from a randomized block design, where factor $B$ is the blocking variable and corresponds to plots of land on which cotton is planted. Each plot was divided into five subplots, and different concentrations of fertilizer were applied to each, with the response being a strength measurement of the cotton harvested. There were three blocks and five different concentrations of fertilizer. Note that there is only one observation for each block and concentration combination. Further discussion of these data can be found in \emph{Experimental Design}, 2nd ed., by W.~G.~Cochran and G.~M.~Cox (John Wiley \& Sons, New York, 1957, pp.~107--108). Suppose we assume the normal regression model with two categorical predictors.
\begin{center}
\begin{tabular}{l|ccc}
& $B = 1$ & $B = 2$ & $B = 3$ \\ \hline
$A = 36$ & 7.62 & 8.00 & 7.93 \\
$A = 54$ & 8.14 & 8.15 & 7.87 \\
$A = 72$ & 7.70 & 7.73 & 7.74 \\
$A = 108$ & 7.17 & 7.57 & 7.80 \\
$A = 144$ & 7.46 & 7.68 & 7.21
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Construct the ANOVA table for testing for no effect due to fertilizer and which also removes the variation due to the blocking variable.
\item Beyond the usual assumptions that we are concerned about, what additional assumption is necessary for this analysis?
\item Actually, the factor $A$ is a quantitative variable. If we were to take this into account by fitting a model that had the same slope for each block but possibly different intercepts, then what benefit would be gained?
\item Carry out the analysis suggested in part (c) and assess whether or not this model makes sense for these data.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First, since there is only one observation in each combination of the two factors, Fertilizer and Plot of Land, we have to assume that no interaction exists between the two in order to be able to detect an effect due to fertilizer. The ANOVA table for testing no effect due to fertilizer follows.
    \begin{center}
    \begin{tabular}{c|ccc}
    Source & Df & SS & MS \\
    \hline
    Fertilizer & 4 & 0.7308 & 0.1827 \\
    Block & 2 & 0.1086 & 0.0543 \\
    Error & 8 & 0.3384 & 0.0423 \\
    Total & 14 & 1.1778 &
    \end{tabular}
    \end{center}
    The $F$ statistic for testing $H_0$: no effect due to fertilizer, is given by $F = 0.1827/0.0423 = 4.3191$ and, since $F \sim F(4, 8)$ under $H_0$, the P-value equals $\prb(F(4, 8) > 4.3191) = .03747$. Therefore, we have some evidence against the null hypothesis of no effect due to Fertilizer.
    \item As mentioned in part (a), since $n_{ij} = 1$, we assume, in addition to the usual assumptions, that there is no interaction between Fertilizer and Plot of Land.
    \item This would increase the number of degrees of freedom available for error, as we would need only 1 degree of freedom to estimate the effect (slope) of Fertilizer. So we would have 11 degrees of freedom for error and thus make our comparisons more accurate.
    \item Using Minitab we fit this model obtaining the following ANOVA table.
    \begin{verbatim}
Analysis of Variance for Response, using Adjusted SS for Tests
Source   DF    Seq SS       MS       F      P
A         1   0.55981  0.55981   12.09  0.005
B         2   0.10864  0.05432    1.17  0.345
Error    11   0.50939  0.04631
Total    14   1.17784
    \end{verbatim}
    From this we can see that there is strong evidence of an effect due to $A$.
    
    To check the validity of this model we provide a normal probability plot of the standardized residuals below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-25d1.pdf}
      \caption{Normal probability plot of the standardized residuals (response is Response).}
      %\label{fig:normal-prob-10-4-25d1}
    \end{figure}
    A plot of the standardized residuals against each of the treatment combination is given below.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-4-25d2.pdf}
      \caption{Standardized residuals versus treatment (response is Response).}
      %\label{fig:residuals-10-4-25d2}
    \end{figure}
    Both plots looks reasonable, indicating no serious concerns about the correctness of the model assumptions.
\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Categorical Response and Quantitative Predictors}
\label{sec:10.5}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now consider the situation in which the response is categorical but at least some of the predictors are quantitative. The essential difficulty in this context lies with the quantitative predictors, so we will focus on the situation in which all the predictors are quantitative. When there are also some categorical predictors, these can be handled in the same way, as we can replace each categorical predictor by a set of dummy quantitative variables, as discussed in Section~\ref{ssec:10.4.5}.

For reasons of simplicity, we will restrict our attention to the situation in which the response variable $Y$ is binary valued, and we will take these values to be 0 and 1. Suppose, then, that there are $k$ quantitative predictors $X_1, \ldots, X_k$. Because $Y \in \{0, 1\}$, we have
\begin{equation*}
\expc(Y \mid X_1 = x_1, \ldots, X_k = x_k) = \prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k) \in [0, 1].
\end{equation*}
Therefore, we cannot write $\expc(Y \mid x_1, \ldots, x_k) = \beta_1 x_1 + \cdots + \beta_k x_k$ without placing some unnatural restrictions on the $\beta_i$ to ensure that $\beta_1 x_1 + \cdots + \beta_k x_k \in [0, 1]$.

Perhaps the simplest way around this is to use a 1--1 function $l : [0, 1] \to \mathbb{R}^1$ and write
\begin{equation*}
l(\prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k)) = \beta_1 x_1 + \cdots + \beta_k x_k
\end{equation*}
so that
\begin{equation*}
\prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k) = l^{-1}(\beta_1 x_1 + \cdots + \beta_k x_k).
\end{equation*}
We refer to $l$ as a \emph{link function}. There are many possible choices for $l$. For example, it is immediate that we can take $l$ to be any inverse cdf for a continuous distribution.

If we take $l = \Phi^{-1}$, i.e., the inverse cdf of the $N(0, 1)$ distribution, then this is called the \emph{probit link}. A more commonly used link, due to some inherent mathematical simplicities, is the \emph{logistic link} given by
\begin{equation}
\label{eq:10.5.1}
l(p) = \ln\left( \frac{p}{1 - p} \right).
\end{equation}
The right-hand side of \eqref{eq:10.5.1} is referred to as the \emph{logit} or \emph{log odds}. The logistic link is the inverse cdf of the logistic distribution (see Exercise~\ref{exer:10.5.1}). We will restrict our discussion to the logistic link hereafter.

The logistic link implies that (see Exercise~\ref{exer:10.5.2})
\begin{equation}
\label{eq:10.5.2}
\prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k) = \frac{\exp(\beta_1 x_1 + \cdots + \beta_k x_k)}{1 + \exp(\beta_1 x_1 + \cdots + \beta_k x_k)},
\end{equation}
which is a relatively simple relationship. We see immediately, however, that
\begin{equation*}
\var(Y \mid X_1 = x_1, \ldots, X_k = x_k) = \prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k)(1 - \prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k)),
\end{equation*}
so the variance of the conditional distribution of $Y$, given the predictors, depends on the values of the predictors. Therefore, these models are not, strictly speaking, regression models as we have defined them. Still when we use the link function given by \eqref{eq:10.5.1}, we refer to this as the \emph{logistic regression model}.

Now suppose we observe $n$ independent observations $(x_{i1}, \ldots, x_{ik}, y_i)$ for $i = 1, \ldots, n$. We then have that, given $(x_{i1}, \ldots, x_{ik})$, the response $y_i$ is an observation from the Bernoulli$(\prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k))$ distribution. Then \eqref{eq:10.5.2} implies that the conditional likelihood, given the values of the predictors, is
\begin{equation*}
\prod_{i=1}^{n} \left( \frac{\exp(\beta_1 x_1 + \cdots + \beta_k x_k)}{1 + \exp(\beta_1 x_1 + \cdots + \beta_k x_k)} \right)^{y_i} \left( \frac{1}{1 + \exp(\beta_1 x_1 + \cdots + \beta_k x_k)} \right)^{1 - y_i}.
\end{equation*}
Inference about the $\beta_i$ then proceeds via the likelihood methods discussed in Chapter~\ref{ch:6}. In fact, we need to use software to obtain the MLE's, and, because the exact sampling distributions of these quantities are not available, the large sample methods discussed in Section~\ref{sec:6.5} are used for approximate confidence intervals and P-values. Note that assessing the null hypothesis $H_0 : \beta_i = 0$ is equivalent to assessing the null hypothesis that the predictor $X_i$ does not have a relationship with the response.

We illustrate the use of logistic regression via an example.

\begin{example}
\label{ex:10.5.1}
The following table of data represent the (number of failures, number of successes) for ingots prepared for rolling under different settings of the predictor variables, $U =$ soaking time and $V =$ heating time, as reported in \emph{Analysis of Binary Data}, by D.~R.~Cox (Methuen, London, 1970). A failure indicates that an ingot is not ready for rolling after the treatment. There were observations at 19 different settings of these variables.
\begin{center}
\begin{tabular}{l|cccc}
& $V = 7$ & $V = 14$ & $V = 27$ & $V = 51$ \\ \hline
$U = 1.0$ & (0, 10) & (0, 31) & (1, 55) & (3, 10) \\
$U = 1.7$ & (0, 17) & (0, 43) & (4, 40) & (0, 1) \\
$U = 2.2$ & (0, 7) & (2, 31) & (0, 21) & (0, 1) \\
$U = 2.8$ & (0, 12) & (0, 31) & (1, 21) & (0, 0) \\
$U = 4.0$ & (0, 9) & (0, 19) & (1, 15) & (0, 1)
\end{tabular}
\end{center}

Including an intercept in the model and linear terms for $U$ and $V$ leads to three predictor variables $X_1 = 1$, $X_2 = U$, $X_3 = V$, and the model takes the form
\begin{equation*}
\prb(Y = 1 \mid X_2 = x_2, X_3 = x_3) = \frac{\exp(\beta_1 + \beta_2 x_2 + \beta_3 x_3)}{1 + \exp(\beta_1 + \beta_2 x_2 + \beta_3 x_3)}.
\end{equation*}

Fitting the model via the method of maximum likelihood leads to the estimates given in the following table. Here, $z$ is the value of estimate divided by its standard error. Because this is approximately distributed $N(0, 1)$ when the corresponding $\beta_i$ equals 0, the P-value for assessing the null hypothesis that $\beta_i = 0$ is $\prb(|Z| \geqslant |z|)$ with $Z \sim N(0, 1)$.
\begin{center}
\begin{tabular}{l|cccc}
Coefficient & Estimate & Std.\ Error & $z$ & P-value \\ \hline
$\beta_1$ & 5.55900 & 1.12000 & 4.96 & 0.000 \\
$\beta_2$ & $-0.05680$ & 0.33120 & $-0.17$ & 0.864 \\
$\beta_3$ & $-0.08203$ & 0.02373 & $-3.46$ & 0.001
\end{tabular}
\end{center}

Of course, we have to feel confident that the model is appropriate before we can proceed to make formal inferences about the $\beta_i$. In this case, we note that the number of successes $s(x_2, x_3)$ in the cell of the table, corresponding to the setting $(X_2, X_3) = (x_2, x_3)$, is an observation from a Binomial$(m(x_2, x_3), \prb(Y = 1 \mid X_2 = x_2, X_3 = x_3))$ distribution, where $m(x_2, x_3)$ is the sum of the number of successes and failures in that cell. So, for example, if $X_2 = U = 1.0$ and $X_3 = V = 7$, then $m(1.0, 7) = 10$ and $s(1.0, 7) = 10$. Denoting the estimate of $\prb(Y = 1 \mid X_2 = x_2, X_3 = x_3)$ by $\hat{p}(x_2, x_3)$, obtained by plugging in the MLE, we have that (see Problem~\ref{exer:10.5.8})
\begin{equation}
\label{eq:10.5.3}
X^2 = \sum_{x_2, x_3} \frac{(s(x_2, x_3) - m(x_2, x_3)\hat{p}(x_2, x_3))^2}{m(x_2, x_3)\hat{p}(x_2, x_3)}
\end{equation}
is asymptotically distributed as a $\chi^2(19 - 3) = \chi^2(16)$ distribution when the model is correct. We determine the degrees of freedom by counting the number of cells where there were observations (19 in this case, as no observations were obtained when $U = 2.8$, $V = 51$) and subtracting the number of parameters estimated. For these data, $X^2 = 13.543$ and the P-value is $\prb(\chi^2(16) \geqslant 13.543) = 0.633$. Therefore, we have no evidence that the model is incorrect and can proceed to make inferences about the $\beta_i$ based on the logistic regression model.

From the preceding table, we see that the null hypothesis $H_0 : \beta_2 = 0$ is not rejected. Accordingly, we drop $X_2$ and fit the smaller model given by
\begin{equation*}
\prb(Y = 1 \mid X_3 = x_3) = \frac{\exp(\beta_1 + \beta_3 x_3)}{1 + \exp(\beta_1 + \beta_3 x_3)}.
\end{equation*}
This leads to the estimates $\beta_1 = 5.4152$ and $\beta_3 = -0.08070$. Note that these are only marginally different from the previous estimates. In Figure~\ref{fig:10.5.1}, we present a graph of the fitted function over the range where we have observed $X_3$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig10_5_1.pdf}
\caption{The fitted probability of obtaining an ingot ready to be rolled as a function of heating time in Example~\ref{ex:10.5.1}.}
\label{fig:10.5.1}
\end{figure}
\end{example}

\paragraph{Summary of Section~\ref{sec:10.5}}

\begin{itemize}
\item We have examined the situation in which we have a single binary-valued response variable and a number of quantitative predictors.
\item One method of expressing a relationship between the response and predictors is via the use of a link function.
\item If we use the logistic link function, then we can carry out a logistic regression analysis using likelihood methods of inference.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:10.5.1}
Prove that the function $f : \mathbb{R}^1 \to \mathbb{R}^1$, defined by $f(x) = e^{-x}/(1 + e^{-x})^2$ for $x \in \mathbb{R}^1$, is a density function with distribution function given by $F(x) = 1/(1 + e^{-x})$ and inverse cdf given by $F^{-1}(p) = \ln p - \ln(1 - p)$ for $p \in [0, 1]$. This is called the \emph{logistic distribution}.
\end{exercise}

\begin{solution}
We have $\int_{-\infty}^{\infty} f(x) \, \mathrm{d}x = \int_0^{\infty} \frac{e^{-x}}{(1 + e^{-x})^2} \, \mathrm{d}x = \frac{1}{1 + e^{-x}} \Big|_{-\infty}^{\infty} = 1$. Hence, $f$ is indeed a density function. The distribution function is then given by $F(x) = \int_{-\infty}^{x} \frac{e^{-t}}{(1 + e^{-t})^2} \, \mathrm{d}t = \frac{1}{1 + e^{-t}} \Big|_{-\infty}^{x} = \frac{1}{1 + e^{-x}}$ as claimed. Let $p = \prb(X \leqslant x) = F(x)$, then $p \in [0, 1]$ and we have $p = (1 + e^{-x})^{-1}$, which implies $F^{-1}(p) = x = \ln(p/(1 - p))$ as claimed.
\end{solution}

\begin{exercise}
\label{exer:10.5.2}
Establish \eqref{eq:10.5.2}.
\end{exercise}

\begin{solution}
Let $p = \prb(Y = 1 \mid x)$. The log odds at $X = x$ is then given by (10.5.1) as follows.
\[
    \ln\left(\frac{p}{1 - p}\right) = \ln\left(\left(\frac{\exp\{\beta_1 + \beta_2 x\}}{1 + \exp\{\beta_1 + \beta_2 x\}}\right) \bigg/ \left(\frac{1}{1 + \exp\{\beta_1 + \beta_2 x\}}\right)\right) = \ln(\exp\{\beta_1 + \beta_2 x\}) = \beta_1 + \beta_2 x
\]
as claimed.
\end{solution}

\begin{exercise}
\label{exer:10.5.3}
Suppose that a logistic regression model for a binary-valued response $Y$ is given by
\begin{equation*}
\prb(Y = 1 \mid x) = \frac{\exp(\beta_1 + \beta_2 x)}{1 + \exp(\beta_1 + \beta_2 x)}.
\end{equation*}
Prove that the log odds at $X = x$ is given by $\beta_1 + \beta_2 x$.
\end{exercise}

\begin{solution}
Let $l = l(p) = \ln(p/(1 - p))$ be the log odds. Then, $e^l = p/(1 - p) = 1/(1/p - 1)$. Hence,
\[
    \frac{e^l}{1 + e^l} = \frac{1}{1 + 1/e^l} = \frac{1}{1 + (1 - p)/p} = \frac{p}{p + (1 - p)} = p.
\]
By substituting $l = \beta_1 + \beta_2 x$, we have
\[
    p = \frac{\exp(\beta_1 + \beta_2 x)}{1 + \exp(\beta_1 + \beta_2 x)}.
\]
\end{solution}

\begin{exercise}
\label{exer:10.5.4}
Suppose that instead of the inverse logistic cdf as the link function, we use the inverse cdf of a Laplace distribution (see Problem~\ref{exer:2.4.22}). Determine the form of $\prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k)$.
\end{exercise}

\begin{solution}
A Laplace distribution having density $f(x) = e^{-|x|}/2$ is used for the inverse cdf. The cdf is $F(x) = \int_{-\infty}^{x} e^{-|y|}/2 \, \mathrm{d}x = e^x/2$ for $x \leqslant 0$ and $F(x) = 1 - e^{-x}/2$ for $x > 0$. Hence, $F^{-1}(p) = \ln(2p)$ for $p \leqslant 1/2$ and $-\ln(2(1 - p))$. Therefore,
\[
    \prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k) = F(\beta_1 x_1 + \cdots + \beta_k x_k) = \begin{cases}
        \exp(\beta_1 x_1 + \cdots + \beta_k x_k)/2 & \text{if } \beta_1 x_1 + \cdots + \beta_k x_k \leqslant 0 \\
        1 - \exp(-(\beta_1 x_1 + \cdots + \beta_k x_k))/2 & \text{if } \beta_1 x_1 + \cdots + \beta_k x_k > 0.
    \end{cases}
\]
\end{solution}

\begin{exercise}
\label{exer:10.5.5}
Suppose that instead of the inverse logistic cdf as the link function, we use the inverse cdf of a Cauchy distribution (see Problem~\ref{exer:2.4.21}). Determine the form of $\prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k)$.
\end{exercise}

\begin{solution}
A Cauchy distribution having density $f(x) = 1/[\pi(1 + x^2)]$ is used for the inverse cdf. The cdf is
\[
    F(x) = \int_{-\infty}^{x} \frac{1}{\pi} \frac{1}{1 + x^2} \, \mathrm{d}x = \int_{-\pi/2}^{\arctan(x)} \frac{1}{\pi} \frac{1}{1 + \tan^2 \theta} \sec^2 \theta \, \mathrm{d}\theta = \int_{-\pi/2}^{\arctan(x)} \frac{1}{\pi} \, \mathrm{d}\theta = \frac{\arctan(x) + \pi/2}{\pi}.
\]
In the third integral, arc-tangent transformation is used, i.e., $x = \tan \theta$ is used. Hence, $F^{-1}(p) = \tan(\pi(p - 1/2))$. Therefore,
\[
    \prb(Y = 1 \mid X_1 = x_1, \ldots, X_k = x_k) = F(\beta_1 x_1 + \cdots + \beta_k x_k) = 1/2 + \arctan(\beta_1 x_1 + \cdots + \beta_k x_k)/\pi.
\]
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:10.5.6}
Use software to replicate the results of Example~\ref{ex:10.5.1}.
\end{exercise}

\begin{solution}
The results should be the same as presented in Example \ref{ex:10.5.1}.
\end{solution}

\begin{exercise}
\label{exer:10.5.7}
Suppose that the following data were obtained for the quantitative predictor $X$ and the binary-valued response variable $Y$.
\begin{center}
\begin{tabular}{l|ccccccccccc}
$X$ & $-5$ & $-4$ & $-3$ & $-2$ & $-1$ & 0 & 1 & 2 & 3 & 4 & 5 \\ \hline
$Y$ & 0, 0 & 0, 0 & 0, 0 & 0, 0 & 1, 0 & 0, 0 & 1, 0 & 0, 1 & 1, 1 & 1, 1 & 1, 1
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Using these data, fit the logistic regression model given by
\begin{equation*}
\prb(Y = 1 \mid x) = \frac{\exp(\beta_1 + \beta_2 x + \beta_3 x^2)}{1 + \exp(\beta_1 + \beta_2 x + \beta_3 x^2)}.
\end{equation*}
\item Does the model fit the data?
\item Test the null hypothesis $H_0 : \beta_3 = 0$.
\item If you decide there is no quadratic effect, refit the model and test for any linear effect.
\item Plot $\prb(Y = 1 \mid x)$ as a function of $x$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Fitting the model using Minitab leads to the estimates given in the following table.
    \begin{center}
    \begin{tabular}{c|cccc}
    Coefficient & Estimate & Std.\ Error & $Z$ & P-value \\
    \hline
    $\beta_1$ & $-1.1850$ & 0.9338 & $-1.27$ & 0.204 \\
    $\beta_2$ & 0.9436 & 0.3966 & 2.38 & 0.017 \\
    $\beta_3$ & 0.0597 & 0.1462 & 0.41 & 0.683
    \end{tabular}
    \end{center}
    \item The Chi-squared statistic for testing the validity of the model is then equal to 4.66204 with P-value given by $\prb(\chi^2(8) > 4.66204) = .79301$. Therefore, we have no evidence that the model is incorrect.
    \item The P-value for testing $H_0: \beta_3 = 0$ is 0.638, so we do not have any evidence against the null hypothesis.
    \item Since the null hypothesis $H_0: \beta_3 = 0$ is not rejected, we dropped the quadratic term and refit the model. This leads to the estimates given in the following table.
    \begin{center}
    \begin{tabular}{c|cccc}
    Coefficient & Estimate & Std.\ Error & $Z$ & P-value \\
    \hline
    $\beta_1$ & $-1.0063$ & 0.8150 & $-1.23$ & 0.217 \\
    $\beta_2$ & 0.9969 & 0.4163 & 2.39 & 0.017
    \end{tabular}
    \end{center}
    The P-value for testing $H_0: \beta_2 = 0$ is 0.017, so we have some evidence against the null hypothesis and we conclude that there is a linear effect.
    \item The plot of $\prb(Y = 1 \mid X = x)$ as a function of $x$ using the estimates found above follows.
    \begin{figure}[!htbp]
      \centering
      %\includegraphics[scale=0.5]{fig10-5-7e.pdf}
      \caption{Plot of $\prb(Y = 1 \mid x)$ as a function of $x$.}
      %\label{fig:logistic-10-5-7e}
    \end{figure}
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:10.5.8}
Prove that \eqref{eq:10.5.3} is the correct form for the chi-squared goodness-of-fit test statistic.
\end{exercise}

\begin{solution}
The cell count (number of successes) is an observation from a $\text{Binomial}(m(x_2, x_3), \prb(Y = 1 \mid X_2 = x_2, X_3 = x_3))$ distribution. Let $p_{(x_2, x_3)}(\theta) = \prb(Y = 1 \mid X_2 = x_2, X_3 = x_3)$, where $\theta = (\beta_2, \beta_3)$. Then by Theorem \ref{thm:9.1.2} we have that
\[
    X^2 = \sum_{(x_2, x_3)} \frac{(s(x_2, x_3) - m(x_2, x_3) \hat{p}(x_2, x_3))^2}{m(x_2, x_3) \hat{p}(x_2, x_3)} \xrightarrow{D} \chi^2(k - 1 - \dim \Omega)
\]
where $k$ is the number of combinations of $(x_2, x_3)$ and $\dim \Omega = 2$. Hence, (10.5.3) is the correct form of the Chi-squared goodness of fit test statistic.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Further Proofs (Advanced)}
\label{sec:10.6}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Proof of Theorem~\ref{thm:10.3.1}}

We want to prove that, when $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$ and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then the least-squares estimates of $\beta_1$ and $\beta_2$ are given by $b_1 = \bar{y} - b_2 \bar{x}$ and
\begin{equation*}
b_2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{equation*}
whenever $\sum_{i=1}^{n} (x_i - \bar{x})^2 \neq 0$.

We need an algebraic result that will simplify our calculations.

\begin{lemma}
\label{lem:10.6.1}
If $(x_1, y_1), \ldots, (x_n, y_n)$ are such that $\sum_{i=1}^{n} (x_i - \bar{x})^2 \neq 0$ and $q, r \in \mathbb{R}^1$, then $\sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)(q + rx_i) = 0$.
\end{lemma}

\begin{proof}
We have
\begin{equation*}
\sum_{i=1}^{n} (y_i - b_1 - b_2 x_i) = n\bar{y} - nb_1 - nb_2 \bar{x} = n(\bar{y} - (\bar{y} - b_2 \bar{x}) - b_2 \bar{x}) = 0,
\end{equation*}
which establishes that $\sum_{i=1}^{n} (y_i - b_1 - b_2 x_i) q = 0$ for any $q$. Now using this, and the formulas in Theorem~\ref{thm:10.3.1}, we obtain
\begin{align*}
\sum_{i=1}^{n} (y_i - b_1 - b_2 x_i) x_i &= \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)(x_i - \bar{x}) \\
&= \sum_{i=1}^{n} (y_i - \bar{y} - b_2(x_i - \bar{x}))(x_i - \bar{x}) \\
&= \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}) - \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x}) = 0.
\end{align*}
This establishes the lemma.
\end{proof}

Returning to the proof of Theorem~\ref{thm:10.3.1}, we have
\begin{align*}
\sum_{i=1}^{n} (y_i - \beta_1 - \beta_2 x_i)^2 &= \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i + (\beta_1 - b_1) + (\beta_2 - b_2) x_i)^2 \\
&= \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2 + 2 \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)((\beta_1 - b_1) + (\beta_2 - b_2) x_i) \\
&\quad + \sum_{i=1}^{n} ((\beta_1 - b_1) + (\beta_2 - b_2) x_i)^2 \\
&= \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2 + \sum_{i=1}^{n} ((\beta_1 - b_1) + (\beta_2 - b_2) x_i)^2
\end{align*}
as the middle term is 0 by Lemma~\ref{lem:10.6.1}. Therefore,
\begin{equation*}
\sum_{i=1}^{n} (y_i - \beta_1 - \beta_2 x_i)^2 \geqslant \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2
\end{equation*}
and $\sum_{i=1}^{n} (y_i - \beta_1 - \beta_2 x_i)^2$ takes its minimum value if and only if
\begin{equation*}
\sum_{i=1}^{n} ((\beta_1 - b_1) + (\beta_2 - b_2) x_i)^2 = 0.
\end{equation*}
This occurs if and only if $(\beta_1 - b_1) + (\beta_2 - b_2) x_i = 0$ for every $i$. Because the $x_i$ are not all the same value, this is true if and only if $\beta_1 = b_1$ and $\beta_2 = b_2$, which completes the proof.

\paragraph{Proof of Theorem~\ref{thm:10.3.2}}

We want to prove that, if $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$ and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then
\begin{enumerate}[(i)]
\item $\expc(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) = \beta_1$
\item $\expc(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) = \beta_2$.
\end{enumerate}

From Theorem~\ref{thm:10.3.1} and $\expc(Y \mid X_1 = x_1, \ldots, X_n = x_n) = \beta_1 + \beta_2 x$, we have that
\begin{equation*}
\expc(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(\beta_1 + \beta_2 x_i - \beta_1 - \beta_2 \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \beta_2 \cdot \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \beta_2.
\end{equation*}
Also, from Theorem~\ref{thm:10.3.1} and what we have just proved,
\begin{equation*}
\expc(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) = \beta_1 + \beta_2 \bar{x} - \beta_2 \bar{x} = \beta_1.
\end{equation*}

\paragraph{Proof of Theorem~\ref{thm:10.3.3}}

We want to prove that, if $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$, $\var(Y \mid X = x) = \sigma^2$ for every $x$, and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then
\begin{enumerate}[(i)]
\item $\var(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)$
\item $\var(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$
\item $\cov(B_1, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{-\sigma^2 \bar{x}}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$.
\end{enumerate}

We first prove (ii). Observe that $b_2$ is a linear combination of the $y_i - \bar{y}$ values, so we can evaluate the conditional variance once we have obtained the conditional variances and covariances of the $Y_i - \bar{Y}$ values. We have that
\begin{equation*}
Y_i - \bar{Y} = \left( 1 - \frac{1}{n} \right) Y_i - \frac{1}{n} \sum_{j \neq i} Y_j,
\end{equation*}
so the conditional variance of $Y_i - \bar{Y}$ is given by
\begin{equation*}
\sigma^2 \left( 1 - \frac{1}{n} \right)^2 + \sigma^2 \cdot \frac{n - 1}{n^2} = \sigma^2 \left( 1 - \frac{1}{n} \right).
\end{equation*}
When $i \neq j$, we can write
\begin{equation*}
Y_i - \bar{Y} = \left( 1 - \frac{1}{n} \right) Y_i - \frac{1}{n} Y_j - \frac{1}{n} \sum_{k \neq i, j} Y_k,
\end{equation*}
and the conditional covariance between $Y_i - \bar{Y}$ and $Y_j - \bar{Y}$ is then given by
\begin{equation*}
-\sigma^2 \cdot 2 \left( 1 - \frac{1}{n} \right) \cdot \frac{1}{n} + \sigma^2 \cdot \frac{n - 2}{n^2} = -\frac{\sigma^2}{n}
\end{equation*}
(note that you can assume that the means of the expectations of the $Y$'s are 0 for this calculation). Therefore, the conditional variance of $B_2$ is given by
\begin{align*}
\var(B_2 \mid x_1, \ldots, x_n) &= \frac{\sigma^2 \left( 1 - \frac{1}{n} \right) \sum_{i=1}^{n} (x_i - \bar{x})^2}{\left( \sum_{i=1}^{n} (x_i - \bar{x})^2 \right)^2} - \frac{\sigma^2}{n} \cdot \frac{\sum_{i \neq j} (x_i - \bar{x})(x_j - \bar{x})}{\left( \sum_{i=1}^{n} (x_i - \bar{x})^2 \right)^2} \\
&= \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2},
\end{align*}
because
\begin{equation*}
\sum_{i \neq j} (x_i - \bar{x})(x_j - \bar{x}) = \left( \sum_{i=1}^{n} (x_i - \bar{x}) \right)^2 - \sum_{i=1}^{n} (x_i - \bar{x})^2 = -\sum_{i=1}^{n} (x_i - \bar{x})^2.
\end{equation*}

For (iii), we have that
\begin{align*}
\cov(B_1, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) &= \cov(\bar{Y} - B_2 \bar{X}, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
&= \cov(\bar{Y}, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) - \bar{x} \var(B_2 \mid X_1 = x_1, \ldots, X_n = x_n)
\end{align*}
and
\begin{align*}
\cov(\bar{Y}, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) &= \frac{\sum_{i=1}^{n} (x_i - \bar{x}) \cov(Y_i - \bar{Y}, \bar{Y} \mid X_1 = x_1, \ldots, X_n = x_n)}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \\
&= \frac{\sigma^2 \left( 1 - \frac{1}{n} \right) \sum_{i=1}^{n} (x_i - \bar{x}) - \frac{1}{n} \sum_{j \neq i} (x_j - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = 0.
\end{align*}
Therefore, $\cov(B_1, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{-\sigma^2 \bar{x}}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$.

Finally, for (i), we have,
\begin{align*}
\var(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) &= \var(\bar{Y} - B_2 \bar{x} \mid X_1 = x_1, \ldots, X_n = x_n) \\
&= \var(\bar{Y} \mid X_1 = x_1, \ldots, X_n = x_n) + \bar{x}^2 \var(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\quad - 2\bar{x} \cov(\bar{Y}, B_2 \mid X_1 = x_1, \ldots, X_n = x_n),
\end{align*}
where $\var(\bar{Y} \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2/n$. Substituting the results for (ii) and (iii) completes the proof of the theorem.

\paragraph{Proof of Corollary~\ref{cor:10.3.1}}

We need to show that
\begin{equation*}
\var(B_1 + B_2 x \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2 \left( \frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right).
\end{equation*}
For this, we have that
\begin{align*}
&\var(B_1 + B_2 x \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\quad = \var(B_1 \mid X_1 = x_1, \ldots, X_n = x_n) + x^2 \var(B_2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\qquad + 2x \cov(B_1, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\quad = \sigma^2 \left( \frac{1}{n} + \frac{x^2 - \bar{x}^2 - 2x\bar{x}}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right) \\
&\quad = \sigma^2 \left( \frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right).
\end{align*}

\paragraph{Proof of Theorem~\ref{thm:10.3.4}}

We want to show that, if $\expc(Y \mid X = x) = \beta_1 + \beta_2 x$, $\var(Y \mid X = x) = \sigma^2$ for every $x$, and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then
\begin{equation*}
\expc(S^2 \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2.
\end{equation*}

We have that
\begin{align*}
\expc((n - 2) S^2 \mid X_1 = x_1, \ldots, X_n = x_n) &= \expc\left( \sum_{i=1}^{n} (Y_i - B_1 - B_2 x_i)^2 \mid X_1 = x_1, \ldots, X_n = x_n \right) \\
&= \sum_{i=1}^{n} \expc((Y_i - \bar{Y} - B_2(x_i - \bar{x}))^2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
&= \sum_{i=1}^{n} \var(Y_i - \bar{Y} - B_2(x_i - \bar{x}) \mid X_1 = x_1, \ldots, X_n = x_n)
\end{align*}
because
\begin{equation*}
\expc(Y_i - \bar{Y} - B_2(x_i - \bar{x}) \mid X_1 = x_1, \ldots, X_n = x_n) = \beta_1 + \beta_2 x_i - \beta_1 - \beta_2 \bar{x} - \beta_2(x_i - \bar{x}) = 0.
\end{equation*}
Now,
\begin{align*}
&\var(Y_i - \bar{Y} - B_2(x_i - \bar{x}) \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\quad = \var(Y_i - \bar{Y} \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\qquad - 2(x_i - \bar{x}) \cov(Y_i - \bar{Y}, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\qquad + (x_i - \bar{x})^2 \var(B_2 \mid X_1 = x_1, \ldots, X_n = x_n)
\end{align*}
and, using the results established about the covariances of the $Y_i - \bar{Y}$ in the proof of Theorem~\ref{thm:10.3.3}, we have that
\begin{equation*}
\var(Y_i - \bar{Y} \mid X_1 = x_1, \ldots, X_n = x_n) = \sigma^2 (1 - 1/n)
\end{equation*}
and
\begin{align*}
&\cov(Y_i - \bar{Y}, B_2 \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\quad = \frac{1}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \sum_{j=1}^{n} (x_j - \bar{x}) \cov(Y_i - \bar{Y}, Y_j - \bar{Y} \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\quad = \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \left( \left( 1 - \frac{1}{n} \right)(x_i - \bar{x}) - \frac{1}{n} \sum_{j \neq i} (x_j - \bar{x}) \right) \\
&\quad = \frac{\sigma^2 (x_i - \bar{x})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{align*}
because $\sum_{j \neq i} (x_j - \bar{x}) = -(x_i - \bar{x})$. Therefore,
\begin{align*}
&\var(Y_i - \bar{Y} - B_2(x_i - \bar{x}) \mid X_1 = x_1, \ldots, X_n = x_n) \\
&\quad = \sigma^2 \left( 1 - \frac{1}{n} \right) - \frac{2\sigma^2 (x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} + \frac{\sigma^2 (x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \\
&\quad = \sigma^2 \left( 1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)
\end{align*}
and
\begin{equation*}
\expc(S^2 \mid X_1 = x_1, \ldots, X_n = x_n) = \frac{\sigma^2}{n - 2} \sum_{i=1}^{n} \left( 1 - \frac{1}{n} - \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right) = \sigma^2,
\end{equation*}
as was stated.

\paragraph{Proof of Lemma~\ref{lem:10.3.1}}

We need to show that, if $(x_1, y_1), \ldots, (x_n, y_n)$ are such that $\sum_{i=1}^{n} (x_i - \bar{x})^2 \neq 0$, then
\begin{equation*}
\sum_{i=1}^{n} (y_i - \bar{y})^2 = b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2 + \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2.
\end{equation*}

We have that
\begin{align*}
\sum_{i=1}^{n} (y_i - \bar{y})^2 &= \sum_{i=1}^{n} y_i^2 - n\bar{y}^2 \\
&= \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i + b_1 + b_2 x_i)^2 - n\bar{y}^2 \\
&= \sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)^2 + \sum_{i=1}^{n} (b_1 + b_2 x_i)^2 - n\bar{y}^2
\end{align*}
because $\sum_{i=1}^{n} (y_i - b_1 - b_2 x_i)(b_1 + b_2 x_i) = 0$ by Lemma~\ref{lem:10.6.1}. Then, using Theorem~\ref{thm:10.3.1}, we have
\begin{equation*}
\sum_{i=1}^{n} (b_1 + b_2 x_i)^2 - n\bar{y}^2 = \sum_{i=1}^{n} (\bar{y} + b_2(x_i - \bar{x}))^2 - n\bar{y}^2 = b_2^2 \sum_{i=1}^{n} (x_i - \bar{x})^2,
\end{equation*}
and this completes the proof.

\paragraph{Proof of Theorem~\ref{thm:10.3.6}}

We want to show that, if $Y$ given $X = x$ is distributed $N(\beta_1 + \beta_2 x, \sigma^2)$ and we observe the independent values $(x_1, y_1), \ldots, (x_n, y_n)$ for $(X, Y)$, then the conditional distributions of $B_1$, $B_2$, and $S^2$, given $X_1 = x_1, \ldots, X_n = x_n$, are as follows.
\begin{enumerate}[(i)]
\item $B_1 \sim N\left( \beta_1, \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right) \right)$
\item $B_2 \sim N\left( \beta_2, \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)$
\item $B_1 + B_2 x \sim N\left( \beta_1 + \beta_2 x, \sigma^2 \left( \frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right) \right)$
\item $(n - 2)S^2/\sigma^2 \sim \chi^2(n - 2)$ independent of $(B_1, B_2)$.
\end{enumerate}

We first prove (i). Because $B_1$ can be written as a linear combination of the $Y_i$, Theorem~\ref{thm:4.6.1} implies that the distribution of $B_1$ must be normal. The result then follows from Theorems~\ref{thm:10.3.2} and~\ref{thm:10.3.3}. A similar proof establishes (ii) and (iii). The proof of (iv) is similar to the proof of Theorem~\ref{thm:4.6.6}, and we leave this to a further course in statistics.

\paragraph{Proof of Corollary~\ref{cor:10.3.2}}

We want to show
\begin{enumerate}[(i)]
\item $\displaystyle \frac{B_1 - \beta_1}{S\left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2}} \sim t(n - 2)$
\item $\displaystyle \frac{(B_2 - \beta_2)\left( \sum_{i=1}^{n} (x_i - \bar{x})^2 \right)^{1/2}}{S} \sim t(n - 2)$
\item $\displaystyle \frac{B_1 + B_2 x - \beta_1 - \beta_2 x}{S\left( \frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2}} \sim t(n - 2)$
\item If $F$ is defined as in \eqref{eq:10.3.8}, then $H_0 : \beta_2 = 0$ is true if and only if $F \sim F(1, n - 2)$.
\end{enumerate}

We first prove (i). Because $B_1$ and $S^2$ are independent,
\begin{equation*}
\frac{B_1 - \beta_1}{\left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2} \sigma} \sim N(0, 1)
\end{equation*}
independent of $(n - 2)S^2/\sigma^2 \sim \chi^2(n - 2)$. Therefore, applying Definition~\ref{def:4.6.2}, we have
\begin{equation*}
\frac{B_1 - \beta_1}{\left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2} \sigma} \cdot \frac{1}{\sqrt{(n - 2)S^2/(n - 2)\sigma^2}} = \frac{B_1 - \beta_1}{S\left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right)^{1/2}} \sim t(n - 2).
\end{equation*}

For (ii), the proof proceeds just as in the proof of (i).

For (iii), the proof proceeds just as in the proof of (i) and also using Corollary~\ref{cor:10.3.1}.

We now prove (iv). Taking the square of the ratio in (ii) and applying Theorem~\ref{thm:4.6.11} implies
\begin{equation*}
G = \frac{(B_2 - \beta_2)^2}{S^2/\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{(B_2 - \beta_2)^2 \sum_{i=1}^{n} (x_i - \bar{x})^2}{S^2} \sim F(1, n - 2).
\end{equation*}
Now observe that $F$ defined by \eqref{eq:10.3.8} equals $G$ when $\beta_2 = 0$. The converse that $F \sim F(1, n - 2)$ only if $\beta_2 = 0$ is somewhat harder to prove and we leave this to a further course.

