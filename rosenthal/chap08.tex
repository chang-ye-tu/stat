\chapter{Optimal Inferences}
\label{ch:8}

\section*{Chapter Outline}
\begin{itemize}
\item Section 1: Optimal Unbiased Estimation
\item Section 2: Optimal Hypothesis Testing
\item Section 3: Optimal Bayesian Inferences
\item Section 4: Decision Theory (Advanced)
\item Section 5: Further Proofs (Advanced)
\end{itemize}

In Chapter \ref{ch:5}, we introduced the basic ingredient of statistical inference---the statistical model. In Chapter \ref{ch:6}, inference methods were developed based on the model alone via the likelihood function. In Chapter \ref{ch:7}, we added the prior distribution on the model parameter, which led to the posterior distribution as the basis for deriving inference methods.

With both the likelihood and the posterior, however, the inferences were derived largely based on intuition. For example, when we had a characteristic of interest $\psi(\theta)$, there was nothing in the theory in Chapters \ref{ch:6} and \ref{ch:7} that forced us to choose a particular estimator, confidence or credible interval, or testing procedure. A complete theory of statistical inference, however, would totally prescribe our inferences.

One attempt to resolve this issue is to introduce a performance measure on inferences and then choose an inference that does best with respect to this measure. For example, we might choose to measure the performance of estimators by their mean-squared error (MSE) and then try to obtain an estimator that had the smallest possible MSE. This is the optimality approach to inference, and it has been quite successful in a number of problems. In this chapter, we will consider several successes for the optimality approach to deriving inferences.

Sometimes the performance measure we use can be considered to be based on what is called a loss function. Loss functions form the basis for yet another approach to statistical inference called decision theory. While it is not always the case that a performance measure is based on a loss function, this holds in some of the most important problems in statistical inference. Decision theory provides a general framework in which to discuss these problems. A brief introduction to decision theory is provided in Section~\ref{sec:8.4} as an advanced topic.

\section{Optimal Unbiased Estimation}
\label{sec:8.1}

Suppose we want to estimate the real-valued characteristic $\psi(\theta)$ for the statistical model $\{f_\theta : \theta \in \Omega\}$. If we have observed the data $s$, an estimate is a value $T(s)$ that the statistician hopes will be close to the true value of $\psi(\theta)$. We refer to $T$ as an estimator of $\psi$. The error in the estimate is given by $|T(s) - \psi(\theta)|$. For a variety of reasons (mostly to do with mathematics) it is more convenient to consider the squared error $(T(s) - \psi(\theta))^2$.

Of course, we would like this squared error to be as small as possible. Because we do not know the true value of $\theta$, this leads us to consider the distributions of the squared error, when $s$ has distribution given by $f_\theta$, for each $\theta \in \Omega$. We would then like to choose the estimator $T$ so that these distributions are as concentrated as possible about 0. A convenient measure of the concentration of these distributions about 0 is given by their means, or
\begin{equation}
\mathrm{MSE}_\theta(T) = \expc_\theta((T - \psi(\theta))^2),
\label{eq:8.1.1}
\end{equation}
called the mean-squared error (recall Definition \ref{def:6.3.1}).

An optimal estimator of $\psi(\theta)$ is then a $T$ that minimizes \eqref{eq:8.1.1} for every $\theta \in \Omega$. In other words, $T$ would be optimal if, for any other estimator $T^*$ defined on $S$, we have that
\[
\mathrm{MSE}_\theta(T) \leqslant \mathrm{MSE}_\theta(T^*)
\]
for each $\theta$. Unfortunately, it can be shown that, except in very artificial circumstances, there is no such $T$, so we need to modify our optimization problem.

This modification takes the form of restricting the estimators $T$ that we will entertain as possible choices for the inference. Consider an estimator $T$ such that $\expc_\theta(T)$ does not exist or is infinite. It can then be shown that \eqref{eq:8.1.1} is infinite (see Challenge~\ref{exer:8.1.26}). So we will first restrict our search to those $T$ for which $\expc_\theta(T)$ is finite for every $\theta$.

Further restrictions on the types of estimators that we consider make use of the following result (recall also Theorem \ref{thm:6.3.1}).

\begin{theorem}
\label{thm:8.1.1}
If $T$ is such that $\expc(T^2)$ is finite, then
\[
\expc((T - c)^2) = \var(T) + (\expc(T) - c)^2,
\]
This is minimized by taking $c = \expc(T)$.
\end{theorem}

\begin{proof}
We have that
\begin{align*}
\expc((T - c)^2) &= \expc((T - \expc(T) + \expc(T) - c)^2) \\
&= \expc((T - \expc(T))^2) + 2\expc(T - \expc(T))(\expc(T) - c) + (\expc(T) - c)^2 \\
&= \var(T) + (\expc(T) - c)^2,
\end{align*}
because $\expc(T - \expc(T)) = \expc(T) - \expc(T) = 0$. As $(\expc(T) - c)^2 \geqslant 0$, and $\var(T)$ does not depend on $c$, the value of the expression is minimized by taking $c = \expc(T)$.
\end{proof}

\subsection{The Rao--Blackwell Theorem and Rao--Blackwellization}
\label{ssec:8.1.1}

We will prove that, when we are looking for $T$ to minimize \eqref{eq:8.1.1}, we can further restrict our attention to estimators $T$ that depend on the data only through the value of a sufficient statistic. This simplifies our search, as sufficiency often results in a reduction of the dimension of the data (recall the discussion and examples in Section \ref{ssec:6.1.1}). First, however, we need the following property of sufficiency.

\begin{theorem}
\label{thm:8.1.2}
A statistic $U$ is sufficient for a model if and only if the conditional distribution of the data $s$ given $U = u$ is the same for every $\theta \in \Omega$.
\end{theorem}

\begin{proof}
See Section~\ref{sec:8.5} for the proof of this result.
\end{proof}

The implication of this result is that information in the data $s$ beyond the value of $U(s) = u$ can tell us nothing about the true value of $\theta$, because this information comes from a distribution that does not depend on the parameter. Notice that Theorem~\ref{thm:8.1.2} is a characterization of sufficiency, alternative to that provided in Section \ref{ssec:6.1.1}.

Consider a simple example that illustrates the content of Theorem~\ref{thm:8.1.2}.

\begin{example}
\label{ex:8.1.1}
Suppose that $S = \{1, 2, 3, 4\}$, $\Omega = \{a, b\}$, where the two probability distributions are given by the following table.
\begin{center}
\begin{tabular}{c|cccc}
 & $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$\theta = a$ & $1/2$ & $1/6$ & $1/6$ & $1/6$ \\
$\theta = b$ & $1/4$ & $1/4$ & $1/4$ & $1/4$
\end{tabular}
\end{center}
Then $L(\theta \mid 2) = L(\theta \mid 3) = L(\theta \mid 4)$, and so $U : S \to \{0, 1\}$, given by $U(1) = 0$ and $U(2) = U(3) = U(4) = 1$ is a sufficient statistic.

As we must have $s = 1$ when we observe $U(s) = 0$, the conditional distribution of the response $s$, given $U(s) = 0$, is degenerate at 1 (i.e., all the probability mass is at the point 1) for both $\theta = a$ and $\theta = b$. When $\theta = a$, the conditional distribution of the response $s$, given $U(s) = 1$, places $1/3$ of its mass at each of the points in $\{2, 3, 4\}$ and similarly when $\theta = b$. So given $U(s) = 1$, the conditional distributions are as in the following table.
\begin{center}
\begin{tabular}{c|cccc}
 & $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$\theta = a$ & $0$ & $1/3$ & $1/3$ & $1/3$ \\
$\theta = b$ & $0$ & $1/3$ & $1/3$ & $1/3$
\end{tabular}
\end{center}
Thus, we see that indeed the conditional distributions are independent of $\theta$.
\end{example}

We now combine Theorems~\ref{thm:8.1.1} and~\ref{thm:8.1.2} to show that we can restrict our attention to estimators $T$ that depend on the data only through the value of a sufficient statistic $U$. By Theorem~\ref{thm:8.1.2} we can denote the conditional probability measure for $s$, given $U(s) = u$, by $\prb(\cdot \mid U = u)$, i.e., this probability measure does not depend on $\theta$, as it is the same for every $\theta \in \Omega$.

For estimator $T$ of $\psi(\theta)$, such that $\expc_\theta(T)$ is finite for every $\theta$, put $T_U(s)$ equal to the conditional expectation of $T$ given the value of $U(s)$; namely,
\[
T_U(s) = \expc_{\prb(\cdot \mid U=U(s))}(T),
\]
i.e., $T_U$ is the average value of $T$ when we average using $\prb(\cdot \mid U = U(s))$. Notice that $T_U(s_1) = T_U(s_2)$ whenever $U(s_1) = U(s_2)$ (this is because $\prb(\cdot \mid U = U(s_1)) = \prb(\cdot \mid U = U(s_2))$), and so $T_U$ depends on the data $s$ only through the value of $U(s)$.

\begin{theorem}[Rao--Blackwell]
\label{thm:8.1.3}
Suppose that $U$ is a sufficient statistic and $\expc_\theta(T^2)$ is finite for every $\theta$. Then $\mathrm{MSE}_\theta(T_U) \leqslant \mathrm{MSE}_\theta(T)$ for every $\theta \in \Omega$.
\end{theorem}

\begin{proof}
  Let $\prb_{\theta,U}$ denote the marginal probability measure of $U$ induced by $\prb_\theta$. By the theorem of total expectation (see Theorem \ref{thm:3.5.2}), we have that
\[
\mathrm{MSE}_\theta(T) = \expc_{\prb_{\theta,U}}\left(\expc_{\prb(\cdot \mid U=u)}((T - \psi(\theta))^2)\right),
\]
where $\expc_{\prb(\cdot \mid U=u)}((T - \psi(\theta))^2)$ denotes the conditional MSE of $T$, given $U = u$. Now by Theorem~\ref{thm:8.1.1},
\begin{equation}
\expc_{\prb(\cdot \mid U=u)}((T - \psi(\theta))^2) = \var_{\prb(\cdot \mid U=u)}(T) + (\expc_{\prb(\cdot \mid U=u)}(T) - \psi(\theta))^2.
\label{eq:8.1.3}
\end{equation}
As both terms in \eqref{eq:8.1.3} are nonnegative, and recalling the definition of $T_U$, we have
\begin{align*}
\mathrm{MSE}_\theta(T) &= \expc_{\prb_{\theta,U}}(\var_{\prb(\cdot \mid U=u)}(T)) + \expc_{\prb_{\theta,U}}((T_U(s) - \psi(\theta))^2) \\
&\geqslant \expc_{\prb_{\theta,U}}((T_U(s) - \psi(\theta))^2).
\end{align*}
Now $(T_U(s) - \psi(\theta))^2 = \expc_{\prb(\cdot \mid U=u)}((T_U(s) - \psi(\theta))^2)$ (Theorem \ref{thm:3.5.4}) and so, by the theorem of total expectation,
\[
\expc_{\prb_{\theta,U}}((T_U(s) - \psi(\theta))^2) = \expc_{\prb_{\theta,U}}\left(\expc_{\prb(\cdot \mid U=u)}((T_U(s) - \psi(\theta))^2)\right) = \expc_{\prb_\theta}((T_U(s) - \psi(\theta))^2) = \mathrm{MSE}_\theta(T_U)
\]
and the theorem is proved.
\end{proof}

Theorem~\ref{thm:8.1.3} shows that we can always improve on (or at least make no worse) any estimator $T$ that possesses a finite second moment, by replacing $T(s)$ by the estimate $T_U(s)$. This process is sometimes referred to as the Rao-Blackwellization of an estimator.

Notice that putting $\expc = \expc_\theta$ and $c = \psi(\theta)$ in Theorem~\ref{thm:8.1.1} implies that
\begin{equation}
\mathrm{MSE}_\theta(T) = \var_\theta(T) + (\expc_\theta(T) - \psi(\theta))^2.
\label{eq:8.1.4}
\end{equation}
So the MSE of $T$ can be decomposed as the sum of the variance of $T$ plus the squared bias of $T$ (this was also proved in Theorem \ref{thm:6.3.1}).

Theorem~\ref{thm:8.1.1} has another important implication, for \eqref{eq:8.1.4} is minimized by taking $\psi(\theta) = \expc_\theta(T)$. This indicates that, on average, the estimator $T$ comes closer (in terms of squared error) to $\expc_\theta(T)$ than to any other value. So, if we are sampling from the distribution specified by $\theta$, $T(s)$ is a natural estimate of $\expc_\theta(T)$. Therefore, for a general characteristic $\psi(\theta)$, it makes sense to restrict attention to estimators that have bias equal to 0. This leads to the following definition.

\begin{definition}
\label{def:8.1.1}
An estimator $T$ of $\psi(\theta)$ is \emph{unbiased} if $\expc_\theta(T) = \psi(\theta)$ for every $\theta \in \Omega$.
\end{definition}

Notice that, for unbiased estimators with finite second moment, \eqref{eq:8.1.4} becomes
\[
\mathrm{MSE}_\theta(T) = \var_\theta(T).
\]
Therefore, our search for an optimal estimator has become the search for an unbiased estimator with smallest variance. If such an estimator exists, we give it a special name.

\begin{definition}
\label{def:8.1.2}
An unbiased estimator of $\psi(\theta)$ with smallest variance for each $\theta \in \Omega$ is called a \emph{uniformly minimum variance unbiased} (UMVU) estimator.
\end{definition}

It is important to note that the Rao--Blackwell theorem (Theorem~\ref{thm:8.1.3}) also applies to unbiased estimators. This is because the Rao--Blackwellization of an unbiased estimator yields an unbiased estimator, as the following result demonstrates.

\begin{theorem}[Rao--Blackwell for unbiased estimators]
\label{thm:8.1.4}
If $T$ has finite second moment, is unbiased for $\psi(\theta)$, and $U$ is a sufficient statistic, then $\expc_\theta(T_U) = \psi(\theta)$ for every $\theta \in \Omega$ (so $T_U$ is also unbiased for $\psi(\theta)$) and $\var_\theta(T_U) \leqslant \var_\theta(T)$.
\end{theorem}

\begin{proof}
  Using the theorem of total expectation (Theorem \ref{thm:3.5.2}), we have
\[
\expc_\theta(T_U) = \expc_{\prb_{\theta,U}}(T_U) = \expc_{\prb_{\theta,U}}\left(\expc_{\prb(\cdot \mid U=u)}(T)\right) = \expc_\theta(T) = \psi(\theta).
\]
So $T_U$ is unbiased for $\psi(\theta)$ and $\mathrm{MSE}_\theta(T) = \var_\theta(T)$, $\mathrm{MSE}_\theta(T_U) = \var_\theta(T_U)$. Applying Theorem~\ref{thm:8.1.3} gives $\var_\theta(T_U) \leqslant \var_\theta(T)$.
\end{proof}

There are many situations in which the theory of unbiased estimation leads to good estimators. However, the following example illustrates that in some problems, there are no unbiased estimators and hence the theory has some limitations.

\begin{example}[The Nonexistence of an Unbiased Estimator]
\label{ex:8.1.2}
Suppose that $(x_1, \ldots, x_n)$ is a sample from the Bernoulli$(\theta)$ and we wish to find a UMVU estimator of $\psi(\theta) = \theta/(1 - \theta)$, the odds in favor of a success occurring. From Theorem~\ref{thm:8.1.4}, we can restrict our search to unbiased estimators $T$ that are functions of the sufficient statistic $n\bar{x}$.

Such a $T$ satisfies $\expc_\theta(T(n\bar{X})) = \theta/(1 - \theta)$ for every $\theta \in [0, 1]$. Recalling that $n\bar{X} \sim \text{Binomial}(n, \theta)$, this implies that
\[
\frac{\theta}{1 - \theta} = \sum_{k=0}^{n} T(k) \binom{n}{k} \theta^k (1 - \theta)^{n-k}
\]
for every $\theta \in [0, 1]$. By the binomial theorem, we have
\[
(1 - \theta)^{n-k} = \sum_{l=0}^{n-k} \binom{n-k}{l} (-1)^l \theta^l.
\]
Substituting this into the preceding expression for $\theta/(1 - \theta)$ and writing this in terms of powers of $\theta$ leads to
\begin{equation}
\frac{\theta}{1 - \theta} = \sum_{m=0}^{n} \left(\sum_{k=0}^{m} T(k) \binom{n}{k} (-1)^{m-k}\right) \theta^m.
\label{eq:8.1.5}
\end{equation}
Now the left-hand side of \eqref{eq:8.1.5} goes to $\infty$ as $\theta \to 1$, but the right-hand side is a polynomial in $\theta$, which is bounded in $[0, 1]$. Therefore, an unbiased estimator of $\psi$ cannot exist.
\end{example}

If a characteristic $\psi(\theta)$ has an unbiased estimator, then it is said to be \emph{U-estimable}. It should be kept in mind, however, that just because a parameter is not U-estimable does not mean that we cannot estimate it! For example, in Example~\ref{ex:8.1.2}, $\psi$ is a 1--1 function of $\theta$, so the MLE of $\psi$ is given by $\bar{x}/(1 - \bar{x})$ (see Theorem \ref{thm:6.2.1}); this seems like a sensible estimator, even if it is biased.

\subsection{Completeness and the Lehmann--Scheff\'e Theorem}
\label{ssec:8.1.2}

In certain circumstances, if an unbiased estimator exists, and is a function of a sufficient statistic $U$, then there is only one such estimator---so it must be UMVU. We need the concept of completeness to establish this.

\begin{definition}
\label{def:8.1.3}
A statistic $U$ is \emph{complete} if any function $h$ of $U$, which satisfies $\expc_\theta(h(U)) = 0$ for every $\theta \in \Omega$, also satisfies $h(U(s)) = 0$ with probability 1 for each $\theta \in \Omega$ (i.e., $\prb_\theta(\{s : h(U(s)) = 0\}) = 1$ for every $\theta \in \Omega$).
\end{definition}

In probability theory, we treat two functions as equivalent if they differ only on a set having probability content 0, as the probability of the functions taking different values at an observed response value is 0. So in Definition~\ref{def:8.1.3}, we need not distinguish between $h$ and the constant 0. Therefore, a statistic $U$ is complete if the only unbiased estimator of 0, based on $U$, is given by 0 itself.

We can now derive the following result.

\begin{theorem}[Lehmann--Scheff\'e]
\label{thm:8.1.5}
If $U$ is a complete sufficient statistic, and if $T$ depends on the data only through the value of $U$, has finite second moment for every $\theta$, and is unbiased for $\psi(\theta)$, then $T$ is UMVU.
\end{theorem}

\begin{proof}
Suppose that $T^*$ is also an unbiased estimator of $\psi(\theta)$. By Theorem~\ref{thm:8.1.4} we can assume that $T^*$ depends on the data only through the value of $U$. Then there exist functions $h$ and $h^*$ such that $T(s) = h(U(s))$ and $T^*(s) = h^*(U(s))$ and
\[
0 = \expc_\theta(T) - \expc_\theta(T^*) = \expc_\theta(h(U)) - \expc_\theta(h^*(U)) = \expc_\theta(h(U) - h^*(U)).
\]
By the completeness of $U$, we have that $h(U) = h^*(U)$ with probability 1 for each $\theta \in \Omega$, which implies that $T = T^*$ with probability 1 for each $\theta \in \Omega$. This says there is essentially only one unbiased estimator for $\psi(\theta)$ based on $U$, and so it must be UMVU.
\end{proof}

The Rao--Blackwell theorem for unbiased estimators (Theorem~\ref{thm:8.1.4}), together with the Lehmann--Scheff\'e theorem, provide a method for obtaining a UMVU estimator of $\psi(\theta)$. Suppose we can find an unbiased estimator $T$ that has finite second moment. If we also have a complete sufficient statistic $U$, then by Theorem~\ref{thm:8.1.4} $T_U(s) = \expc_{\prb(\cdot \mid U=U(s))}(T)$ is unbiased for $\psi(\theta)$ and depends on the data only through the value of $U$, because $T_U(s_1) = T_U(s_2)$ whenever $U(s_1) = U(s_2)$. Therefore, by Theorem~\ref{thm:8.1.5}, $T_U$ is UMVU for $\psi(\theta)$.

It is not necessary, in a given problem, that a complete sufficient statistic exist. In fact, it can be proved that the only candidate for this is a minimal sufficient statistic (recall the definition in Section \ref{ssec:6.1.1}). So in a given problem, we must obtain a minimal sufficient statistic and then determine whether or not it is complete. We illustrate this via an example.

\begin{example}[Location Normal]
\label{ex:8.1.3}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu \in \mathbb{R}^1$ is unknown and $\sigma_0^2 > 0$ is known. In Example \ref{ex:6.1.7}, we showed that $\bar{x}$ is a minimal sufficient statistic for this model.

In fact, $\bar{x}$ is also complete for this model. The proof of this is a bit involved and is presented in Section~\ref{sec:8.5}.

Given that $\bar{x}$ is a complete, minimal sufficient statistic, this implies that $T(\bar{x})$ is a UMVU estimator of its mean $\expc(T(\bar{X}))$ whenever $T$ has a finite second moment for every $\mu \in \mathbb{R}^1$. In particular, $\bar{x}$ is the UMVU estimator of $\mu$ because $\expc(\bar{X}) = \mu$ and $\expc(\bar{X}^2) = (\sigma_0^2/n) + \mu^2 < \infty$. Furthermore, $\bar{x} + \sigma_0 z_p$ is the UMVU estimator of $\expc(\bar{X} + \sigma_0 z_p) = \mu + \sigma_0 z_p$ (the $p$th quantile of the true distribution).
\end{example}

The arguments needed to show the completeness of a minimal sufficient statistic in a problem are often similar to the one required in Example~\ref{ex:8.1.3} (see Challenge~\ref{exer:8.1.27}). Rather than pursue such technicalities here, we quote some important examples in which the minimal sufficient statistic is complete.

\begin{example}[Location-Scale Normal]
\label{ex:8.1.4}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma^2)$ distribution, where $\mu \in \mathbb{R}^1$ and $\sigma > 0$ are unknown. The parameter in this model is two-dimensional and is given by $(\mu, \sigma^2) \in \mathbb{R}^1 \times (0, \infty)$.

We showed, in Example \ref{ex:6.1.8}, that $(\bar{x}, s^2)$ is a minimal sufficient statistic for this model. In fact, it can be shown that $(\bar{x}, s^2)$ is a complete minimal sufficient statistic. Therefore, $T(\bar{x}, s^2)$ is a UMVU estimator of $\expc_\theta(T(\bar{X}, S^2))$ whenever the second moment of $T(\bar{x}, s^2)$ is finite for every $(\mu, \sigma^2)$. In particular, $\bar{x}$ is the UMVU estimator of $\mu$ and $s^2$ is UMVU for $\sigma^2$.
\end{example}

\begin{example}[Distribution-Free Models]
\label{ex:8.1.5}
Suppose that $(x_1, \ldots, x_n)$ is a sample from some continuous distribution on $\mathbb{R}^1$. The statistical model comprises all continuous distributions on $\mathbb{R}^1$.

It can be shown that the order statistics $(x_{(1)}, \ldots, x_{(n)})$ make up a complete minimal sufficient statistic for this model. Therefore, $T(x_{(1)}, \ldots, x_{(n)})$ is UMVU for
\[
\expc(T(X_{(1)}, \ldots, X_{(n)}))
\]
whenever
\begin{equation}
\expc(T^2(X_{(1)}, \ldots, X_{(n)})) < \infty
\label{eq:8.1.6}
\end{equation}
for every continuous distribution. In particular, if $T : \mathbb{R}^n \to \mathbb{R}^1$ is bounded, then this is the case. For example, if
\[
T(x_{(1)}, \ldots, x_{(n)}) = \frac{1}{n} \sum_{i=1}^{n} \indc_A(x_{(i)}),
\]
the relative frequency of the event $A$ in the sample, then $T(x_{(1)}, \ldots, x_{(n)})$ is UMVU for $\expc(T(X_{(1)}, \ldots, X_{(n)})) = \prb(A)$.

Now change the model assumption so that $(x_1, \ldots, x_n)$ is a sample from some continuous distribution on $\mathbb{R}^1$ that possesses its first $m$ moments. Again, it can be shown that the order statistics make up a complete minimal sufficient statistic. Therefore, $T(x_{(1)}, \ldots, x_{(n)})$ is UMVU for $\expc(T(X_{(1)}, \ldots, X_{(n)}))$ whenever \eqref{eq:8.1.6} holds for every continuous distribution possessing its first $m$ moments. For example, if $m = 2$, then this implies that $T(x_{(1)}, \ldots, x_{(n)}) = \bar{x}$ is UMVU for $\expc(\bar{X})$. When $m = 4$, we have that $s^2$ is UMVU for the population variance (see Exercise~\ref{exer:8.1.2}).
\end{example}

\subsection{The Cram\'er--Rao Inequality (Advanced)}
\label{ssec:8.1.3}

There is a fundamental inequality that holds for the variance of an estimator $T$. This is given by the Cram\'er--Rao inequality (sometimes called the information inequality). It is a corollary to the following inequality.

\begin{theorem}[Covariance inequality]
\label{thm:8.1.6}
Suppose $T, U : S \to \mathbb{R}^1$ and $\expc_\theta(T^2) < \infty$, $0 < \expc_\theta(U^2) < \infty$ for every $\theta \in \Omega$. Then
\[
\var_\theta(T) \geqslant \frac{(\cov_\theta(T, U))^2}{\var_\theta(U)}
\]
for every $\theta \in \Omega$. Equality holds if and only if
\[
T(s) = \expc_\theta(T) + \frac{\cov_\theta(T, U)}{\var_\theta(U)}(U(s) - \expc_\theta(U(s)))
\]
with probability 1 for every $\theta \in \Omega$ (i.e., if and only if $T(s)$ and $U(s)$ are linearly related).
\end{theorem}

\begin{proof}
  This result follows immediately from the Cauchy--Schwartz inequality (Theorem \ref{thm:3.6.3}).
\end{proof}

Now suppose that $\Omega$ is an open subinterval of $\mathbb{R}^1$ and we take
\begin{equation}
U_\theta(s) = S(\theta\,|\,s) = \frac{\partial \ln f_\theta(s)}{\partial \theta},
\label{eq:8.1.7}
\end{equation}
i.e., $U$ is the score function. Assume that the conditions discussed in Section \ref{sec:6.5} hold, so that $\expc_\theta(S(\theta \mid s)) = 0$ for all $\theta$, and, Fisher's information $I(\theta) = \var_\theta(S(\theta \mid s))$ is finite. Then using
\[
\frac{\partial \ln f_\theta(s)}{\partial \theta} = \frac{\partial f_\theta(s)}{\partial \theta} \frac{1}{f_\theta(s)},
\]
we have
\begin{align}
\cov_\theta(T, U) &= \expc_\theta\left(T(s) \frac{\partial \ln f_\theta(s)}{\partial \theta}\right) \notag \\
&= \expc_\theta\left(T(s) \frac{\partial f_\theta(s)}{\partial \theta} \frac{1}{f_\theta(s)}\right) \notag \\
&= \sum_s \left(T(s) \frac{\partial f_\theta(s)}{\partial \theta} \frac{1}{f_\theta(s)}\right) f_\theta(s) = \frac{\partial}{\partial \theta} \sum_s T(s) f_\theta(s) = \frac{\partial \expc_\theta(T)}{\partial \theta},
\label{eq:8.1.8}
\end{align}
in the discrete case, where we have assumed conditions like those discussed in Section \ref{sec:6.5}, so we can pull the partial derivative through the sum. A similar argument gives the equality \eqref{eq:8.1.8} in the continuous case as well.

The covariance inequality, applied with $U$ specified as in \eqref{eq:8.1.7} and using \eqref{eq:8.1.8}, gives the following result.

\begin{corollary}[Cram\'er--Rao or information inequality]
\label{cor:8.1.1}
Under conditions,
\[
\var_\theta(T) \geqslant \left(\frac{\partial \expc_\theta(T)}{\partial \theta}\right)^2 (I(\theta))^{-1}
\]
for every $\theta \in \Omega$. Equality holds if and only if
\[
T(s) = \expc_\theta(T) + \frac{\partial \expc_\theta(T)}{\partial \theta} (I(\theta))^{-1} S(\theta \mid s)
\]
with probability 1 for every $\theta \in \Omega$.
\end{corollary}

The Cram\'er--Rao inequality provides a fundamental lower bound on the variance of an estimator $T$. From \eqref{eq:8.1.4}, we know that the variance is a relevant measure of the accuracy of an estimator only when the estimator is unbiased, so we restate Corollary~\ref{cor:8.1.1} for this case.

\begin{corollary}
\label{cor:8.1.2}
Under the conditions of Corollary~\ref{cor:8.1.1}, when $T$ is an unbiased estimator of $\psi(\theta)$,
\[
\var_\theta(T) \geqslant (\psi'(\theta))^2 (I(\theta))^{-1}
\]
for every $\theta \in \Omega$. Equality holds if and only if
\begin{equation}
T(s) = \psi(\theta) + \psi'(\theta)(I(\theta))^{-1} S(\theta \mid s)
\label{eq:8.1.9}
\end{equation}
with probability 1 for every $\theta \in \Omega$.
\end{corollary}

Notice that when $\psi(\theta) = \theta$, then Corollary~\ref{cor:8.1.2} says that the variance of the unbiased estimator $T$ is bounded below by the reciprocal of the Fisher information. More generally, when $\psi$ is a 1--1, smooth transformation, we have (using Challenge 6.5.19) that the variance of an unbiased $T$ is again bounded below by the reciprocal of the Fisher information, but this time the model uses the parameterization in terms of $\psi = \psi(\theta)$.

Corollary~\ref{cor:8.1.2} has several interesting implications. First, if we obtain an unbiased estimator $T$ with variance at the lower bound, then we know immediately that it is UMVU. Second, we know that any unbiased estimator that achieves the lower bound is of the form given in \eqref{eq:8.1.9}. Note that the right-hand side of \eqref{eq:8.1.9} must be independent of $\theta$ in order for this to be an estimator. If this is not the case, then there are no UMVU estimators whose variance achieves the lower bound. The following example demonstrates that there are cases in which UMVU estimators exist, but their variance does not achieve the lower bound.

\begin{example}[Poisson$(\lambda)$ Model]
\label{ex:8.1.6}
Suppose that $(x_1, \ldots, x_n)$ is a sample from the Poisson$(\lambda)$ distribution where $\lambda > 0$ is unknown. The log-likelihood is given by $l(\lambda \mid x_1, \ldots, x_n) = n\bar{x} \ln \lambda - n\lambda$, so the score function is given by $S(\lambda \mid x_1, \ldots, x_n) = n\bar{x}/\lambda - n$. Now
\[
\frac{\partial S(\lambda \mid x_1, \ldots, x_n)}{\partial \lambda} = -\frac{n\bar{x}}{\lambda^2},
\]
and thus
\[
I(\lambda) = \expc_\lambda\left(\frac{n\bar{X}}{\lambda^2}\right) = \frac{n}{\lambda}.
\]

Suppose we are estimating $\lambda$. Then the Cram\'er--Rao lower bound is given by $I^{-1}(\lambda) = \lambda/n$. Noting that $\bar{x}$ is unbiased for $\lambda$ and that $\var_\lambda(\bar{X}) = \lambda/n$, we see immediately that $\bar{x}$ is UMVU and achieves the lower bound.

Now suppose that we are estimating $\psi(\lambda) = e^{-\lambda} = \prb(\{0\})$. The Cram\'er--Rao lower bound equals $e^{-2\lambda}\lambda/n$ and
\[
\psi(\lambda) + \psi'(\lambda)I^{-1}(\lambda)S(\lambda \mid x_1, \ldots, x_n) = e^{-\lambda} - e^{-\lambda} \frac{\lambda}{n}\left(\frac{n\bar{x}}{\lambda} - n\right) = e^{-\lambda}(1 - \bar{x} + \lambda),
\]
which is clearly not independent of $\lambda$. So there does not exist a UMVU estimator for $\psi$ that attains the lower bound.

Does there exist a UMVU estimator for $\psi$? Observe that when $n = 1$, then $\indc_{\{0\}}(x_1)$ is an unbiased estimator of $\psi$. As it turns out, $\bar{x}$ is (for every $n$) a complete minimal sufficient statistic for this model, so by the Lehmann--Scheff\'e theorem $\indc_{\{0\}}(x_1)$ is UMVU for $\psi$. Furthermore, $\indc_{\{0\}}(X_1)$ has variance
\[
\prb(X_1 = 0)(1 - \prb(X_1 = 0)) = e^{-\lambda}(1 - e^{-\lambda})
\]
since $\indc_{\{0\}}(X_1) \sim \text{Bernoulli}(e^{-\lambda})$. This implies that $e^{-\lambda}(1 - e^{-\lambda}) > e^{-2\lambda}\lambda$.

In general, we have that
\[
\frac{1}{n} \sum_{i=1}^{n} \indc_{\{0\}}(x_i)
\]
is an unbiased estimator of $\psi$, but it is not a function of $\bar{x}$. Thus we cannot apply the Lehmann--Scheff\'e theorem, but we can Rao--Blackwellize this estimator. Therefore, the UMVU estimator of $\psi$ is given by
\[
\frac{1}{n} \sum_{i=1}^{n} \expc(\indc_{\{0\}}(X_i) \mid \bar{X} = \bar{x}).
\]
To determine this estimator in closed form, we reason as follows. The conditional probability function of $(X_1, \ldots, X_n)$ given $\bar{X} = \bar{x}$, because $n\bar{X}$ is distributed Poisson$(n\lambda)$, is
\[
\frac{\lambda^{x_1}}{x_1!} \cdots \frac{\lambda^{x_n}}{x_n!} e^{-n\lambda} \left\{\frac{(n\lambda)^{n\bar{x}}}{(n\bar{x})!} e^{-n\lambda}\right\}^{-1} = \binom{n\bar{x}}{x_1 \cdots x_n} \left(\frac{1}{n}\right)^{x_1} \cdots \left(\frac{1}{n}\right)^{x_n},
\]
i.e., $(X_1, \ldots, X_n)$ given $\bar{X} = \bar{x}$ is distributed Multinomial$(n\bar{x}, 1/n, \ldots, 1/n)$. Accordingly, the UMVU estimator is given by
\[
\expc(\indc_{\{0\}}(X_1) \mid \bar{X} = \bar{x}) = \prb(X_1 = 0 \mid \bar{X} = \bar{x}) = \left(1 - \frac{1}{n}\right)^{n\bar{x}}
\]
because $X_i \mid \bar{X} = \bar{x} \sim \text{Binomial}(n\bar{x}, 1/n)$ for each $i = 1, \ldots, n$.

Certainly, it is not at all obvious from the functional form that this estimator is unbiased, let alone UMVU. So this result can be viewed as a somewhat remarkable application of the theory.
\end{example}

Recall now Theorems \ref{thm:6.5.2} and \ref{thm:6.5.3}. The implications of these results, with some additional conditions, are that the MLE of $\theta$ is asymptotically unbiased for $\theta$ and that the asymptotic variance of the MLE is at the information lower bound. This is often interpreted to mean that, with large samples, the MLE makes full use of the information about $\theta$ contained in the data.

\subsection*{Summary of Section \ref{sec:8.1}}

\begin{itemize}
\item An estimator comes closest (using squared distance) on average to its mean (see Theorem~\ref{thm:8.1.1}), so we can restrict attention to unbiased estimators for quantities of interest.
\item The Rao--Blackwell theorem says that we can restrict attention to functions of a sufficient statistic when looking for an estimator minimizing MSE.
\item When a sufficient statistic is complete, then any function of that sufficient statistic is UMVU for its mean.
\item The Cram\'er--Rao lower bound gives a lower bound on the variance of an unbiased estimator and a method for obtaining an estimator that has variance at this lower bound when such an estimator exists.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:8.1.1}
Suppose that a statistical model is given by the two distributions in the following table.
\begin{center}
\begin{tabular}{c|cccc}
 & $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$f_a(s)$ & $1/3$ & $1/6$ & $1/12$ & $5/12$ \\
$f_b(s)$ & $1/2$ & $1/4$ & $1/6$ & $1/12$
\end{tabular}
\end{center}
If $T : \{1, 2, 3, 4\} \to \{1, 2, 3, 4\}$ is defined by $T(1) = T(2) = 1$ and $T(s) = s$ otherwise, then prove that $T$ is a sufficient statistic. Derive the conditional distributions of $s$ given $T(s)$ and show that these are independent of $\theta$.
\end{exercise}

\begin{solution}
We have that $L(1 \mid \cdot) = (3/2)L(2 \mid \cdot)$ and so by Section \ref{ssec:6.1.1} $T$ is a sufficient statistic. Given $T = 1$, then the conditional distributions of $s$ are given by the following table.
\begin{center}
\begin{tabular}{c|cccc}
 & $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$f_a(s \mid T = 1)$ & $\frac{1/3}{1/3 + 1/6} = \frac{2}{3}$ & $\frac{1/6}{1/3 + 1/6} = \frac{1}{3}$ & 0 & 0 \\
$f_b(s \mid T = 1)$ & $\frac{1/2}{1/2 + 1/4} = \frac{2}{3}$ & $\frac{1/4}{1/2 + 1/4} = \frac{1}{3}$ & 0 & 0 \\
\end{tabular}
\end{center}
We see that these are the same (i.e., independent of $\theta$). When $T = 3$ the conditional distributions of $s$ are given by
\begin{center}
\begin{tabular}{c|cccc}
 & $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$f_a(s \mid T = 3)$ & 0 & 0 & 1 & 0 \\
$f_b(s \mid T = 3)$ & 0 & 0 & 1 & 0 \\
\end{tabular}
\end{center}
and when $T = 4$ the conditional distributions are given by
\begin{center}
\begin{tabular}{c|cccc}
 & $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$f_a(s \mid T = 4)$ & 0 & 0 & 0 & 1 \\
$f_b(s \mid T = 4)$ & 0 & 0 & 0 & 1 \\
\end{tabular}
\end{center}
and these are also independent of $\theta$.
\end{solution}

\begin{exercise}
\label{exer:8.1.2}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a distribution with mean $\mu$ and variance $\sigma^2$. Prove that $s^2 = (n-1)^{-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$ is unbiased for $\sigma^2$.
\end{exercise}

\begin{solution}
Using $\var(x) = \expc(x^2) - (\expc(x))^2$, $\var(\bar{x}) = \var(x)/n$ we have that
\begin{align*}
\expc\left(\sum_{i=1}^{n}(x_i - \bar{x})^2\right) &= \expc\left(\sum_{i=1}^{n}x_i^2 - n\bar{x}^2\right) = \sum_{i=1}^{n}\expc(x_i^2) - n\expc(\bar{x}^2) \\
&= n\expc(x_1^2) - n\expc(\bar{x}^2) = n(\sigma^2 + \mu^2) - n\left(\frac{\sigma^2}{n} + \mu^2\right) = (n-1)\sigma^2
\end{align*}
and the result follows. This estimator will be UMVU whenever $(\bar{x}, s^2)$ is a complete sufficient statistic for the class of possible distributions that we are sampling from.
\end{solution}

\begin{exercise}
\label{exer:8.1.3}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu \in \mathbb{R}^1$ is unknown and $\sigma_0^2$ is known. Determine a UMVU estimator of the second moment $\mu^2 + \sigma_0^2$.
\end{exercise}

\begin{solution}
From Example \ref{ex:8.1.3} we know that $\bar{x}$ is a complete sufficient statistic. Therefore, any function of $\bar{x}$ is a UMVU estimator of its mean. We have that $\expc(\bar{x}^2) = \mu^2 + \sigma_0^2/n$ and so $\bar{x}^2 - \sigma_0^2/n + \sigma_0^2 = \bar{x}^2 + (1 - 1/n)\sigma_0^2$ is UMVU for $\mu^2 + \sigma_0^2$.
\end{solution}

\begin{exercise}
\label{exer:8.1.4}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu \in \mathbb{R}^1$ is unknown and $\sigma_0^2$ is known. Determine a UMVU estimator of the first quartile $\mu + \sigma_0 z_{0.25}$.
\end{exercise}

\begin{solution}
We have that $\expc(\bar{x} + \sigma_0 z_{.25}) = \expc(\bar{x}) + \sigma_0 z_{.25} = \mu + \sigma_0 z_{.25}$. Since $\bar{x}$ is complete this implies that $\bar{x} + \sigma_0 z_{.25}$ is UMVU.
\end{solution}

\begin{exercise}
\label{exer:8.1.5}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu \in \mathbb{R}^1$ is unknown and $\sigma_0^2$ is known. Is $2\bar{x} + 3$ a UMVU estimator of anything? If so, what is it UMVU for? Justify your answer.
\end{exercise}

\begin{solution}
This is a UMVU estimator of $5 + 2\mu$.
\end{solution}

\begin{exercise}
\label{exer:8.1.6}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Bernoulli$(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown. Determine a UMVU estimator of $\theta$ (use the fact that a minimal sufficient statistic for this model is complete).
\end{exercise}

\begin{solution}
We have that $\expc(\bar{x}) = \theta$ and since $\bar{x}$ is complete it is UMVU for $\theta$.
\end{solution}

\begin{exercise}
\label{exer:8.1.7}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Gamma$(\alpha_0, \lambda)$ distribution, where $\alpha_0$ is known and $\lambda > 0$ is unknown. Using the fact that $\bar{x}$ is a complete sufficient statistic (see Challenge~\ref{exer:8.1.27}), determine a UMVU estimator of $\lambda^{-1}$.
\end{exercise}

\begin{solution}
We have that the mean of a $\text{Gamma}(\alpha_0, \beta)$ random variable is given by
\[
\expc(X) = \int_0^{\infty} x \frac{(\beta x)^{\alpha_0 - 1}}{\Gamma(\alpha_0)} e^{-\beta x} \beta \, \mathrm{d}x = \frac{1}{\beta\Gamma(\alpha_0)} \int_0^{\infty} (\beta x)^{\alpha_0} e^{-\beta x} \beta \, \mathrm{d}x = \frac{\Gamma(\alpha_0 + 1)}{\beta\Gamma(\alpha_0)} = \frac{\alpha_0}{\beta}.
\]
Therefore, $\bar{x}/\alpha_0$ is an unbiased estimator of $\beta^{-1}$, and since $\bar{x}$ is complete, this implies that $\bar{x}/\alpha_0$ is UMVU.
\end{solution}

\begin{exercise}
\label{exer:8.1.8}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu_0, \sigma^2)$ distribution, where $\mu_0$ is known and $\sigma^2 > 0$ is unknown. Show that $\sum_{i=1}^{n} (x_i - \mu_0)^2$ is a sufficient statistic for this problem. Using the fact that it is complete, determine a UMVU estimator for $\sigma^2$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(x_1, \ldots, x_n \mid \sigma^2) = \sigma^{-2n} \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i - \mu_0)^2\right\}$. By factorization (Theorem \ref{thm:6.1.1}), $\sum_{i=1}^{n}(x_i - \mu_0)^2$ is sufficient. Further, $\expc_{\sigma^2}\left(\sum_{i=1}^{n}(x_i - \mu_0)^2\right) = n\sigma^2$, so $n^{-1}\sum_{i=1}^{n}(x_i - \mu_0)^2$ is unbiased for $\sigma^2$. Since this sufficient statistic is complete, we have that $n^{-1}\sum_{i=1}^{n}(x_i - \mu_0)^2$ is UMVU for $\sigma^2$.
\end{solution}

\begin{exercise}
\label{exer:8.1.9}
Suppose a statistical model comprises all continuous distributions on $\mathbb{R}^1$. Based on a sample of $n$, determine a UMVU estimator of $\prb((-1, 1))$, where $\prb$ is the true probability measure. Justify your answer.
\end{exercise}

\begin{solution}
The parameter of interest is $\psi = \prb((-1, 1))$. The statistic $\indc_{(-1,1)}(X_1)$ is unbiased because $\expc[\indc_{(-1,1)}(X_1)] = \prb(X_1 \in (-1, 1)) = \psi$. Example \ref{ex:8.1.5} says $U = (X_{(1)}, \ldots, X_{(n)})$ is a complete minimal sufficient for the model. Hence, $T = \expc[\indc_{(-1,1)}(X_1) \mid U]$ is the UMVU estimator by Theorem \ref{thm:8.1.5}. By symmetry $\expc[\indc_{(-1,1)}(X_1) \mid U] = \cdots = \expc[\indc_{(-1,1)}(X_n) \mid U]$. Since $\sum_{i=1}^{n} \indc_{(-1,1)}(X_{(i)}) = \expc\left[\sum_{i=1}^{n} \indc_{(-1,1)}(X_{(i)}) \mid U\right] = \expc\left[\sum_{i=1}^{n} \indc_{(-1,1)}(X_i) \mid U\right] = n\expc[\indc_{(-1,1)}(X_1) \mid U] = nT$, the UMVU estimator $T$ is $n^{-1}\sum_{i=1}^{n} \indc_{(-1,1)}(X_i)$.
\end{solution}

\begin{exercise}
\label{exer:8.1.10}
Suppose a statistical model comprises all continuous distributions on $\mathbb{R}^1$ that have a finite second moment. Based on a sample of $n$, determine a UMVU estimator of $\mu^2$, where $\mu$ is the true mean. Justify your answer. (Hint: Find an unbiased estimator for $n = 2$, Rao--Blackwellize this estimator for a sample of $n$, and then use the Lehmann--Scheff\'e theorem.)
\end{exercise}

\begin{solution}
Let $X_1, \ldots, X_n$ be a random sample. The parameter of interest is $\psi = \mu^2$ and $\expc[X_1 X_2] = \expc[X_1]\expc[X_2] = \mu^2$. Hence, $X_1 X_2$ is an unbiased estimator of $\psi = \mu^2$. As in Example \ref{ex:8.1.5}, the order statistic $U = (X_{(1)}, \ldots, X_{(n)})$ is complete and sufficient. By the Lehmann--Scheff\'e theorem, $T = \expc[X_1 X_2 \mid U]$ is UMVU. Then via symmetry
\begin{align*}
T &= \binom{n}{2}^{-1} \sum_{i < j} X_i X_j = \frac{1}{2}\binom{n}{2}^{-1} \sum_{i \neq j} X_i X_j = \frac{1}{2}\binom{n}{2}^{-1} \sum_{i=1}^{n} X_i(n\bar{X} - X_i) \\
&= \frac{1}{n(n-1)}\left[(X_1 + \cdots + X_n)^2 - (X_1^2 + \cdots + X_n^2)\right]
\end{align*}
is the UMVU estimator.
\end{solution}

\begin{exercise}
\label{exer:8.1.11}
The estimator determined in Exercise~\ref{exer:8.1.10} is also unbiased for $\mu^2$ when the statistical model comprises all continuous distributions on $\mathbb{R}^1$ that have a finite first moment. Is this estimator still UMVU for $\mu^2$?
\end{exercise}

\begin{solution}
Yes, $T$ will still be UMVU because it is the only unbiased estimator, due to completeness of the order statistic in the family of all continuous distributions with first moment.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:8.1.12}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Uniform$[0, \theta]$ distribution, where $\theta > 0$ is unknown. Show that $x_{(n)}$ is a sufficient statistic and determine its distribution. Using the fact that $x_{(n)}$ is complete, determine a UMVU estimator of $\theta$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(x_1, \ldots, x_n \mid \theta) = \theta^{-n}$ whenever $\theta > x_{(n)}$ and 0 otherwise. Therefore, when we know $x_{(n)}$ we know the likelihood function and so $x_{(n)}$ is sufficient. Then $x_{(n)}$ has density given by $\theta^{-n} n x^{n-1}$ for $0 < x < \theta$ and $\expc(x_{(n)}) = \int_0^{\theta} \theta^{-n} n x^n \, \mathrm{d}x = \theta^{-n} \frac{n}{n+1} x^{n+1}\big|_0^{\theta} = \frac{n}{n+1}\theta$. So $\frac{n+1}{n}x_{(n)}$ is UMVU for $\theta$.
\end{solution}

\begin{exercise}
\label{exer:8.1.13}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Bernoulli$(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown. Then determine the conditional distribution of $(x_1, \ldots, x_n)$, given the value of the sufficient statistic $\bar{x}$.
\end{exercise}

\begin{solution}
We have that the joint conditional probability function of $(x_1, \ldots, x_n)$ given $n\bar{x}$ is
\[
\prod_{i=1}^{n} \theta^{x_i}(1-\theta)^{1-x_i} \bigg/ \left\{\binom{n}{n\bar{x}} \theta^{n\bar{x}}(1-\theta)^{n - n\bar{x}}\right\} = 1\bigg/\binom{n}{n\bar{x}}.
\]
This is the uniform distribution on the set of all sequences of 0's and 1's of length $n$ that have $n\bar{x}$ 1's.
\end{solution}

\begin{exercise}
\label{exer:8.1.14}
Prove that $L(\theta, a) = (\theta - a)^2$ satisfies
\[
L(\theta, \lambda a_1 + (1 - \lambda)a_2) \leqslant \lambda L(\theta, a_1) + (1 - \lambda) L(\theta, a_2)
\]
when $a$ ranges in a subinterval of $\mathbb{R}^1$. Use this result together with Jensen's inequality (Theorem \ref{thm:3.6.4}) to prove the Rao--Blackwell theorem.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
(\theta - \alpha a_1 - (1-\alpha)a_2)^2 &= \theta^2 - 2\theta(\alpha a_1 + (1-\alpha)a_2) + (\alpha a_1 + (1-\alpha)a_2)^2 \\
&= \alpha(\theta - a_1)^2 + (1-\alpha)(\theta - a_2)^2 - \alpha a_1^2 - (1-\alpha)a_2^2 + (\alpha a_1 + (1-\alpha)a_2)^2 \\
&= \alpha(\theta - a_1)^2 + (1-\alpha)(\theta - a_2)^2 - \alpha(1-\alpha)a_1^2 - \alpha(1-\alpha)a_2^2 + \alpha(1-\alpha)2a_1 a_2 \\
&= \alpha(\theta - a_1)^2 + (1-\alpha)(\theta - a_2)^2 - \alpha(1-\alpha)(a_1 - a_2)^2 \\
&\leqslant \alpha(\theta - a_1)^2 + (1-\alpha)(\theta - a_2)^2.
\end{align*}
Then by Jensen's inequality we have that $\text{MSE}_\theta(T) = \expc_\theta((T - \psi(\theta))^2) = \expc_\theta(\expc_{\prb(\cdot \mid U)}(T - \psi(\theta))^2) \geqslant \expc_\theta((\expc_{\prb(\cdot \mid U)}(T) - \psi(\theta))^2) = \text{MSE}_\theta(T_U)$.
\end{solution}

\begin{exercise}
\label{exer:8.1.15}
Prove that $L(\theta, a) = |\theta - a|$ satisfies
\[
L(\theta, \lambda a_1 + (1 - \lambda)a_2) \leqslant \lambda L(\theta, a_1) + (1 - \lambda)L(\theta, a_2)
\]
when $a$ ranges in a subinterval of $\mathbb{R}^1$. Use this result together with Jensen's inequality (Theorem \ref{thm:3.6.4}) to prove the Rao--Blackwell theorem for absolute error. (Hint: First show that $|x + y| \leqslant |x| + |y|$ for any $x$ and $y$.)
\end{exercise}

\begin{solution}
We have that
\[
|x + y| = \begin{cases} x + y & x + y \geqslant 0 \\ -(x + y) & x + y \leqslant 0 \end{cases} \leqslant |x| + |y|
\]
for any $x, y$. Therefore,
\begin{align*}
|\theta - \alpha a_1 - (1-\alpha)a_2| &= |\alpha(\theta - a_1) + (1-\alpha)(\theta - a_2)| \\
&\leqslant |\alpha(\theta - a_1)| + |(1-\alpha)(\theta - a_2)| = \alpha|\theta - a_1| + (1-\alpha)|\theta - a_2|.
\end{align*}
Then by Jensen's inequality $\expc_\theta(|T - \psi(\theta)|) = \expc_\theta(\expc_{\prb(\cdot \mid U)}(|T - \psi(\theta)|)) \geqslant \expc_\theta(|\expc_{\prb(\cdot \mid U)}(T) - \psi(\theta)|) = \expc_\theta(|T_U - \psi(\theta)|)$.
\end{solution}

\begin{exercise}
\label{exer:8.1.16}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma^2)$ distribution, where $(\mu, \sigma^2) \in \mathbb{R}^1 \times (0, \infty)$ is unknown. Show that the optimal estimator (in the sense of minimizing the MSE), of the form $cs^2$ for $\sigma^2$, is given by $c = (n-1)/(n+1)$. Determine the bias of this estimator and show that it goes to 0 as $n \to \infty$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
\text{MSE}_{(\mu, \sigma^2)}(cs^2) &= \expc_{(\mu, \sigma^2)}\left((cs^2 - \sigma^2)^2\right) = \sigma^4 \expc_{(\mu, \sigma^2)}\left(\left(\frac{c}{n-1}X - 1\right)^2\right) \\
&= \sigma^4 \left(\left(\frac{c}{n-1}\right)^2 \expc_{(\mu, \sigma^2)}(X^2) - \frac{2c}{n-1}\expc_{(\mu, \sigma^2)}(X) + 1\right)
\end{align*}
where $X = (n-1)s^2/\sigma^2 \sim \chi^2(n-1)$. So $\expc_{(\mu, \sigma^2)}(X) = n - 1$ and $\var_{(\mu, \sigma^2)}(X) = 2(n-1)$, which implies $\expc_{(\mu, \sigma^2)}(X^2) = 2(n-1) + (n-1)^2$. Differentiating the above expression with respect to $c$, and setting the derivative equal to 0, gives that the optimal value satisfies
\[
\frac{c}{(n-1)^2}\expc_{(\mu, \sigma^2)}(X^2) - \frac{1}{n-1}\expc_{(\mu, \sigma^2)}(X) = 0
\]
or
\[
c = \frac{(n-1)\expc_{(\mu, \sigma^2)}(X)}{\expc_{(\mu, \sigma^2)}(X^2)} = \frac{(n-1)(n-1)}{2(n-1) + (n-1)^2} = \frac{n-1}{n+1}.
\]
We have that the bias equals
\[
\expc_{(\mu, \sigma^2)}(cs^2) - \sigma^2 = (c - 1)\sigma^2 = \left(\frac{n-1}{n+1} - 1\right)\sigma^2 = -\frac{2\sigma^2}{n+1}.
\]
\end{solution}

\begin{exercise}
\label{exer:8.1.17}
Prove that if a statistic $T$ is complete for a model and $U = h(T)$ for a 1--1 function $h$, then $U$ is also complete.
\end{exercise}

\begin{solution}
Suppose that $c$ is a function such that $\expc_\theta(c(U)) = 0$ for every $\theta$. Then $\expc_\theta(c(h(T))) = 0$ for every $\theta$, and the completeness of $T$ implies that $\prb_\theta(\{s : c(h(T(s))) = 0\}) = 1$ for every $\theta$. Now suppose $u$ is such that $c(u) \neq 0$. Then $\prb_\theta(U = u) = \prb_\theta(h(T) = u) = \prb_\theta(T = h^{-1}(u)) = \prb_\theta(\{s : T(s) = h^{-1}(u)\}) = 0$ since $c(h(T(s))) = c(u)$ for $s$ in $\{s : T(s) = h^{-1}(u)\}$. This implies that $U$ is complete.
\end{solution}

\begin{exercise}
\label{exer:8.1.18}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma^2)$ distribution, where $(\mu, \sigma^2) \in \mathbb{R}^1 \times (0, \infty)$ is unknown. Derive a UMVU estimator of the standard deviation $\sigma$. (Hint: Calculate the expected value of the sample standard deviation $s$.)
\end{exercise}

\begin{solution}
We have that $X = (n-1)s^2/\sigma^2 \sim \chi^2(n-1) = \text{Gamma}((n-1)/2, 1/2)$ and so
\begin{align*}
\expc(X^{1/2}) &= \frac{1}{\Gamma\left(\frac{n-1}{2}\right)} \int_0^{\infty} x^{1/2} \left(\frac{x}{2}\right)^{\frac{n-1}{2} - 1} e^{-x/2} \frac{1}{2} \, \mathrm{d}x \\
&= \frac{2^{1/2}}{\Gamma\left(\frac{n-1}{2}\right)} \int_0^{\infty} \left(\frac{x}{2}\right)^{\frac{n}{2} - 1} e^{-x/2} \frac{1}{2} \, \mathrm{d}x = \frac{2^{1/2}\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}.
\end{align*}
Therefore, $(n-1)^{1/2}\Gamma\left(\frac{n-1}{2}\right)s / 2^{1/2}\Gamma\left(\frac{n}{2}\right)$ is an unbiased estimator of $\sigma$. Since it is a function of the complete sufficient statistic, it is UMVU.
\end{solution}

\begin{exercise}
\label{exer:8.1.19}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma^2)$ distribution, where $(\mu, \sigma^2) \in \mathbb{R}^1 \times (0, \infty)$ is unknown. Derive a UMVU estimator of the first quartile $\mu + \sigma z_{0.25}$. (Hint: Problem~\ref{exer:8.1.17}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:8.1.18} we have that $\bar{x} + (n-1)^{1/2}\Gamma\left(\frac{n-1}{2}\right)s z_{.25} / 2^{1/2}\Gamma\left(\frac{n}{2}\right)$ is an unbiased estimator of the first quartile. Since it is a function of a complete sufficient statistic, it is UMVU.
\end{solution}

\begin{exercise}
\label{exer:8.1.20}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu \in \Omega = \{\mu_1, \mu_2\}$ is unknown and $\sigma_0^2 > 0$ is known. Establish that $\bar{x}$ is a minimal sufficient statistic for this model but that it is not complete.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(x_1, \ldots, x_n \mid \mu) = \exp\left\{-n(\bar{x} - \mu)^2/2\sigma_0^2\right\}$ for $\mu \in \{\mu_1, \mu_2\}$. Clearly, given $\bar{x}$ we can determine the likelihood function so $\bar{x}$ is sufficient. Now the log-likelihood function takes the values $-n(\bar{x} - \mu_1)^2/2\sigma_0^2$ and $-n(\bar{x} - \mu_2)^2/2\sigma_0^2$, which give
\[
-\frac{n(\bar{x} - \mu_1)^2}{2\sigma_0^2} + \frac{n(\bar{x} - \mu_2)^2}{2\sigma_0^2} = \frac{n\bar{x}(\mu_1 - \mu_2)}{\sigma_0^2} - \frac{n(\mu_1^2 - \mu_2^2)}{2\sigma_0^2},
\]
so we can determine $\bar{x}$ from the likelihood, and $\bar{x}$ is a minimal sufficient statistic.

Now supposing $\mu_1 < \mu_2$, we have that
\begin{align*}
\prb_\mu(\mu_1 < \bar{x} < \mu_2) &= \prb_\mu\left(\frac{\mu_1 - \mu}{\sigma_0/\sqrt{n}} < \frac{\bar{x} - \mu}{\sigma_0/\sqrt{n}} < \frac{\mu_2 - \mu}{\sigma_0/\sqrt{n}}\right) \\
&= \Phi\left(\frac{\mu_2 - \mu}{\sigma_0/\sqrt{n}}\right) - \Phi\left(\frac{\mu_1 - \mu}{\sigma_0/\sqrt{n}}\right) \\
&= \begin{cases} \Phi\left(\frac{\mu_2 - \mu_1}{\sigma_0/\sqrt{n}}\right) - \Phi(0) & \mu = \mu_1 \\ \Phi(0) - \Phi\left(\frac{\mu_1 - \mu_2}{\sigma_0/\sqrt{n}}\right) & \mu = \mu_2 \end{cases}
\end{align*}
and, since $\Phi\left(\frac{\mu_2 - \mu_1}{\sigma_0/\sqrt{n}}\right) = 1 - \Phi\left(\frac{\mu_1 - \mu_2}{\sigma_0/\sqrt{n}}\right)$, we see that this probability is independent of $\mu \in \{\mu_1, \mu_2\}$. Now $\indc_{(\mu_1, \mu_2)}(\bar{x})$ is unbiased for $\prb_\mu(\mu_1 < \bar{x} < \mu_2)$ and therefore $\indc_{(\mu_1, \mu_2)}(\bar{x}) - \prb_\mu(\mu_1 < \bar{x} < \mu_2)$ is an unbiased estimator of 0 that is not 0 with probability 1. Therefore, $\bar{x}$ is not complete.
\end{solution}

\begin{exercise}
\label{exer:8.1.21}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu \in \mathbb{R}^1$ is unknown and $\sigma_0^2$ is known. Determine the information lower bound, for an unbiased estimator, when we consider estimating the second moment $\mu^2 + \sigma_0^2$. Does the UMVU estimator in Exercise~\ref{exer:8.1.3} attain the information lower bound?
\end{exercise}

\begin{solution}
The log-likelihood function is given by $-n(\bar{x} - \mu)^2/2\sigma_0^2$, so $S(\mu \mid x_1, \ldots, x_n) = n(\bar{x} - \mu)/\sigma_0^2$. Then $S'(\mu \mid x_1, \ldots, x_n) = -n/\sigma_0^2$, so $I(\mu) = n/\sigma_0^2$. Since $\psi(\mu) = \mu^2 + \sigma_0^2$, then $\psi'(\mu) = 2\mu$ and the information lower bound for an unbiased estimator is given by $4\mu^2\sigma_0^2/n$. The estimator that obtains this lower bound is given by $\mu^2 + \sigma_0^2 - 2\mu\sigma_0^2 \cdot \frac{n(\bar{x} - \mu)}{n\sigma_0^2} = -2\mu\bar{x} + 3\mu^2 + \sigma_0^2$, which is not equal to the UMVU estimator obtained in Exercise \ref{exer:8.1.3}. Therefore, the UMVU estimator cannot obtain the lower bound.
\end{solution}

\begin{exercise}
\label{exer:8.1.22}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Gamma$(\alpha_0, \lambda)$ distribution, where $\alpha_0$ is known and $\lambda > 0$ is unknown. Determine the information lower bound for the estimation of $\lambda^{-1}$ using unbiased estimators, and determine if the UMVU estimator obtained in Exercise~\ref{exer:8.1.7} attains this.
\end{exercise}

\begin{solution}
The log-likelihood function is given by $l(\beta \mid x_1, \ldots, x_n) = n\alpha_0\ln\beta - \beta n\bar{x}$, so $S(\beta \mid x_1, \ldots, x_n) = n\alpha_0/\beta - n\bar{x}$, $S'(\beta \mid x_1, \ldots, x_n) = -n\alpha_0/\beta^2$, which implies $I(\beta) = n\alpha_0/\beta^2$. Since $\psi(\beta) = \beta^{-1}$, $\psi'(\beta) = -\beta^{-2}$ the information lower bound for unbiased estimators is given by $(1/\beta^4)(\beta^2/n\alpha_0) = 1/n\alpha_0\beta^2$. Note that by Exercise \ref{exer:8.1.7} $\bar{x}/\alpha_0$ is UMVU for $\beta^{-1}$ and this has variance $\alpha_0/n\alpha_0^2\beta^2 = 1/n\alpha_0\beta^2$, which is the Cram\'er--Rao lower bound.
\end{solution}

\begin{exercise}
\label{exer:8.1.23}
Suppose that $(x_1, \ldots, x_n)$ is a sample from the distribution with density $f_\theta(x) = \theta x^{\theta-1}$ for $x \in [0, 1]$ and $\theta > 0$ is unknown. Determine the information lower bound for estimating $\theta$ using unbiased estimators. Does a UMVU estimator with variance at the lower bound exist for this problem?
\end{exercise}

\begin{solution}
The log-likelihood function is given by $l(\theta \mid x_1, \ldots, x_n) = n\ln\theta + (\theta - 1)\sum_{i=1}^{n}\ln x_i$, so $S(\theta \mid x_1, \ldots, x_n) = n/\theta + \sum_{i=1}^{n}\ln x_i$, $S'(\theta \mid x_1, \ldots, x_n) = -n/\theta^2$, which implies $I(\theta) = n/\theta^2$. Since $\psi(\theta) = \theta$, $\psi'(\theta) = 1$ the information lower bound for unbiased estimators is given by $\theta^2/n$. This is attained by the estimator $\theta + \frac{\theta^2}{n}\left(\frac{n}{\theta} + \sum_{i=1}^{n}\ln x_i\right) = 2\theta + \frac{\theta^2}{n}\sum_{i=1}^{n}\ln x_i$. Since this depends on $\theta$, this implies that any UMVU estimator, if it exists, cannot have variance at the lower bound.
\end{solution}

\begin{exercise}
\label{exer:8.1.24}
Suppose that a statistic $T$ is a complete statistic based on some statistical model. A submodel is a statistical model that comprises only some of the distributions in the original model. Why is it not necessarily the case that $T$ is complete for a submodel?
\end{exercise}

\begin{solution}
The definition of completeness is $\expc_\theta[g(T)] = 0$ for all $\theta \in \Omega$ implies $\prb_\theta(g(T) = 0) = 1$ for all $\theta \in \Omega$. To be a complete statistic for a submodel $\Omega_0 \subset \Omega$, $T$ must satisfy that $\expc_\theta[g(T)] = 0$ for all $\theta \in \Omega_0$ implies $\prb_\theta(g(T) = 0) = 1$ for all $\theta \in \Omega_0$. Hence, the restriction is shrunken from $\Omega$ to $\Omega_0$. This smaller restriction may cause incompleteness of $T$. For example $T = (\bar{X}, S^2)$ is complete for $N(\mu, \sigma^2)$ model as in Example \ref{ex:8.1.4}. If we consider the model $\Omega_0 = \{N(\theta, \theta^2) : \theta > 0\} \subset \Omega$, the statistic $T$ is not complete because $\expc_\theta[n\bar{X}^2 - (n+1)S^2] = 0$ even though $\prb_\theta(n\bar{X}^2 - (n+1)S^2 = 0) = 0$.
\end{solution}

\begin{exercise}
\label{exer:8.1.25}
Suppose that a statistic $T$ is a complete statistic based on some statistical model. If we construct a larger model that contains all the distributions in the original model and is such that any set that has probability content equal to 0 for every distribution in the original model also has probability content equal to 0 for every distribution in the larger model, then prove that $T$ is complete for the larger model as well.
\end{exercise}

\begin{solution}
Let $\Omega_0$ be a submodel of $\Omega$. Assume that for a Borel set $B$, $\prb_\theta(B) = 0$ for $\theta \in \Omega$ if $\prb_\theta(B) = 0$ for $\theta \in \Omega_0$. Suppose $T$ is a complete statistic for the submodel $\Omega_0$. Suppose there is a function $g$ such that $\expc_\theta[g(T)] = 0$ for all $\theta \in \Omega$. Since $\Omega_0 \subset \Omega$, $\expc_\theta[g(T)] = 0$ for all $\theta \in \Omega_0$. The completeness of $T$ in $\Omega_0$ implies $\prb_\theta(g(T) = 0) = 1$ for all $\theta \in \Omega_0$. Let $B = \{g(T) \neq 0\}$. Then $\prb_\theta(B) = 0$ for all $\theta \in \Omega_0$. Thus, $\prb_\theta(B) = 0$ for all $\theta \in \Omega$ by the assumption. Therefore $T$ is complete for the model $\Omega$ as well.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:8.1.26}
If $X$ is a random variable such that $\expc(X)$ either does not exist or is infinite, then show that $\expc((X - c)^2) = \infty$ for any constant $c$.
\end{exercise}

\begin{solution}
We can assume that $c = 0$ (or else replace $X$ by $Y = X - c$). We have that $X(s) = X(s)\indc_{(-\infty, -1)}(X(s)) + X(s)\indc_{[-1,1]}(X(s)) + X(s)\indc_{(1,\infty)}(X(s))$. Then $\expc(X) = \expc(X\indc_{(-\infty,-1)}(X)) + \expc(X\indc_{[-1,1]}(X)) + \expc(X\indc_{(1,\infty)}(X))$ and $-1 \leqslant \expc(X\indc_{[-1,1]}(X)) \leqslant 1$. Also, $\expc(X^2) = \expc(X^2 \indc_{(-\infty,-1)}(X)) + \expc(X^2 \indc_{[-1,1]}(X)) + \expc(X^2 \indc_{(1,\infty)}(X))$, and each term in this sum is nonnegative (possibly infinite).

Now suppose $\expc(X\indc_{(1,\infty)}(X)) = \infty$. Then, when $X > 1$, we have $X^2 > X$ and so $\expc(X^2 \indc_{(1,\infty)}(X)) \geqslant \expc(X\indc_{(1,\infty)}(X)) = \infty$. If $\expc(X\indc_{(-\infty,-1)}(X)) = -\infty$ then, when $X < -1$, we have $-X^2 < X$, so $\expc(-X^2 \indc_{(-\infty,-1)}(X)) \leqslant \expc(X\indc_{(-\infty,-1)}(X)) = -\infty$, which implies $\expc(X^2 \indc_{(-\infty,-1)}(X)) = \infty$. In both cases, we have that $\expc(X^2) = \infty$.
\end{solution}

\begin{exercise}
\label{exer:8.1.27}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Gamma$(\alpha_0, \lambda)$ distribution, where $\alpha_0$ is known and $\lambda > 0$ is unknown. Show that $\bar{x}$ is a complete minimal sufficient statistic.
\end{exercise}

\begin{solution}
The log-likelihood function is given by $l(\beta \mid x_1, \ldots, x_n) = n\alpha_0\ln\beta - \beta n\bar{x}$, so $\bar{x}$ determines the likelihood and as such is sufficient. Now $S(\beta \mid x_1, \ldots, x_n) = n\alpha_0/\beta - n\bar{x}$, and, setting the score function equal to 0, we obtain the MLE of $\beta$ as $\alpha_0/\bar{x}$, so we can also obtain $\bar{x}$ from the likelihood. This implies that $\bar{x}$ is minimal sufficient.

We know that $\bar{x} \sim \text{Gamma}(n\alpha_0, n\beta)$. We will present the argument when $n = 1$ and the proof is the same for $n > 1$.

Now suppose $h$ is such that $\expc_\beta(h(X)) = 0$ for every $\beta > 0$. Suppose that $h \geqslant 0$. Then this implies that $\int_0^{\infty} h(x)x^{\alpha - 1}e^{-\beta x} \, \mathrm{d}x = 0$ for any $\beta$, but since the integrand is nonnegative, this can only happen when $h(x) = 0$. Similarly if $h \leqslant 0$.

Now write $h = h^+ - h^-$ where $h^+(x) = \max\{0, h(x)\}$, $h^-(x) = \max\{0, -h(x)\}$, and we must have $\int_0^{\infty} h^+(x)x^{\alpha - 1}e^{-\beta x} \, \mathrm{d}x = \int_0^{\infty} h^-(x)x^{\alpha - 1}e^{-\beta x} \, \mathrm{d}x$. This implies $\int_0^{\infty} h^+(x)x^{\alpha - 1}e^{-x} \, \mathrm{d}x = \int_0^{\infty} h^-(x)x^{\alpha - 1}e^{-x} \, \mathrm{d}x$. Now suppose $h^+ > 0$ and $h^- > 0$ on subintervals of $(0, \infty)$. Note they cannot both be nonzero on the same subinterval. Then we have that
\[
\frac{\int_0^{\infty} e^{-(\beta - 1)x}h^+(x)x^{\alpha - 1}e^{-x} \, \mathrm{d}x}{\int_0^{\infty} h^+(x)x^{\alpha - 1}e^{-x} \, \mathrm{d}x} = \frac{\int_0^{\infty} e^{-(\beta - 1)x}h^-(x)x^{\alpha - 1}e^{-x} \, \mathrm{d}x}{\int_0^{\infty} h^-(x)x^{\alpha - 1}e^{-x} \, \mathrm{d}x}
\]
or $m^+(\beta - 1) = m^-(\beta - 1)$ for every $\beta > 1$, where $m^+$ is the mgf of the distribution on $(0, \infty)$ with density given by $h^+(x)x^{\alpha - 1}e^{-x}/\int_0^{\infty} h^+(x)x^{\alpha - 1}e^{-x} \, \mathrm{d}x$, and $m^-$ is the mgf of the distribution on $(0, \infty)$ with density given by $h^-(x)x^{\alpha - 1}e^{-x}/\int_0^{\infty} h^-(x)x^{\alpha - 1}e^{-x} \, \mathrm{d}x$. But the equality of the mgf's implies the equality of the distributions (Theorem \ref{thm:3.4.6}), and these distributions are concentrated on disjoint subsets, so we have a contradiction to the supposition that $h^+ > 0$ and $h^- > 0$ on subintervals of $(0, \infty)$. This implies that $h^+ = h^- = 0$ and so $h = 0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{solution}

\section{Optimal Hypothesis Testing}
\label{sec:8.2}

Suppose we want to assess a hypothesis about the real-valued characteristic $\psi(\theta)$ for the model $\{f_\theta : \theta \in \Omega\}$. Typically, this will take the form $H_0 : \psi(\theta) = \psi_0$, where we have specified a value for $\psi$. After observing data $s$, we want to assess whether or not we have evidence against $H_0$.

In Section \ref{ssec:6.3.3}, we discussed methods for assessing such a hypothesis based on the plug-in MLE for $\psi(\theta)$. These involved computing a P-value as a measure of how surprising the data $s$ are when the null hypothesis is assumed to be true. If $s$ is surprising for each of the distributions $f_\theta$ for which $\psi(\theta) = \psi_0$, then we have evidence against $H_0$. The development of such procedures was largely based on the intuitive justification for the likelihood function.

\subsection{The Power Function of a Test}
\label{ssec:8.2.1}

Closely associated with a specific procedure for computing a P-value is the concept of a power function $\beta(\theta)$, as defined in Section \ref{ssec:6.3.6}. For this, we specified a critical value $\alpha$, such that we declare the results of the test statistically significant whenever the P-value is less than or equal to $\alpha$. The power $\beta(\theta)$ is then the probability of the P-value being less than or equal to $\alpha$ when we are sampling from $f_\theta$. The greater the value of $\beta(\theta)$, when $\psi(\theta) \neq \psi_0$, the better the procedure is at detecting departures from $H_0$. The power function is thus a measure of the sensitivity of the testing procedure to detecting departures from $H_0$.

Recall the following fundamental example.

\begin{example}[Location Normal Model]
\label{ex:8.2.1}
Suppose we have a sample $(x_1, \ldots, x_n)$ from the $\mathrm{N}(\mu, \sigma_0^2)$ model, where $\mu \in \mathbb{R}^1$ is unknown and $\sigma_0^2 > 0$ is known, and we want to assess the null hypothesis $H_0 : \mu = \mu_0$. In Example \ref{ex:6.3.9}, we showed that a sensible test for this problem is based on the $z$-statistic
\[
z = \frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}},
\]
with $Z \sim \mathrm{N}(0, 1)$ under $H_0$. The P-value is then given by
\[
\prb_{\mu_0}\left(|Z| \geqslant \left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right) = 2\left(1 - \Phi\left(\left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right)\right),
\]
where $\Phi$ denotes the $\mathrm{N}(0, 1)$ distribution function.

In Example \ref{ex:6.3.18}, we showed that, for critical value $\alpha$, the power function of the $z$-test is given by
\begin{align*}
\beta(\mu) &= \prb_\mu\left(2\left(1 - \Phi\left(\left|\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right)\right) < \alpha\right) \\
&= \prb_\mu\left(\Phi\left(\left|\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right) > 1 - \frac{\alpha}{2}\right) \\
&= 1 - \Phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-(\alpha/2)}\right) + \Phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - z_{1-(\alpha/2)}\right)
\end{align*}
because $\bar{X} \sim \mathrm{N}(\mu, \sigma_0^2/n)$.

We see that specifying a value for $\alpha$ specifies a set of data values
\[
R = \left\{(x_1, \ldots, x_n) : \Phi\left(\left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right) > 1 - \frac{\alpha}{2}\right\}
\]
such that the results of the test are determined to be statistically significant whenever $(x_1, \ldots, x_n) \in R$. Using the fact that $\Phi$ is 1--1 increasing, we can also write $R$ as
\[
R = \left\{(x_1, \ldots, x_n) : \left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right| > \Phi^{-1}\left(1 - \frac{\alpha}{2}\right)\right\} = \left\{(x_1, \ldots, x_n) : \left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right| > z_{1-(\alpha/2)}\right\}.
\]
Furthermore, the power function is given by $\beta(\mu) = \prb_\mu(R)$ and $\beta(\mu_0) = \prb_{\mu_0}(R) = \alpha$.
\end{example}

\subsection{Type I and Type II Errors}
\label{ssec:8.2.2}

We now adopt a different point of view. We are going to look for tests that are optimal for testing the null hypothesis $H_0 : \psi(\theta) = \psi_0$. First, we will assume that, having observed the data $s$, we will decide to either accept or reject $H_0$. If we reject $H_0$, then this is equivalent to accepting the alternative $H_a : \psi(\theta) \neq \psi_0$. Our performance measure for assessing testing procedures will then be the probability that the testing procedure makes an error.

There are two types of error. We can make a type I error---rejecting $H_0$ when it is true---or make a type II error---accepting $H_0$ when $H_0$ is false. Note that if we reject $H_0$, then this implies that we are accepting the alternative hypothesis $H_a : \psi(\theta) \neq \psi_0$.

It turns out that, except in very artificial circumstances, there are no testing procedures that simultaneously minimize the probabilities of making the two kinds of errors. Accordingly, we will place an upper bound $\alpha$, called the critical value, on the probability of making a type I error. We then search among those tests whose probability of making a type I error is less than or equal to $\alpha$, for a testing procedure that minimizes the probability of making a type II error.

Sometimes hypothesis testing problems for real-valued parameters are distinguished as being one-sided or two-sided. For example, if $\theta$ is real-valued, then $H_0 : \theta = \theta_0$ versus $H_a : \theta \neq \theta_0$ is a two-sided testing problem, while $H_0 : \theta \leqslant \theta_0$ versus $H_a : \theta > \theta_0$ or $H_0 : \theta \geqslant \theta_0$ versus $H_a : \theta < \theta_0$ are examples of one-sided problems. Notice, however, that if we define
\[
\psi(\theta) = \indc_{(\theta_0, \infty)}(\theta),
\]
then $H_0 : \theta \leqslant \theta_0$ versus $H_a : \theta > \theta_0$ is equivalent to the problem $H_0 : \psi(\theta) = 0$ versus $H_a : \psi(\theta) \neq 0$. Similarly, if we define
\[
\psi(\theta) = \indc_{(-\infty, \theta_0)}(\theta),
\]
then $H_0 : \theta \geqslant \theta_0$ versus $H_a : \theta < \theta_0$ is equivalent to the problem $H_0 : \psi(\theta) = 0$ versus $H_a : \psi(\theta) \neq 0$. So the formulation we have adopted for testing problems about a general $\psi$ includes the one-sided problems as special cases.

\subsection{Rejection Regions and Test Functions}
\label{ssec:8.2.3}

One approach to specifying a testing procedure is to select a subset $R \subset S$ before we observe $s$. We then reject $H_0$ whenever $s \in R$ and accept $H_0$ whenever $s \notin R$. The set $R$ is referred to as a rejection region. Putting an upper bound on the probability of rejecting $H_0$ when it is true leads to the following.

\begin{definition}
\label{def:8.2.1}
A rejection region $R$ satisfying
\begin{equation}
\prb_\theta(R) \leqslant \alpha
\label{eq:8.2.1}
\end{equation}
whenever $\psi(\theta) = \psi_0$ is called a \emph{size $\alpha$ rejection region} for $H_0$.
\end{definition}

So \eqref{eq:8.2.1} expresses the bound on the probability of making a type I error.

Among all size $\alpha$ rejection regions $R$, we want to find the one (if it exists) that will minimize the probability of making a type II error. This is equivalent to finding the size $\alpha$ rejection region $R$ that maximizes the probability of rejecting the null hypothesis when it is false. This probability can be expressed in terms of the power function of $R$ and is given by $\beta(\theta) = \prb_\theta(R)$ whenever $\psi(\theta) \neq \psi_0$.

To fully specify the optimality approach to testing hypotheses, we need one additional ingredient. Observe that our search for an optimal size $\alpha$ rejection region $R$ is equivalent to finding the indicator function $\indc_R$ that satisfies $\beta(\theta) = \expc_\theta(\indc_R) = \prb_\theta(R) \leqslant \alpha$, when $\psi(\theta) = \psi_0$, and maximizes $\beta(\theta) = \expc_\theta(\indc_R) = \prb_\theta(R)$, when $\psi(\theta) \neq \psi_0$. It turns out that, in a number of problems, there is no such rejection region.

On the other hand, there is often a solution to the more general problem of finding a function $\phi : S \to [0, 1]$ satisfying
\begin{equation}
\beta(\theta) = \expc_\theta(\phi) \leqslant \alpha,
\label{eq:8.2.2}
\end{equation}
when $\psi(\theta) = \psi_0$, and maximizes
\[
\beta(\theta) = \expc_\theta(\phi),
\]
when $\psi(\theta) \neq \psi_0$. We have the following terminology.

\begin{definition}
\label{def:8.2.2}
We call $\phi : S \to [0, 1]$ a \emph{test function} and $\beta(\theta) = \expc_\theta(\phi)$ the \emph{power function} associated with the test function $\phi$. If $\phi$ satisfies \eqref{eq:8.2.2} when $\psi(\theta) = \psi_0$, it is called a \emph{size $\alpha$ test function}. If $\phi$ satisfies $\expc_\theta(\phi) = \alpha$ when $\psi(\theta) = \psi_0$, it is called an \emph{exact size $\alpha$ test function}. A size $\alpha$ test function $\phi$ that maximizes $\beta(\theta) = \expc_\theta(\phi)$ when $\psi(\theta) \neq \psi_0$ is called a \emph{uniformly most powerful} (UMP) size $\alpha$ test function.
\end{definition}

Note that $\phi = \indc_R$ is a test function with power function given by $\beta(\theta) = \expc_\theta(\indc_R) = \prb_\theta(R)$.

For observed data $s$, we interpret $\phi(s) = 0$ to mean that we accept $H_0$ and interpret $\phi(s) = 1$ to mean that we reject $H_0$. In general, we interpret $\phi(s)$ to be the conditional probability that we reject $H_0$ given the data $s$. Operationally, this means that, after we observe $s$, we generate a Bernoulli$(\phi(s))$ random variable. If we get a 1, we reject $H_0$; if we get a 0, we accept $H_0$. Therefore, by the theorem of total expectation, $\expc_\theta(\phi)$ is the unconditional probability of rejecting $H_0$. The randomization that occurs when $0 < \phi(s) < 1$ may seem somewhat counterintuitive, but it is forced on us by our search for a UMP size $\alpha$ test, as we can increase power by doing this in certain problems.

\subsection{The Neyman--Pearson Theorem}
\label{ssec:8.2.4}

For a testing problem specified by a null hypothesis $H_0 : \psi(\theta) = \psi_0$ and a critical value $\alpha$, we want to find a UMP size $\alpha$ test function $\phi$. Note that a UMP size $\alpha$ test function $\phi_0$ for $H_0 : \psi(\theta) = \psi_0$ is characterized (letting $\beta_\phi$ denote the power function of $\phi$) by
\[
\beta_{\phi_0}(\theta) \leqslant \alpha,
\]
when $\psi(\theta) = \psi_0$, and by
\[
\beta_{\phi_0}(\theta) \geqslant \beta_\phi(\theta),
\]
when $\psi(\theta) \neq \psi_0$, for any other size $\alpha$ test function $\phi$.

Still, this optimization problem does not have a solution in general. In certain problems, however, an optimal solution can be found. The following result gives one such example. It is fundamental to the entire theory of optimal hypothesis testing.

\begin{theorem}[Neyman--Pearson]
\label{thm:8.2.1}
Suppose that $\Omega = \{\theta_0, \theta_1\}$ and that we want to test $H_0 : \theta = \theta_0$. Then an exact size $\alpha$ test function $\phi_0$ exists of the form
\begin{equation}
\phi_0(s) = \begin{cases}
1 & f_{\theta_1}(s)/f_{\theta_0}(s) > c_0 \\
\gamma & f_{\theta_1}(s)/f_{\theta_0}(s) = c_0 \\
0 & f_{\theta_1}(s)/f_{\theta_0}(s) < c_0
\end{cases}
\label{eq:8.2.3}
\end{equation}
for some $\gamma \in [0, 1]$ and $c_0 \geqslant 0$. This test is UMP size $\alpha$.
\end{theorem}

\begin{proof}
See Section~\ref{sec:8.5} for the proof of this result.
\end{proof}

The following result can be established by a simple extension of the proof of the Neyman--Pearson theorem.

\begin{corollary}
\label{cor:8.2.1}
If $\phi$ is a UMP size $\alpha$ test, then $\phi(s) = \phi_0(s)$ everywhere except possibly on the boundary $B = \{s : f_{\theta_1}(s)/f_{\theta_0}(s) = c_0\}$. Furthermore, $\phi$ has exact size $\alpha$ unless the power of a UMP size $\alpha$ test equals 1.
\end{corollary}

\begin{proof}
See Challenge~\ref{exer:8.2.22}.
\end{proof}

Notice the intuitive nature of the test given by the Neyman--Pearson theorem, for \eqref{eq:8.2.3} indicates that we categorically reject $H_0$ as being true when the likelihood ratio of $\theta_1$ versus $\theta_0$ is greater than the constant $c_0$, and we accept $H_0$ when it is smaller. When the likelihood ratio equals $c_0$, we randomly decide to reject $H_0$ with probability $\gamma$. Also, Corollary~\ref{cor:8.2.1} says that a UMP size $\alpha$ test is basically unique, although there are possibly different randomization strategies on the boundary.

The proof of the Neyman--Pearson theorem reveals that $c_0$ is the smallest real number such that
\begin{equation}
\prb_{\theta_0}\left(\frac{f_{\theta_1}(s)}{f_{\theta_0}(s)} > c_0\right) \leqslant \alpha
\label{eq:8.2.4}
\end{equation}
and
\begin{equation}
\gamma = \begin{cases}
\dfrac{\alpha - \prb_{\theta_0}\left(\frac{f_{\theta_1}(s)}{f_{\theta_0}(s)} > c_0\right)}{\prb_{\theta_0}\left(\frac{f_{\theta_1}(s)}{f_{\theta_0}(s)} = c_0\right)} & \prb_{\theta_0}\left(\frac{f_{\theta_1}(s)}{f_{\theta_0}(s)} = c_0\right) \neq 0 \\[12pt]
0 & \text{otherwise}.
\end{cases}
\label{eq:8.2.5}
\end{equation}
We use \eqref{eq:8.2.4} and \eqref{eq:8.2.5} to calculate $c_0$ and $\gamma$, and so determine the UMP size $\alpha$ test, in a particular problem.

Note that the test is nonrandomized whenever $\prb_{\theta_0}(f_{\theta_1}(s)/f_{\theta_0}(s) > c_0) = \alpha$, as then $\gamma = 0$, i.e., we categorically accept or reject $H_0$ after seeing the data. This always occurs whenever the distribution of $f_{\theta_1}(s)/f_{\theta_0}(s)$ is continuous when $s \sim \prb_{\theta_0}$. Interestingly, it can happen that the distribution of the ratio is not continuous even when the distribution of $s$ is continuous (see Problem~\ref{exer:8.2.17}).

Before considering some applications of the Neyman--Pearson theorem, we establish the analog of the Rao--Blackwell theorem for hypothesis testing problems. Given the value of the sufficient statistic $U(s) = u$, we denote the conditional probability measure for the response $s$ by $\prb(\cdot \mid U = u)$ (by Theorem~\ref{thm:8.1.2}, this probability measure does not depend on $\theta$). For test function $\phi$ put $\phi_U(s)$ equal to the conditional expectation of $\phi$ given the value of $U(s)$; namely,
\[
\phi_U(s) = \expc_{\prb(\cdot \mid U=U(s))}(\phi).
\]

\begin{theorem}
\label{thm:8.2.2}
Suppose that $U$ is a sufficient statistic and $\phi$ is a size $\alpha$ test function for $H_0 : \psi(\theta) = \psi_0$. Then $\phi_U$ is a size $\alpha$ test function for $H_0 : \psi(\theta) = \psi_0$ that depends on the data only through the value of $U$. Furthermore, $\phi$ and $\phi_U$ have the same power function.
\end{theorem}

\begin{proof}
It is clear that $\phi_U(s_1) = \phi_U(s_2)$ whenever $U(s_1) = U(s_2)$, and so $\phi_U$ depends on the data only through the value of $U$. Now let $\prb_{\theta,U}$ denote the marginal probability measure of $U$ induced by $\prb_\theta$. Then by the theorem of total expectation, we have $\expc_\theta(\phi) = \expc_{\prb_{\theta,U}}(\expc_{\prb(\cdot \mid U=u)}(\phi)) = \expc_{\prb_{\theta,U}}(\phi_U) = \expc_\theta(\phi_U)$. Now $\expc_\theta(\phi) \leqslant \alpha$ when $\psi(\theta) = \psi_0$, which implies that $\expc_\theta(\phi_U) \leqslant \alpha$ when $\psi(\theta) = \psi_0$, and $\beta(\theta) = \expc_\theta(\phi) = \expc_\theta(\phi_U)$ when $\psi(\theta) \neq \psi_0$.
\end{proof}

This result allows us to restrict our search for a UMP size $\alpha$ test to those test functions that depend on the data only through the value of a sufficient statistic.

We now consider some applications of the Neyman--Pearson theorem. The following example shows that this result can lead to solutions to much more general problems than the simple case being addressed.

\begin{example}[Optimal Hypothesis Testing in the Location Normal Model]
\label{ex:8.2.2}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu \in \Omega = \{\mu_0, \mu_1\}$ and $\sigma_0^2 > 0$ is known, and we want to test $H_0 : \mu = \mu_0$ versus $H_a : \mu = \mu_1$. The likelihood function is given by
\[
L(\mu \mid x_1, \ldots, x_n) = \exp\left(-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right),
\]
and $\bar{x}$ is a sufficient statistic for this restricted model.

By Theorem~\ref{thm:8.2.2}, we can restrict our attention to test functions that depend on the data through $\bar{x}$. Now $\bar{X} \sim \mathrm{N}(\mu, \sigma_0^2/n)$ so that
\begin{align*}
\frac{f_{\mu_1}(\bar{x})}{f_{\mu_0}(\bar{x})} &= \frac{\exp\left(-\frac{n}{2\sigma_0^2}(\bar{x} - \mu_1)^2\right)}{\exp\left(-\frac{n}{2\sigma_0^2}(\bar{x} - \mu_0)^2\right)} \\
&= \exp\left(-\frac{n}{2\sigma_0^2}\left(\bar{x}^2 - 2\bar{x}\mu_1 + \mu_1^2 - \bar{x}^2 + 2\bar{x}\mu_0 - \mu_0^2\right)\right) \\
&= \exp\left(-\frac{n}{\sigma_0^2}(\mu_1 - \mu_0)\bar{x}\right) \exp\left(-\frac{n}{2\sigma_0^2}(\mu_1^2 - \mu_0^2)\right).
\end{align*}
Therefore,
\begin{align*}
\prb_{\mu_0}\left(\frac{f_{\mu_1}(\bar{X})}{f_{\mu_0}(\bar{X})} > c_0\right) &= \prb_{\mu_0}\left(\exp\left(-\frac{n}{\sigma_0^2}(\mu_1 - \mu_0)\bar{X}\right) \exp\left(-\frac{n}{2\sigma_0^2}(\mu_1^2 - \mu_0^2)\right) > c_0\right) \\
&= \prb_{\mu_0}\left(\exp\left(-\frac{n}{\sigma_0^2}(\mu_1 - \mu_0)\bar{X}\right) > c_0 \exp\left(\frac{n}{2\sigma_0^2}(\mu_1^2 - \mu_0^2)\right)\right) \\
&= \prb_{\mu_0}\left((\mu_1 - \mu_0)\bar{X} > -\frac{\sigma_0^2}{n}\ln\left(c_0 \exp\left(\frac{n}{2\sigma_0^2}(\mu_1^2 - \mu_0^2)\right)\right)\right) \\
&= \begin{cases}
\prb_{\mu_0}\left(\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} > c_0'\right) & \mu_1 > \mu_0 \\[6pt]
\prb_{\mu_0}\left(\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} < c_0'\right) & \mu_1 < \mu_0,
\end{cases}
\end{align*}
where
\[
c_0' = \frac{\sqrt{n}}{\sigma_0}\left(-\frac{\sigma_0^2}{n(\mu_1 - \mu_0)}\ln\left(c_0 \exp\left(\frac{n}{2\sigma_0^2}(\mu_1^2 - \mu_0^2)\right)\right) - \mu_0\right).
\]

Using \eqref{eq:8.2.4}, when $\mu_1 > \mu_0$, we select $c_0$ so that $c_0' = z_{1-\alpha}$; when $\mu_1 < \mu_0$, we select $c_0$ so that $c_0' = z_\alpha$. These choices imply that
\[
\prb_{\mu_0}\left(\frac{f_{\mu_1}(\bar{X})}{f_{\mu_0}(\bar{X})} > c_0\right) = \alpha
\]
and, by \eqref{eq:8.2.5}, $\gamma = 0$.

So the UMP size $\alpha$ test is nonrandomized. When $\mu_1 > \mu_0$, the test is given by
\begin{equation}
\phi_0(\bar{x}) = \begin{cases}
1 & \bar{x} \geqslant \mu_0 + \frac{\sigma_0}{\sqrt{n}} z_{1-\alpha} \\[6pt]
0 & \bar{x} < \mu_0 + \frac{\sigma_0}{\sqrt{n}} z_{1-\alpha}.
\end{cases}
\label{eq:8.2.6}
\end{equation}
When $\mu_1 < \mu_0$, the test is given by
\begin{equation}
\phi_0^*(\bar{x}) = \begin{cases}
1 & \bar{x} \leqslant \mu_0 + \frac{\sigma_0}{\sqrt{n}} z_\alpha \\[6pt]
0 & \bar{x} > \mu_0 + \frac{\sigma_0}{\sqrt{n}} z_\alpha.
\end{cases}
\label{eq:8.2.7}
\end{equation}

Notice that the test function in \eqref{eq:8.2.6} does not depend on $\mu_1$ in any way. The subsequent implication is that this test function is UMP size $\alpha$ for $H_0 : \mu = \mu_0$ versus $H_a : \mu = \mu_1$ for any $\mu_1 > \mu_0$. This implies that $\phi_0$ is UMP size $\alpha$ for $H_0 : \mu = \mu_0$ versus the alternative $H_a : \mu > \mu_0$.

Furthermore, we have
\begin{align*}
\beta_{\phi_0}(\mu) &= \prb_\mu\left(\bar{X} \geqslant \mu_0 + \frac{\sigma_0}{\sqrt{n}} z_{1-\alpha}\right) \\
&= \prb_\mu\left(\frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} \geqslant \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha}\right) \\
&= 1 - \Phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha}\right).
\end{align*}
Note that this is increasing in $\mu$, which implies that $\phi_0$ is a size $\alpha$ test function for $H_0 : \mu \leqslant \mu_0$ versus $H_a : \mu > \mu_0$. Observe that, if $\phi$ is a size $\alpha$ test function for $H_0 : \mu \leqslant \mu_0$ versus $H_a : \mu > \mu_0$, then it is also a size $\alpha$ test for $H_0 : \mu = \mu_0$ versus $H_a : \mu > \mu_0$. From this, we conclude that $\phi_0$ is UMP size $\alpha$ for $H_0 : \mu \leqslant \mu_0$ versus $H_a : \mu > \mu_0$. Similarly (see Problem~\ref{exer:8.2.12}), it can be shown that $\phi_0^*$ in \eqref{eq:8.2.7} is UMP size $\alpha$ for $H_0 : \mu \geqslant \mu_0$ versus $H_a : \mu < \mu_0$.

We might wonder if a UMP size $\alpha$ test exists for the two-sided problem $H_0 : \mu = \mu_0$ versus $H_a : \mu \neq \mu_0$. Suppose that $\phi$ is a size $\alpha$ UMP test for this problem. Then $\phi$ is also size $\alpha$ for $H_0 : \mu = \mu_0$ versus $H_a : \mu = \mu_1$ when $\mu_1 > \mu_0$. Using Corollary~\ref{cor:8.2.1} and the preceding developments (which also shows that there does not exist a test of the form \eqref{eq:8.2.3} having power equal to 1 for this problem), this implies that $\phi = \phi_0$ (the boundary $B$ has probability 0 here). But $\phi$ is also UMP size $\alpha$ for $H_0 : \mu = \mu_0$ versus $H_a : \mu = \mu_1$ when $\mu_1 < \mu_0$; thus, by the same reasoning, $\phi = \phi_0^*$. But clearly $\phi_0 \neq \phi_0^*$, so there is no UMP size $\alpha$ test for the two-sided problem.

Intuitively, we would expect that the size $\alpha$ test given by
\begin{equation}
\phi^*(\bar{x}) = \begin{cases}
1 & \left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right| \geqslant z_{1-\alpha/2} \\[6pt]
0 & \left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right| < z_{1-\alpha/2}
\end{cases}
\label{eq:8.2.8}
\end{equation}
would be a good test to use, but it is not UMP size $\alpha$. It turns out, however, that the test in \eqref{eq:8.2.8} is UMP size $\alpha$ among all tests satisfying $\beta_\phi(\mu_0) \leqslant \alpha$ and $\beta_\phi(\mu) \geqslant \alpha$ when $\mu \neq \mu_0$.
\end{example}

Example~\ref{ex:8.2.2} illustrated a hypothesis testing problem for which no UMP size $\alpha$ test exists. Sometimes, however, by requiring that the test possess another very natural property, we can obtain an optimal test.

\begin{definition}
\label{def:8.2.3}
A test $\phi$ that satisfies $\beta_\phi(\theta) \leqslant \alpha$, when $\psi(\theta) = \psi_0$, and $\beta_\phi(\theta) \geqslant \alpha$, when $\psi(\theta) \neq \psi_0$, is said to be an \emph{unbiased size $\alpha$ test} for the hypothesis testing problem $H_0 : \psi(\theta) = \psi_0$.
\end{definition}

So \eqref{eq:8.2.8} is a UMP unbiased size $\alpha$ test. An unbiased test has the property that the probability of rejecting the null hypothesis, when the null hypothesis is false, is always greater than the probability of rejecting the null hypothesis, when the null hypothesis is true. This seems like a very reasonable property. In particular, it can be proved that any UMP size $\alpha$ is always an unbiased size $\alpha$ test (Problem~\ref{exer:8.2.14}). We do not pursue the theory of unbiased tests further in this text.

We now consider an example which shows that we cannot dispense with the use of randomized tests.

\begin{example}[Optimal Hypothesis Testing in the Bernoulli$(\theta)$ Model]
\label{ex:8.2.3}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Bernoulli$(\theta)$ distribution, where $\theta \in \Omega = \{\theta_0, \theta_1\}$, and we want to test $H_0 : \theta = \theta_0$ versus $H_a : \theta = \theta_1$, where $\theta_1 > \theta_0$. Then $n\bar{x}$ is a minimal sufficient statistic and, by Theorem~\ref{thm:8.2.2}, we can restrict our attention to test functions that depend on the data only through $n\bar{x}$.

Now $n\bar{X} \sim \text{Binomial}(n, \theta)$, so
\[
\frac{f_{\theta_1}(n\bar{x})}{f_{\theta_0}(n\bar{x})} = \frac{\theta_1^{n\bar{x}}(1 - \theta_1)^{n-n\bar{x}}}{\theta_0^{n\bar{x}}(1 - \theta_0)^{n-n\bar{x}}} = \left(\frac{\theta_1}{\theta_0}\right)^{n\bar{x}} \left(\frac{1 - \theta_1}{1 - \theta_0}\right)^{n-n\bar{x}}.
\]
Therefore,
\begin{align*}
\prb_{\theta_0}\left(\frac{f_{\theta_1}(n\bar{X})}{f_{\theta_0}(n\bar{X})} > c_0\right) &= \prb_{\theta_0}\left(\left(\frac{\theta_1}{\theta_0}\right)^{n\bar{X}} \left(\frac{1 - \theta_1}{1 - \theta_0}\right)^{n-n\bar{X}} > c_0\right) \\
&= \prb_{\theta_0}\left(\left(\frac{\theta_1}{1 - \theta_1} \cdot \frac{1 - \theta_0}{\theta_0}\right)^{n\bar{X}} > c_0 \left(\frac{1 - \theta_1}{1 - \theta_0}\right)^{-n}\right) \\
&= \prb_{\theta_0}\left(n\bar{X} \ln\left(\frac{\theta_1}{1 - \theta_1} \cdot \frac{1 - \theta_0}{\theta_0}\right) > \ln c_0 \left(\frac{1 - \theta_1}{1 - \theta_0}\right)^{-n}\right) \\
&= \prb_{\theta_0}\left(n\bar{X} > \frac{\ln c_0 \left(\frac{1 - \theta_1}{1 - \theta_0}\right)^{-n}}{\ln\left(\frac{\theta_1}{1 - \theta_1} \cdot \frac{1 - \theta_0}{\theta_0}\right)}\right) = \prb_{\theta_0}(n\bar{X} > c_0')
\end{align*}
because
\[
\ln\left(\frac{\theta_1}{1 - \theta_1} \cdot \frac{1 - \theta_0}{\theta_0}\right) > 0
\]
as $\theta/(1 - \theta)$ is increasing in $\theta$, which implies $\theta_1/(1 - \theta_1) > \theta_0/(1 - \theta_0)$.

Now, using \eqref{eq:8.2.4}, we choose $c_0$ so that $c_0'$ is an integer satisfying
\[
\prb_{\theta_0}(n\bar{X} > c_0') \leqslant \alpha \quad \text{and} \quad \prb_{\theta_0}(n\bar{X} > c_0' - 1) > \alpha.
\]
Because $n\bar{X} \sim \text{Binomial}(n, \theta_0)$ is a discrete distribution, we see that, in general, we will not be able to achieve $\prb_{\theta_0}(n\bar{X} > c_0') = \alpha$ exactly. So, using \eqref{eq:8.2.5},
\[
\gamma = \frac{\alpha - \prb_{\theta_0}(n\bar{X} > c_0')}{\prb_{\theta_0}(n\bar{X} = c_0')}
\]
will not be equal to 0. Then
\[
\phi_0(n\bar{x}) = \begin{cases}
1 & n\bar{x} > c_0' \\
\gamma & n\bar{x} = c_0' \\
0 & n\bar{x} < c_0'
\end{cases}
\]
is UMP size $\alpha$ for $H_0 : \theta = \theta_0$ versus $H_a : \theta = \theta_1$. Note that we can use statistical software (or Table D.6) for the binomial distribution to obtain $c_0'$.

For example, suppose $n = 6$ and $\theta_0 = 0.25$. The following table gives the values of the Binomial$(6, 0.25)$ distribution function to three decimal places.
\begin{center}
\begin{tabular}{c|ccccccc}
$x$ & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
$F(x)$ & 0.178 & 0.534 & 0.831 & 0.962 & 0.995 & 1.000 & 1.000
\end{tabular}
\end{center}
Therefore, if $\alpha = 0.05$, we have that $c_0' = 3$ because $\prb_{0.25}(n\bar{X} > 3) = 1 - 0.962 = 0.038$ and $\prb_{0.25}(n\bar{X} > 2) = 1 - 0.831 = 0.169$. This implies that
\[
\gamma = \frac{0.05 - (1 - 0.962)}{0.962} = 0.012.
\]
So with this test, we reject $H_0 : \theta = \theta_0$ categorically if the number of successes is greater than 3, accept $H_0 : \theta = \theta_0$ categorically when the number of successes is less than 3, and when the number of 1's equals 3, we randomly reject $H_0 : \theta = \theta_0$ with probability 0.012 (e.g., generate $U \sim \text{Uniform}[0, 1]$ and reject whenever $U \leqslant 0.012$).

Notice that the test $\phi_0$ does not involve $\theta_1$, so indeed it is UMP size $\alpha$ for $H_0 : \theta = \theta_0$ versus $H_a : \theta > \theta_0$. Furthermore, using Problem~\ref{exer:8.2.18}, we have
\[
\prb_\theta(n\bar{X} > c_0') = \sum_{k=c_0'+1}^{n} \binom{n}{k} \theta^k (1 - \theta)^{n-k} = 1 - \frac{\Gamma(n+1)}{\Gamma(c_0'+1)\Gamma(n-c_0')} \int_0^{1-\theta} u^{c_0'}(1-u)^{n-c_0'-1} \,\mathrm{d}u.
\]
Because
\[
\int_0^{1-\theta} u^{c_0'}(1-u)^{n-c_0'-1} \,\mathrm{d}u
\]
is decreasing in $\theta$, we must have that $\prb_\theta(n\bar{X} > c_0')$ is increasing in $\theta$. Arguing as in Example~\ref{ex:8.2.2}, we conclude that $\phi_0$ is UMP size $\alpha$ for $H_0 : \theta \leqslant \theta_0$ versus $H_a : \theta > \theta_0$. Similarly, we obtain a UMP size $\alpha$ test for $H_0 : \theta \geqslant \theta_0$ versus $H_a : \theta < \theta_0$. As in Example~\ref{ex:8.2.2}, there is no UMP size $\alpha$ test for $H_0 : \theta = \theta_0$ versus $H_a : \theta \neq \theta_0$, but there is a UMP unbiased size $\alpha$ test for this problem.
\end{example}

\subsection{Likelihood Ratio Tests (Advanced)}
\label{ssec:8.2.5}

In the examples considered so far, the Neyman--Pearson theorem has led to solutions to problems in which $H_0$ or $H_a$ are not just single values of the parameter, even though the theorem was only stated for the single-value case. We also noted, however, that this is not true in general (for example, the two-sided problems discussed in Examples~\ref{ex:8.2.2} and~\ref{ex:8.2.3}).

The method of generalized likelihood ratio tests for $H_0 : \psi(\theta) = \psi_0$ has been developed to deal with the general case. This is motivated by the Neyman--Pearson theorem, for observe that in \eqref{eq:8.2.3},
\[
\frac{f_{\theta_1}(s)}{f_{\theta_0}(s)} = \frac{L(\theta_1 \mid s)}{L(\theta_0 \mid s)}.
\]
Therefore, \eqref{eq:8.2.3} can be thought of as being based on the ratio of the likelihood at $\theta_1$ to the likelihood at $\theta_0$, and we reject $H_0 : \theta = \theta_0$ when the likelihood gives much more support to $\theta_1$ than to $\theta_0$. The amount of the additional support required for rejection is determined by $c_0$. The larger $c_0$ is, the larger the likelihood $L(\theta_1 \mid s)$ has to be relative to $L(\theta_0 \mid s)$ before we reject $H_0 : \theta = \theta_0$.

Denote the overall MLE of $\theta$ by $\hat{\theta}(s)$, and the MLE, when $\theta \in H_0$, by $\hat{\theta}_{H_0}(s)$. So we have
\[
L(\theta \mid s) \leqslant L(\hat{\theta}_{H_0}(s) \mid s)
\]
for all $\theta$ such that $\psi(\theta) = \psi_0$. The generalized likelihood ratio test then rejects $H_0$ when
\begin{equation}
\frac{L(\hat{\theta}(s) \mid s)}{L(\hat{\theta}_{H_0}(s) \mid s)}
\label{eq:8.2.9}
\end{equation}
is large, as this indicates evidence against $H_0$ being true.

How do we determine when \eqref{eq:8.2.9} is large enough to reject? Denoting the observed data by $s_0$, we do this by computing the P-values
\begin{equation}
\prb_\theta\left(\frac{L(\hat{\theta}(s) \mid s)}{L(\hat{\theta}_{H_0}(s) \mid s)} > \frac{L(\hat{\theta}(s_0) \mid s_0)}{L(\hat{\theta}_{H_0}(s_0) \mid s_0)}\right)
\label{eq:8.2.10}
\end{equation}
when $\theta \in H_0$. Small values of \eqref{eq:8.2.10} are evidence against $H_0$. Of course, when $\psi(\theta) = \psi_0$ for more than one value of $\theta$, then it is not clear which value of \eqref{eq:8.2.10} to use. It can be shown, however, that under conditions such as those discussed in Section \ref{sec:6.5}, if $s$ corresponds to a sample of $n$ values from a distribution, then
\[
2 \ln \frac{L(\hat{\theta}(s) \mid s)}{L(\hat{\theta}_{H_0}(s) \mid s)} \xrightarrow{D} \chi^2(\dim \Omega - \dim H_0)
\]
as $n \to \infty$, whenever the true value of $\theta$ is in $H_0$. Here, $\dim \Omega$ and $\dim H_0$ are the dimensions of these sets. This leads us to a test that rejects $H_0$ whenever
\begin{equation}
2 \ln \frac{L(\hat{\theta}(s) \mid s)}{L(\hat{\theta}_{H_0}(s) \mid s)}
\label{eq:8.2.11}
\end{equation}
is greater than a particular quantile of the $\chi^2(\dim \Omega - \dim H_0)$ distribution.

For example, suppose that in a location-scale normal model, we are testing $H_0 : \mu = \mu_0$. Then $\Omega = \mathbb{R}^1 \times [0, \infty)$, $H_0 = \{\mu_0\} \times [0, \infty)$, $\dim \Omega = 2$, $\dim H_0 = 1$, and, for a size 0.05 test, we reject whenever \eqref{eq:8.2.11} is greater than $\chi^2_{0.95}(1)$. Note that, strictly speaking, likelihood ratio tests are not derived via optimality considerations. We will not discuss likelihood ratio tests further in this text.

\subsection*{Summary of Section \ref{sec:8.2}}

\begin{itemize}
\item In searching for an optimal hypothesis testing procedure, we place an upper bound on the probability of making a type I error (rejecting $H_0$ when it is true) and search for a test that minimizes the probability of making a type II error (accepting $H_0$ when it is false).
\item The Neyman--Pearson theorem prescribes an optimal size $\alpha$ test when $H_0$ and $H_a$ each specify a single value for the full parameter $\theta$.
\item Sometimes the Neyman--Pearson theorem leads to solutions to hypothesis testing problems when the null or alternative hypotheses allow for more than one possible value for $\theta$, but in general we must resort to likelihood ratio tests for such problems.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:8.2.1}
Suppose that a statistical model is given by the two distributions in the following table.
\begin{center}
\begin{tabular}{c|cccc}
 & $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$f_a(s)$ & $1/3$ & $1/6$ & $1/12$ & $5/12$ \\
$f_b(s)$ & $1/2$ & $1/4$ & $1/6$ & $1/12$
\end{tabular}
\end{center}
Determine the UMP size 0.10 test for testing $H_0 : \theta = a$ versus $H_a : \theta = b$. What is the power of this test? Repeat this with the size equal to 0.05.
\end{exercise}

\begin{solution}
The ratio $f_b(s)/f_a(s)$ has the following distribution when $\theta = a$: $\prb_a(f_b(s)/f_a(s) = 3/2) = \prb_a(\{1, 2\}) = 1/2$, $\prb_a(f_b(s)/f_a(s) = 2) = \prb_a(\{3\}) = 1/12$, and $\prb_a(f_b(s)/f_a(s) = 1/5) = \prb_a(\{4\}) = 5/12$. When $\alpha = .1$, using (8.2.4) and (8.2.5), we have that $c_0 = 3/2$ and $\gamma = ((1/10) - (1/12))/(1/2) = 1/30$. The power of the test is $\prb_b(\{3\}) + \prb_b(\{1, 2\})/30 = 1/6 + (3/4)/30 = 23/120$. When $\alpha = .05$ we have that $c_0 = 2$ and $\gamma = ((1/20) - 0)/(1/12) = 3/5$. The power of the test is $\prb_b(\{3\})(3/5) = (1/6)(3/5) = 1/10$.
\end{solution}

\begin{exercise}
\label{exer:8.2.2}
Suppose for the hypothesis testing problem of Exercise~\ref{exer:8.2.1}, a statistician decides to generate $U \sim \text{Uniform}[0, 1]$ and reject $H_0$ whenever $U \leqslant 0.05$. Show that this test has size 0.05. Explain why this is not a good choice of test and why the test derived in Exercise~\ref{exer:8.2.1} is better. Provide numerical evidence for this.
\end{exercise}

\begin{solution}
Such a test completely ignores the data and so makes no use of any information that this provides about the true value of $\theta$. The power of such a test is clearly $1/20$, and this is smaller than the power $1/10$ of the optimal size $.05$ test derived in Exercise \ref{exer:8.2.1}.
\end{solution}

\begin{exercise}
\label{exer:8.2.3}
Suppose an investigator knows that an industrial process yields a response variable that follows an $\mathrm{N}(1, 2)$ distribution. Some changes have been made in the industrial process, and the investigator believes that these have possibly made a change in the mean of the response (not the variance), increasing its value. The investigator wants the probability of a type I error occurring to be less than 1\%. Determine an appropriate testing procedure for this problem based on a sample of size 10.
\end{exercise}

\begin{solution}
By (8.2.6) the optimal $.01$ test is of the form (using $z_{.99} = 2.3263$)
\[
\varphi_0(\bar{x}) = \begin{cases} 1 & \bar{x} \geqslant 1 + \frac{\sqrt{2}}{\sqrt{10}} \cdot 2.3263 \\ 0 & \bar{x} < 1 + \frac{\sqrt{2}}{\sqrt{10}} \cdot 2.3263 \end{cases} = \begin{cases} 1 & \bar{x} \geqslant 2.0404 \\ 0 & \bar{x} < 2.0404. \end{cases}
\]
\end{solution}

\begin{exercise}
\label{exer:8.2.4}
Suppose you have a sample of 20 from an $\mathrm{N}(\mu, 1)$ distribution. You form a 0.975-confidence interval for $\mu$ and use it to test $H_0 : \mu = 0$ by rejecting $H_0$ whenever 0 is not in the confidence interval.
\begin{enumerate}[(a)]
\item What is the size of this test?
\item Determine the power function of this test.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
\item Let $C$ be the 0.975-confidence interval for $\mu$. Then, $\prb_\mu(C) = 0.975$. The size of the test is the rejecting probability of $H_0$. Hence, the size is $\alpha = \prb_0(0 \notin C) = 1 - \prb_0(C) = 1 - 0.975 = 0.025$.
\item The confidence interval $C$ is $[\bar{x} - z_{0.9875}/\sqrt{20}, \bar{x} + z_{0.9875}/\sqrt{20}]$. Since $\bar{x} \sim N(\theta, 1/20)$ if $\theta$ is true, the power function is given by
\begin{align*}
\beta(\theta) &= \prb_\theta(0 \notin C) = \prb_\theta(\bar{x} < -z_{0.9875}/\sqrt{20} \text{ or } \bar{x} > z_{0.9875}/\sqrt{20}) \\
&= \Phi(-(z_{0.9875} + \theta)/\sqrt{20}) + 1 - \Phi((z_{0.9875} - \theta)/\sqrt{20}).
\end{align*}
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:8.2.5}
Suppose you have a sample of size $n = 1$ from a Uniform$[0, \theta]$ distribution, where $\theta > 0$ is unknown. You test $H_0 : \theta \leqslant 1$ by rejecting $H_0$ whenever the sampled value is greater than 1.
\begin{enumerate}[(a)]
\item What is the size of this test?
\item Determine the power function of this test.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
\item Since $\prb_\theta(X > 1) = 0$ for all $\theta \leqslant 1$, the size $\alpha = \sup_{\theta \in H_0} \prb_\theta(X > 1) = 0$.
\item Suppose $\theta > 1$. The power function is $\beta(\theta) = \prb_\theta(X > 1) = \int_1^{\theta}(1/\theta) \, \mathrm{d}x = 1 - 1/\theta$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:8.2.6}
Suppose you are testing a null hypothesis $H_0 : \theta = \theta_0$, where $\theta \in \mathbb{R}^1$. You use a size 0.05 testing procedure and accept $H_0$. You feel you have a fairly large sample, but when you compute the power at $\theta_{0.2}$, you obtain a value of 0.10 where $\theta_{0.2}$ represents the smallest difference from $\theta_0$ that is of practical importance. Do you believe it makes sense to conclude that the null hypothesis is true? Justify your conclusion.
\end{exercise}

\begin{solution}
The power is too low to confidently say that $H_0$ is true. A small power indicates that we have a low probability of detecting practically significant deviations from 0 with this test.
\end{solution}

\begin{exercise}
\label{exer:8.2.7}
Suppose you want to test the null hypothesis $H_0 : \mu = 0$ based on a sample of $n$ from an $\mathrm{N}(\mu, 1)$ distribution, where $\mu \in \{0, 2\}$. How large does $n$ have to be so that the power at $\mu = 2$, of the optimal size 0.05 test, is equal to 0.99?
\end{exercise}

\begin{solution}
The test is $H_0: \mu = 0$ versus $H_a: \mu = 2$. Hence, the UMP size $\alpha$ test has the rejection region $\phi_{1/n}(\bar{x} - 2)/\phi_{1/n}(\bar{x}) = \exp(2n(\bar{x} - 1)) > k_\alpha$. The rejection region is equivalent to $\bar{x} > k_\alpha'$ for some $k_\alpha' > 0$. Since $\alpha = \prb_0(\bar{x} > k_\alpha') = 1 - \Phi(\sqrt{n}k_\alpha') = 1 - \Phi(z_{1-\alpha})$, the critical point is $k_\alpha' = z_{1-\alpha}/\sqrt{n}$. The power function is given by
\[
\beta(2) = \prb_2(\bar{x} > k_\alpha') = \prb_2(\bar{x} > z_{1-\alpha}/\sqrt{n}) = 1 - \Phi(z_{1-\alpha} - 2\sqrt{n}) \geqslant 0.99 = 1 - \Phi(z_{0.01}).
\]
The solution is
\[
n \geqslant (z_{1-\alpha} - z_{0.01})^2/4 = (z_{0.95} - z_{0.01})^2/4 = (1.6449 - (-2.3263))^2/4 = 3.9426.
\]
Hence, we need at least $n = 4$ samples.
\end{solution}

\begin{exercise}
\label{exer:8.2.8}
Suppose we have available two different test procedures in a problem and these have the same power function. Explain why, from the point of view of optimal hypothesis testing theory, we should not care which test is used.
\end{exercise}

\begin{solution}
What we care about in optimal hypothesis testing theory is type I and II errors, i.e., significance level and power function. Hence, we must ignore the difference of two test procedures whenever two tests have the same significance level and the same power function.
\end{solution}

\begin{exercise}
\label{exer:8.2.9}
Suppose you have a UMP size $\alpha$ test $\phi$ for testing the hypothesis $H_0 : \psi(\theta) = \psi_0$, where $\psi$ is real-valued. Explain how the graph of the power function of another size $\alpha$ test that was not UMP would differ from the graph of the power function of $\phi$.
\end{exercise}

\begin{solution}
Suppose we have two size $\alpha$ test functions $\varphi$ and $\varphi'$ for this testing problem, with corresponding power functions $\beta_\varphi$ and $\beta_{\varphi'}$. Since $\varphi$ is UMP we must have that $\beta_\varphi(\theta) \geqslant \beta_{\varphi'}(\theta)$ for all $\theta > 0$. This implies that the graph of $\beta_\varphi$ lies above the graph of $\beta_{\varphi'}$.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:8.2.10}
Suppose you have a coin and you want to test the hypothesis that the coin is fair, i.e., you want to test $H_0 : \theta = 1/2$ where $\theta$ is the probability of getting a head on a single toss. You decide to reject $H_0$ using the rejection region $R = \{0, 1, 7, 8\}$ based on $n = 10$ tosses. Tabulate the power function for this procedure for $\theta \in \{0, 1/8, 2/8, \ldots, 7/8, 1\}$.
\end{exercise}

\begin{solution}
The power $\beta(\theta)$ is given by
\[
\beta(\theta) = \expc_\theta(\indc_R(X)) = \prb_\theta(X \in R) = \sum_{x \in R} \binom{n}{x} \theta^x (1 - \theta)^{n-x}.
\]
The result is given by the following table.
\begin{center}
\begin{tabular}{c|ccccccccc}
$\theta$ & 0 & 1/8 & 2/8 & 3/8 & 4/8 & 5/8 & 6/8 & 7/8 & 8/8 \\
\hline
$\beta$ & 1.000 & 0.639 & 0.248 & 0.101 & 0.172 & 0.384 & 0.532 & 0.334 & 0.000 \\
\end{tabular}
\end{center}

The R code for the computation is given below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Define the rejection region R = {0, 1, 7, 8}
R <- c(0, 1, 7, 8)
n <- 10

# Define theta values
theta <- seq(0, 1, by = 1/8)

# Calculate power for each theta
power <- numeric(length(theta))
for (k in seq_along(theta)) {
  th <- theta[k]
  if (th == 0) {
    power[k] <- 1  # P(X in {0,1,7,8}) = P(X=0) = 1 when theta=0
  } else if (th == 1) {
    power[k] <- 0  # P(X in {0,1,7,8}) = P(X=10)=0 not in R when theta=1
  } else {
    power[k] <- sum(dbinom(R, n, th))
  }
}

# Print results
results <- data.frame(theta = theta, power = round(power, 3))
print(results)
\end{minted}
\caption{Power function computation for Exercise 8.2.10 (power\_8210.R)}
\label{lst:power_8210}
\end{listing}
\end{solution}

\begin{exercise}
\label{exer:8.2.11}
On the same graph, plot the power functions for the two-sided $z$-test of $H_0 : \mu = 0$ for samples of sizes $n = 1, 4, 10, 20$, and $100$ based on $\alpha = 0.05$.
\begin{enumerate}[(a)]
\item What do you observe about these graphs?
\item Explain how these graphs demonstrate the unbiasedness of this test.
\end{enumerate}
\end{exercise}

\begin{solution}
We use notations in Example \ref{ex:8.2.1} with $\sigma_0^2 = 1$. The choice of $\sigma_0^2$ does not make any difference except the scale of $\mu$, i.e., $\beta_{\sigma_0^2}(\mu) = \beta_1(\sigma_0\mu)$. The rejection region is given by $R = \{(x_1, \ldots, x_n) : |\bar{x} - \mu_0| > z_{1-\alpha/2}/\sqrt{n}\}$ where $\mu_0 = 0$. The power function is
\begin{align*}
\beta(\mu) &= \expc_\mu \indc_R((X_1, \ldots, X_n)) = \prb_\mu(R) \\
&= \prb_\mu(\bar{X} > z_{1-\alpha/2}/\sqrt{n} \text{ or } \bar{X} < -z_{1-\alpha/2}/\sqrt{n}) \\
&= 1 - \Phi(z_{1-\alpha/2} - \mu\sqrt{n}) + \Phi(-z_{1-\alpha/2} - \mu\sqrt{n}) \\
&= \Phi(-z_{1-\alpha/2} + \mu\sqrt{n}) + \Phi(-z_{1-\alpha/2} - \mu\sqrt{n}).
\end{align*}
The graph of the power functions is drawn below.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig_8211.pdf}
\caption{Power functions for different sample sizes $n = 1, 4, 10, 20, 100$ with $\alpha = 0.05$.}
%\label{fig:power-functions-8211}
\end{figure}

The R code for this graph is as below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Power function computation for Exercise 8.2.11
alpha <- 0.05
z_crit <- qnorm(1 - alpha/2)

# Create mu values
mu <- seq(-7, 7, length.out = 2001)

# Sample sizes
n_values <- c(1, 4, 10, 20, 100)

# Calculate power for each sample size
power_matrix <- matrix(NA, nrow = length(mu), ncol = length(n_values))
colnames(power_matrix) <- paste0("n=", n_values)

for (j in seq_along(n_values)) {
  n <- n_values[j]
  power_matrix[, j] <- pnorm(-z_crit + mu * sqrt(n)) + 
                       pnorm(-z_crit - mu * sqrt(n))
}

# Plot
plot(mu, power_matrix[, 1], type = "l", col = 1, 
     xlab = expression(mu), ylab = "Power",
     ylim = c(0, 1), main = "Power Functions")
for (j in 2:length(n_values)) {
  lines(mu, power_matrix[, j], col = j)
}
legend("topright", legend = paste0("n=", n_values), 
       col = 1:length(n_values), lty = 1)
\end{minted}
\caption{Power function plot for Exercise 8.2.11}
\label{lst:power_8211}
\end{listing}

\begin{enumerate}[(a)]
\item The power is increasing at any fixed parameter $\mu$ as the sample size $n$ is increasing.
\item A test $\varphi$ is unbiased if $\beta(\theta) \geqslant \alpha$ for all $\theta \in H_a$. Since all power functions are above 0.05, all tests are unbiased.
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:8.2.12}
Prove that $\phi_0^*$ in \eqref{eq:8.2.7} is UMP size $\alpha$ for $H_0 : \mu \geqslant \mu_0$ versus $H_a : \mu < \mu_0$.
\end{exercise}

\begin{solution}
From the argument in the text we have that (8.2.7) is UMP size $\alpha$ for $H_0: \mu = \mu_0$ versus $H_a: \mu = \mu_1$ for some $\mu_1 < \mu_0$. Since the test does not depend on $\mu_1$, this says that (8.2.7) is UMP size $\alpha$ for $H_0: \mu = \mu_0$ versus $H_a: \mu < \mu_0$. Now the power function is given by $\beta_{\varphi_0}(\mu) = \prb_\mu(\bar{x} \leqslant \mu_0 + \frac{\sigma_0}{\sqrt{n}}z_\alpha) = \prb_\mu\left(\frac{\bar{x} - \mu}{\sigma_0/\sqrt{n}} \leqslant \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_\alpha\right) = \Phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_\alpha\right)$. Note that this is decreasing in $\mu$. This implies that $\varphi_0$ is a size $\alpha$ test function for $H_0: \mu \geqslant \mu_0$ versus $H_a: \mu < \mu_0$. Observe that, if $\varphi$ is a size $\alpha$ test function for $H_0: \mu \geqslant \mu_0$ versus $H_a: \mu < \mu_0$, then it is also a size $\alpha$ test for $H_0: \mu = \mu_0$ versus $H_a: \mu < \mu_0$. From this we conclude that $\varphi_0$ is UMP size $\alpha$ for $H_0: \mu \leqslant \mu_0$ versus $H_a: \mu > \mu_0$.
\end{solution}

\begin{exercise}
\label{exer:8.2.13}
Prove that the test function $\phi(s) = \alpha$ for every $s \in S$ is an exact size $\alpha$ test function. What is the interpretation of this test function?
\end{exercise}

\begin{solution}
We have that $\expc_\theta(\varphi) = \alpha$ for every $\theta$, so it is of exact size $\alpha$. For this test, no matter what data is obtained, we randomly decide to reject $H_0$ with probability $\alpha$.
\end{solution}

\begin{exercise}
\label{exer:8.2.14}
Using the test function in Problem~\ref{exer:8.2.13}, show that a UMP size $\alpha$ test is also a UMP unbiased size $\alpha$ test.
\end{exercise}

\begin{solution}
Suppose that $\varphi_0$ is a size $\alpha$ UMP test for a specific problem and let $\varphi$ be the test function of Problem \ref{exer:8.2.13}. Then for $\theta$ such that the alternative is true we have that $\expc_\theta(\varphi_0) \geqslant \expc_\theta(\varphi) = \alpha$, so $\varphi_0$ is unbiased.
\end{solution}

\begin{exercise}
\label{exer:8.2.15}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Gamma$(\alpha_0, \lambda)$ distribution, where $\alpha_0$ is known and $\lambda > 0$ is unknown. Determine the UMP size $\alpha$ test for testing $H_0 : \lambda = \lambda_0$ versus $H_a : \lambda = \lambda_1$, where $\lambda_1 > \lambda_0$. Is this test UMP size $\alpha$ for $H_0 : \lambda \leqslant \lambda_0$ versus $H_a : \lambda > \lambda_0$?
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\beta \mid x_1, \ldots, x_n) = \beta^{n\alpha_0}\exp\{-\beta n\bar{x}\}$. Therefore, we reject $H_0: \beta = \beta_0$ versus $H_a: \beta = \beta_1$ whenever $\beta_1^{n\alpha_0}\exp\{-\beta_1 n\bar{x}\}/\beta_0^{n\alpha_0}\exp\{-\beta_0 n\bar{x}\} > c_0$ or, equivalently, whenever $(\beta_0 - \beta_1)n\bar{x} > n\alpha_0(\beta_0 - \beta_1) + \ln c_0$ or, since $\beta_0 < \beta_1$, whenever $n\bar{x} < (n\alpha_0(\beta_0 - \beta_1) + \ln c_0)/(\beta_0 - \beta_1)$. When $H_0$ is true we have that $n\bar{X} \sim \text{Gamma}(n\alpha_0, \beta_0)$, so with $x_{1-\alpha}(\beta_0)$ denoting the $(1-\alpha)$th quantile of this distribution, the UMP size $\alpha$ test is to reject whenever $n\bar{x} \leqslant x_\alpha(\beta_0)$.

Since this test does not depend on $\beta_1$, it is also UMP size $\alpha$ for $H_0: \beta = \beta_0$ versus $H_a: \beta > \beta_0$. Now observe that when $X \sim \text{Gamma}(\alpha, \beta)$, $Z = \beta X \sim \text{Gamma}(\alpha, 1)$. Therefore, $\prb_\beta(n\bar{x} \leqslant x_{1-\alpha}(\beta_0)) = \prb_\beta(\beta n\bar{x} \leqslant \beta x_\alpha(\beta_0)) = \prb_1(Z \leqslant \beta x_\alpha(\beta_0))$, where $Z \sim \text{Gamma}(\alpha, 1)$. The above implies that the power function is increasing in $\beta$. Therefore, the above test is size $\alpha$ for $H_0: \beta \leqslant \beta_0$ versus $H_a: \beta > \beta_0$. Now suppose $\varphi$ is also size $\alpha$ for $H_0: \beta \leqslant \beta_0$ versus $H_a: \beta > \beta_0$. Then $\varphi$ is also size $\alpha$ for $H_0: \beta = \beta_0$ versus $H_a: \beta > \beta_0$ and so must have its power function uniformly less than or equal to the power function for the above test when $\beta > \beta_0$. This implies that the above test is UMP size $\alpha$ for $H_0: \beta \leqslant \beta_0$ versus $H_a: \beta > \beta_0$.
\end{solution}

\begin{exercise}
\label{exer:8.2.16}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu_0, \sigma^2)$ distribution, where $\mu_0$ is known and $\sigma^2 > 0$ is unknown. Determine the UMP size $\alpha$ test for testing $H_0 : \sigma^2 = \sigma_0^2$ versus $H_a : \sigma^2 = \sigma_1^2$ where $\sigma_0^2 < \sigma_1^2$. Is this test UMP size $\alpha$ for $H_0 : \sigma^2 \leqslant \sigma_0^2$ versus $H_a : \sigma^2 > \sigma_0^2$?
\end{exercise}

\begin{solution}
Without loss of generality, assume $\mu_0 = 0$. Then for $H_0: \sigma^2 = \sigma_0^2$ versus $H_a: \sigma^2 = \sigma_1^2$, the UMP size $\alpha$ test rejects $H_0$ whenever
\[
\frac{L(\sigma_1^2 \mid x_1, \ldots, x_n)}{L(\sigma_0^2 \mid x_1, \ldots, x_n)} = \frac{\sigma_1^{-2n}\exp\left\{-\frac{1}{2\sigma_1^2}\sum_{i=1}^{n}x_i^2\right\}}{\sigma_0^{-2n}\exp\left\{-\frac{1}{2\sigma_0^2}\sum_{i=1}^{n}x_i^2\right\}} > c_0
\]
or, equivalently, whenever $n(\sigma_0^2 - \sigma_1^2) + \frac{1}{2}\left(\frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2}\right)\sum_{i=1}^{n}x_i^2 > \ln c_0$ or, using $\sigma_0^2 < \sigma_1^2$, whenever
\[
\frac{1}{\sigma_0^2}\sum_{i=1}^{n}x_i^2 > \frac{2}{\sigma_0^2}\left(\frac{1}{\sigma_0^2} - \frac{1}{\sigma_1^2}\right)^{-1}(\ln c_0 - n(\sigma_0^2 - \sigma_1^2)).
\]
Under $H_0$ we have that $\frac{1}{\sigma_0^2}\sum_{i=1}^{n}x_i^2 \sim \chi^2(n)$, so the test is to reject whenever $\frac{1}{\sigma_0^2}\sum_{i=1}^{n}x_i^2 > x_{1-\alpha}$, where $x_{1-\alpha}$ is the $(1-\alpha)$th quantile of the $\chi^2(n)$ distribution. Since the test does not involve $\sigma_1^2$, it is UMP size $\alpha$ for $H_0: \sigma^2 = \sigma_0^2$ versus $H_a: \sigma^2 > \sigma_0^2$. The power function of this test is given by $\prb_{\sigma^2}\left(\frac{1}{\sigma_0^2}\sum_{i=1}^{n}x_i^2 \geqslant x_{1-\alpha}\right) = \prb_{\sigma^2}\left(\frac{1}{\sigma^2}\sum_{i=1}^{n}x_i^2 \geqslant \frac{\sigma_0^2}{\sigma^2}x_{1-\alpha}\right) = \prb\left(Z \geqslant \frac{\sigma_0^2}{\sigma^2}x_{1-\alpha}\right)$ where $Z = \left(\sum_{i=1}^{n}x_i^2\right)/\sigma^2 \sim \chi^2(n)$, so the power function is increasing in $\sigma^2$. This implies that the above test is of size $\alpha$ for $H_0: \sigma^2 \leqslant \sigma_0^2$ versus $H_a: \sigma^2 > \sigma_0^2$. Now suppose $\varphi$ is also size $\alpha$ for $H_0: \sigma^2 \leqslant \sigma_0^2$ versus $H_a: \sigma^2 > \sigma_0^2$. Then $\varphi$ is also size $\alpha$ for $H_0: \sigma^2 = \sigma_0^2$ versus $H_a: \sigma^2 > \sigma_0^2$ and so must have its power function uniformly less than or equal to the power function for the above test when $\sigma^2 > \sigma_0^2$. This implies that the above test is UMP size $\alpha$ for $H_0: \sigma^2 \leqslant \sigma_0^2$ versus $H_a: \sigma^2 > \sigma_0^2$.
\end{solution}

\begin{exercise}
\label{exer:8.2.17}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Uniform$[0, \theta]$ distribution, where $\theta > 0$ is unknown. Determine the UMP size $\alpha$ test for testing $H_0 : \theta = \theta_0$ versus $H_a : \theta = \theta_1$, where $\theta_0 < \theta_1$. Is this test function UMP size $\alpha$ for $H_0 : \theta \leqslant \theta_0$ versus $H_a : \theta > \theta_0$?
\end{exercise}

\begin{solution}
For $H_0: \theta = \theta_0$ versus $H_a: \theta = \theta_1$, the UMP size $\alpha$ test rejects $H_0$ whenever $L(\theta_1 \mid x_1, \ldots, x_n)/L(\theta_0 \mid x_1, \ldots, x_n) = \theta_1^{-n}\indc_{(0,\theta_1)}(x_{(n)})/\theta_0^{-n}\indc_{(0,\theta_0)}(x_{(n)}) > c_0$ or, equivalently, whenever $\indc_{(0,\theta_1)}(x_{(n)})/\indc_{(0,\theta_0)}(x_{(n)}) > \theta_0^{-n}c_0/\theta_1^{-n}$. So we reject categorically whenever $x_{(n)} > \theta_0$ because the likelihood ratio equals $\infty$. When $0 \leqslant x_{(n)} \leqslant 1$ the likelihood ratio equals $\theta_1^{-n}/\theta_0^{-n}$. This implies that the UMP size $\alpha$ rejects whenever the likelihood ratio is greater than $\theta_1^{-n}/\theta_0^{-n}$ (i.e., equals $\infty$) and otherwise we randomly reject with probability $\alpha$ (i.e., when the likelihood ratio does not equal $\infty$).

Note that the above test does not depend on $\theta_1$ and so is UMP size $\alpha$ for $H_0: \theta = \theta_0$ versus $H_a: \theta > \theta_0$. Further, if $\theta < \theta_0$ we have that $\prb_\theta\left(\frac{L(\theta_1 \mid x_1, \ldots, x_n)}{L(\theta \mid x_1, \ldots, x_n)} < \infty\right) = 1$, so the above test has size $\alpha$ for $H_0: \theta \leqslant \theta_0$ versus $H_a: \theta > \theta_0$. Now suppose $\varphi$ is size $\alpha$ for $H_0: \theta \leqslant \theta_0$ versus $H_a: \theta > \theta_0$. Then $\varphi$ is also size $\alpha$ for $H_0: \theta = \theta_0$ versus $H_a: \theta > \theta_0$ and so must have its power function uniformly less than or equal to the power function for the above test when $\theta > \theta_0$. This implies that the above test is UMP size $\alpha$ for $H_0: \theta \leqslant \theta_0$ versus $H_a: \theta > \theta_0$.
\end{solution}

\begin{exercise}
\label{exer:8.2.18}
Suppose that $F$ is the distribution function for the Binomial$(n, \theta)$ distribution. Then prove that
\[
F(x) = \frac{\Gamma(n+1)}{\Gamma(x+1)\Gamma(n-x)} \int_{1-\theta}^{1} y^x (1-y)^{n-x-1} \,\mathrm{d}y
\]
for $x = 0, 1, \ldots, n-1$. This establishes a relationship between the binomial probability distribution and the beta function. (Hint: Integration by parts.)
\end{exercise}

\begin{solution}
Suppose $X \sim \text{Binomial}(n, \theta)$ and put
\[
F(x) = \frac{\Gamma(n+1)}{\Gamma(x+1)\Gamma(n-x)} \int_\theta^1 y^x(1-y)^{n-x-1} \, \mathrm{d}y.
\]
So $F(0) = \frac{\Gamma(n+1)}{\Gamma(0+1)\Gamma(n-0)}\int_\theta^1 y^0(1-y)^{n-0-1} \, \mathrm{d}y = n\int_\theta^1(1-y)^{n-1} \, \mathrm{d}y = (1-\theta)^n = \prb(X = 0)$. Using integration by parts we put $u = y^x$, giving $\mathrm{d}u = xy^{x-1}$ and $\mathrm{d}v = (1-y)^{n-x-1}$, giving $v = -(1-y)^{n-x}/(n-x)$, so that
\begin{align*}
F(x) &= \frac{\Gamma(n+1)}{\Gamma(x+1)\Gamma(n-x)} \int_\theta^1 y^x(1-y)^{n-x-1} \, \mathrm{d}y \\
&= \frac{\Gamma(n+1)}{\Gamma(x+1)\Gamma(n-x)} \left[-\frac{y^x(1-y)^{n-x}}{n-x}\bigg|_\theta^1 + \frac{x}{n-x}\int_\theta^1 y^{x-1}(1-y)^{n-x} \, \mathrm{d}y\right] \\
&= \binom{n}{x}\theta^x(1-\theta)^{n-x} + \frac{\Gamma(n+1)}{\Gamma(x)\Gamma(n-x+1)}\int_\theta^1 y^{x-1}(1-y)^{n-x} \, \mathrm{d}y \\
&= \prb(X = x) + F(x-1).
\end{align*}
Continuing this recursively establishes the result.
\end{solution}

\begin{exercise}
\label{exer:8.2.19}
Suppose that $F$ is the distribution function for the Poisson$(\lambda)$ distribution. Then prove that
\[
F(x) = \frac{1}{x!} \int_\lambda^\infty y^x e^{-y} \,\mathrm{d}y
\]
for $x = 0, 1, \ldots$. This establishes a relationship between the Poisson probability distribution and the gamma function. (Hint: Integration by parts.)
\end{exercise}

\begin{solution}
Let $X \sim \text{Poisson}(\lambda)$ and put $F(x) = \frac{1}{x!}\int_\lambda^{\infty} y^x e^{-y} \, \mathrm{d}y$. Then $F(0) = \frac{1}{0!}\int_\lambda^{\infty} y^0 e^{-y} \, \mathrm{d}y = e^{-\lambda} = \prb(X = 0)$. Using integration by parts with $u = y^x$, giving $\mathrm{d}u = xy^{x-1}$, $\mathrm{d}v = e^{-y}$, giving $v = -e^{-y}$, we have that
\begin{align*}
F(x) &= \frac{1}{x!}\left\{-y^x e^{-y}\big|_\lambda^{\infty} + x\int_\lambda^{\infty} y^{x-1}e^{-y} \, \mathrm{d}y\right\} = \frac{\lambda^x e^{-\lambda}}{x!} + F(x-1) \\
&= \prb(X = x) + F(x-1).
\end{align*}
Continuing this recursively establishes the result.
\end{solution}

\begin{exercise}
\label{exer:8.2.20}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Poisson$(\lambda)$ distribution, where $\lambda > 0$ is unknown. Determine the UMP size $\alpha$ test for $H_0 : \lambda = \lambda_0$ versus $H_a : \lambda = \lambda_1$, where $\lambda_0 < \lambda_1$. Is this test function UMP size $\alpha$ for $H_0 : \lambda \leqslant \lambda_0$ versus $H_a : \lambda > \lambda_0$? (Hint: You will need the result of Problem~\ref{exer:8.2.19}.)
\end{exercise}

\begin{solution}
The UMP size $\alpha$ test for $H_0: \lambda = \lambda_0$ versus $H_0: \lambda = \lambda_1$ is of the form
\[
\frac{L(\lambda_1 \mid x_1, \ldots, x_n)}{L(\lambda_0 \mid x_1, \ldots, x_n)} = \frac{(\lambda_1)^{n\bar{x}}e^{-\lambda_1}}{(\lambda_0)^{n\bar{x}}e^{-\lambda_0}} > c_0
\]
or, equivalently, whenever $n\bar{x}(\ln\lambda_1 - \ln\lambda_0) > (\lambda_1 - \lambda_0)\ln c_0$, and since $\lambda_1 > \lambda_0$, this is equivalent to rejecting whenever $n\bar{x} > (\lambda_1 - \lambda_0)\ln c_0/(\ln\lambda_1 - \ln\lambda_0)$. Now recall that $n\bar{x} \sim \text{Poisson}(n\lambda_0)$ under $H_0$ so we must determine the smallest $k$ such that $\prb_{\lambda_0}(n\bar{x} > k) \leqslant \alpha$ and then put $\gamma = (\alpha - \prb_{\lambda_0}(n\bar{x} > k))/\prb_{\lambda_0}(n\bar{x} = k)$. Since this test does not involve $\lambda_1$, it is UMP size $\alpha$ for $H_0: \lambda = \lambda_0$ versus $H_0: \lambda > \lambda_0$. From Problem \ref{exer:8.2.19} we have that $\prb_\lambda(n\bar{x} > k) \leqslant 1 - \frac{1}{x!}\int_\lambda^{\infty} y^x e^{-y} \, \mathrm{d}y$, and we see that this is increasing in $\lambda$. Therefore, this test is UMP size $\alpha$ for $H_0: \lambda \leqslant \lambda_0$ versus $H_0: \lambda > \lambda_0$.
\end{solution}

\begin{exercise}
\label{exer:8.2.21}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma^2)$ distribution, where $(\mu, \sigma^2) \in \mathbb{R}^1 \times (0, \infty)$ is unknown. Derive the form of the exact size $\alpha$ likelihood ratio test for testing $H_0 : \mu = \mu_0$ versus $H_0 : \mu \neq \mu_0$.
\end{exercise}

\begin{solution}
When $H_0: \mu = \mu_0$ holds, the log-likelihood and score functions are given by $l(x_1, \ldots, x_n \mid \sigma^2) = -n\ln\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x - \mu_0)^2$, $S(x_1, \ldots, x_n \mid \sigma^2) = -\frac{n}{\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^{n}(x - \mu_0)^2$. Then $S(x_1, \ldots, x_n \mid \sigma^2) = 0$ leads to the MLE $\hat{\mu}_{H_0} = \mu_0$, $\hat{\sigma}^2_{H_0} = \frac{1}{n}\sum_{i=1}^{n}(x - \mu_0)^2$, and the maximized log-likelihood equals
\[
l(x_1, \ldots, x_n \mid \hat{\sigma}^2_{H_0}) = n\ln n - n\ln\sum_{i=1}^{n}(x - \mu_0)^2 - \frac{n}{2}.
\]

When $H_a: \mu \neq \mu_0$ holds, the log-likelihood function is given by $l(x_1, \ldots, x_n \mid \mu, \sigma^2) = -n\ln\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x - \mu)^2$. By Example \ref{ex:6.2.6} the MLE is given by $\hat{\mu} = \bar{x}$, $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(x - \bar{x})^2$ and the maximized log-likelihood is given by $l(x_1, \ldots, x_n \mid \hat{\mu}, \hat{\sigma}^2) = n\ln n - n\ln\sum_{i=1}^{n}(x - \bar{x})^2 - \frac{n}{2}$. Then the likelihood ratio test rejects whenever
\begin{align*}
2(l(x_1, \ldots, x_n \mid \hat{\mu}, \hat{\sigma}^2) - l(x_1, \ldots, x_n \mid \hat{\sigma}^2_{H_0})) &= 2\left(-n\ln\sum_{i=1}^{n}(x - \bar{x})^2 + n\ln\sum_{i=1}^{n}(x - \mu_0)^2\right) \\
&= 2n\ln\frac{\sum_{i=1}^{n}(x - \mu_0)^2}{\sum_{i=1}^{n}(x - \bar{x})^2} > x_{1-\alpha}
\end{align*}
where $x_{1-\alpha}$ is the $(1-\alpha)$th quantile of the $\chi^2(2-1) = \chi^2(1)$ distribution.
\end{solution}

\begin{exercise}[Optimal confidence intervals]
\label{exer:8.2.22}
Suppose that for model $\{f_\theta : \theta \in \Omega\}$ we have a UMP size $\alpha$ test function $\phi_{\psi_0}$ for $H_0 : \psi(\theta) = \psi_0$, for each possible value of $\psi_0$. Suppose further that each $\phi_{\psi_0}$ only takes values in $\{0, 1\}$, i.e., each $\phi_{\psi_0}$ is a nonrandomized size $\alpha$ test function.
\begin{enumerate}[(a)]
\item Prove that
\[
C(s) = \{\psi_0 : \phi_{\psi_0}(s) = 0\}
\]
satisfies
\[
\prb_\theta(\psi(\theta) \in C(s)) \geqslant 1 - \alpha
\]
for every $\theta \in \Omega$. Conclude that $C(s)$ is a $(1-\alpha)$-confidence set for $\psi(\theta)$.
\item If $C^*$ is a $(1-\alpha)$-confidence set for $\psi(\theta)$, then prove that the test function defined by
\[
\phi^*_{\psi_0}(s) = \begin{cases}
1 & \psi_0 \notin C^*(s) \\
0 & \psi_0 \in C^*(s)
\end{cases}
\]
is size $\alpha$ for $H_0 : \psi(\theta) = \psi_0$.
\item Suppose that for each value $\psi_0$, the test function $\phi_{\psi_0}$ is UMP size $\alpha$ for testing $H_0 : \psi(\theta) = \psi_0$ versus $H_0 : \psi(\theta) \neq \psi_0$. Then prove that
\begin{equation}
\prb_\theta(\psi(\theta') \in C(s))
\label{eq:8.2.12}
\end{equation}
is minimized, when $\psi(\theta') \neq \psi_0$, among all $(1-\alpha)$-confidence sets for $\psi(\theta)$. The probability \eqref{eq:8.2.12} is the probability of $C$ containing the false value $\psi(\theta')$, and a $(1-\alpha)$-confidence region that minimizes this probability when $\psi(\theta') \neq \psi_0$ is called a \emph{uniformly most accurate} (UMA) $(1-\alpha)$-confidence region for $\psi(\theta)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
\item We have that
\begin{align*}
\prb_\theta(\psi(\theta) \in C(s)) &= \prb_\theta(\{s : \psi(\theta) \in C(s)\}) = \prb_\theta(\{s : \varphi_{\psi(\theta)}(s) = 0\}) \\
&= 1 - \prb_\theta(\{s : \varphi_{\psi(\theta)}(s) = 1\}) = 1 - \expc_\theta(\varphi_{\psi(\theta)}) \geqslant 1 - \alpha
\end{align*}
since $\expc_\theta(\varphi_{\psi(\theta)}) \leqslant \alpha$.
\item We have that $\expc_\theta(\varphi^*_{\psi(\theta)}) = \prb_\theta(\psi(\theta) \notin C^*(s)) = 1 - \prb_\theta(\psi(\theta) \in C^*(s)) \leqslant 1 - (1 - \alpha) = \alpha$ since $\prb_\theta(\psi(\theta) \in C^*(s)) \geqslant 1 - \alpha$.
\item Suppose now that, for each value of $\psi_0$, the test function $\varphi_{\psi_0}$ is UMP size $\alpha$ for $H_0: \psi(\theta) = \psi_0$ versus $H_a: \psi(\theta) \neq \psi_0$. Then, if $C$ is the confidence set corresponding to this family of tests, we have that $\prb_\theta(\psi(\theta^*) \in C(s)) = \prb_\theta(\{s : \psi(\theta^*) \in C(s)\}) = 1 - \prb_\theta(\psi(\theta^*) \notin C^*(s)) = 1 - \expc_\theta(\varphi_{\psi(\theta^*)})$, and since $\expc_\theta(\varphi_{\psi(\theta^*)})$ is maximized when $\psi(\theta) \neq \psi(\theta^*)$, part (b) implies that the probability of covering a false value is uniformly minimized by $C$.
\end{enumerate}
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:8.2.23}
Prove Corollary~\ref{cor:8.2.1} in the discrete case.
\end{exercise}

\begin{solution}
Suppose that a test $\varphi$ is size $\alpha$ and UMP for $H_0: \theta = \theta_0$ versus $H_a: \theta = \theta_1$. Let $\varphi_0$ be as in Theorem \ref{thm:8.2.1}. Following the proof of Theorem \ref{thm:8.2.1}, let $S^* = \{s : \varphi_0(s) \neq \varphi(s)\} \cap \{s : f_{\theta_1}(s) \neq c_0 f_{\theta_0}(s)\}$. Then $\expc_{\theta_1}(\varphi) = \expc_{\theta_1}(\varphi_0)$ and, since $\varphi$ is size $\alpha$, we have that $0 \geqslant \expc_{\theta_1}(\varphi_0) - \expc_{\theta_1}(\varphi) - c_0(\alpha - \expc_{\theta_0}(\varphi)) = \sum_{s \in S}(\varphi_0(s) - \varphi(s))(f_{\theta_1}(s) - c_0 f_{\theta_0}(s)) = \sum_{s \in S^*}(\varphi_0(s) - \varphi(s))(f_{\theta_1}(s) - c_0 f_{\theta_0}(s)) \geqslant 0$ since $(\varphi_0(s) - \varphi(s))(f_{\theta_1}(s) - c_0 f_{\theta_0}(s)) > 0$ on $S^*$. But this implies that $S^* = \emptyset$ and we have that $\varphi_0(s) = \varphi(s)$ whenever $f_{\theta_1}(s) \neq c_0 f_{\theta_0}(s)$. The values $\varphi_0$ and $\varphi$ may differ on $B = \{s : f_{\theta_1}(s) = c_0 f_{\theta_0}(s)\}$.

Since $\expc_{\theta_1}(\varphi_0) - \expc_{\theta_1}(\varphi) = 0$ the inequalities above establish that $c_0(\alpha - \expc_{\theta_0}(\varphi)) = 0$. If $c_0 = 0$, then $\varphi_0(s) = 1$ whenever $f_{\theta_1}(s) > 0$, so the power of the UMP test is 1. If $c_0 \neq 1$, then $\expc_{\theta_0}(\varphi) = \alpha$ and $\varphi$ has exact size $\alpha$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{solution}

\section{Optimal Bayesian Inferences}
\label{sec:8.3}

We now add the prior probability measure $\Pi$ with density $\pi$. As we will see, this completes the specification of an optimality problem, as now there is always a solution. Solutions to Bayesian optimization problems are known as Bayes rules.

In Section~\ref{sec:8.1}, the unrestricted optimization problem was to find the estimator $T$ of $\psi(\theta)$ that minimizes $\mathrm{MSE}_\theta(T) = \expc_\theta((T - \psi(\theta))^2)$, for each $\theta \in \Omega$. The Bayesian version of this problem is to minimize
\begin{equation}
\expc_\Pi(\mathrm{MSE}_\theta(T)) = \expc_\Pi(\expc_\theta((T - \psi(\theta))^2)).
\label{eq:8.3.1}
\end{equation}
By the theorem of total expectation (Theorem \ref{thm:3.5.2}), \eqref{eq:8.3.1} is the expected value of the squared error $(T(s) - \psi(\theta))^2$ under the joint distribution on $(\theta, s)$ induced by the conditional distribution for $s$, given $\theta$ (the sampling model), and by the marginal distribution for $\theta$ (the prior distribution of $\theta$). Again, by the theorem of total expectation, we can write this as
\begin{equation}
\expc_\Pi(\mathrm{MSE}_\theta(T)) = \expc_M(\expc_{\Pi(\cdot \mid s)}((T - \psi(\theta))^2)),
\label{eq:8.3.2}
\end{equation}
where $\Pi(\cdot \mid s)$ denotes the posterior probability measure for $\theta$, given the data $s$ (the conditional distribution of $\theta$ given $s$), and $M$ denotes the prior predictive probability measure for $s$ (the marginal distribution of $s$).

We have the following result.

\begin{theorem}
\label{thm:8.3.1}
When \eqref{eq:8.3.1} is finite, a Bayes rule is given by
\[
T(s) = \expc_{\Pi(\cdot \mid s)}(\psi(\theta)),
\]
namely, the posterior expectation of $\psi(\theta)$.
\end{theorem}

\begin{proof}
First, consider the expected posterior squared error
\[
\expc_{\Pi(\cdot \mid s)}\left((T_0(s) - \psi(\theta))^2\right)
\]
of an estimate $T_0(s)$. By Theorem~\ref{thm:8.1.1} this is minimized by taking $T_0(s)$ equal to $T(s) = \expc_{\Pi(\cdot \mid s)}(\psi(\theta))$ (note that the ``random'' quantity here is $\theta$).

Now suppose that $T_0$ is any estimator of $\psi(\theta)$. Then we have just shown that
\[
0 \leqslant \expc_{\Pi(\cdot \mid s)}\left((T(s) - \psi(\theta))^2\right) \leqslant \expc_{\Pi(\cdot \mid s)}\left((T_0(s) - \psi(\theta))^2\right)
\]
and thus,
\[
\expc_\Pi(\mathrm{MSE}_\theta(T)) = \expc_M\left(\expc_{\Pi(\cdot \mid s)}((T(s) - \psi(\theta))^2)\right) \leqslant \expc_M\left(\expc_{\Pi(\cdot \mid s)}((T_0(s) - \psi(\theta))^2)\right) = \expc_\Pi(\mathrm{MSE}_\theta(T_0)).
\]
Therefore, $T$ minimizes \eqref{eq:8.3.1} and is a Bayes rule.
\end{proof}

So we see that, under mild conditions, the optimal Bayesian estimation problem always has a solution and there is no need to restrict ourselves to unbiased estimators, etc.

For the hypothesis testing problem $H_0 : \psi(\theta) = \psi_0$, we want to find the test function $\phi$ that minimizes the prior probability of making an error (type I or type II). Such a $\phi$ is a Bayes rule. We have the following result.

\begin{theorem}
\label{thm:8.3.2}
A Bayes rule for the hypothesis testing problem $H_0 : \psi(\theta) = \psi_0$ is given by
\[
\phi_0(s) = \begin{cases}
1 & \Pi(\{\psi(\theta) = \psi_0\} \mid s) \leqslant \Pi(\{\psi(\theta) \neq \psi_0\} \mid s) \\
0 & \text{otherwise}.
\end{cases}
\]
\end{theorem}

\begin{proof}
Consider test function $\phi$ and let $\indc_{\{\psi(\theta) = \psi_0\}}(\theta)$ denote the indicator function of the set $\{\theta : \psi(\theta) = \psi_0\}$ (so $\indc_{\{\psi(\theta) = \psi_0\}}(\theta) = 1$ when $\psi(\theta) = \psi_0$ and equals 0 otherwise). Observe that $\phi(s)$ is the probability of rejecting $H_0$, having observed $s$, which is an error when $\indc_{\{\psi(\theta) = \psi_0\}}(\theta) = 1$; $1 - \phi(s)$ is the probability of accepting $H_0$, having observed $s$, which is an error when $\indc_{\{\psi(\theta) = \psi_0\}}(\theta) = 0$. Therefore, given $s$ and $\theta$, the probability of making an error is
\[
e(\theta, s) = \phi(s) \indc_{\{\psi(\theta) = \psi_0\}}(\theta) + (1 - \phi(s))(1 - \indc_{\{\psi(\theta) = \psi_0\}}(\theta)).
\]
By the theorem of total expectation, the prior probability of making an error (taking the expectation of $e(\theta, s)$ under the joint distribution of $(\theta, s)$) is
\begin{equation}
\expc_M\left(\expc_{\Pi(\cdot \mid s)}(e(\theta, s))\right).
\label{eq:8.3.3}
\end{equation}
As in the proof of Theorem~\ref{thm:8.3.1}, if we can find $\phi$ that minimizes $\expc_{\Pi(\cdot \mid s)}(e(\theta, s))$ for each $s$, then $\phi$ also minimizes \eqref{eq:8.3.3} and is a Bayes rule.

Using Theorem \ref{thm:3.5.4} to pull $\phi(s)$ through the conditional expectation, and the fact that $\expc_{\Pi(\cdot \mid s)}(\indc_A(\theta)) = \Pi(A \mid s)$ for any event $A$, then
\[
\expc_{\Pi(\cdot \mid s)}(e(\theta, s)) = \phi(s)\Pi(\{\psi(\theta) = \psi_0\} \mid s) + (1 - \phi(s))(1 - \Pi(\{\psi(\theta) = \psi_0\} \mid s)).
\]
Because $\phi(s) \in [0, 1]$, we have
\begin{align*}
&\min\{\Pi(\{\psi(\theta) = \psi_0\} \mid s), 1 - \Pi(\{\psi(\theta) = \psi_0\} \mid s)\} \\
&\quad \leqslant \phi(s)\Pi(\{\psi(\theta) = \psi_0\} \mid s) + (1 - \phi(s))(1 - \Pi(\{\psi(\theta) = \psi_0\} \mid s)).
\end{align*}
Therefore, the minimum value of $\expc_{\Pi(\cdot \mid s)}(e(\theta, s))$ is attained by $\phi(s) = \phi_0(s)$.
\end{proof}

Observe that Theorem~\ref{thm:8.3.2} says that the Bayes rule rejects $H_0$ whenever the posterior probability of the null hypothesis is less than or equal to the posterior probability of the alternative. This is an intuitively satisfying result.

The following problem does arise with this approach, however. We have
\begin{equation}
\Pi(\{\psi(\theta) = \psi_0\} \mid s) = \frac{\expc_\Pi(\indc_{\{\theta : \psi(\theta) = \psi_0\}}(\theta) f_\theta(s))}{m(s)} \leqslant \frac{\max_{\{\theta : \psi(\theta) = \psi_0\}} f_\theta(s) \Pi(\{\psi(\theta) = \psi_0\})}{m(s)}.
\label{eq:8.3.4}
\end{equation}
When $\Pi(\{\psi(\theta) = \psi_0\}) = 0$, \eqref{eq:8.3.4} implies that $\Pi(\{\psi(\theta) = \psi_0\} \mid s = 0)$ for every $s$. Therefore, using the Bayes rule, we would always reject $H_0$ no matter what data $s$ are obtained, which does not seem sensible. As discussed in Section \ref{ssec:7.2.3}, we have to be careful to make sure we use a prior $\Pi$ that assigns positive mass to $H_0$ if we are going to use the optimal Bayes approach to a hypothesis testing problem.

\subsection*{Summary of Section \ref{sec:8.3}}

\begin{itemize}
\item Optimal Bayesian procedures are obtained by minimizing the expected performance measure using the posterior distribution.
\item In estimation problems, when using squared error as the performance measure, the posterior mean is optimal.
\item In hypothesis testing problems, when minimizing the probability of making an error as the performance measure, then computing the posterior probability of the null hypothesis and accepting $H_0$ when this is greater than 1/2 is optimal.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:8.3.1}
Suppose that $S = \{1, 2, 3\}$, $\Omega = \{1, 2\}$, with data distributions given by the following table. We place a uniform prior on $\Omega$ and want to estimate $\theta$.
\begin{center}
\begin{tabular}{c|ccc}
 & $s = 1$ & $s = 2$ & $s = 3$ \\
\hline
$f_1(s)$ & $1/6$ & $1/6$ & $2/3$ \\
$f_2(s)$ & $1/4$ & $1/4$ & $1/2$
\end{tabular}
\end{center}
Using a Bayes rule, test the hypothesis $H_0 : \theta = 2$ when $s = 2$ is observed.
\end{exercise}

\begin{solution}
The posterior distribution of $\theta$ is given by
\[
\Pi(\theta = 1 \mid 2) = \frac{\frac{1}{2} \cdot \frac{1}{6}}{\frac{1}{2} \cdot \frac{1}{6} + \frac{1}{2} \cdot \frac{1}{4}} = \frac{2}{5}, \quad \Pi(\theta = 2 \mid 2) = \frac{\frac{1}{2} \cdot \frac{1}{4}}{\frac{1}{2} \cdot \frac{1}{6} + \frac{1}{2} \cdot \frac{1}{4}} = \frac{3}{5},
\]
so $\Pi(\theta = 2 \mid 2) > \Pi(\theta = 1 \mid 2)$ and we accept $H_0: \theta = 2$.
\end{solution}

\begin{exercise}
\label{exer:8.3.2}
For the situation described in Exercise~\ref{exer:8.3.1}, determine the Bayes rule estimator of $\theta$ when using expected squared error as our performance measure for estimators.
\end{exercise}

\begin{solution}
The Bayes rule is given by the posterior mean and this is given by $\frac{2}{5} + \frac{3}{5} \cdot 2 = \frac{8}{5}$.
\end{solution}

\begin{exercise}
\label{exer:8.3.3}
Suppose that we have a sample $(x_1, \ldots, x_n)$ from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu$ is unknown and $\sigma_0^2$ is known, and we want to estimate $\mu$ using expected squared error as our performance measure for estimators. If we use the prior distribution $\mu \sim \mathrm{N}(\mu_*, \tau_0^2)$, then determine the Bayes rule for this problem. Determine the limiting Bayes rule as $\tau_0 \to \infty$.
\end{exercise}

\begin{solution}
In Example \ref{ex:7.1.2} we determined that the posterior distribution of $\mu$ is given by the
\[
N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right), \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right)
\]
distribution. Then the Bayes rule is given by the posterior mean
\[
\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right) \to \left(\frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{n}{\sigma_0^2}\bar{x}\right) = \bar{x}
\]
as $\tau_0 \to \infty$.
\end{solution}

\begin{exercise}
\label{exer:8.3.4}
Suppose that we observe a sample $(x_1, \ldots, x_n)$ from a Bernoulli$(\theta)$ distribution, where $\theta$ is completely unknown, and we want to estimate $\theta$ using expected squared error as our performance measure for estimators. If we use the prior distribution $\theta \sim \text{Beta}(\alpha, \beta)$, then determine a Bayes rule for this problem.
\end{exercise}

\begin{solution}
From Example \ref{ex:7.1.1} we have that the posterior distribution of $\theta$ is $\text{Beta}(n\bar{x} + \alpha, n(1 - \bar{x}) + \beta)$. The Bayes rule is given by the posterior mean and this is evaluated in Example \ref{ex:7.2.2} to be $(n\bar{x} + \alpha)/(n + \alpha + \beta)$.
\end{solution}

\begin{exercise}
\label{exer:8.3.5}
Suppose that $(x_1, \ldots, x_n)$ is a sample from a Gamma$(\alpha_0, \lambda)$ distribution, where $\alpha_0$ is known, and $\lambda \sim \text{Gamma}(\alpha_*, \lambda_*)$, where $\alpha_*$ and $\lambda_*$ are known. If we want to estimate $\lambda$ using expected squared error as our performance measure for estimators, then determine the Bayes rule. Use the weak (or strong) law of large numbers to determine what this estimator converges to as $n \to \infty$.
\end{exercise}

\begin{solution}
The likelihood is given by $L(\beta \mid x_1, \ldots, x_n) = \beta^{n\alpha_0}\exp\{-\beta n\bar{x}\}$ and the prior density is $\pi(\beta) \propto \beta^{\tau_0 - 1}e^{-\upsilon_0\beta}$. Therefore, the posterior density is proportional to $\beta^{n\alpha_0 + \tau_0 - 1}\exp\{-(n\bar{x} + \upsilon_0)\beta\}$, and from this we deduce that the posterior distribution of $\beta$ is $\text{Gamma}(n\alpha_0 + \tau_0, n\bar{x} + \upsilon_0)$. The Bayes rule is given by the posterior mean and this equals $(n\alpha_0 + \tau_0)/(n\bar{x} + \upsilon_0)$. By the weak law of large numbers this converges in probability to (since $\expc_\beta(\bar{x}) = \alpha_0/\beta$) $\alpha_0/(\alpha_0/\beta) = \beta$ as $n \to \infty$.
\end{solution}

\begin{exercise}
\label{exer:8.3.6}
For the situation described in Exercise~\ref{exer:8.3.5}, determine the Bayes rule for estimating $\lambda^{-1}$ when using expected squared error as our performance measure for estimators.
\end{exercise}

\begin{solution}
The Bayes rule is given by the posterior mean of $1/\beta$ and this equals
\begin{align*}
&\frac{(n\bar{x} + \upsilon_0)^{n\alpha_0 + \tau_0}}{\Gamma(n\alpha_0 + \tau_0)} \int_0^{\infty} \left(\frac{1}{\beta}\right)\beta^{n\alpha_0 + \tau_0 - 1}\exp\{-(n\bar{x} + \upsilon_0)\beta\} \, \mathrm{d}\beta \\
&= \frac{(n\bar{x} + \upsilon_0)^{n\alpha_0 + \tau_0}}{\Gamma(n\alpha_0 + \tau_0)} \int_0^{\infty} \beta^{n\alpha_0 + \tau_0 - 2}\exp\{-(n\bar{x} + \upsilon_0)\beta\} \, \mathrm{d}\beta \\
&= \frac{(n\bar{x} + \upsilon_0)^{n\alpha_0 + \tau_0}}{\Gamma(n\alpha_0 + \tau_0)} \cdot \frac{\Gamma(n\alpha_0 + \tau_0 - 1)}{(n\bar{x} + \upsilon_0)^{n\alpha_0 + \tau_0 - 1}} = \frac{n\bar{x} + \upsilon_0}{n\alpha_0 + \tau_0 - 1}
\end{align*}
and this converges to $(\alpha_0/\beta)/\alpha_0 = \beta^{-1}$ as $n \to \infty$.
\end{solution}

\begin{exercise}
\label{exer:8.3.7}
Suppose that we have a sample $(x_1, \ldots, x_n)$ from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu$ is unknown and $\sigma_0^2$ is known, and we want to find the test of $H_0 : \mu = \mu_0$ that minimizes the prior probability of making an error (type I or type II). If we use the prior distribution $\mu \sim p_0 \indc_{\{\mu_0\}} + (1 - p_0)\mathrm{N}(\mu_0, \tau_0^2)$, where $p_0 \in (0, 1)$ is known (i.e., the prior is a mixture of a distribution degenerate at $\mu_0$ and an $\mathrm{N}(\mu_0, \tau_0^2)$ distribution), then determine the Bayes rule for this problem. Determine the limiting Bayes rule as $\tau_0 \to \infty$. (Hint: Make use of the computations in Example \ref{ex:7.2.13}.)
\end{exercise}

\begin{solution}
By Theorem \ref{thm:8.3.2} the Bayes rule is given by $\varphi(\bar{x}) = 1$ whenever the posterior probability of $H_0$ is less than or equal to the posterior probability of $H_0^c$. Equivalently, $\varphi(\bar{x}) = 1$ whenever the posterior probability of $H_0$ is less than or equal to $1/2$. By \eqref{eq:7.2.9} and Theorem \ref{thm:7.2.1} the posterior probability of $H_0$ is given by
\[
\Pi(\psi(\theta) = \psi_0 \mid s) = \frac{r \cdot \text{BF}_{H_0}}{1 + r \cdot \text{BF}_{H_0}}
\]
where $r = p_0/(1 - p_0)$, $\text{BF}_{H_0} = m_1(s)/m_2(s)$ and $m_i(s)$ is the prior predictive density of $s = (x_1, \ldots, x_n)$ under the prior $\Pi_i$, where $\Pi_1$ is the prior degenerate at $\mu_0$, and $\Pi_2$ is the $N(\mu_0, \sigma_0^2)$ prior. Note that $\Pi(\psi(\theta) = \psi_0 \mid s) \leqslant 1/2$ if and only if $\text{BF}_{H_0} \leqslant r^{-1}$. Then following Example \ref{ex:7.2.13} we have that
\begin{align*}
m_2(x_1, \ldots, x_n) &= (2\pi\sigma_0^2)^{-n/2}\exp\left(-\frac{n-1}{2\sigma_0^2}s^2\right)\tau_0^{-1}\exp\left(\frac{1}{2}\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right)^2\right) \\
&\quad \times \exp\left(-\frac{1}{2}\left(\frac{\mu_0^2}{\tau_0^2} + \frac{n\bar{x}^2}{\sigma_0^2}\right)\right)\left(\frac{n}{\sigma_0^2} + \frac{1}{\tau_0^2}\right)^{-1/2}.
\end{align*}
Because $\Pi_1$ is degenerate at $\mu_0$, it is immediate that the prior predictive under $\Pi_1$ is given by
\[
m_1(x_1, \ldots, x_n) = (2\pi\sigma_0^2)^{-n/2}\exp\left(-\frac{n-1}{2\sigma_0^2}s^2\right)\exp\left(-\frac{n}{2\sigma_0^2}(\bar{x} - \mu_0)^2\right).
\]
Therefore, $\text{BF}_{H_0}$ equals
\[
\text{BF}_{H_0} = \frac{\exp\left(-\frac{n}{2\sigma_0^2}(\bar{x} - \mu_0)^2\right)}{\tau_0^{-1}\left(\frac{n}{\sigma_0^2} + \frac{1}{\tau_0^2}\right)^{-1/2}\exp\left(\frac{1}{2}\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right)^2 - \frac{1}{2}\left(\frac{\mu_0^2}{\tau_0^2} + \frac{n\bar{x}^2}{\sigma_0^2}\right)\right)}
\]
and we reject whenever this is less than $(1 - p_0)/p_0$. As $\tau_0^2 \to \infty$ the denominator converges to 0 and so in the limit we never reject $H_0$.
\end{solution}

\begin{exercise}
\label{exer:8.3.8}
Suppose that we have a sample $(x_1, \ldots, x_n)$ from a Bernoulli$(\theta)$ distribution, where $\theta$ is unknown, and we want to find the test of $H_0 : \theta = \theta_0$ that minimizes the prior probability of making an error (type I or type II). If we use the prior distribution $\theta \sim p_0 \indc_{\{\theta_0\}} + (1 - p_0)\text{Uniform}[0, 1]$, where $p_0 \in (0, 1)$ is known (i.e., the prior is a mixture of a distribution degenerate at $\theta_0$ and a uniform distribution), then determine the Bayes rule for this problem.
\end{exercise}

\begin{solution}
Since the posterior distribution given data is the same as the posterior distribution given a minimal sufficient statistic, we base our calculations on $T = X_1 + \cdots + X_n \sim \text{Binomial}(n, \theta)$. Let $\Pi_1$ be the prior degenerate at $\theta_0$ and $\Pi_2$ be the $U[0, 1]$ prior. By \eqref{eq:7.2.9} and Theorem \ref{thm:7.2.1} the posterior probability of $H_0$ is given by
\[
\Pi(\psi(\theta) = \psi_0 \mid s) = \frac{r \cdot \text{BF}_{H_0}}{1 + r \cdot \text{BF}_{H_0}}
\]
where $r = p_0/(1 - p_0)$, $\text{BF}_{H_0} = m_1(t)/m_2(t)$ and $m_i(t)$ is the prior predictive density of $T$ when the prior $\Pi_i$ is being used. By Theorem \ref{thm:8.3.2} the Bayes rule is given by $\varphi(\bar{x}) = 1$ whenever $\Pi(\psi(\theta) = \psi_0 \mid s) \leqslant 1/2$, or, equivalently, $\text{BF}_{H_0} \leqslant r^{-1}$.

We have that $m_1(t) = \binom{n}{t}\theta_0^t(1 - \theta_0)^{n-t}$ and
\[
m_2(t) = \int_0^1 \binom{n}{t}\theta^t(1 - \theta)^{n-t} \, \mathrm{d}\theta = \binom{n}{t}\frac{\Gamma(t+1)\Gamma(n - t + 1)}{\Gamma(n + 2)}.
\]
Therefore,
\[
\text{BF}_{H_0} = \frac{\Gamma(n + 2)}{\Gamma(t + 1)\Gamma(n - t + 1)}\theta_0^t(1 - \theta_0)^{n-t}
\]
and we reject whenever this is less than $(1 - p_0)/p_0$.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:8.3.9}
Suppose that $\Omega = \{\theta_1, \theta_2\}$, that we put a prior $\pi$ on $\Omega$, and that we want to estimate $\theta$. Suppose our performance measure for estimators is the probability of making an incorrect choice of $\theta$. If the model is denoted $\{f_\theta : \theta \in \Omega\}$, then obtain the form of the Bayes rule when data $s$ are observed.
\end{exercise}

\begin{solution}
Suppose $T(s) \in \{\theta_1, \theta_2\}$ for each $s$. The Bayes rule will minimize
\begin{align*}
\expc_\Pi(\prb_\theta(T(s) \neq \theta)) &= \expc_\Pi(\expc_\theta(1 - \indc_{\{\theta\}}(T(s)))) \\
&= 1 - \expc_\Pi(\expc_\theta(\indc_{\{\theta\}}(T(s)))) = 1 - \expc_M(\expc_{\Pi(\cdot \mid s)}(\indc_{\{\theta\}}(T(s)))).
\end{align*}
Therefore, the Bayes rule at $s$ is given by $T(s)$ which maximizes
\[
\expc_{\Pi(\cdot \mid s)}(\indc_{\{\theta\}}(T(s))) = \Pi(\{\theta_1\} \mid s)\indc_{\{\theta_1\}}(T(s)) + \Pi(\{\theta_2\} \mid s)\indc_{\{\theta_2\}}(T(s))
\]
and this is clearly given by
\[
T(s) = \begin{cases} \theta_1 & \Pi(\{\theta_1\} \mid s) > \Pi(\{\theta_2\} \mid s) \\ \theta_2 & \Pi(\{\theta_2\} \mid s) > \Pi(\{\theta_1\} \mid s) \end{cases}
\]
and when $\Pi(\{\theta_1\} \mid s) = \Pi(\{\theta_2\} \mid s)$ we can take $T(s)$ to be either $\theta_1$ or $\theta_2$. So the Bayes rule is given by the posterior mode.
\end{solution}

\begin{exercise}
\label{exer:8.3.10}
For the situation described in Exercise~\ref{exer:8.3.1}, use the Bayes rule obtained via the method of Problem~\ref{exer:8.3.9} to estimate $\theta$ when $s = 2$. What advantage does this estimate have over that obtained in Exercise~\ref{exer:8.3.2}?
\end{exercise}

\begin{solution}
Since $\Pi(\theta = 2 \mid 2) > \Pi(\theta = 1 \mid 2)$, we have that the Bayes rule takes the value $T(s) = 2$ for this data. An advantage for this estimator over the posterior mean is that the posterior mode is always an element of the parameter space, while the posterior mean may not be, as in Exercise \ref{exer:8.3.1}. So if we use the posterior mean, we may estimate the parameter by a value that it could never possibly take.
\end{solution}

\begin{exercise}
\label{exer:8.3.11}
Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma^2)$ distribution where $(\mu, \sigma^2) \in \mathbb{R}^1 \times (0, \infty)$ is unknown, and want to estimate $\mu$ using expected squared error as our performance measure for estimators. Using the prior distribution given by
\[
\mu \mid \sigma^2 \sim \mathrm{N}(\mu_0, \tau_0^2 \sigma^2),
\]
and using
\[
\frac{1}{\sigma^2} \sim \text{Gamma}(\alpha_0, \beta_0),
\]
where $\mu_0$, $\tau_0^2$, $\alpha_0$, and $\beta_0$ are fixed and known, then determine the Bayes rule for $\mu$.
\end{exercise}

\begin{solution}
In Example \ref{ex:7.1.4} we derived the posterior distribution of $(\mu, 1/\sigma^2)$, and in Example \ref{ex:7.2.1} we derived the marginal posterior distribution of $\mu$ to be
\[
\mu_x + \sqrt{\frac{2\beta_x}{(2\alpha_0 + n)(n + 1/\tau_0^2)}}Z
\]
where $Z \sim t(n + 2\alpha_0)$ where $\mu_x = (n + 1/\tau_0^2)^{-1}(\mu_0/\tau_0^2 + n\bar{x})$ and
\[
\beta_x = \beta_0 + \frac{n}{2}\bar{x}^2 + \frac{\mu_0^2}{2\tau_0^2} + \frac{n-1}{2}s^2 - \frac{1}{2}\left(n + \frac{1}{\tau_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + n\bar{x}\right)^2.
\]
We know the Bayes rule is given by the posterior mean of $\mu$ and this equals
\[
\mu_x + \sqrt{\frac{2\beta_x}{(2\alpha_0 + n)(n + 1/\tau_0^2)}}\expc_{\Pi(\cdot \mid x_1, \ldots, x_n)}(Z) = \mu_x
\]
since the mean of a $\text{Student}(\lambda)$ random variable is 0 (provided $\lambda > 1$). So the Bayes rule is given by $\mu_x$.
\end{solution}

\begin{exercise}[Model selection]
\label{exer:8.3.12}
Generalize Problem~\ref{exer:8.3.9} to the case $\Omega = \{\theta_1, \ldots, \theta_k\}$.
\end{exercise}

\begin{solution}
Suppose $T(s) \in \{\theta_1, \ldots, \theta_k\}$ for each $s$. The Bayes rule will minimize
\begin{align*}
\expc_\Pi(\prb_\theta(T(s) \neq \theta)) &= \expc_\Pi(\expc_\theta(1 - \indc_{\{\theta\}}(T(s)))) \\
&= 1 - \expc_\Pi(\expc_\theta(\indc_{\{\theta\}}(T(s)))) = 1 - \expc_M(\expc_{\Pi(\cdot \mid s)}(\indc_{\{\theta\}}(T(s)))).
\end{align*}
Therefore, the Bayes rule at $s$ is given by $T(s)$, which maximizes
\[
\expc_{\Pi(\cdot \mid s)}(\indc_{\{\theta\}}(T(s))) = \sum_{i=1}^{k} \Pi(\{\theta_i\} \mid s)\indc_{\{\theta_i\}}(T(s))
\]
and this is clearly given by $T(s) = \theta_i$ whenever $\Pi(\{\theta_i\} \mid s) > \Pi(\{\theta_j\} \mid s)$ for every $j \neq i$. When more than one value of $\theta$ maximizes $\Pi(\{\theta\} \mid s)$ we can take $T(s)$ to be any of these values.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:8.3.13}
In Section \ref{ssec:7.2.4}, we described the Bayesian prediction problem. Using the notation found there, suppose we wish to predict $t \in \mathbb{R}^1$ using a predictor $\tilde{T}(s)$. If we assess the accuracy of a predictor by
\[
\expc((\tilde{T}(s) - t)^2) = \expc_\Pi(\expc_{\prb_\theta}(\expc_{Q_\theta(\cdot \mid s)}((\tilde{T}(s) - t)^2))),
\]
then determine the prior predictor that minimizes this quantity (assume all relevant expectations are finite). If we observe $s_0$, then determine the best predictor. (Hint: Assume all the probability measures are discrete.)
\end{exercise}

\begin{solution}
We have that
\begin{align*}
\expc\left((t^*(s) - t)^2\right) &= \expc_\Pi\left(\expc_{\prb_\theta}\left(\expc_{Q_\theta(\cdot \mid s)}\left((T^*(s) - t)^2\right)\right)\right) \\
&= \expc_M\left(\expc_{\prb_\theta(\cdot \mid s)}\left(\expc_{Q_\theta(\cdot \mid s)}\left((T^*(s) - t)^2\right)\right)\right) \\
&= \expc_M\left(\expc_{Q(\cdot \mid s)}\left((T^*(s) - t)^2\right)\right)
\end{align*}
where $Q(\cdot \mid s)$ is the posterior predictive measure for $t$ given $s$ (has density or probability function $q(t \mid s)$ as specified in Section \ref{ssec:7.2.4}). This is minimized if we can find $T^*(s)$ that minimizes $\expc_{Q(\cdot \mid s)}((T^*(s) - t)^2)$ for each $s$. By Theorem \ref{thm:8.1.1} this is minimized by taking $T^*(s) = \expc_{Q(\cdot \mid s)}(t)$, the posterior predictive mean of $t$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{solution}

\section{Decision Theory (Advanced)}
\label{sec:8.4}

To determine an optimal inference, we chose a performance measure and then attempted to find an inference, of a given type, that has optimal performance with respect to this measure. For example, when considering estimates of a real-valued characteristic of interest $\psi(\theta)$, we took the performance measure to be MSE and then searched for the estimator that minimizes this for each value of $\theta$.

Decision theory is closely related to the optimal approach to deriving inferences, but it is a little more specialized. In the decision framework, we take the point of view that, in any statistical problem, the statistician is faced with making a decision, e.g., deciding on a particular value for $\psi(\theta)$. Furthermore, associated with a decision is the notion of a loss incurred whenever the decision is incorrect. A decision rule is a procedure, based on the observed data $s$, that the statistician uses to select a decision. The decision problem is then to find a decision rule that minimizes the average loss incurred.

There are a number of real-world contexts in which losses are an obvious part of the problem, e.g., the monetary losses associated with various insurance plans that an insurance company may consider offering. So the decision theory approach has many applications. It is clear in many practical problems, however, that losses (as well as performance measures) are somewhat arbitrary components of a statistical problem, often chosen simply for convenience. In such circumstances, the approaches to deriving inferences described in Chapters 6 and 7 are preferred by many statisticians.

So the decision theory model for inference adds another ingredient to the sampling model (or to the sampling model and prior) to derive inferences---the loss function. To formalize this, we conceive of a set of possible actions or decisions that the statistician could take after observing the data $s$. This set of possible actions is denoted by $A$ and is called the action space. To connect these actions with the statistical model, there is a correct action function $A : \Omega \to A$ such that $A(\theta)$ is the correct action to take when $\theta$ is the true value of the parameter. Of course, because we do not know $\theta$, we do not know the correct action $A(\theta)$, so there is uncertainty involved in our decision. Consider a simple example.

\begin{example}
\label{ex:8.4.1}
Suppose you are told that an urn containing 100 balls has either 50 white and 50 black balls or 60 white and 40 black balls. Five balls are drawn from the urn without replacement and their colors are observed. The statistician's job is to make a decision about the true proportion of white balls in the urn based on these data.

The statistical model then comprises two distributions $\{\prb_1, \prb_2\}$ where, using parameter space $\Omega = \{1, 2\}$, $\prb_1$ is the Hypergeometric$(100, 50, 5)$ distribution (see Example \ref{ex:2.3.7}) and $\prb_2$ is the Hypergeometric$(100, 60, 5)$ distribution. The action space is $A = \{0.5, 0.6\}$, and $A : \Omega \to A$ is given by $A(1) = 0.5$ and $A(2) = 0.6$. The data are given by the colors of the five balls drawn.
\end{example}

We suppose now that there is also a loss or penalty $L(\theta, a)$ incurred when we select action $a \in A$ and $\theta$ is true. If we select the correct action, then the loss is 0; it is greater than 0 otherwise.

\begin{definition}
\label{def:8.4.1}
A \emph{loss function} is a function $L$ defined on $\Omega \times A$ and taking values in $[0, \infty)$ such that $L(\theta, a) = 0$ if and only if $a = A(\theta)$.
\end{definition}

Sometimes the loss can be an actual monetary loss. Actually, decision theory is a little more general than what we have just described, as we can allow for negative losses (gains or profits), but the restriction to nonnegative losses is suitable for purely statistical applications.

In a specific problem, the statistician chooses a loss function that is believed to lead to reasonable statistical procedures. This choice is dependent on the particular application. Consider some examples.

\begin{example}[Example~\ref{ex:8.4.1} continued]
\label{ex:8.4.2}
Perhaps a sensible choice in this problem would be
\[
L(\theta, a) = \begin{cases}
1 & \theta = 1, a = 0.6 \\
2 & \theta = 2, a = 0.5 \\
0 & \text{otherwise}.
\end{cases}
\]
Here we have decided that selecting $a = 0.5$ when it is not correct is a more serious error than selecting $a = 0.6$ when it is not correct. If we want to treat errors symmetrically, then we could take
\[
L(\theta, a) = \indc_{\{(1, 0.6), (2, 0.5)\}}(\theta, a),
\]
i.e., the losses are 1 or 0.
\end{example}

\begin{example}[Estimation as a Decision Problem]
\label{ex:8.4.3}
Suppose we have a marginal parameter $\psi(\theta)$ of interest, and we want to specify an estimate $T(s)$ after observing $s \in S$. Here, the action space is $A = \{\psi(\theta) : \theta \in \Omega\}$ and $A(\theta) = \psi(\theta)$. Naturally, we want $T(s) \in A$.

For example, suppose $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma^2)$ distribution, where $(\mu, \sigma^2) \in \Omega = \mathbb{R}^1 \times \mathbb{R}^+$ is unknown, and we want to estimate $\psi(\mu, \sigma^2) = \mu$. In this case, $A = \mathbb{R}^1$ and a possible estimator is the sample average $T(x_1, \ldots, x_n) = \bar{x}$.

There are many possible choices for the loss function. Perhaps a natural choice is to use
\begin{equation}
L(\theta, a) = |\psi(\theta) - a|,
\label{eq:8.4.1}
\end{equation}
the absolute deviation between $\psi(\theta)$ and $a$. Alternatively, it is common to use
\begin{equation}
L(\theta, a) = (\psi(\theta) - a)^2,
\label{eq:8.4.2}
\end{equation}
the squared deviations between $\psi(\theta)$ and $a$.
\end{example}

We refer to \eqref{eq:8.4.2} as squared error loss. Notice that \eqref{eq:8.4.2} is just the square of the Euclidean distance between $\psi(\theta)$ and $a$. It might seem more natural to actually use the distance \eqref{eq:8.4.1} as the loss function. It turns out, however, that there are a number of mathematical conveniences that arise from using squared distance.

\begin{example}[Hypothesis Testing as a Decision Problem]
\label{ex:8.4.4}
In this problem, we have a characteristic of interest $\psi(\theta)$ and want to assess the plausibility of the value $\psi_0$ after viewing the data $s$. In a hypothesis testing problem, this is written as $H_0 : \psi(\theta) = \psi_0$ versus $H_a : \psi(\theta) \neq \psi_0$. As in Section~\ref{sec:8.2}, we refer to $H_0$ as the null hypothesis and to $H_a$ as the alternative hypothesis.

The purpose of a hypothesis testing procedure is to decide which of $H_0$ or $H_a$ is true based on the observed data $s$. So in this problem, the action space is $A = \{H_0, H_a\}$ and the correct action function is
\[
A(\theta) = \begin{cases}
H_0 & \psi(\theta) = \psi_0 \\
H_a & \psi(\theta) \neq \psi_0.
\end{cases}
\]

An alternative, and useful, way of thinking of the two hypotheses is as subsets of $\Omega$. We write $H_0 = \psi^{-1}\{\psi_0\}$ as the subset of all $\theta$ values that make the null hypothesis true, and $H_a = H_0^c$ is the subset of all $\theta$ values that make the null hypothesis false. Then, based on the data $s$, we want to decide if the true value of $\theta$ is in $H_0$ or if $\theta$ is in $H_a$. If $H_0$ (or $H_a$) is composed of a single point, then it is called a simple hypothesis or a point hypothesis; otherwise, it is referred to as a composite hypothesis.

For example, suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma^2)$ distribution where $\theta = (\mu, \sigma^2) \in \Omega = \mathbb{R}^1 \times \mathbb{R}^+$, $\psi(\theta) = \mu$, and we want to test the null hypothesis $H_0 : \mu = 0$ versus the alternative $H_a : \mu \neq 0$. Then $H_0 = \{0\} \times \mathbb{R}^+$ and $H_a = \{0\}^c \times \mathbb{R}^+$. For the same model, let
\[
\psi(\theta) = \indc_{(-\infty, 0] \times \mathbb{R}^+}(\mu, \sigma^2),
\]
i.e., $\psi$ is the indicator function for the subset $(-\infty, 0] \times \mathbb{R}^+$. Then testing $H_0 : \psi = 1$ versus the alternative $H_a : \psi = 0$ is equivalent to testing that the mean is less than or equal to 0 versus the alternative that it is greater than 0. This one-sided hypothesis testing problem is often denoted as $H_0 : \mu \leqslant 0$ versus $H_a : \mu > 0$.

There are a number of possible choices for the loss function, but the most commonly used is of the form
\[
L(\theta, a) = \begin{cases}
0 & \theta \in H_0, a = H_0 \text{ or } \theta \in H_a, a = H_a \\
b & \theta \notin H_0, a = H_0 \\
c & \theta \notin H_a, a = H_a.
\end{cases}
\]
If we reject $H_0$ when $H_0$ is true (a type I error), we incur a loss of $c$; if we accept $H_0$ when $H_0$ is false (a type II error), we incur a loss of $b$. When $b = c$, we can take $b = c = 1$ and produce the commonly used 0--1 loss function.
\end{example}

A statistician faced with a decision problem---i.e., a model, action space, correct action function, and loss function---must now select a rule for choosing an element of the action space $A$ when the data $s$ are observed. A decision function is a procedure that specifies how an action is to be selected in the action space $A$.

\begin{definition}
\label{def:8.4.2}
A \emph{nonrandomized decision function} $d$ is a function $d : S \to A$.
\end{definition}

So after observing $s$, we decide that the appropriate action is $d(s)$.

Actually, we will allow our decision procedures to be a little more general than this, as we permit a random choice of an action after observing $s$.

\begin{definition}
\label{def:8.4.3}
A \emph{decision function} $\delta$ is such that $\delta(s, \cdot)$ is a probability measure on the action space $A$ for each $s \in S$ (so $\delta(s, A')$ is the probability that the action taken is in $A' \subset A$).
\end{definition}

Operationally, after observing $s$, a random mechanism with distribution specified by $\delta(s, \cdot)$ is used to select the action from the set of possible actions. Notice that if $\delta(s, \cdot)$ is a probability measure degenerate at the point $d(s)$ (so $\delta(s, \{d(s)\}) = 1$) for each $s$, then $\delta$ is equivalent to the nonrandomized decision function $d$ and conversely (see Problem~\ref{exer:8.4.8}).

The use of randomized decision procedures may seem rather unnatural, but, as we will see, sometimes they are an essential ingredient of decision theory. In many estimation problems, the use of randomized procedures provides no advantage, but this is not the case in hypothesis testing problems. We let $\mathcal{D}$ denote the set of all decision functions $\delta$ for the specific problem of interest.

The decision problem is to choose a decision function $\delta \in \mathcal{D}$. The selected $\delta$ will then be used to generate decisions in applications. We base this choice on how the various decision functions $\delta$ perform with respect to the loss function. Intuitively, we want to choose $\delta$ to make the loss as small as possible. For a particular $\theta$, because $s \sim f_\theta$ and $a \sim \delta(s, \cdot)$, the loss $L(\theta, a)$ is a random quantity. Therefore, rather than minimizing specific losses, we speak instead about minimizing some aspect of the distribution of the losses for each $\theta \in \Omega$. Perhaps a reasonable choice is to minimize the average loss. Accordingly, we define the risk function associated with $\delta \in \mathcal{D}$ as the average loss incurred by $\delta$. The risk function plays a central role in determining an appropriate decision function for a problem.

\begin{definition}
\label{def:8.4.4}
The \emph{risk function} associated with decision function $\delta$ is given by
\begin{equation}
R_\delta(\theta) = \expc_\theta(\expc_{\delta(s,\cdot)}(L(\theta, a))).
\label{eq:8.4.3}
\end{equation}
\end{definition}

Notice that to calculate the risk function we first calculate the average of $L(\theta, a)$, based on $s$ fixed and $a \sim \delta(s, \cdot)$. Then we average this conditional average with respect to $s \sim f_\theta$. By the theorem of total expectation, this is the average loss. When $\delta(s, \cdot)$ is degenerate at $d(s)$ for each $s$, then \eqref{eq:8.4.3} simplifies (see Problem~\ref{exer:8.4.8}) to
\[
R_\delta(\theta) = \expc_\theta(L(\theta, d(s))).
\]
Consider the following examples.

\begin{example}
\label{ex:8.4.5}
Suppose that $S = \{1, 2, 3\}$, $\Omega = \{1, 2\}$, and the distributions are given by the following table.
\begin{center}
\begin{tabular}{c|ccc}
 & $s = 1$ & $s = 2$ & $s = 3$ \\
\hline
$f_1(s)$ & $1/3$ & $1/3$ & $1/3$ \\
$f_2(s)$ & $1/2$ & $1/2$ & $0$
\end{tabular}
\end{center}
Further suppose that $A = \Omega$, $A(\theta) = \theta$, and the loss function is given by $L(\theta, a) = 1$ when $\theta \neq a$ but is 0 otherwise.

Now consider the decision function $\delta$ specified by the following table.
\begin{center}
\begin{tabular}{c|cc}
 & $a = 1$ & $a = 2$ \\
\hline
$\delta(1, \{a\})$ & $1/4$ & $3/4$ \\
$\delta(2, \{a\})$ & $1/4$ & $3/4$ \\
$\delta(3, \{a\})$ & $1$ & $0$
\end{tabular}
\end{center}
So when we observe $s = 1$, we randomly choose the action $a = 1$ with probability 1/4 and choose the action $a = 2$ with probability 3/4, etc. Notice that this decision function does the sensible thing and selects the decision $a = 1$ when we observe $s = 3$, as we know unequivocally that $\theta = 1$ in this case.

We have
\begin{align*}
\expc_{\delta(1,\cdot)}(L(\theta, a)) &= \frac{1}{4}L(\theta, 1) + \frac{3}{4}L(\theta, 2) \\
\expc_{\delta(2,\cdot)}(L(\theta, a)) &= \frac{1}{4}L(\theta, 1) + \frac{3}{4}L(\theta, 2) \\
\expc_{\delta(3,\cdot)}(L(\theta, a)) &= L(\theta, 1),
\end{align*}
so the risk function of $\delta$ is then given by
\begin{align*}
R_\delta(1) &= \expc_1(\expc_{\delta(s,\cdot)}(L(1, a))) \\
&= \frac{1}{3}\left(\frac{1}{4}L(1, 1) + \frac{3}{4}L(1, 2)\right) + \frac{1}{3}\left(\frac{1}{4}L(1, 1) + \frac{3}{4}L(1, 2)\right) + \frac{1}{3}L(1, 1) \\
&= \frac{3}{12} + \frac{3}{12} + 0 = \frac{1}{2}
\end{align*}
and
\begin{align*}
R_\delta(2) &= \expc_2(\expc_{\delta(s,\cdot)}(L(2, a))) \\
&= \frac{1}{2}\left(\frac{1}{4}L(2, 1) + \frac{3}{4}L(2, 2)\right) + \frac{1}{2}\left(\frac{1}{4}L(2, 1) + \frac{3}{4}L(2, 2)\right) + 0 \cdot L(2, 1) \\
&= \frac{1}{8} + \frac{1}{8} + 0 = \frac{1}{4}.
\end{align*}
\end{example}

\begin{example}[Estimation]
\label{ex:8.4.6}
We will restrict our attention to nonrandomized decision functions and note that these are also called estimators. The risk function associated with estimator $T$ and loss function \eqref{eq:8.4.1} is given by
\[
R_T(\theta) = \expc_\theta(|\psi(\theta) - T|)
\]
and is called the mean absolute deviation (MAD). The risk function associated with the estimator $T$ and loss function \eqref{eq:8.4.2} is given by
\[
R_T(\theta) = \expc_\theta((\psi(\theta) - T)^2)
\]
and is called the MSE.

We want to choose the estimator $T$ to minimize $R_T(\theta)$ for every $\theta \in \Omega$. Note that, when using \eqref{eq:8.4.2}, this decision problem is exactly the same as the optimal estimation problem discussed in Section~\ref{sec:8.1}.
\end{example}

\begin{example}[Hypothesis Testing]
\label{ex:8.4.7}
We note that for a given decision function $\delta$ for this problem, and a data value $s$, the distribution $\delta(s, \cdot)$ is characterized by $\phi(s) = \delta(s, H_a)$, which is the probability of rejecting $H_0$ when $s$ has been observed. This is because the probability measure $\delta(s, \cdot)$ is concentrated on two points, so we need only give its value at one of these to completely specify it. We call $\phi$ the test function associated with $\delta$ and observe that a decision function for this problem is also specified by a test function $\phi$.

We have immediately that
\begin{equation}
\expc_{\delta(s,\cdot)}(L(\theta, a)) = (1 - \phi(s))L(\theta, H_0) + \phi(s)L(\theta, H_a).
\label{eq:8.4.4}
\end{equation}
Therefore, when using the 0--1 loss function,
\begin{align*}
R_\delta(\theta) &= \expc_\theta((1 - \phi(s))L(\theta, H_0) + \phi(s)L(\theta, H_a)) \\
&= L(\theta, H_0) + \expc_\theta(\phi(s))(L(\theta, H_a) - L(\theta, H_0)) \\
&= \begin{cases}
\expc_\theta(\phi(s)) & \theta \in H_0 \\
1 - \expc_\theta(\phi(s)) & \theta \in H_a.
\end{cases}
\end{align*}

Recall that in Section \ref{ssec:6.3.6}, we introduced the power function associated with a hypothesis assessment procedure that rejected $H_0$ whenever the P-value was smaller than some prescribed value. The power function, evaluated at $\theta$, is the probability that such a procedure rejects $H_0$ when $\theta$ is the true value. Because $\phi(s)$ is the conditional probability, given $s$, that $H_0$ is rejected, the theorem of total expectation implies that $\expc_\theta(\phi(s))$ equals the unconditional probability that we reject $H_0$ when $\theta$ is the true value. So in general, we refer to the function
\[
\beta_\phi(\theta) = \expc_\theta(\phi(s))
\]
as the power function of the decision procedure $\delta$ or, equivalently, as the power function of the test function $\phi$.

Therefore, minimizing the risk function in this case is equivalent to choosing $\phi$ to minimize $\beta_\phi(\theta)$ for every $\theta \in H_0$ and to maximize $\beta_\phi(\theta)$ for every $\theta \in H_a$. Accordingly, this decision problem is exactly the same as the optimal inference problem discussed in Section~\ref{sec:8.2}.
\end{example}

Once we have written down all the ingredients for a decision problem, it is then clear what form a solution to the problem will take. In particular, any decision function $\delta_0$ that satisfies
\[
R_{\delta_0}(\theta) \leqslant R_\delta(\theta)
\]
for every $\theta \in \Omega$ and $\delta \in \mathcal{D}$ is an optimal decision function and is a solution. If two decision functions have the same risk functions, then, from the point of view of decision theory, they are equivalent. So it is conceivable that there might be more than one solution to a decision problem.

Actually, it turns out that an optimal decision function exists only in extremely unrealistic cases, namely, the data always tell us categorically what the correct decision is (see Problem~\ref{exer:8.4.9}). We do not really need statistical inference for such situations. For example, suppose we have two coins---coin A has two heads and coin B has two tails. As soon as we observe an outcome from a coin toss, we know exactly which coin was tossed and there is no need for statistical inference.

Still, we can identify some decision rules that we do not want to use. For example, if $\delta \in \mathcal{D}$ is such that there exists $\delta' \in \mathcal{D}$ satisfying $R_{\delta'}(\theta) \leqslant R_\delta(\theta)$ for every $\theta$, and if there is at least one $\theta$ for which $R_{\delta'}(\theta) < R_\delta(\theta)$, then naturally we strictly prefer $\delta'$ to $\delta$.

\begin{definition}
\label{def:8.4.5}
A decision function $\delta$ is said to be \emph{admissible} if there is no $\delta'$ that is strictly preferred to it.
\end{definition}

A consequence of decision theory is that we should use only admissible decision functions. Still, there are many admissible decision functions and typically none is optimal. Furthermore, a procedure that is only admissible may be a very poor choice (see Challenge~\ref{exer:8.4.11}).

There are several routes out of this impasse for decision theory. One approach is to use reduction principles. By this we mean that we look for an optimal decision function in some subclass $\mathcal{D}_0 \subset \mathcal{D}$ that is considered appropriate. So we then look for a $\delta_0 \in \mathcal{D}_0$ such that $R_{\delta_0}(\theta) \leqslant R_\delta(\theta)$ for every $\theta \in \Omega$ and $\delta \in \mathcal{D}_0$; i.e., we look for an optimal decision function in $\mathcal{D}_0$. Consider the following example.

\begin{example}[Size $\alpha$ Tests for Hypothesis Testing]
\label{ex:8.4.8}
Consider a hypothesis testing problem $H_0$ versus $H_a$. Recall that in Section~\ref{sec:8.2}, we restricted attention to those test functions $\phi$ that satisfy $\expc_\theta(\phi) \leqslant \alpha$ for every $\theta \in H_0$. Such a $\phi$ is called a size $\alpha$ test function for this problem. So in this case, we are restricting to the class $\mathcal{D}_0$ of all decision functions $\delta$ for this problem, which correspond to size $\alpha$ test functions.

In Section~\ref{sec:8.2}, we showed that sometimes there is an optimal $\delta \in \mathcal{D}_0$. For example, when $H_0$ and $H_a$ are simple, the Neyman--Pearson theorem (Theorem~\ref{thm:8.2.1}) provides an optimal $\phi$; thus, $\delta$, defined by $\delta(s, H_a) = \phi(s)$, is optimal. We also showed in Section~\ref{sec:8.2}, however, that in general there is no optimal size $\alpha$ test function $\phi$ and so there is no optimal $\delta \in \mathcal{D}_0$. In this case, further reduction principles are necessary.
\end{example}

Another approach to selecting a $\delta \in \mathcal{D}$ is based on choosing one particular real-valued characteristic of the risk function of $\delta$ and ordering the decision functions based on that. There are several possibilities.

One way is to introduce a prior $\pi$ into the problem and then look for the decision procedure $\delta \in \mathcal{D}$ that has smallest prior risk
\[
r_\delta = \expc_\pi(R_\delta(\theta)).
\]
We then look for a rule that has prior risk equal to $\min_{\delta \in \mathcal{D}} r_\delta$ (or $\inf_{\delta \in \mathcal{D}} r_\delta$). This approach is called Bayesian decision theory.

\begin{definition}
\label{def:8.4.6}
The quantity $r_\delta$ is called the \emph{prior risk} of $\delta$, $\min_{\delta \in \mathcal{D}} r_\delta$ is called the \emph{Bayes risk}, and a rule with prior risk equal to the Bayes risk is called a \emph{Bayes rule}.
\end{definition}

We derived Bayes rules for several problems in Section~\ref{sec:8.3}. Interestingly, Bayesian decision theory always effectively produces an answer to a decision problem. This is a very desirable property for any theory of statistics.

Another way to order decision functions uses the maximum (or supremum) risk. So for a decision function $\delta$, we calculate
\[
\max_{\theta \in \Omega} R_\delta(\theta)
\]
(or $\sup_{\theta \in \Omega} R_\delta(\theta)$) and then select a $\delta \in \mathcal{D}$ that minimizes this quantity. Such a $\delta$ has the smallest, largest risk or the smallest, worst behavior.

\begin{definition}
\label{def:8.4.7}
A decision function $\delta_0$ satisfying
\begin{equation}
\max_{\theta \in \Omega} R_{\delta_0}(\theta) = \min_{\delta \in \mathcal{D}} \max_{\theta \in \Omega} R_\delta(\theta)
\label{eq:8.4.5}
\end{equation}
is called a \emph{minimax decision function}.
\end{definition}

Again, this approach will always effectively produce an answer to a decision problem (see Problem~\ref{exer:8.4.10}).

Much more can be said about decision theory than this brief introduction to the basic concepts. Many interesting, general results have been established for the decision theoretic approach to statistical inference.

\subsection*{Summary of Section \ref{sec:8.4}}

\begin{itemize}
\item The decision theoretic approach to statistical inference introduces an action space $A$ and a loss function $L$.
\item A decision function $\delta$ prescribes a probability distribution $\delta(s, \cdot)$ on $A$. The statistician generates a decision in $A$ using this distribution after observing $s$.
\item The problem in decision theory is to select $\delta$; for this, the risk function $R_\delta(\theta)$ is used. The value $R_\delta(\theta)$ is the average loss incurred when using the decision function $\delta$, and the goal is to minimize risk.
\item Typically, no optimal decision function $\delta$ exists. So, to select a $\delta$, various reduction criteria are used to reduce the class of possible decision functions, or the decision functions are ordered using some real-valued characteristic of their risk functions, e.g., maximum risk or average risk with respect to some prior.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:8.4.1}
Suppose we observe a sample $(x_1, \ldots, x_n)$ from a Bernoulli$(\theta)$ distribution, where $\theta$ is completely unknown, and we want to estimate $\theta$ using squared error loss. Write out all the ingredients of this decision problem. Calculate the risk function of the estimator $T(x_1, \ldots, x_n) = \bar{x}$. Graph the risk function when $n = 10$.
\end{exercise}

\begin{solution}
The model is given by the collection of probability functions $\{\theta^{n\bar{x}}(1 - \theta)^{n - n\bar{x}} : \theta \in [0, 1]\}$ on the set of all sequences $(x_1, \ldots, x_n)$ of 0's and 1's. The action space is $\mathcal{A} = [0, 1]$, the correct action function is $A(\theta) = \theta$, and the loss function is $L(\theta, a) = (\theta - a)^2$.

The risk function for $T$ is given by $R_T(\theta) = \expc_\theta((\theta - \bar{x})^2) = \var_\theta(\bar{x}) = \theta(1 - \theta)/n$. This is plotted below.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig_841.pdf}
\caption{Risk function for the sample mean estimator of the Bernoulli parameter.}
%\label{fig:risk-841}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:8.4.2}
Suppose we have a sample $(x_1, \ldots, x_n)$ from a Poisson$(\lambda)$ distribution, where $\lambda$ is completely unknown, and we want to estimate $\lambda$ using squared error loss. Write out all the ingredients of this decision problem. Consider the estimator $T(x_1, \ldots, x_n) = \bar{x}$ and calculate its risk function. Graph the risk function when $n = 25$.
\end{exercise}

\begin{solution}
The model is given by the collection of probability functions
\[
\{\lambda^{n\bar{x}}e^{-n\lambda}/\Pi_{i=1}^{n}x_i! : \lambda \geqslant 0\}
\]
on the set of all sequences $(x_1, \ldots, x_n)$ of nonnegative integers. The action space is $\mathcal{A} = [0, \infty)$, the correct action function is $A(\lambda) = \lambda$, and the loss function is $L(\lambda, a) = (\lambda - a)^2$.

The risk function for $T$ is given by $R_T(\lambda) = \expc_\lambda((\lambda - \bar{x})^2) = \var_\lambda(\bar{x}) = \lambda/n$. This is plotted below for $n = 25$.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig_842.pdf}
\caption{Risk function for the sample mean estimator of the Poisson rate parameter with $n = 25$.}
%\label{fig:risk-842}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:8.4.3}
Suppose we have a sample $(x_1, \ldots, x_n)$ from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu$ is unknown and $\sigma_0^2$ is known, and we want to estimate $\mu$ using squared error loss. Write out all the ingredients of this decision problem. Consider the estimator $T(x_1, \ldots, x_n) = \bar{x}$ and calculate its risk function. Graph the risk function when $n = 25$, $\sigma_0^2 = 2$.
\end{exercise}

\begin{solution}
The model is given by the collection of density functions
\[
\left\{\frac{1}{\sqrt{2\pi}\sigma_0}\exp\left\{-\frac{1}{2\sigma_0^2}\sum_{i=1}^{n}(x_i - \mu)^2\right\} : \mu \in \mathbb{R}^1\right\}
\]
on the set of all sequences $(x_1, \ldots, x_n)$ of real numbers. The action space is $\mathcal{A} = \mathbb{R}^1$, the correct action function is $A(\mu) = \mu$, and the loss function is $L(\mu, a) = (\mu - a)^2$.

The risk function for $T$ is given by $R_T(\mu) = \expc_\mu((\mu - \bar{x})^2) = \var_\mu(\bar{x}) = \frac{\sigma_0^2}{n}$. This is plotted below for $n = 25$ and $\sigma_0^2 = 2$ (note $2/25 = 0.08$).

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig_843.pdf}
\caption{Risk function for the sample mean estimator of the normal mean with $n = 25$ and $\sigma_0^2 = 2$.}
%\label{fig:risk-843}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:8.4.4}
Suppose we observe a sample $(x_1, \ldots, x_n)$ from a Bernoulli$(\theta)$ distribution, where $\theta$ is completely unknown, and we want to test the null hypothesis that $\theta = 1/2$ versus the alternative that it is not equal to this quantity, and we use 0-1 loss. Write out all the ingredients of this decision problem. Suppose we reject the null hypothesis whenever we observe $n\bar{x} \in \{0, 1, n-1, n\}$. Determine the form of the test function and its associated power function. Graph the power function when $n = 10$.
\end{exercise}

\begin{solution}
The model is given by the collection of probability functions $\{\theta^{n\bar{x}}(1 - \theta)^{n - n\bar{x}} : \theta \in [0, 1]\}$ on the set of all sequences $(x_1, \ldots, x_n)$ of 0's and 1's. The action space is $\mathcal{A} = \{H_0, H_a\}$, where $H_0: \theta = 1/2$, the correct action function is
\[
A(\theta) = \begin{cases} H_0 & \theta = 1/2 \\ H_a & \theta \neq 1/2 \end{cases}
\]
and the loss function is
\[
L(\theta, a) = \begin{cases} 0 & \theta = 1/2, a = H_0 \text{ or } \theta \neq 1/2, a = H_a \\ 1 & \theta = 1/2, a = H_a \text{ or } \theta \neq 1/2, a = H_0. \end{cases}
\]

The test function $\varphi$ is given by
\[
\varphi(n\bar{x}) = \begin{cases} 0 & n\bar{x} \notin \{0, 1, n-1, n\} \\ 1 & n\bar{x} \in \{0, 1, n-1, n\}. \end{cases}
\]

The risk function for $\varphi$ is given by
\begin{align*}
R_\varphi(\theta) &= \prb_\theta(\varphi(n\bar{x}) = 1) = \prb_\theta(\{0, 1, n-1, n\}) \\
&= \binom{n}{0}(1 - \theta)^n + \binom{n}{1}\theta(1 - \theta)^{n-1} + \binom{n}{n-1}\theta^{n-1}(1 - \theta) + \binom{n}{n}\theta^n.
\end{align*}

A plot of $R_\varphi$, when $n = 10$ (the power equals $2.1484 \times 10^{-2}$ at $\theta = 1/2$) follows.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig_844.pdf}
\caption{Risk function for the hypothesis test with $n = 10$.}
%\label{fig:risk-844}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:8.4.5}
Consider the decision problem with sample space $S = \{1, 2, 3, 4\}$, parameter space $\Omega = \{a, b\}$, with the parameter indexing the distributions given in the following table.
\begin{center}
\begin{tabular}{c|cccc}
 & $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$f_a(s)$ & $1/4$ & $1/4$ & $0$ & $1/2$ \\
$f_b(s)$ & $1/2$ & $0$ & $1/4$ & $1/4$
\end{tabular}
\end{center}
Suppose that the action space $A = \Omega$, with $A(\theta) = \theta$, and the loss function is given by $L(\theta, a) = 1$ when $a \neq A(\theta)$ and is equal to 0 otherwise.
\begin{enumerate}[(a)]
\item Calculate the risk function of the deterministic decision function given by $d(1) = d(2) = d(3) = a$ and $d(4) = b$.
\item Is $d$ in part (a) optimal?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
\item The risk function is given by
\begin{align*}
R_d(a) &= \expc_a(L(a, d(s))) \\
&= \frac{1}{4}L(a, d(1)) + \frac{1}{4}L(a, d(2)) + 0 \cdot L(a, d(3)) + \frac{1}{2}L(a, d(4)) \\
&= \frac{1}{4}L(a, a) + \frac{1}{4}L(a, a) + \frac{1}{2}L(a, b) = \frac{1}{2}L(a, b) = \frac{1}{2}, \\
R_d(b) &= \expc_b(L(b, d(s))) \\
&= \frac{1}{2}L(b, d(1)) + 0 \cdot L(b, d(2)) + \frac{1}{4}L(b, d(3)) + \frac{1}{4}L(b, d(4)) \\
&= \frac{1}{2}L(b, a) + \frac{1}{4}L(b, a) + \frac{1}{4}L(b, b) = \frac{1}{2}L(b, a) + \frac{1}{4}L(b, a) = \frac{3}{4}.
\end{align*}

\item Consider the risk function of the decision function $d^*$ given by $d^*(1) = b$, $d^*(2) = a$, $d^*(3) = b$, $d^*(4) = a$. The risk function is given by
\begin{align*}
R_{d^*}(a) &= \expc_a(L(a, d^*(s))) \\
&= \frac{1}{4}L(a, d^*(1)) + \frac{1}{4}L(a, d^*(2)) + 0 \cdot L(a, d^*(3)) + \frac{1}{2}L(a, d^*(4)) \\
&= \frac{1}{4}L(a, b) + \frac{1}{4}L(a, a) + \frac{1}{2}L(a, a) = \frac{1}{4}L(a, b) = \frac{1}{4}, \\
R_{d^*}(b) &= \expc_b(L(b, d^*(s))) \\
&= \frac{1}{2}L(b, d^*(1)) + 0 \cdot L(b, d^*(2)) + \frac{1}{4}L(b, d^*(3)) + \frac{1}{4}L(b, d^*(4)) \\
&= \frac{1}{2}L(b, b) + \frac{1}{4}L(b, b) + \frac{1}{4}L(b, a) = \frac{1}{4}L(b, a) = \frac{1}{4},
\end{align*}
so $R_{d^*}(a) < R_d(a)$, $R_{d^*}(b) < R_d(b)$ and $d$ is not optimal.
\end{enumerate}
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:8.4.6}
Suppose we have a sample $(x_1, \ldots, x_n)$ from a Poisson$(\lambda)$ distribution, where $\lambda$ is completely unknown, and we want to test the hypothesis that $\lambda \leqslant \lambda_0$ versus the alternative that $\lambda > \lambda_0$, using the 0--1 loss function. Write out all the ingredients of this decision problem. Suppose we decide to reject the null hypothesis whenever $n\bar{x} > \lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor$ and randomly reject the null hypothesis with probability 1/2 when $n\bar{x} = \lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor$. Determine the form of the test function and its associated power function. Graph the power function when $\lambda_0 = 1$ and $n = 5$.
\end{exercise}

\begin{solution}
The model is given by the collection of probability functions
\[
\left\{\left(\prod_{i=1}^{n}x_i!\right)^{-1}\lambda^{n\bar{x}}e^{-n\lambda} : \lambda \geqslant 0\right\}
\]
on the set of all sequences $(x_1, \ldots, x_n)$ of nonnegative integers. The action space is $\mathcal{A} = \{H_0, H_a\}$, where $H_0: \lambda \leqslant \lambda_0$. The correct action function is
\[
A(\lambda) = \begin{cases} H_0 & \lambda \leqslant \lambda_0 \\ H_a & \lambda > \lambda_0 \end{cases}
\]
and the loss function is
\[
L(\lambda, a) = \begin{cases} 0 & \lambda \leqslant \lambda_0, a = H_0 \text{ or } \lambda > \lambda_0, a = H_a \\ 1 & \lambda \leqslant \lambda_0, a = H_a \text{ or } \lambda > \lambda_0, a = H_0. \end{cases}
\]

The test function $\varphi$ is given by
\[
\varphi(x_1, \ldots, x_n) = \begin{cases} 0 & n\bar{x} < \lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor \\ 1/2 & n\bar{x} = \lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor \\ 1 & n\bar{x} > \lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor. \end{cases}
\]

The power function for $\varphi$ is given by (using $n\bar{x} \sim \text{Poisson}(\lambda)$)
\begin{align*}
\beta_\varphi(\lambda) &= \prb_\lambda(\varphi(x_1, \ldots, x_n) = 1) \\
&= \frac{1}{2}\prb_\lambda\left(n\bar{x} = \lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor\right) + \prb_\lambda\left(n\bar{x} > \lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor\right) \\
&= \frac{1}{2}\frac{(n\lambda)^{\lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor}}{(\lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor)!}\exp\{-n\lambda\} + \sum_{k = \lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor + 1}^{\infty} \frac{(n\lambda)^k}{k!}\exp\{-n\lambda\}.
\end{align*}

When $\lambda_0 = 1$ and $n = 5$ then $\lfloor n\lambda_0 + 2\sqrt{n\lambda_0} \rfloor = 9$, so the power function is given by
\begin{align*}
R_\varphi(\lambda) &= \frac{1}{2}\frac{(10\lambda)^9}{(9)!}\exp\{-5\lambda\} + \sum_{k=10}^{\infty}\frac{(5\lambda)^k}{k!}\exp\{-5\lambda\} \\
&= \frac{1}{2}\frac{(5\lambda)^9}{(9)!}\exp\{-5\lambda\} + 1 - \sum_{k=0}^{9}\frac{(5\lambda)^k}{k!}\exp\{-5\lambda\}.
\end{align*}
This is plotted below.

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig_846.pdf}
\caption{Power function for the Poisson test with $\lambda_0 = 1$ and $n = 5$.}
%\label{fig:power-846}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:8.4.7}
Suppose we have a sample $(x_1, \ldots, x_n)$ from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu$ is unknown and $\sigma_0^2$ is known, and we want to test the null hypothesis that the mean response is $\mu_0$ versus the alternative that the mean response is not equal to $\mu_0$, using the 0--1 loss function. Write out all the ingredients of this decision problem. Suppose that we decide to reject whenever $\bar{x} \notin [\mu_0 - 2\sigma_0/\sqrt{n}, \mu_0 + 2\sigma_0/\sqrt{n}]$. Determine the form of the test function and its associated power function. Graph the power function when $\mu_0 = 0$, $\sigma_0 = 3$, and $n = 10$.
\end{exercise}

\begin{solution}
The model is given by the collection of density functions
\[
\left\{\frac{1}{\sqrt{2\pi}\sigma_0}\exp\left\{-\frac{1}{2\sigma_0^2}\sum_{i=1}^{n}(x_i - \mu)^2\right\} : \mu \in \mathbb{R}^1\right\}
\]
on the set of all sequences $(x_1, \ldots, x_n)$ of real numbers. The action space is $\mathcal{A} = \{H_0, H_a\}$, where $H_0: \mu = \mu_0$. The correct action function is
\[
A(\lambda) = \begin{cases} H_0 & \lambda \leqslant \lambda_0 \\ H_a & \lambda > \lambda_0 \end{cases}
\]
and the loss function is
\[
L(\lambda, a) = \begin{cases} 0 & \mu = \mu_0, a = H_0 \text{ or } \mu \neq \mu_0, a = H_a \\ 1 & \mu = \mu_0, a = H_a \text{ or } \mu \neq \mu_0, a = H_0. \end{cases}
\]

The test function $\varphi$ is given by
\[
\varphi(x_1, \ldots, x_n) = \begin{cases} 0 & \bar{x} \in [\mu_0 - 2\sigma_0/\sqrt{n}, \mu_0 + 2\sigma_0/\sqrt{n}] \\ 1 & \bar{x} \notin [\mu_0 - 2\sigma_0/\sqrt{n}, \mu_0 + 2\sigma_0/\sqrt{n}]. \end{cases}
\]

The power function for $\varphi$ is given by (using $\bar{x} \sim N(\mu, \sigma_0^2/n)$)
\begin{align*}
\beta_\varphi(\mu) &= \prb_\mu(\varphi(x_1, \ldots, x_n) = 1) = 1 - \prb_\mu\left(\bar{x} \in [\mu_0 - 2\sigma_0/\sqrt{n}, \mu_0 + 2\sigma_0/\sqrt{n}]\right) \\
&= 1 - \prb_\mu\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - 2 < \frac{\bar{x} - \mu}{\sigma_0/\sqrt{n}} < \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + 2\right) \\
&= 1 - \left(\Phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + 2\right) - \Phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - 2\right)\right).
\end{align*}

When $\mu_0 = 0$, $\sigma_0 = 3$, $n = 10$ the power function is
\[
\beta_\varphi(\mu) = 1 - \left(\Phi\left(-\frac{\mu}{3/\sqrt{10}} + 2\right) - \Phi\left(-\frac{\mu}{3/\sqrt{10}} - 2\right)\right).
\]
This is plotted below (power equals 0.0455 at $\mu = 0$).

\begin{figure}[!htbp]
\centering
%\includegraphics[scale=0.5]{fig_847.pdf}
\caption{Power function for the normal mean test with $\mu_0 = 0$, $\sigma_0 = 3$, and $n = 10$.}
%\label{fig:power-847}
\end{figure}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:8.4.8}
Prove that a decision function $\delta$ that gives a probability measure $\delta(s, \cdot)$ degenerate at $d(s)$ for each $s \in S$ is equivalent to specifying a function $d : S \to A$ and conversely. For such a $\delta$, prove that $R_\delta(\theta) = \expc_\theta(L(\theta, d(s)))$.
\end{exercise}

\begin{solution}
Suppose we have that $\delta(s, \cdot)$ is degenerate at $d(s)$ for each $s$. Then clearly $d: S \to \mathcal{A}$.

Now suppose we have $d: S \to \mathcal{A}$ and define
\[
\delta(s, B) = \begin{cases} 1 & d(s) \in B \\ 0 & \text{otherwise} \end{cases}
\]
for $B \subset \mathcal{A}$. Then $\delta(s, \mathcal{A}) = 1$ and, if $B_1, B_2, \ldots$ are mutually disjoint subsets of $\mathcal{A}$, then $d(s) \in B_i$ for one $i$ (and only one) if and only if $d(s) \in \cup_{j=1}^{\infty}B_j$, so $\delta(s, \cup_{j=1}^{\infty}B_j) = \sum_{j=1}^{\infty}\delta(s, B_j)$. Therefore, $\delta(s, \cdot)$ is a probability measure for each $s$ and $\delta$ is a decision function.

Now, using the fact that $\delta(s, \cdot)$ is a discrete probability measure degenerate at $d(s)$, we have that $R_\delta(\theta) = \expc_\theta(\expc_{\delta(s, \cdot)}(L(\theta, a))) = \expc_\theta(\delta(s, \{d(s)\})(L(\theta, d(s)))) = \expc_\theta(L(\theta, d(s)))$ since $\delta(s, \{d(s)\}) = 1$.
\end{solution}

\begin{exercise}
\label{exer:8.4.9}
Suppose we have a decision problem and that each probability distribution in the model is discrete.
\begin{enumerate}[(a)]
\item Prove that $\delta$ is optimal in $\mathcal{D}$ if and only if $\delta(s, \cdot)$ is degenerate at $A(\theta)$ for each $s$ for which $\prb_\theta(\{s\}) > 0$.
\item Prove that if there exist $\theta_1, \theta_2 \in \Omega$ such that $A(\theta_1) \neq A(\theta_2)$, and $\prb_{\theta_1}, \prb_{\theta_2}$ are not concentrated on disjoint sets, then there is no optimal $\delta \in \mathcal{D}$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item Consider the decision function $d_{\theta_0}(s) \equiv A(\theta_0)$. Then note that $R_{d_{\theta_0}}(\theta_0) = 0$. Then, if $\delta$ is optimal, we must have that $R_\delta(\theta_0) \leqslant R_{d_{\theta_0}}(\theta_0)$ for every $\theta_0$, so $R_\delta(\theta) \equiv 0$. But this implies that $\expc_{\delta(s, \cdot)}(L(\theta, a)) = 0$ at every $s$, where $\prb_\theta(\{s\}) > 0$. Since $L(\theta, a) \geqslant 0$, then Challenge \ref{exer:3.3.29} implies that $\delta(s, \{L(\theta, a) = 0\}) = 1$ and, since $L(\theta, a) = 0$ if and only if $a = A(\theta)$, this implies that $\delta(s, \cdot)$ is degenerate at $A(\theta)$ for each $s$ for which $\prb_\theta(\{s\}) > 0$.
\item Part (a) proved that, for an optimal $\delta$, $\delta(s, \cdot)$ is degenerate at $A(\theta)$ for each $s$ for which $\prb_\theta(\{s\}) > 0$. But if there exists $s$ such that $\prb_{\theta_1}(\{s\}) > 0$ and $\prb_{\theta_2}(\{s\}) > 0$ and $A(\theta_1) \neq A(\theta_2)$, then this cannot happen and so no optimal $\delta$ can exist.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:8.4.10}
If decision function $\delta$ has constant risk and is admissible, then prove that $\delta$ is minimax.
\end{exercise}

\begin{solution}
Suppose $\delta$ is not minimax. Then there exists decision function $\delta^*$ such that $\sup_\theta R_{\delta^*}(\theta) < \sup_\theta R_\delta(\theta)$. But since $R_\delta(\theta)$ is constant in $\theta$ this implies that $R_{\delta^*}(\theta) < R_\delta(\theta)$ for every $\theta$ and so $\delta$ is not admissible, contradicting the hypothesis. Therefore, $\delta$ must be minimax.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:8.4.11}
Suppose we have a decision problem in which $\theta_0 \in \Omega$ is such that $\prb_{\theta_0}(C) = 0$ implies that $\prb_\theta(C) = 0$ for every $\theta \in \Omega$. Further assume that there is no optimal decision function (see Problem~\ref{exer:8.4.9}). Then prove that the nonrandomized decision function $d$ given by $d(s) \equiv A(\theta_0)$ is admissible. What does this result tell you about the concept of admissibility?
\end{exercise}

\begin{solution}
We have that $R_d(\theta_0) = \expc_{\theta_0}(L(\theta_0, d(s))) = 0$. Now suppose that $d$ is not admissible. Then there exists decision function $\delta$ such that $R_\delta(\theta) \leqslant R_d(\theta)$ for every $\theta$ and $R_\delta(\theta) < R_d(\theta)$ for some $\theta$. But this implies that $0 = R_\delta(\theta_0) = \expc_{\theta_0}(\expc_{\delta(s, \cdot)}(L(\theta_0, a))) = 0$ and then Challenge \ref{exer:3.3.29} implies that the set $C = \{s : \expc_{\delta(s, \cdot)}(L(\theta_0, a)) > 0\}$ satisfies $\prb_{\theta_0}(C) = 0$. But by hypothesis this implies that $\prb_\theta(C) = 0$ for every $\theta$. This in turn implies that $R_\delta(\theta) = 0$ for every $\theta$. This says $\delta$ is optimal and contradicts the hypothesis that no such decision function exists.

In most practical problems, there does not exist an optimal decision function. So this result says that, in the typical decision problem, constants are admissible, i.e., decision functions that completely ignore the data are admissible. So the property of admissibility for a decision function is not a very strong one.
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:8.4.12}
Comment on the following statement: A natural requirement for any theory of inference is that it produce an answer for every inference problem posed. Have we discussed any theories so far that you believe will satisfy this?
\end{exercise}

\begin{exercise}
\label{exer:8.4.13}
Decision theory produces a decision in a given problem. It says nothing about how likely it is that the decision is in error. Some statisticians argue that a valid approach to inference must include some quantification of our uncertainty concerning any statement we make about an unknown, as only then can a recipient judge the reliability of the inference. Comment on this.
\end{exercise}

\section{Further Proofs (Advanced)}
\label{sec:8.5}

\subsection*{Proof of Theorem~\ref{thm:8.1.2}}

We want to show that a statistic $U$ is sufficient for a model if and only if the conditional distribution of the data $s$ given $U = u$ is the same for every $\theta \in \Omega$.

We prove this in the discrete case so that $f_\theta(s) = \prb_\theta(\{s\})$. The general case requires more mathematics, and we leave that to a further course.

Let $u$ be such that $\prb_\theta(U^{-1}\{u\}) > 0$ where $U^{-1}\{u\} = \{s : U(s) = u\}$, so $U^{-1}\{u\}$ is the set of values of $s$ such that $U(s) = u$. We have
\begin{equation}
\prb_\theta(s = s_1 \mid U = u) = \frac{\prb_\theta(s = s_1, U = u)}{\prb_\theta(U = u)}.
\label{eq:8.5.1}
\end{equation}
Whenever $s_1 \notin U^{-1}\{u\}$,
\[
\prb_\theta(s = s_1, U = u) = \prb_\theta(\{s_1\} \cap \{s : U(s) = u\}) = \prb_\theta(\emptyset) = 0
\]
independently of $\theta$. Therefore, $\prb_\theta(s = s_1 \mid U = u) = 0$ independently of $\theta$.

So let us suppose that $s_1 \in U^{-1}\{u\}$. Then
\[
\prb_\theta(s = s_1, U = u) = \prb_\theta(\{s_1\} \cap \{s : U(s) = u\}) = \prb_\theta(\{s_1\}) = f_\theta(s_1).
\]
If $U$ is a sufficient statistic, the factorization theorem (Theorem \ref{thm:6.1.1}) implies $f_\theta(s) = h(s)g_\theta(U(s))$ for some $h$ and $g$. Therefore, since
\[
\prb_\theta(U = u) = \sum_{s \in U^{-1}\{u\}} f_\theta(s),
\]
\eqref{eq:8.5.1} equals
\[
\frac{f_\theta(s_1)}{\sum_{s \in U^{-1}\{u\}} f_\theta(s)} = \frac{f_\theta(s_1)}{\sum_{s \in U^{-1}\{u\}} c(s, s_1) f_\theta(s_1)} = \frac{1}{\sum_{s \in U^{-1}\{u\}} c(s, s_1)}
\]
where
\[
\frac{f_\theta(s)}{f_\theta(s_1)} = \frac{h(s)}{h(s_1)} = c(s, s_1).
\]
We conclude that \eqref{eq:8.5.1} is independent of $\theta$.

Conversely, if \eqref{eq:8.5.1} is independent of $\theta$, then for $s_1, s_2 \in U^{-1}\{u\}$ we have
\[
\prb_\theta(U = u) = \frac{\prb_\theta(s = s_2)}{\prb_\theta(s = s_2 \mid U = u)}.
\]
Thus
\begin{align*}
f_\theta(s_1) &= \prb_\theta(s = s_1) = \prb_\theta(s = s_1 \mid U = u) \prb_\theta(U = u) \\
&= \prb_\theta(s = s_1 \mid U = u) \frac{\prb_\theta(s = s_2)}{\prb_\theta(s = s_2 \mid U = u)} \\
&= \frac{\prb_\theta(s = s_1 \mid U = u)}{\prb_\theta(s = s_2 \mid U = u)} f_\theta(s_2) = c(s_1, s_2) f_\theta(s_2),
\end{align*}
where
\[
c(s_1, s_2) = \frac{\prb_\theta(s = s_1 \mid U = u)}{\prb_\theta(s = s_2 \mid U = u)}.
\]
By the definition of sufficiency in Section \ref{ssec:6.1.1}, this establishes the sufficiency of $U$.

\subsection*{Establishing the Completeness of $\bar{x}$ in Example~\ref{ex:8.1.3}}

Suppose that $(x_1, \ldots, x_n)$ is a sample from an $\mathrm{N}(\mu, \sigma_0^2)$ distribution, where $\mu \in \mathbb{R}^1$ is unknown and $\sigma_0^2 > 0$ is known. In Example \ref{ex:6.1.7}, we showed that $\bar{x}$ is a minimal sufficient statistic.

Suppose that the function $h$ is such that $\expc_\mu(h(\bar{X})) = 0$ for every $\mu \in \mathbb{R}^1$. Then defining
\[
h^+(\bar{x}) = \max(0, h(\bar{x})) \quad \text{and} \quad h^-(\bar{x}) = \max(0, -h(\bar{x})),
\]
we have $h(\bar{x}) = h^+(\bar{x}) - h^-(\bar{x})$. Therefore, setting
\[
c^+(\mu) = \expc_\mu(h^+(\bar{X})) \quad \text{and} \quad c^-(\mu) = \expc_\mu(h^-(\bar{X})),
\]
we must have
\[
\expc_\mu(h(\bar{X})) = \expc_\mu(h^+(\bar{X})) - \expc_\mu(h^-(\bar{X})) = c^+(\mu) - c^-(\mu) = 0,
\]
and so $c^+(\mu) = c^-(\mu)$. Because $h^+$ and $h^-$ are nonnegative functions, we have that $c^+(\mu) \geqslant 0$ and $c^-(\mu) \geqslant 0$.

If $c^+(\mu) = 0$, then we have that $h^+(\bar{x}) = 0$ with probability 1, because a nonnegative function has mean 0 if and only if it is 0 with probability 1 (see Challenge 3.3.22). Then $h^-(\bar{x}) = 0$ with probability 1 also, and we conclude that $h(\bar{x}) = 0$ with probability 1.

If $c^+(\mu') > 0$, then $h^+(\bar{x}) > 0$ for all $\bar{x}$ in a set $A$ having positive probability with respect to the $\mathrm{N}(\mu', \sigma_0^2/n)$ distribution (otherwise $h^+(\bar{x}) = 0$ with probability 1, which implies, as above, that $c^+(\mu') = 0$). This implies that $c^+(\mu) > 0$ for every $\mu$ because every $\mathrm{N}(\mu, \sigma_0^2/n)$ distribution assigns positive probability to $A$ as well (you can think of $A$ as a subinterval of $\mathbb{R}^1$).

Now note that
\[
g^+(\bar{x}) = h^+(\bar{x}) \frac{1}{\sqrt{2\pi\sigma_0}} \exp(-n\bar{x}^2/2\sigma_0^2)
\]
is nonnegative and is strictly positive on $A$. We can write
\begin{align}
c^+(\mu) &= \expc_\mu(h^+(\bar{X})) = \int_{-\infty}^{\infty} h^+(\bar{x}) \frac{1}{\sqrt{2\pi\sigma_0}} \exp(-n(\bar{x} - \mu)^2/2\sigma_0^2) \,\mathrm{d}\bar{x} \notag \\
&= \exp(-n\mu^2/2\sigma_0^2) \int_{-\infty}^{\infty} \exp(n\bar{x}\mu/\sigma_0^2) g^+(\bar{x}) \,\mathrm{d}\bar{x}.
\label{eq:8.5.2}
\end{align}
Setting $\mu = 0$ establishes that $0 < \int_{-\infty}^{\infty} g^+(\bar{x}) \,\mathrm{d}\bar{x} < \infty$, because $0 < c^+(\mu) < \infty$ for every $\mu$. Therefore,
\[
\frac{g^+(\bar{x})}{\int_{-\infty}^{\infty} g^+(\bar{x}) \,\mathrm{d}\bar{x}}
\]
is a probability density of a distribution concentrated on $A^+ = \{\bar{x} : h(\bar{x}) > 0\}$. Furthermore, using \eqref{eq:8.5.2} and the definition of moment-generating function in Section \ref{sec:3.4},
\begin{equation}
\frac{c^+(\mu) \exp(n\mu^2/2\sigma_0^2)}{\int_{-\infty}^{\infty} g^+(\bar{x}) \,\mathrm{d}\bar{x}}
\label{eq:8.5.3}
\end{equation}
is the moment-generating function of this distribution evaluated at $n\mu/\sigma_0^2$.

Similarly, we define
\[
g^-(\bar{x}) = h^-(\bar{x}) \frac{1}{\sqrt{2\pi\sigma_0}} \exp(-n\bar{x}^2/2\sigma_0^2)
\]
so that
\[
\frac{g^-(\bar{x})}{\int_{-\infty}^{\infty} g^-(\bar{x}) \,\mathrm{d}\bar{x}}
\]
is a probability density of a distribution concentrated on $A^- = \{\bar{x} : h(\bar{x}) < 0\}$. Also,
\begin{equation}
\frac{c^-(\mu) \exp(n\mu^2/2\sigma_0^2)}{\int_{-\infty}^{\infty} g^-(\bar{x}) \,\mathrm{d}\bar{x}}
\label{eq:8.5.4}
\end{equation}
is the moment-generating function of this distribution evaluated at $n\mu/\sigma_0^2$.

Because $c^+(\mu) = c^-(\mu)$, we have that (setting $\mu = 0$)
\[
\int_{-\infty}^{\infty} g^+(\bar{x}) \,\mathrm{d}\bar{x} = \int_{-\infty}^{\infty} g^-(\bar{x}) \,\mathrm{d}\bar{x}.
\]
This implies that \eqref{eq:8.5.3} equals \eqref{eq:8.5.4} for every $\mu$, and so the moment-generating functions of these two distributions are the same everywhere. By Theorem \ref{thm:3.4.6}, these distributions must be the same. But this is impossible, as the distribution given by $g^+$ is concentrated on $A^+$ whereas the distribution given by $g^-$ is concentrated on $A^-$ and $A^+ \cap A^- = \emptyset$. Accordingly, we conclude that we cannot have $c^+(\mu) > 0$, and we are done.

\subsection*{The Proof of Theorem~\ref{thm:8.2.1} (the Neyman--Pearson Theorem)}

We want to prove that when $\Omega = \{\theta_0, \theta_1\}$, and we want to test $H_0 : \theta = \theta_0$, then an exact size $\alpha$ test function $\phi_0$ exists of the form
\begin{equation}
\phi_0(s) = \begin{cases}
1 & f_{\theta_1}(s)/f_{\theta_0}(s) > c_0 \\
\gamma & f_{\theta_1}(s)/f_{\theta_0}(s) = c_0 \\
0 & f_{\theta_1}(s)/f_{\theta_0}(s) < c_0
\end{cases}
\label{eq:8.5.5}
\end{equation}
for some $\gamma \in [0, 1]$ and $c_0 \geqslant 0$, and this test is UMP size $\alpha$.

We develop the proof of this result in the discrete case. The proof in the more general context is similar.

First, we note that $\{s : f_{\theta_0}(s) = f_{\theta_1}(s) = 0\}$ has $\prb_\theta$ measure equal to 0 for both $\theta = \theta_0$ and $\theta = \theta_1$. Accordingly, without loss we can remove this set from the sample space and assume hereafter that $f_{\theta_0}(s)$ and $f_{\theta_1}(s)$ cannot be simultaneously 0. Therefore, the ratio $f_{\theta_1}(s)/f_{\theta_0}(s)$ is always defined.

Suppose that $\alpha = 1$. Then setting $c = 0$ and $\gamma = 1$ in \eqref{eq:8.5.5}, we see that $\phi_0(s) \equiv 1$, and so $\expc_{\theta_1}(\phi_0) = 1$. Therefore, $\phi_0$ is UMP size $\alpha$, because no test can have power greater than 1.

Suppose that $\alpha = 0$. Setting $c_0 = \infty$ and $\gamma = 1$ in \eqref{eq:8.5.5}, we see that $\phi_0(s) = 0$ if and only if $f_{\theta_0}(s) > 0$ (if $f_{\theta_0}(s) = 0$, then $f_{\theta_1}(s)/f_{\theta_0}(s) = \infty$ and conversely). So $\phi_0$ is the indicator function for the set $A = \{s : f_{\theta_0}(s) = 0\}$, and therefore $\expc_{\theta_0}(\phi_0) = 0$. Further, any size 0 test function $\phi$ must be 0 on $A^c$ to have $\expc_{\theta_0}(\phi) = 0$. On $A$ we have that $0 \leqslant \phi(s) \leqslant 1 = \phi_0(s)$ and so $\expc_{\theta_1}(\phi) \leqslant \expc_{\theta_1}(\phi_0)$. Therefore, $\phi_0$ is UMP size $\alpha$.

Now assume that $0 < \alpha < 1$. Consider the distribution function of the likelihood ratio when $\theta = \theta_0$, namely,
\[
1 - \Psi^*(c) = \prb_{\theta_0}(f_{\theta_1}(s)/f_{\theta_0}(s) \leqslant c).
\]
So $1 - \Psi^*(c)$ is a nondecreasing function of $c$ with $1 - \Psi^*(-\infty) = 0$ and $1 - \Psi^*(\infty) = 1$.

Let $c_0$ be the smallest value of $c$ such that $1 - \alpha \leqslant 1 - \Psi^*(c)$ (recall that $1 - \Psi^*(c)$ is right continuous because it is a distribution function). Then we have that $\Psi^*(c_0 - 0) = 1 - \lim_{\epsilon \downarrow 0} \Psi^*(c_0 - \epsilon) \leqslant 1 - \alpha \leqslant 1 - \Psi^*(c_0)$ and (using the fact that the jump in a distribution function at a point equals the probability of the point)
\[
\prb_{\theta_0}(f_{\theta_1}(s)/f_{\theta_0}(s) = c_0) = (1 - \Psi^*(c_0)) - (1 - \Psi^*(c_0 - 0)) = \Psi^*(c_0 - 0) - \Psi^*(c_0).
\]
Using this value of $c_0$ in \eqref{eq:8.5.5}, put
\[
\gamma = \begin{cases}
\dfrac{\Psi^*(c_0) - \alpha}{\Psi^*(c_0-0) - \Psi^*(c_0)} & \Psi^*(c_0 - 0) \neq \Psi^*(c_0) \\[12pt]
0 & \text{otherwise},
\end{cases}
\]
and note that $\gamma \in [0, 1]$. Then we have
\begin{align*}
\expc_{\theta_0}(\phi_0) &= \gamma \prb_{\theta_0}(f_{\theta_1}(s)/f_{\theta_0}(s) = c_0) + \prb_{\theta_0}(f_{\theta_1}(s)/f_{\theta_0}(s) > c_0) \\
&= \gamma \cdot \Psi^*(c_0) + \Psi^*(c_0) = \alpha,
\end{align*}
so $\phi_0$ has exact size $\alpha$.

Now suppose that $\phi$ is another size $\alpha$ test and $\expc_{\theta_1}(\phi) \geqslant \expc_{\theta_1}(\phi_0)$. We partition the sample space as $S = S_0 \cup S_1 \cup S_2$ where
\begin{align*}
S_0 &= \{s : \phi_0(s) - \phi(s) = 0\}, \\
S_1 &= \{s : \phi_0(s) - \phi(s) < 0\}, \\
S_2 &= \{s : \phi_0(s) - \phi(s) > 0\}.
\end{align*}
Note that
\[
S_1 = \{s : \phi_0(s) - \phi(s) < 0, f_{\theta_1}(s)/f_{\theta_0}(s) \leqslant c_0\}
\]
because $f_{\theta_1}(s)/f_{\theta_0}(s) > c_0$ implies $\phi_0(s) = 1$, which implies $\phi_0(s) - \phi(s) = 1 - \phi(s) \geqslant 0$ as $0 \leqslant \phi(s) \leqslant 1$. Also
\[
S_2 = \{s : \phi_0(s) - \phi(s) > 0, f_{\theta_1}(s)/f_{\theta_0}(s) \leqslant c_0\}
\]
because $f_{\theta_1}(s)/f_{\theta_0}(s) < c_0$ implies $\phi_0(s) = 0$, which implies $\phi_0(s) - \phi(s) = -\phi(s) \leqslant 0$ as $0 \leqslant \phi(s) \leqslant 1$.

Therefore,
\begin{align*}
0 &\leqslant \expc_{\theta_1}(\phi_0) - \expc_{\theta_1}(\phi) = \expc_{\theta_1}(\phi_0 - \phi) \\
&= \expc_{\theta_1}(\indc_{S_1}(s)(\phi_0(s) - \phi(s))) + \expc_{\theta_1}(\indc_{S_2}(s)(\phi_0(s) - \phi(s))).
\end{align*}
Now note that
\begin{align*}
\expc_{\theta_1}(\indc_{S_1}(s)(\phi_0(s) - \phi(s))) &= \sum_{s \in S_1} (\phi_0(s) - \phi(s)) f_{\theta_1}(s) \\
&\geqslant c_0 \sum_{s \in S_1} (\phi_0(s) - \phi(s)) f_{\theta_0}(s) = c_0 \expc_{\theta_0}(\indc_{S_1}(s)(\phi_0(s) - \phi(s)))
\end{align*}
because $\phi_0(s) - \phi(s) < 0$ and $f_{\theta_1}(s)/f_{\theta_0}(s) \leqslant c_0$ when $s \in S_1$. Similarly, we have that
\begin{align*}
\expc_{\theta_1}(\indc_{S_2}(s)(\phi_0(s) - \phi(s))) &= \sum_{s \in S_2} (\phi_0(s) - \phi(s)) f_{\theta_1}(s) \\
&\geqslant c_0 \sum_{s \in S_2} (\phi_0(s) - \phi(s)) f_{\theta_0}(s) = c_0 \expc_{\theta_0}(\indc_{S_2}(s)(\phi_0(s) - \phi(s)))
\end{align*}
because $\phi_0(s) - \phi(s) > 0$ and $f_{\theta_1}(s)/f_{\theta_0}(s) \leqslant c_0$ when $s \in S_2$.

Combining these inequalities, we obtain
\begin{align*}
0 &\leqslant \expc_{\theta_1}(\phi_0) - \expc_{\theta_1}(\phi) \leqslant c_0 \expc_{\theta_0}(\phi_0 - \phi) \\
&= c_0(\expc_{\theta_0}(\phi_0) - \expc_{\theta_0}(\phi)) = c_0(\alpha - \expc_{\theta_0}(\phi)) \leqslant 0
\end{align*}
because $\expc_{\theta_0}(\phi) \leqslant 0$. Therefore, $\expc_{\theta_1}(\phi_0) = \expc_{\theta_1}(\phi)$, which proves that $\phi_0$ is UMP among all size $\alpha$ tests.
