\chapter{Sampling Distributions and Limits}
\label{ch:4}

\noindent\textbf{CHAPTER OUTLINE}
\begin{itemize}
\item Section 1 \quad Sampling Distributions
\item Section 2 \quad Convergence in Probability
\item Section 3 \quad Convergence with Probability 1
\item Section 4 \quad Convergence in Distribution
\item Section 5 \quad Monte Carlo Approximations
\item Section 6 \quad Normal Distribution Theory
\item Section 7 \quad Further Proofs (Advanced)
\end{itemize}

In many applications of probability theory, we will be faced with the following problem. Suppose that $X_1, X_2, \ldots, X_n$ is an identically and independently distributed (i.i.d.)\ sequence, i.e., $X_1, X_2, \ldots, X_n$ is a sample from some distribution, and we are interested in the distribution of a new random variable $Y = h(X_1, X_2, \ldots, X_n)$ for some function $h$. In particular, we might want to compute the distribution function of $Y$, or perhaps its mean and variance. The distribution of $Y$ is sometimes referred to as its \emph{sampling distribution}, as $Y$ is based on a sample from some underlying distribution.

We will see that some of the methods developed in earlier chapters are useful in solving such problems --- especially when it is possible to compute an exact solution, e.g., obtain an exact expression for the probability or density function of $Y$. Section~\ref{sec:4.6} contains a number of exact distribution results for a variety of functions of normal random variables. These have important applications in statistics.

Quite often, however, exact results are impossible to obtain, as the problem is just too complex. In such cases, we must develop an approximation to the distribution of $Y$.

For many important problems, a version of $Y$ is defined for each sample size $n$ (e.g., a sample mean or sample variance), so that we can consider a sequence of random variables $Y_1, Y_2, \ldots$, etc. This leads us to consider the limiting distribution of such a sequence so that, when $n$ is large, we can approximate the distribution of $Y_n$ by the limit, which is often much simpler. This approach leads to a famous result, known as the central limit theorem, discussed in Section~\ref{sec:4.4}.

Sometimes we cannot even develop useful approximations for large $n$ due to the difficulty of the problem or perhaps because $n$ is just too small in a particular application. Fortunately, however, we can then use the Monte Carlo approach where the power of the computer becomes available. This is discussed in Section~\ref{sec:4.5}.

In Chapter~\ref{ch:5} we will see that, in statistical applications, we typically do not know much about the underlying distribution of the $X_i$ from which we are sampling. We then collect a sample and a value, such as $Y$, that will serve as an estimate of a characteristic of the underlying distribution, e.g., the sample mean $\bar{X}$ will serve as an estimate of the mean of the distribution of the $X_i$. We then want to know what happens to these estimates as $n$ grows. If we have chosen our estimates well, then the estimates will converge to the quantities we are estimating as $n$ increases. Such an estimate is called \emph{consistent}. In Sections~4.2 and~4.3, we will discuss the most important consistency theorems --- namely, the weak and strong laws of large numbers.

\section{Sampling Distributions}
\label{sec:4.1}

Let us consider a very simple example.

\begin{example}
\label{ex:4.1.1}
Suppose we obtain a sample $X_1, X_2$ of size $n = 2$ from the discrete distribution with probability function given by
\[
p_X(x) = \begin{cases}
1/2 & x = 1 \\
1/4 & x = 2 \\
1/4 & x = 3 \\
0 & \text{otherwise.}
\end{cases}
\]
Let us take $Y_2 = (X_1 X_2)^{1/2}$. This is the geometric mean of the sample values (the geometric mean of $n$ positive numbers $x_1, \ldots, x_n$ is defined as $(x_1 \cdots x_n)^{1/n}$).

To determine the distribution of $Y_2$, we first list the possible values for $Y_2$, the samples that give rise to these values, and their probabilities of occurrence. The values of these probabilities specify the sampling distribution of $Y$. We have the following table.

\begin{center}
\begin{tabular}{lll}
$y$ & Sample & $p_{Y_2}(y)$ \\
\hline
$1$ & $(1,1)$ & $(1/2)(1/2) = 1/4$ \\
$\sqrt{2}$ & $(1,2), (2,1)$ & $(1/2)(1/4) + (1/4)(1/2) = 1/4$ \\
$\sqrt{3}$ & $(1,3), (1,3)$ & $(1/2)(1/4) + (1/4)(1/2) = 1/4$ \\
$2$ & $(2,2)$ & $(1/4)(1/4) = 1/16$ \\
$\sqrt{6}$ & $(2,3), (3,2)$ & $(1/4)(1/4) + (1/4)(1/4) = 1/8$ \\
$3$ & $(3,3)$ & $(1/4)(1/4) = 1/16$
\end{tabular}
\end{center}

Now suppose instead we have a sample $X_1, \ldots, X_{20}$ of size $n = 20$ and we want to find the distribution of $Y_{20} = (X_1 \cdots X_{20})^{1/20}$. Obviously, we can proceed as above, but this time the computations are much more complicated, as there are now $3^{20} = 3{,}486{,}784{,}401$ possible samples, as opposed to the $3^2 = 9$ samples used to form the previous table. Directly computing $p_{Y_{20}}$, as we have done for $p_{Y_2}$, would be onerous --- even for a computer! So what can we do here?

One possibility is to look at the distribution of $Y_n = (X_1 \cdots X_n)^{1/n}$ when $n$ is large and see if we can approximate this in some fashion. The results of Section~\ref{ssec:4.4.1} show that
\[
\ln Y_n = \frac{1}{n} \sum_{i=1}^{n} \ln X_i
\]
has an approximate normal distribution when $n$ is large. In fact, the approximating normal distribution when $n = 20$ turns out to be an $\mathrm{N}(0.447940, 0.105167)$ distribution. We have plotted this density in Figure~\ref{fig:4.1.1}.

Another approach is to use the methods of Section~\ref{sec:2.10} to generate $N$ samples of size $n = 20$ from $p_X$, calculate $\ln Y_{20}$ for each ($\ln$ is a 1--1 transformation, and we transform to avoid the potentially large values assumed by $Y_{20}$), and then use these $N$ values to approximate the distribution of $\ln Y_{20}$. For example, in Figure~\ref{fig:4.1.2} we have provided a plot of a density histogram (see Section~\ref{ssec:5.4.3} for more discussion of histograms) of $N = 10^4$ values of $\ln Y_{20}$ calculated from $N = 10^4$ samples of size $n = 20$ generated (using the computer) from $p_X$. The area of each rectangle corresponds to the proportion of values of $\ln Y_{20}$ that were in the interval given by the base of the rectangle. As we will see in Sections~4.2, 4.3, and~4.4, these areas approximate the actual probabilities that $\ln Y_{20}$ falls in these intervals. These approximations improve as we increase $N$.

Notice the similarity in the shapes of Figures~\ref{fig:4.1.1} and~\ref{fig:4.1.2}. Figure~\ref{fig:4.1.2} is not symmetrical about its center, however, as it is somewhat skewed. This is an indication that the normal approximation is not entirely adequate when $n = 20$.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_1_1.pdf}
  \caption{Plot of the approximating $\mathrm{N}(0.447940, 0.105167)$ density to the distribution of $\ln Y_{20}$ in Example~\ref{ex:4.1.1}.}
  \label{fig:4.1.1}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_1_2.pdf}
  \caption{Plot of $N = 10^4$ values of $\ln Y_{20}$ obtained by generating $N = 10^4$ samples from $p_X$ in Example~\ref{ex:4.1.1}.}
  \label{fig:4.1.2}
\end{figure}

Sometimes we are lucky and can work out the sampling distribution of
\[
Y = h(X_1, X_2, \ldots, X_n)
\]
exactly in a form useful for computing probabilities and expectations for $Y$. In general, however, when we want to compute $\prb(Y \in B) = \prb_Y(B)$, we will have to determine the set of samples $(X_1, X_2, \ldots, X_n)$ such that $Y \in B$, as given by
\[
h^{-1}(B) = \{(x_1, x_2, \ldots, x_n) : h(x_1, x_2, \ldots, x_n) \in B\},
\]
and then compute $\prb((X_1, X_2, \ldots, X_n) \in h^{-1}(B))$. This is typically an intractable problem and approximations or simulation (Monte Carlo) methods will be essential. Techniques for deriving such approximations will be discussed in subsequent sections of this chapter. In particular, we will develop an important approximation to the sampling distribution of the sample mean
\[
\bar{X} = h(X_1, X_2, \ldots, X_n) = \frac{1}{n} \sum_{i=1}^{n} X_i.
\]

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:4.1}}
\begin{itemize}
\item A sampling distribution is the distribution of a random variable corresponding to a function of some i.i.d.\ sequence.
\item Sampling distributions can sometimes be computed by direct computation or by approximations such as the central limit theorem.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:4.1.1}
Suppose that $X_1, X_2, X_3$ are i.i.d.\ from $p_X$ in Example~\ref{ex:4.1.1}. Determine the exact distribution of $Y_3 = (X_1 X_2 X_3)^{1/3}$.
\end{exercise}

\begin{solution}
\begin{align*}
    \prb(Y_3 = 1) &= (1/2)(1/2)(1/2) = 1/8 \\
    \prb(Y_3 = 2) &= (1/4)(1/4)(1/4) = 1/64 \\
    \prb(Y_3 = 3) &= (1/4)(1/4)(1/4) = 1/64 \\
    \prb(Y_3 = 2^{1/3}) &= (1/2)(1/2)(1/4) + (1/2)(1/4)(1/2) + (1/4)(1/2)(1/2) = 3/16 \\
    \prb(Y_3 = 3^{1/3}) &= (1/2)(1/2)(1/4) + (1/2)(1/4)(1/2) + (1/4)(1/2)(1/2) = 3/16 \\
    \prb(Y_3 = 4^{1/3}) &= (1/2)(1/4)(1/4) + (1/4)(1/2)(1/4) + (1/4)(1/4)(1/2) = 3/32 \\
    \prb(Y_3 = 9^{1/3}) &= (1/2)(1/4)(1/4) + (1/4)(1/2)(1/4) + (1/4)(1/4)(1/2) = 3/32 \\
    \prb(Y_3 = 12^{1/3}) &= (1/4)(1/4)(1/4) + (1/4)(1/4)(1/4) + (1/4)(1/4)(1/4) = 3/64 \\
    \prb(Y_3 = 18^{1/3}) &= (1/4)(1/4)(1/4) + (1/4)(1/4)(1/4) + (1/4)(1/4)(1/4) = 3/64 \\
    \prb(Y_3 = 6^{1/3}) &= (1/2)(1/4)(1/4) + (1/4)(1/2)(1/4) + (1/4)(1/4)(1/2) \\
    &\quad + (1/2)(1/4)(1/4) + (1/4)(1/2)(1/4) + (1/4)(1/4)(1/2) = 3/16
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:4.1.2}
Suppose that a fair six-sided die is tossed $n = 2$ independent times. Compute the exact distribution of the sample mean.
\end{exercise}

\begin{solution}
If $Z$ is the sample mean, then $\prb(Z = 1) = 1/36$, $\prb(Z = 1.5) = 2/36$, $\prb(Z = 2) = 3/36$, $\prb(Z = 2.5) = 4/36$, $\prb(Z = 3) = 5/36$, $\prb(Z = 3.5) = 6/36$, $\prb(Z = 4) = 5/36$, $\prb(Z = 4.5) = 4/36$, $\prb(Z = 5) = 3/36$, $\prb(Z = 5.5) = 2/36$, and $\prb(Z = 6) = 1/36$.
\end{solution}

\begin{exercise}
\label{exer:4.1.3}
Suppose that an urn contains a proportion $p$ of chips labelled $0$ and proportion $1 - p$ of chips labelled $1$. For a sample of $n = 2$ drawn with replacement, determine the distribution of the sample mean.
\end{exercise}

\begin{solution}
If $Z$ is the sample mean, then $\prb(Z = 0) = p^2$, $\prb(Z = 0.5) = 2p(1 - p)$, and $\prb(Z = 1) = (1 - p)^2$.
\end{solution}

\begin{exercise}
\label{exer:4.1.4}
Suppose that an urn contains $N$ chips labelled $0$ and $M$ chips labelled $1$. For a sample of $n = 2$ drawn without replacement, determine the distribution of the sample mean.
\end{exercise}

\begin{solution}
If $Z$ is the sample mean, then
\[
    \prb(Z = 0) = \frac{N}{N + M} \frac{N - 1}{N + M - 1}, \quad \prb(Z = 0.5) = 2 \frac{N}{N + M} \frac{M}{N + M - 1}, \quad \prb(Z = 1) = \frac{M}{N + M} \frac{M - 1}{N + M - 1}.
\]
\end{solution}

\begin{exercise}
\label{exer:4.1.5}
Suppose that a symmetrical die is tossed $n = 20$ independent times. Work out the exact sampling distribution of the maximum of this sample.
\end{exercise}

\begin{solution}
For $1 \leqslant j \leqslant 6$, $\prb(\max = j) = (j/6)^{20} - ((j - 1)/6)^{20}$.
\end{solution}

\begin{exercise}
\label{exer:4.1.6}
Suppose three fair dice are rolled, and let $Y$ be the number of 6's showing. Compute the exact distribution of $Y$.
\end{exercise}

\begin{solution}
Let $X_1, X_2, X_3$ be the numbers showing on the three dice. Then, $Y = \indc_{\{6\}}(X_1) + \indc_{\{6\}}(X_2) + \indc_{\{6\}}(X_3)$. Since $X_i$'s are independent, $\indc_{\{6\}}(X_i)$'s are i.i.d.\ $\text{Bernoulli}(1/6)$. It gives $Y \sim \text{Binomial}(3, 1/6)$.
\end{solution}

\begin{exercise}
\label{exer:4.1.7}
Suppose two fair dice are rolled, and let $W$ be the product of the two numbers showing. Compute the exact distribution of $W$.
\end{exercise}

\begin{solution}
Let $X, Y$ be the two numbers showing on the two dice. Then, $W = XY$ and $\prb(W = w) = |\{(x, y) : w = xy \text{ for } 1 \leqslant x, y \leqslant 6\}|$ because $X$ and $Y$ are a uniform distribution on $\{1, \ldots, 6\}$. Since $1 \leqslant X, Y \leqslant 6$, the range of $W = XY$ is $[1, 36]$. However, not all values between 1 and 36 can be a value of $w$ with positive probability. For example, any number having prime factor greater than 6 can't be a possible value of $W$. Hence, the random variable $W$ has a positive probability only at the values 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 16, 18, 20, 24, 25, 30, and 36.
\[
    \prb(W = w) = \begin{cases}
        1/36 & \text{if } w = 1, 9, 16, 25, 36, \\
        1/18 & \text{if } w = 2, 3, 5, 8, 10, 15, 18, 20, 24, 30, \\
        1/12 & \text{if } w = 4, \\
        1/9 & \text{if } w = 6, 12, \\
        0 & \text{otherwise}.
    \end{cases}
\]
\end{solution}

\begin{exercise}
\label{exer:4.1.8}
Suppose two fair dice are rolled, and let $Z$ be the difference of the two numbers showing (i.e., the first number minus the second number). Compute the exact distribution of $Z$.
\end{exercise}

\begin{solution}
Let $X, Y$ be the numbers showing on the two dice. Then, $Z = X - Y$. The range of $Z$ is $[-5, 5]$. Since $X$ and $Y$ are independent and have the same distribution, $X - Y$ and $Y - X$ have the same distribution. Hence, $\prb(Z = -z) = \prb(X - Y = -z) = \prb(Y - X = z) = \prb(Z = z)$. For $z = 0, \ldots, 5$, $\prb(Z = z) = |\{(x, y) : z = x - y\}|/36 = |\{(1, 1 + z), \ldots, (6 - z, 6)\}|/36 = (6 - z)/36$. Thus, $p_Z(z) = \prb(Z = z) = (6 - |z|)/36$ for $|z| \leqslant 5$ and otherwise $p_Z(z) = 0$.
\end{solution}

\begin{exercise}
\label{exer:4.1.9}
Suppose four fair coins are flipped, and let $Y$ be the number of pairs of coins which land the same way (i.e., the number of pairs that are either both heads or both tails). Compute the exact distribution of $Y$.
\end{exercise}

\begin{solution}
Let $X$ be the number of heads. If $X = 0$ (or $X = 4$), then all four coins show tails (or heads). Hence, $Y = 2$. If $X = 1$ (or $X = 3$), then there is only one pair of tails (or heads). Hence $Y = 1$. If $X = 2$, there are one pair of heads and one pair of tails. Hence $Y = 2$. The other values can't be a value of $Y$. Hence, $\prb(Y = 1) = \prb(X = 1 \text{ or } X = 3) = \binom{4}{1}(1/2)^4 + \binom{4}{3}(1/2)^4 = 1/2$ and $\prb(Y = 2) = \prb(X \in \{0, 2, 4\}) = \binom{4}{0}(1/2)^4 + \binom{4}{2}(1/2)^4 + \binom{4}{4}(1/2)^4 = 1/2$. In sum, $p_Y(y) = \prb(Y = y) = 1/2$ for $y = 1, 2$ otherwise $p_Y(y) = 0$.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:4.1.10}
Generate a sample of $N = 10^3$ values of $Y_{50}$ in Example~\ref{ex:4.1.1}. Calculate the mean and standard deviation of this sample.
\end{exercise}

\begin{solution}
Using R we place the values 1, 2, 3 and probabilities 0.5, 0.25, 0.25. Then we replace the entries by their logs and generate 1000 samples of size 50, calculate the mean of each of these samples, and exponentiate this. The mean and standard deviation of the values is what we want. We get the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
values <- c(1, 2, 3)
probs <- c(0.5, 0.25, 0.25)
log_values <- log(values)

# Generate 1000 samples of size 50
n_samples <- 1000
sample_size <- 50
samples <- matrix(sample(log_values, n_samples * sample_size, 
                         replace = TRUE, prob = probs), 
                  nrow = n_samples, ncol = sample_size)

# Calculate row means and exponentiate
row_means <- rowMeans(samples)
geom_means <- exp(row_means)

# Compute mean and standard deviation
k1 <- mean(geom_means)
k2 <- sd(geom_means)
cat("K1:", k1, "\n")
cat("K2:", k2, "\n")
\end{minted}
\caption{Geometric mean simulation (sol04\_ex4110.R)}
\label{lst:sol04_ex4110}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 1.57531
K2: 0.103538
\end{verbatim}
\end{solution}

\begin{exercise}
\label{exer:4.1.11}
Suppose that $X_1, X_2, \ldots, X_{10}$ is an i.i.d.\ sequence from an $\mathrm{N}(0,1)$ distribution. Generate a sample of $N = 10^3$ values from the distribution of $\max(X_1, X_2, \ldots, X_{10})$. Calculate the mean and standard deviation of this sample.
\end{exercise}

\begin{solution}
Using R we get the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_samples <- 1000
sample_size <- 10

# Generate 1000 samples of size 10 from Normal(0, 1)
samples <- matrix(rnorm(n_samples * sample_size, mean = 0, sd = 1),
                  nrow = n_samples, ncol = sample_size)

# Calculate row maxima
row_max <- apply(samples, 1, max)

# Compute mean and standard deviation
k1 <- mean(row_max)
k2 <- sd(row_max)
cat("K1:", k1, "\n")
cat("K2:", k2, "\n")
\end{minted}
\caption{Maximum of normal samples (sol04\_ex4111.R)}
\label{lst:sol04_ex4111}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 1.54637
K2: 0.617012
\end{verbatim}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:4.1.12}
Suppose that $X_1, X_2, \ldots, X_n$ is a sample from the Poisson$(\lambda)$ distribution. Determine the exact sampling distribution of $Y = X_1 + X_2 + \cdots + X_n$. (Hint: Determine the moment-generating function of $Y$ and use the uniqueness theorem.)
\end{exercise}

\begin{solution}
We know that $m_Y(s) = (m_{X_1}(s))^n = (e^{\lambda(e^s - 1)})^n = e^{n\lambda(e^s - 1)}$. We recognize this as the moment generating function of $\text{Poisson}(n\lambda)$. Hence, $Y \sim \text{Poisson}(n\lambda)$.
\end{solution}

\begin{exercise}
\label{exer:4.1.13}
Suppose that $X_1, X_2$ is a sample from the Uniform$[0,1]$ distribution. Determine the exact sampling distribution of $Y = X_1 + X_2$. (Hint: Determine the density of $Y$.)
\end{exercise}

\begin{solution}
The density of $Y$, for $0 \leqslant y \leqslant 2$, is given by $f_Y(y) = \int_{-\infty}^{\infty} f_{X_1}(t) f_{X_2}(y - t) \, \mathrm{d}t = \int_{\max(y-1,0)}^{\min(y,1)} (1)(1) \, \mathrm{d}t = \min(y, 1) - \max(y - 1, 0)$, which is equal to $y$ for $0 \leqslant y \leqslant 1$, and to $2 - y$ for $1 \leqslant y \leqslant 2$. Otherwise, $f_Y(y) = 0$.
\end{solution}

\begin{exercise}
\label{exer:4.1.14}
Suppose that $X_1, X_2$ is a sample from the Uniform$[0,1]$ distribution. Determine the exact sampling distribution of $Y = (X_1 X_2)^{1/2}$. (Hint: Determine the density of $\ln Y$ and then transform.)
\end{exercise}

\begin{solution}
$\ln Y = (\ln X_1 + \ln X_2)/2$. Since $X_1 \sim \text{Uniform}(0, 1)$, then $-\ln X_1 \sim \text{Exponential}(1) = \text{Gamma}(1, 1)$. Hence, $W \equiv -\ln X_1 - \ln X_2 \sim \text{Gamma}(2, 1)$, so $f_W(w) = we^{-w}$ for $w > 0$ (otherwise 0). Then $Y = e^{-W/2} \equiv h(W)$ and $W = -2\ln Y \equiv h^{-1}(Y)$, so the density of $Y$ satisfies
\[
    f_Y(y) = f_W(-2\ln y)/|h'(h^{-1}(y))| = \frac{(-2\ln y)e^{2\ln y}}{|-\frac{1}{2}e^{-(-2\ln y)/2}|} = \frac{(-2\ln y)y^2}{|-y/2|} = -4y\ln y
\]
for $0 \leqslant y \leqslant 1$ (otherwise 0).
\end{solution}


\section{Convergence in Probability}
\label{sec:4.2}

Notions of convergence are fundamental to much of mathematics. For example, if $a_n = 1 - 1/n$, then $a_1 = 0$, $a_2 = 1/2$, $a_3 = 2/3$, $a_4 = 3/4$, etc. We see that the values of $a_n$ are getting ``closer and closer'' to $1$, and indeed we know from calculus that $\lim_{n \to \infty} a_n = 1$ in this case.

For random variables, notions of convergence are more complicated. If the values themselves are random, then how can they ``converge'' to anything? On the other hand, we can consider various probabilities associated with the random variables and see if they converge in some sense.

The simplest notion of convergence of random variables is convergence in probability, as follows. (Other notions of convergence will be developed in subsequent sections.)

\begin{definition}
\label{def:4.2.1}
Let $X_1, X_2, \ldots$ be an infinite sequence of random variables, and let $Y$ be another random variable. Then the sequence $\{X_n\}$ \emph{converges in probability} to $Y$, if for all $\epsilon > 0$, $\lim_{n \to \infty} \prb(|X_n - Y| > \epsilon) = 0$, and we write $X_n \xrightarrow{P} Y$.
\end{definition}

In Figure~\ref{fig:4.2.1}, we have plotted the differences $X_n - Y$ for selected values of $n$ for 10 generated sequences $\{X_n - Y\}$ for a typical situation where the random variables $X_n$ converge to a random variable $Y$ in probability. We have also plotted the horizontal lines at $\pm\epsilon$ for $\epsilon = 0.25$. From this we can see the increasing concentration of the distribution of $X_n - Y$ about $0$, as $n$ increases, as required by Definition~\ref{def:4.2.1}. In fact, the 10 observed values of $X_{100} - Y$ all satisfy the inequality $|X_{100} - Y| \leqslant 0.25$.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_2_1.pdf}
  \caption{Plot of 10 replications of $X_n - Y$ illustrating the convergence in probability of $X_n$ to $Y$.}
  \label{fig:4.2.1}
\end{figure}

We consider some applications of this definition.

\begin{example}
\label{ex:4.2.1}
Let $Y$ be any random variable, and let $X_1 = X_2 = X_3 = \cdots = Y$. (That is, the random variables are all identical to each other.) In that case, $|X_n - Y| = 0$, so of course
\[
\lim_{n \to \infty} \prb(|X_n - Y| > \epsilon) = 0
\]
for all $\epsilon > 0$. Hence, $X_n \xrightarrow{P} Y$.
\end{example}

\begin{example}
\label{ex:4.2.2}
Suppose $\prb(X_n = 1) = 1/(n+1)$ and $\prb(Y = 1) = 1$. Then $\prb(|X_n - Y| > \epsilon) = 0$ whenever $n > 1/\epsilon$. Hence, $\prb(|X_n - Y| > \epsilon) \to 0$ as $n \to \infty$ for all $\epsilon > 0$. Hence, the sequence $\{X_n\}$ converges in probability to $Y$. (Here, the distributions of $X_n$ and $Y$ are all degenerate.)
\end{example}

\begin{example}
\label{ex:4.2.3}
Let $U \sim \text{Uniform}[0,1]$. Define $X_n$ by
\[
X_n = \begin{cases}
3 & U \geqslant \frac{2}{3} - \frac{1}{n} \\
8 & \text{otherwise},
\end{cases}
\]
and define $Y$ by
\[
Y = \begin{cases}
3 & U \geqslant \frac{2}{3} \\
8 & \text{otherwise}.
\end{cases}
\]
Then
\[
\prb(|X_n - Y| > \epsilon) = P\left(X_n \neq Y\right) = P\left(\frac{2}{3} - \frac{1}{n} \leqslant U < \frac{2}{3}\right) = \frac{1}{n}.
\]
Hence, $\prb(|X_n - Y| > \epsilon) \to 0$ as $n \to \infty$ for all $\epsilon > 0$, and the sequence $\{X_n\}$ converges in probability to $Y$. (This time, the distributions of $X_n$ and $Y$ are not degenerate.)
\end{example}

A common case is where the distributions of the $X_n$ are not degenerate, but $Y$ is just a constant, as in the following example.

\begin{example}
\label{ex:4.2.4}
Suppose $Z_n \sim \text{Exponential}(n)$ and let $Y = 0$. Then
\[
\prb(|Z_n - Y| > \epsilon) = \prb(Z_n > \epsilon) = \int_{\epsilon}^{\infty} n e^{-nx} \,\mathrm{d}x = e^{-n\epsilon}.
\]
Hence, again $\prb(|Z_n - Y| > \epsilon) \to 0$ as $n \to \infty$ for all $\epsilon > 0$, so the sequence $\{Z_n\}$ converges in probability to $Y$.
\end{example}

\subsection{The Weak Law of Large Numbers}
\label{ssec:4.2.1}

One of the most important applications of convergence in probability is the weak law of large numbers. Suppose $X_1, X_2, \ldots$ is a sequence of independent random variables that each have the same mean $\mu$. For large $n$, what can we say about their average
\[
M_n = \frac{1}{n}(X_1 + \cdots + X_n)?
\]

We refer to $M_n$ as the \emph{sample average}, or \emph{sample mean}, for $X_1, \ldots, X_n$. When the sample size $n$ is fixed, we will often use $\bar{X}$ as a notation for sample mean instead of $M_n$.

For example, if we flip a sequence of fair coins, and if $X_i = 1$ or $X_i = 0$ as the $i$th coin comes up heads or tails, then $M_n$ represents the fraction of the first $n$ coins that came up heads. We might expect that for large $n$, this fraction will be close to $1/2$, i.e., to the expected value of the $X_i$.

The weak law of large numbers provides a precise sense in which average values $M_n$ tend to get close to $\expc(X_i)$, for large $n$.

\begin{theorem}[Weak law of large numbers]
\label{thm:4.2.1}
Let $X_1, X_2, \ldots$ be a sequence of independent random variables, each having the same mean $\mu$ and each having variance less than or equal to $\sigma^2 < \infty$. Then for all $\epsilon > 0$, $\lim_{n \to \infty} \prb(|M_n - \mu| > \epsilon) = 0$.

That is, the averages converge in probability to the common mean, or $M_n \xrightarrow{P} \mu$.
\end{theorem}

\begin{proof}
Using linearity of expected value, we see that $\expc(M_n) = \mu$. Also, using independence, we have
\[
\var(M_n) = \frac{1}{n^2} \bigl(\var(X_1) + \var(X_2) + \cdots + \var(X_n)\bigr) \leqslant \frac{1}{n^2}(1 + \cdots + 1)\sigma^2 = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}.
\]
Hence, by Chebychev's inequality (Theorem~3.6.2), we have
\[
\prb(|M_n - \mu| > \epsilon) \leqslant \frac{\var(M_n)}{\epsilon^2} \leqslant \frac{\sigma^2}{n\epsilon^2}.
\]
This converges to $0$ as $n \to \infty$, which proves the theorem.
\end{proof}

It is a fact that, in Theorem~\ref{thm:4.2.1}, if we require the $X_i$ variables to be i.i.d.\ instead of merely independent, then we do not even need the $X_i$ to have finite variance. But we will not discuss this result further here. Consider some applications of the weak law of large numbers.

\begin{example}
\label{ex:4.2.5}
Consider flipping a sequence of identical fair coins. Let $M_n$ be the fraction of the first $n$ coins that are heads. Then $M_n = (X_1 + \cdots + X_n)/n$, where $X_i = 1$ if the $i$th coin is heads, otherwise $X_i = 0$. Hence, by the weak law of large numbers, we have
\begin{align*}
\lim_{n \to \infty} \prb(M_n \leqslant 0.49) &= \lim_{n \to \infty} \prb(M_n - 0.5 \leqslant -0.01) \\
&\leqslant \lim_{n \to \infty} \prb(|M_n - 0.5| \geqslant 0.01 \text{ or } M_n - 0.5 \leqslant -0.01) \\
&= \lim_{n \to \infty} \prb(|M_n - 0.5| \geqslant 0.01) = 0,
\end{align*}
and, similarly, $\lim_{n \to \infty} \prb(M_n \geqslant 0.51) = 0$. This illustrates that for large $n$, it is very likely that $M_n$ is very close to $0.5$.
\end{example}

\begin{example}
\label{ex:4.2.6}
Consider flipping a sequence of identical coins, each of which has probability $p$ of coming up heads. Let $M_n$ again be the fraction of the first $n$ coins that are heads. Then by the weak law of large numbers, for any $\epsilon > 0$, $\lim_{n \to \infty} \prb(p - \epsilon < M_n < p + \epsilon) = 1$. We thus see that for large $n$, it is very likely that $M_n$ is very close to $p$. (The previous example corresponds to the special case $p = 1/2$.)
\end{example}

\begin{example}
\label{ex:4.2.7}
Let $X_1, X_2, \ldots$ be i.i.d.\ with distribution $\mathrm{N}(3, 5)$. Then $\expc(M_n) = 3$, and by the weak law of large numbers, $\prb(3 - \epsilon < M_n < 3 + \epsilon) \to 1$ as $n \to \infty$. Hence, for large $n$, the average value $M_n$ is very close to $3$.
\end{example}

\begin{example}
\label{ex:4.2.8}
Let $W_1, W_2, \ldots$ be i.i.d.\ with distribution Exponential$(6)$. Then $\expc(M_n) = 1/6$, and by the weak law of large numbers, $\prb(1/6 - \epsilon < M_n < 1/6 + \epsilon) \to 1$ as $n \to \infty$. Hence, for large $n$, the average value $M_n$ is very close to $1/6$.
\end{example}

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:4.2}}
\begin{itemize}
\item A sequence $\{X_n\}$ of random variables converges in probability to $Y$ if
\[
\lim_{n \to \infty} \prb(|X_n - Y| > \epsilon) = 0.
\]
\item The weak law of large numbers says that if $\{X_n\}$ is i.i.d.\ (or is independent with constant mean and bounded variance), then the averages $M_n = (X_1 + \cdots + X_n)/n$ converge in probability to $\expc(X_i)$.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:4.2.1}
Let $U \sim \text{Uniform}[5,10]$, and let $Z = \indc_{U \in [5,7]}$ and $Z_n = \indc_{U \in [5, 7 + 1/n^2]}$. Prove that $Z_n \to Z$ in probability.
\end{exercise}

\begin{solution}
Note that $Z_n = Z$ unless $7 \leqslant U < 7 + 1/n^2$. Hence, for any $c > 0$, $\prb(|Z_n - Z| \geqslant c) \leqslant \prb(7 \leqslant U < 7 + 1/n^2) = 1/5n^2 \to 0$ as $n \to \infty$, so $Z_n \to Z$ in probability.
\end{solution}

\begin{exercise}
\label{exer:4.2.2}
Let $Y \sim \text{Uniform}[0,1]$, and let $X_n = Y^n$. Prove that $X_n \to 0$ in probability.
\end{exercise}

\begin{solution}
For any $c > 0$, $\prb(|X_n - 0| \geqslant c) = \prb(Y^n \geqslant c) = \prb(Y \geqslant c^{1/n}) = 1 - c^{1/n} \to 0$ as $n \to \infty$, so $X_n \to 0$ in probability.
\end{solution}

\begin{exercise}
\label{exer:4.2.3}
Let $W_1, W_2, \ldots$ be i.i.d.\ with distribution Exponential$(3)$. Prove that for some $n$, we have $\prb((W_1 + W_2 + \cdots + W_n)/n < 2) \geqslant 0.999$.
\end{exercise}

\begin{solution}
By the weak law of large numbers, since $\expc(W_i) = 1/3$, $\lim_{n \to \infty} \prb(|\frac{1}{n}(W_1 + \cdots + W_n) - \frac{1}{3}| \geqslant 1/6) = 0$, so there is $n$ with $\prb(|\frac{1}{n}(W_1 + \cdots + W_n) - \frac{1}{3}| \geqslant 1/6) < 0.001$. But then $\prb(W_1 + \cdots + W_n < n/2) = 1 - \prb(W_1 + \cdots + W_n \geqslant n/2) \geqslant 1 - \prb(|\frac{1}{n}(W_1 + \cdots + W_n) - \frac{1}{3}| \geqslant 1/6) \geqslant 1 - 0.001 = 0.999$.
\end{solution}

\begin{exercise}
\label{exer:4.2.4}
Let $Y_1, Y_2, \ldots$ be i.i.d.\ with distribution $\mathrm{N}(2, 5)$. Prove that for some $n$, we have $\prb((Y_1 + Y_2 + \cdots + Y_n)/n < \infty) \geqslant 0.999$.
\end{exercise}

\begin{solution}
By the weak law of large numbers, since $\expc(Y_i) = 2$, $\lim_{n \to \infty} \prb(|\frac{1}{n}(Y_1 + \cdots + Y_n) - 2| \geqslant 1) = 0$, so there is $n$ with $\prb(|\frac{1}{n}(Y_1 + \cdots + Y_n) - 2| \geqslant 1) < 0.001$. But then $\prb(Y_1 + \cdots + Y_n > n) = 1 - \prb(Y_1 + \cdots + Y_n \leqslant n) \geqslant 1 - \prb(|\frac{1}{n}(Y_1 + \cdots + Y_n) - 2| \geqslant 1) \geqslant 1 - 0.001 = 0.999$.
\end{solution}

\begin{exercise}
\label{exer:4.2.5}
Let $X_1, X_2, \ldots$ be i.i.d.\ with distribution Poisson$(8)$. Prove that for some $n$, we have $\prb((X_1 + X_2 + \cdots + X_n) \leqslant 9n) \geqslant 0.001$.
\end{exercise}

\begin{solution}
By the weak law of large numbers, since $\expc(X_i) = 8$, $\lim_{n \to \infty} \prb(|\frac{1}{n}(X_1 + \cdots + X_n) - 8| \geqslant 1) = 0$, so there is $n$ with $\prb(|(X_1 + \cdots + X_n)/n - 8| \geqslant 1) < 0.001$. But then $\prb(X_1 + \cdots + X_n > 9n) \leqslant \prb(|(X_1 + \cdots + X_n)/n - 8| \geqslant 1) \leqslant 0.001$.
\end{solution}

\begin{exercise}
\label{exer:4.2.6}
Suppose $X \sim \text{Uniform}[0,1]$, and let $Y_n = \frac{n+1}{n} X$. Prove that $Y_n \xrightarrow{P} X$.
\end{exercise}

\begin{solution}
Fix $c > 0$. Then $\prb(|Y_n - X| \geqslant c) = \prb(|\frac{n-1}{n}X - X| \geqslant c) = \prb(|X|/n \geqslant c) = \prb(X \geqslant nc) = \max(0, 1 - nc)$. Hence, $\prb(|Y_n - X| \geqslant c) \to 0$ as $n \to \infty$ for all $c > 0$, so the sequence $\{Y_n\}$ converges in probability to $X$.
\end{solution}

\begin{exercise}
\label{exer:4.2.7}
Let $H_n$ be the number of heads when flipping $n$ fair coins, let $X_n = e^{-H_n}$, and let $Y = 0$. Prove that $X_n \xrightarrow{P} Y$.
\end{exercise}

\begin{solution}
For all $c > 0$ and $n > -2\ln c$, using Chebyshev's inequality, we have
\[
    \prb(|X_n - Y| \geqslant c) = \prb(e^{-H_n} \geqslant c) = \prb(H_n \leqslant -\ln c) \leqslant \prb(|H_n - n/2| \geqslant |n/2 + \ln c|) \leqslant \frac{\var(H_n)}{|n/2 + \ln c|^2} = \frac{n}{(n + 2\ln c)^2} \to 0
\]
as $n \to \infty$. So, $\{X_n\}$ converges in probability to $Y$.
\end{solution}

\begin{exercise}
\label{exer:4.2.8}
Let $Z_n \sim \text{Uniform}[0, n]$, let $W_n = 5Z_n/(Z_n + 1)$, and let $W = 5$. Prove that $W_n \xrightarrow{P} W$.
\end{exercise}

\begin{solution}
Fix $c > 0$.
\[
    \prb(|W_n - W| \geqslant c) = \prb(5 - 5Z_n/(Z_n + 1) \geqslant c) = \prb(5/(Z_n + 1) \geqslant c) = \prb(Z_n \leqslant -1 + 5/c) = \max(0, -1 + 5/c)/n.
\]
So, $\prb(|W_n - W| \geqslant c) \to 0$ as $n \to \infty$ for all $c > 0$. Hence, $W_n \xrightarrow{P} W$.
\end{solution}

\begin{exercise}
\label{exer:4.2.9}
Consider flipping $n$ fair coins. Let $H_n$ be the total number of heads, and let $F_n$ be the number of heads on coins $1$ through $n - 1$ (i.e., omitting the $n$th coin). Let $X_n = H_n/(H_n + 1)$, and $Y_n = F_n/(H_n + 1)$, and $Z = 0$. Prove that $X_n - Y_n \xrightarrow{P} Z$.
\end{exercise}

\begin{solution}
By definition, $H_n - 1 \leqslant F_n \leqslant H_n$. For $c > 0$ and $n \geqslant 2/c$, using Chebyshev's inequality,
\begin{align*}
    \prb(|X_n - Y_n - Z| \geqslant c) &= \prb(|H_n - F_n|/(H_n + 1) \geqslant c) \leqslant \prb(1/(H_1 + 1) \geqslant c) \\
    &= \prb(H_n \leqslant (1/c) - 1) = \prb(H_n - n/2 \leqslant (1/c) - 1 - n/2) \\
    &\leqslant \prb(|H_n - n/2| \geqslant |1 + n/2 - 1/c|) \leqslant \frac{\var(H_n)}{|1 + n/2 - 1/c|^2} \\
    &= \frac{n}{(n + 2 - 2/c)^2} \to 0
\end{align*}
as $n \to \infty$. Hence, $X_n - Y_n \xrightarrow{P} Z$.
\end{solution}

\begin{exercise}
\label{exer:4.2.10}
Let $Z_n$ be the sum of the squares of the numbers showing when we roll $n$ fair dice. Find (with proof) a number $m$ such that $\frac{1}{n} Z_n \xrightarrow{P} m$. (Hint: Use the weak law of large numbers.)
\end{exercise}

\begin{solution}
Let $X_i$ be the numbers showing on $i$th rolling. Then, $Z = X_1^2 + \cdots + X_n^2$. Since $X_i$'s are independent and identically distributed and $\expc(X_i^2) = \sum_{j=1}^{6} j^2 \frac{1}{6} = 91/6$, by the weak law of large numbers,
\[
    \frac{1}{n}Z_n = \frac{1}{n}(X_1^2 + \cdots + X_n^2) \xrightarrow{P} \expc(X_1^2) = \frac{91}{6}.
\]
Hence, $m = 91/6$.
\end{solution}

\begin{exercise}
\label{exer:4.2.11}
Consider flipping $n$ fair nickels and $n$ fair dimes. Let $X_n$ equal $4$ times the number of nickels showing heads, plus $5$ times the number of dimes showing heads. Find (with proof) a number $r$ such that $\frac{1}{n} X_n \xrightarrow{P} r$.
\end{exercise}

\begin{solution}
Let $Y_n$ and $Z_n$ be the numbers of heads in nickel and dime flippings. Then, $X_n = 4Y_n + 5Z_n$. By the weak law of large numbers, $Y_n/n \xrightarrow{P} 1/2$ and $Z_n/n \xrightarrow{P} 1/2$. It is easy to guess $X_n/n \xrightarrow{P} 4(1/2) + 5(1/2) = 9/2$. We will show this using Chebyshev's inequality. Note that $\expc(X_n/n) = 4\expc(Y_n)/n + 5\expc(Z_n)/n = 4(n/2)/n + 5(n/2)/n = 9/2$ and $\var(X_n/n) = \var(5Y_n/n + 4Z_n/n) = 25\var(Y_n)/n^2 + 16\var(Z_n)/n = 25(n/4)/n^2 + 16(n/4)/n^2 = 41/(4n)$.
\[
    \prb(|X_n/n - 9/2| \geqslant c) \leqslant \frac{\var(X_n/n)}{c^2} = \frac{41}{4c^2 n} \to 0
\]
as $n \to \infty$. Hence, $X_n/n \xrightarrow{P} 9/2$. Therefore $r = 9/2$.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:4.2.12}
Generate i.i.d.\ $X_1, \ldots, X_n$ distributed Exponential$(5)$ and compute $M_n$ when $n = 20$. Repeat this $N$ times, where $N$ is large (if possible, take $N = 10^5$; otherwise as large as is feasible), and compute the proportion of values of $M_n$ that lie between $0.19$ and $0.21$. Repeat this with $n = 50$. What property of convergence in probability do your results illustrate?
\end{exercise}

\begin{solution}
The following results were generated using R (k1 holds the proportions).

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_samples <- 100000

# n = 20
samples_20 <- matrix(rexp(n_samples * 20, rate = 1/0.2), 
                     nrow = n_samples, ncol = 20)
means_20 <- rowMeans(samples_20)
k1_20 <- mean(means_20 >= 0.19 & means_20 <= 0.21)
cat("K1 (n=20):", k1_20, "\n")

# n = 50
samples_50 <- matrix(rexp(n_samples * 50, rate = 1/0.2), 
                     nrow = n_samples, ncol = 50)
means_50 <- rowMeans(samples_50)
k1_50 <- mean(means_50 >= 0.19 & means_50 <= 0.21)
cat("K1 (n=50):", k1_50, "\n")
\end{minted}
\caption{Exponential sample means concentration (sol04\_ex4212.R)}
\label{lst:sol04_ex4212}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1 (n=20): 0.176500
K1 (n=50): 0.276850
\end{verbatim}

We see that about 18\% of the $M_{20}$ values are between the limits, while about 28\% of the $M_{50}$ values are between the limits. This reflects the increasing concentration of the distributions of $M_n$ as $n$ increases.
\end{solution}

\begin{exercise}
\label{exer:4.2.13}
Generate i.i.d.\ $X_1, \ldots, X_n$ distributed Poisson$(7)$ and compute $M_n$ when $n = 20$. Repeat this $N$ times, where $N$ is large (if possible, take $N = 10^5$; otherwise as large as is feasible), and compute the proportion of values of $M_n$ that lie between $6.99$ and $7.01$. Repeat this with $n = 100$. What property of convergence in probability do your results illustrate?
\end{exercise}

\begin{solution}
The following results were generated using R (k1 holds the proportions).

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_samples <- 100000

# n = 20
samples_20 <- matrix(rpois(n_samples * 20, lambda = 7), 
                     nrow = n_samples, ncol = 20)
means_20 <- rowMeans(samples_20)
k1_20 <- mean(means_20 >= 6.99 & means_20 <= 7.01)
cat("K1 (n=20):", k1_20, "\n")

# n = 100
samples_100 <- matrix(rpois(n_samples * 100, lambda = 7), 
                      nrow = n_samples, ncol = 100)
means_100 <- rowMeans(samples_100)
k1_100 <- mean(means_100 >= 6.99 & means_100 <= 7.01)
cat("K1 (n=100):", k1_100, "\n")
\end{minted}
\caption{Poisson sample means concentration (sol04\_ex4213.R)}
\label{lst:sol04_ex4213}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1 (n=20): 0.0328400
K1 (n=100): 0.0463600
\end{verbatim}

We see that about 3.2\% of the $M_{20}$ values are between the limits, while about 4.6\% of the $M_{50}$ values are between the limits. This reflects the increasing concentration of the distributions of $M_n$ as $n$ increases, although it is not highly concentrated yet.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:4.2.14}
Give an example of random variables $X_1, X_2, \ldots$ such that $X_n$ converges to $0$ in probability, but $\expc(|X_n|) \geqslant 1$ for all $n$. (Hint: Suppose $\prb(X_n = n) = 1/n$ and $\prb(X_n = 0) = 1 - 1/n$.)
\end{exercise}

\begin{solution}
Let $\prb(X_n = n) = 1/n$ and $\prb(X_n = 0) = 1 - 1/n$. Then $\expc(X_n) = n(1/n) + 0(1 - 1/n) = 1$. But for any $c > 0$, $\prb(|X_n - 0| \geqslant c) \leqslant \prb(X_n = n) = 1/n \to 0$ as $n \to \infty$, so $X_n \to 0$ in probability.
\end{solution}

\begin{exercise}
\label{exer:4.2.15}
Prove that $X_n \xrightarrow{P} 0$ if and only if $|X_n| \xrightarrow{P} 0$.
\end{exercise}

\begin{solution}
$|X_n| \xrightarrow{P} 0$ if and only if, for any $c > 0$, $\prb(||X_n| - 0| \geqslant c) \to 0$ as $n \to \infty$. But $\prb(||X_n| - 0| \geqslant c) = \prb(||X_n|| \geqslant c) = \prb(|X_n| \geqslant c) = \prb(|X_n - 0| \geqslant c)$, so this holds if and only if $X_n \xrightarrow{P} 0$.
\end{solution}

\begin{exercise}
\label{exer:4.2.16}
Prove or disprove that $X_n \xrightarrow{P} 5$ if and only if $|X_n| \xrightarrow{P} 5$.
\end{exercise}

\begin{solution}
This is false. For example, suppose $X_n = -5$ for all $n$. Then for $0 < c < 10$, $\prb(|X_n - 5| \geqslant c) = 1 \not\to 0$ as $n \to \infty$, so $X_n \not\to 5$ in probability. On the other hand, $|X_n| = 5$ for all $n$, so of course $|X_n| \to 5$ in probability.
\end{solution}

\begin{exercise}
\label{exer:4.2.17}
Suppose $X_n \xrightarrow{P} X$, and $Y_n \xrightarrow{P} Y$. Let $Z_n = X_n + Y_n$ and $Z = X + Y$. Prove that $Z_n \xrightarrow{P} Z$.
\end{exercise}

\begin{solution}
If $|X_n - X| < c/2$ and $|Y_n - Y| < c/2$, then $|(X_n - X) + (Y_n - Y)| \leqslant |X_n - X| + |Y_n - Y| < c$. Hence, the event $|(X_n - X) + (Y_n - Y)| \geqslant c$ is contained in the union of two events $|X_n - X| \geqslant c/2$ and $|Y_n - Y| \geqslant c/2$. From the assumption, $\lim_{n \to \infty} \prb(|X_n - X| \geqslant c) = \lim_{n \to \infty} \prb(|Y_n - Y| \geqslant c) = 0$ for all $c > 0$.
\begin{align*}
    \lim_{n \to \infty} \prb(|Z_n - Z| \geqslant c) &\leqslant \lim_{n \to \infty} \prb(|X_n - X| \geqslant c/2 \text{ or } |Y_n - Y| \geqslant c/2) \\
    &\leqslant \lim_{n \to \infty}[\prb(|X_n - X| \geqslant c/2) + \prb(|Y_n - Y| \geqslant c/2)] \\
    &= \lim_{n \to \infty} \prb(|X_n - X| \geqslant c/2) + \lim_{n \to \infty} \prb(|Y_n - Y| \geqslant c/2) = 0.
\end{align*}
Hence, $Z_n \xrightarrow{P} Z$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:4.2.18}
Suppose $X_n \xrightarrow{P} X$, and $f$ is a continuous function. Prove that $f(X_n) \xrightarrow{P} f(X)$.
\end{exercise}

\begin{solution}
Fix $c$. For an arbitrary $\eta > 0$, there is a number $M_0 > 0$ such that $\prb(|X| > M_0) < \eta$. Since $f$ is uniformly continuous on $[-2M_0, 2M_0]$, there is a number $\delta \in (0, M_0)$ such that $|f(x) - f(y)| < c$ for all $x, y \in [-2M_0, 2M_0]$ satisfying $|x - y| < \delta$. Then, the event $A_n = (|f(X_n) - f(X)| \geqslant c)$ can be separated into three parts $A \cap B$, $A \cap B^c \cap C_n$ and $A \cap B^c \cap C_n^c$ where $B = (|X| > M_0)$ and $C_n = (|X_n - X| < \delta)$. It is easy to check $A \cap B^c \cap C_n = \emptyset$. Note that $\prb(C_n^c) \to 0$ as $n \to \infty$. Hence, we get
\[
    \prb(|f(X_n) - f(X)| \geqslant c) = \prb(A_n \cap B) + \prb(A_n \cap B^c \cap C_n) + \prb(A_n \cap B^c \cap C_n^c) \leqslant \prb(B) + 0 + \prb(C_n^c) \leqslant \eta + \prb(C_n^c).
\]
Thus, $\lim_{n \to \infty} \prb(A) \leqslant \eta + \lim_{n \to \infty} \prb(C_n^c) = \eta$. Since we can take $\eta > 0$ arbitrarily small, we get $\lim_{n \to \infty} \prb(|f(X_n) - f(X)| \geqslant c) = 0$. Therefore, $f(X_n) \xrightarrow{P} f(X)$.
\end{solution}


\section{Convergence with Probability 1}
\label{sec:4.3}

A notion of convergence for random variables that is closely associated with the convergence of a sequence of real numbers is provided by the concept of convergence with probability 1. This property is given in the following definition.

\begin{definition}
\label{def:4.3.1}
Let $X_1, X_2, \ldots$ be an infinite sequence of random variables. We shall say that the sequence $\{X_i\}$ \emph{converges with probability 1} (or \emph{converges almost surely} (a.s.)) to a random variable $Y$, if $\prb(\lim_{n \to \infty} X_n = Y) = 1$, and we write
\[
X_n \xrightarrow{\text{a.s.}} Y.
\]
\end{definition}

In Figure~\ref{fig:4.3.1}, we illustrate this convergence by graphing the sequence of differences $X_n - Y$ for a typical situation where the random variables $X_n$ converge to a random variable $Y$ with probability 1. We have also plotted the horizontal lines at $\pm\epsilon$ for $\epsilon = 0.1$. Notice that inevitably all the values $X_n - Y$ are in the interval $(-0.1, 0.1)$, or, in other words, the values of $X_n$ are within $0.1$ of the values of $Y$.

Definition~\ref{def:4.3.1} indicates that for any given $\epsilon > 0$, there will exist a value $N_\epsilon$ such that $|X_n - Y| < \epsilon$ for every $n \geqslant N_\epsilon$. The value of $N_\epsilon$ will vary depending on the observed value of the sequence $\{X_n - Y\}$, but it always exists. Contrast this with the situation depicted in Figure~\ref{fig:4.2.1}, which only says that the probability distribution $X_n - Y$ concentrates about $0$ as $n$ grows and not that the individual values of $X_n - Y$ will necessarily all be near $0$ (also see Example~\ref{ex:4.3.2}).

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_3_1.pdf}
  \caption{Plot of a single replication $X_n - Y$ illustrating the convergence with probability 1 of $X_n$ to $Y$.}
  \label{fig:4.3.1}
\end{figure}

Consider an example of this.

\begin{example}
\label{ex:4.3.1}
Consider again the setup of Example~\ref{ex:4.2.3}, where $U \sim \text{Uniform}[0,1]$,
\[
X_n = \begin{cases}
3 & U \geqslant \frac{2}{3} - \frac{1}{n} \\
8 & \text{otherwise}
\end{cases}
\]
and
\[
Y = \begin{cases}
3 & U \geqslant \frac{2}{3} \\
8 & \text{otherwise}.
\end{cases}
\]
If $U > 2/3$, then $Y = 8$ and also $X_n = 8$ for all $n$, so clearly $X_n \to Y$. If $U < 2/3$, then for large enough $n$ we will also have
\[
U < \frac{2}{3} - \frac{1}{n},
\]
so again $X_n \to Y$. On the other hand, if $U = 2/3$, then we will always have $X_n = 8$, even though $Y = 3$. Hence, $X_n \to Y$ except when $U = 2/3$. Because $\prb(U = 2/3) = 0$, we do have $X_n \to Y$ with probability 1.
\end{example}

One might wonder what the relationship is between convergence in probability and convergence with probability 1. The following theorem provides an answer.

\begin{theorem}
\label{thm:4.3.1}
Let $Z, Z_1, Z_2, \ldots$ be random variables. Suppose $Z_n \to Z$ with probability 1. Then $Z_n \to Z$ in probability. That is, if a sequence of random variables converges almost surely, then it converges in probability to the same limit.
\end{theorem}

\begin{proof}
  See Section~\ref{sec:4.7} for the proof of this result.
\end{proof}

On the other hand, the converse to Theorem~\ref{thm:4.3.1} is false, as the following example shows.

\begin{example}
\label{ex:4.3.2}
Let $U$ have the uniform distribution on $[0,1]$. We construct an infinite sequence of random variables $\{X_n\}$ by setting
\begin{align*}
X_1 &= \indc_{[0,1/2)}(U), \quad X_2 = \indc_{[1/2,1]}(U), \\
X_3 &= \indc_{[0,1/4)}(U), \quad X_4 = \indc_{[1/4,1/2)}(U), \quad X_5 = \indc_{[1/2,3/4)}(U), \quad X_6 = \indc_{[3/4,1]}(U), \\
X_7 &= \indc_{[0,1/8)}(U), \quad X_8 = \indc_{[1/8,1/4)}(U), \ldots
\end{align*}
where $\indc_A$ is the indicator function of the event $A$, i.e., $\indc_A(s) = 1$ if $s \in A$, and $\indc_A(s) = 0$ if $s \notin A$.

Note that we first subdivided $[0,1]$ into two equal-length subintervals and defined $X_1$ and $X_2$ as the indicator functions for the two subintervals. Next we subdivided $[0,1]$ into four equal-length subintervals and defined $X_3$, $X_4$, $X_5$, and $X_6$ as the indicator functions for the four subintervals. We continued this process by next dividing $[0,1]$ into eight equal-length subintervals, then 16 equal-length subintervals, etc., to obtain an infinite sequence of random variables.

Each of these random variables $X_n$ takes the values $0$ and $1$ only and so must follow a Bernoulli distribution. In particular, $X_1 \sim \text{Bernoulli}(1/2)$, $X_2 \sim \text{Bernoulli}(1/2)$, $X_3 \sim \text{Bernoulli}(1/4)$, etc.

Then for $\epsilon \in (0,1)$ we have that $\prb(|X_n - 0| > \epsilon) = \prb(X_n = 1)$. Because the intervals for $U$ that make $X_n = 0$ are getting smaller and smaller, we see that $\prb(X_n = 1)$ is converging to $0$. Hence, $\{X_n\}$ converges to $0$ in probability.

On the other hand, $\{X_n\}$ does not converge to $0$ almost surely. Indeed, no matter what value $U$ takes on, there will always be infinitely many different $n$ for which $X_n = 1$. Hence, we will have $X_n = 1$ infinitely often, so that we will not have $\{X_n\}$ converging to $0$ for any particular value of $U$. Thus, $\prb(\lim_{n \to \infty} X_n = 0) = 0$, and $\{X_n\}$ does not converge to $0$ with probability 1.
\end{example}

Theorem~\ref{thm:4.3.1} and Example~\ref{ex:4.3.2} together show that convergence with probability 1 is a stronger notion than convergence in probability.

Now, the weak law of large numbers (Section~\ref{ssec:4.2.1}) concludes only that the averages $M_n$ are converging in probability to $\expc(X_i)$. A stronger version of this result would instead conclude convergence with probability 1. We consider that now.

\subsection{The Strong Law of Large Numbers}
\label{ssec:4.3.1}

The following is a strengthening of the weak law of large numbers because it concludes convergence with probability 1 instead of just convergence in probability.

\begin{theorem}[Strong law of large numbers]
\label{thm:4.3.2}
Let $X_1, X_2, \ldots$ be a sequence of i.i.d.\ random variables, each having finite mean $\mu$. Then
\[
P\left(\lim_{n \to \infty} M_n = \mu\right) = 1.
\]
That is, the averages converge with probability 1 to the common mean, or $M_n \xrightarrow{\text{a.s.}} \mu$.
\end{theorem}

\begin{proof}
See \emph{A First Look at Rigorous Probability Theory}, Second Edition, by J.~S.\ Rosenthal (World Scientific Publishing Co., 2006) for a proof of this result.
\end{proof}

This result says that sample averages converge with probability 1 to $\mu$.

Like Theorem~\ref{thm:4.2.1}, it says that for large $n$ the averages $M_n$ are usually close to $\expc(X_i) = \mu$ for large $n$. But it says in addition that if we wait long enough (i.e., if $n$ is large enough), then eventually the averages will all be close to $\mu$, for all sufficiently large $n$. In other words, the sample mean is consistent for $\mu$.

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:4.3}}
\begin{itemize}
\item A sequence $\{X_n\}$ of random variables converges with probability 1 (or converges almost surely) to $Y$ if, $\prb(\lim_{n \to \infty} X_n = Y) = 1$.
\item Convergence with probability 1 implies convergence in probability.
\item The strong law of large numbers says that if $\{X_n\}$ is i.i.d., then the averages $M_n = (X_1 + \cdots + X_n)/n$ converge with probability 1 to $\expc(X_i)$.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:4.3.1}
Let $U \sim \text{Uniform}[5,10]$, and let $Z = \indc_{[5,7]}(U)$ (i.e., $Z$ is the indicator function of $[5,7]$) and $Z_n = \indc_{[5, 7 + 1/n^2]}(U)$. Prove that $Z_n \to Z$ with probability 1.
\end{exercise}

\begin{solution}
Note that $Z_n = Z$ unless $7 \leqslant U < 7 + 1/n^2$. Hence, if $U < 7$ then $Z_n = Z$ for all $n$, so of course $Z_n \to Z$. Also, if $U > 7$, then $Z_n = Z$ whenever $1/n^2 < 7 - U$, i.e., $n > 1/\sqrt{7 - U}$, so again $Z_n \to Z$. Hence, $\prb(Z_n \to Z) \geqslant \prb(U \neq 7) = 1 - \prb(U = 7) = 1 - 0 = 1$, i.e., $Z_n \to Z$ with probability 1.
\end{solution}

\begin{exercise}
\label{exer:4.3.2}
Let $Y \sim \text{Uniform}[0,1]$, and let $X_n = Y^n$. Prove that $X_n \to 0$ with probability 1.
\end{exercise}

\begin{solution}
If $0 \leqslant y < 1$ then $\lim_{n \to \infty} y^n = 0$. Hence, $\prb(X_n \to 0) = \prb(Y^n \to 0) \geqslant \prb(0 \leqslant Y < 1) = 1$, i.e., $Y_n \to 0$ with probability 1.
\end{solution}

\begin{exercise}
\label{exer:4.3.3}
Let $W_1, W_2, \ldots$ be i.i.d.\ with distribution Exponential$(3)$. Prove that with probability 1, for some $n$, we have $(W_1 + W_2 + \cdots + W_n)/n < 2$.
\end{exercise}

\begin{solution}
Since $\expc(W_i) = 1/3$, by the strong law of large numbers, $\prb((W_1 + \cdots + W_n)/n \to 1/3) = 1$. But $\{(W_1 + \cdots + W_n)/n \to 1/3\} \subseteq \{\exists n; (W_1 + \cdots + W_n)/n < 1/2\} = \{\exists n; W_1 + \cdots + W_n < n/2\}$, so also $\prb(\exists n; W_1 + \cdots + W_n < n/2) = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.3.4}
Let $Y_1, Y_2, \ldots$ be i.i.d.\ with distribution $\mathrm{N}(2, 5)$. Prove that with probability 1, for some $n$, we have $(Y_1 + Y_2 + \cdots + Y_n)/n < \infty$.
\end{exercise}

\begin{solution}
Since $\expc(Y_i) = 2$, by the strong law of large numbers, $\prb(\frac{1}{n}(Y_1 + \cdots + Y_n) \to 2) = 1$. But $\{(Y_1 + \cdots + Y_n)/n \to 2\} \subseteq \{\exists n; (Y_1 + \cdots + Y_n)/n > 1\} = \{\exists n; Y_1 + \cdots + Y_n > n\}$, so also $\prb(\exists n; Y_1 + \cdots + Y_n > n) = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.3.5}
Suppose $X_n \to X$ with probability 1, and also $Y_n \to Y$ with probability 1. Prove that $\prb(X_n \to X \text{ and } Y_n \to Y) = 1$.
\end{exercise}

\begin{solution}
By subadditivity, $\prb(X_n \to X \text{ and } Y_n \to Y) = 1 - \prb(X_n \not\to X \text{ or } Y_n \not\to Y) \geqslant 1 - \prb(X_n \not\to X) - \prb(Y_n \not\to Y) = 1 - 0 - 0 = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.3.6}
Suppose $Z_1, Z_2, \ldots$ are i.i.d.\ with finite mean $\mu$. Let $M_n = (Z_1 + \cdots + Z_n)/n$. Determine (with explanation) whether the following statements are true or false.
\begin{enumerate}[(a)]
\item With probability 1, $M_n = \mu$ for some $n$.
\item With probability 1, $-0.01 < M_n - \mu < 0.01$ for some $n$.
\item With probability 1, $-0.01 < M_n - \mu < 0.01$ for all but finitely many $n$.
\item For any $x \in \mathbb{R}^1$, with probability 1, $x - 0.01 < M_n < x + 0.01$ for some $n$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item False, e.g., if $Z_i$ are continuous, then $\prb(M_n = a) = 0$ for any $a$.
    \item True, by the strong law of large numbers.
    \item True, by the strong law of large numbers (the given property is implied by the fact that $\lim_{n \to \infty} A_n = m$).
    \item False, e.g., if $x < m - 0.02$ and $M_n = m$ for all $n$, then this will not occur.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.3.7}
Let $\{X_n\}$ be i.i.d., with $X_n \sim \text{Uniform}[3,7]$. Let $Y_n = (X_1 + X_2 + \cdots + X_n)/n$. Find (with proof) a number $m$ such that $Y_n \xrightarrow{\text{a.s.}} m$. (Hint: Use the strong law of large numbers.)
\end{exercise}

\begin{solution}
The expectation of $X_i$ is $\expc(X_i) = \int_{\mathbb{R}} x \cdot \indc_{[3,7]}(x)/4 \, \mathrm{d}x = 5$. By the strong law of large numbers,
\[
    Y_n = \frac{1}{n}(X_1 + \cdots + X_n) \xrightarrow{a.s.} \expc(X_1) = 5.
\]
Hence, $m = 5$.
\end{solution}

\begin{exercise}
\label{exer:4.3.8}
Let $Z_n$ be the sum of the squares of the numbers showing when we roll $n$ fair dice. Find (with proof) a number $m$ such that $\frac{1}{n} Z_n \xrightarrow{\text{a.s.}} m$.
\end{exercise}

\begin{solution}
Let $X_i$ be the number showing on the $i$th dice. Then $Z_n = X_1^2 + \cdots + X_n^2$. Note that $\expc(X_i^2) = \sum_{j=1}^{6} j^2 \cdot (1/6) = 91/6$. By the strong law of large numbers,
\[
    \frac{1}{n}Z_n = \frac{1}{n}(X_1^2 + \cdots + X_n^2) \xrightarrow{a.s.} \expc(X_1^2) = \frac{91}{6}.
\]
Hence, $m = 91/6$.
\end{solution}

\begin{exercise}
\label{exer:4.3.9}
Consider flipping $n$ fair nickels and $n$ fair dimes. Let $X_n$ equal $4$ times the number of nickels showing heads, plus $5$ times the number of dimes showing heads. Find (with proof) a number $r$ such that $\frac{1}{n} X_n \xrightarrow{\text{a.s.}} r$.
\end{exercise}

\begin{solution}
Let $Y_n$ and $Z_n$ be the number of nickels and dimes showing heads, respectively. Then, $X_n = 4Y_n + 5Z_n$. By the strong law of large numbers, $Y_n/n \xrightarrow{a.s.} 1/2$ and $Z_n/n \xrightarrow{a.s.} 1/2$. Let $A$ be the event such that $Y_n/n \to 1/2$ on $A$ and $B$ be the event such that $Z_n/n \to 1/2$ on $B$. Then, $\prb(A) = \prb(B) = 1$. It is easy to check that $X_n/n = (4Y_n + 5Z_n)/n \to 4(1/2) + 5(1/2) = 9/2$ on $A \cap B$. The probability of $A \cap B$ is $\prb(A \cap B) = \prb(A) + \prb(B) - \prb(A \cup B) = 1 + 1 - 1 = 1$. Hence, $X_n/n \xrightarrow{a.s.} 9/2$. Therefore $r = 9/2$.
\end{solution}

\begin{exercise}
\label{exer:4.3.10}
Suppose $Y_n \xrightarrow{\text{a.s.}} Y$. Does this imply that $\prb(|Y_5 - Y| \leqslant |Y_4 - Y|) = 0$? Explain.
\end{exercise}

\begin{solution}
Suppose $Y = Y_1 = Y_2 = Y_3 = 0 = Y_5 = Y_6 = \cdots$ and $Y_5 = 1$. Then, $\lim_{n \to \infty} Y_n = 0 = Y$. Hence, $Y_n \xrightarrow{a.s.} Y$. However, $\prb(|Y_5 - Y| > |Y_4 - Y|) = \prb(|Y_5| > 0) = 1$. Hence, $\prb(|Y_5 - Y| > |Y_4 - Y|) = 0$ doesn't hold. Any convergence deals with very large $n$'s. Hence, we can ignore a finite number of $Y_1, \ldots, Y_k$ in convergence for fixed $k$.
\end{solution}

\begin{exercise}
\label{exer:4.3.11}
Consider repeatedly flipping a fair coin. Let $H_n$ be the number of heads on the first $n$ flips, and let $Z_n = H_n/n$.
\begin{enumerate}[(a)]
\item Prove that, with probability 1, there is some $m$ such that $|Z_n - 1/2| < 0.001$ for all $n \geqslant m$.
\item Let $r$ be the smallest positive integer satisfying $|Z_r - 1/2| < 0.001$. Must we have $|Z_n - 1/2| < 0.001$ for all $n \geqslant r$? Why or why not?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Suppose there is no such $m$. Then, there is a sequence $n_k$ such that $|Z_{n_k} - 1/2| \geqslant 0.001$ and $Z_{n_k} \to c$. Then, $|c - 1/2| \geqslant 0.001$. That means $Z_{n_k} \not\to 1/2$. It contradicts to the strong law of large numbers. Hence, there must exist a number $m$ such that $|Z_n - 1/2| < 0.001$ for all $n \geqslant m$.
    \item Suppose the flipping sequence starts with HT. Then $Z_2 = 1/2$. So, $r = 2$. However, $Z_3 = 1/3$ or $Z_3 = 2/3$. Thus, the statement that $|Z_n - 1/2| < 0.001$ for all $n \geqslant r$ is false. Usual limit theorems deal with large $n$. That means we can ignore the first finite observations.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.3.12}
Suppose $\prb(X = 0) = \prb(X = 1) = 1/2$, and let $X_n = X$ for $n = 1, 2, 3, \ldots$. (That is, the random variables $\{X_n\}$ are all identical.) Let $Y_n = (X_1 + X_2 + \cdots + X_n)/n$.
\begin{enumerate}[(a)]
\item Prove that $\prb(\lim_{n \to \infty} Y_n = 0) = \prb(\lim_{n \to \infty} Y_n = 1) = 1/2$.
\item Prove that there is no number $m$ such that $\prb(\lim_{n \to \infty} Y_n = m) = 1$.
\item Why does part (b) not contradict the law of large numbers?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Since $X_n = X$ for all $n \geqslant 1$, $Y_n = (X_1 + \cdots + X_n)/n = (X + \cdots + X)/n = nX/n = X$. Thus $\lim_{n \to \infty} Y_n = X$. The probability $\prb(\lim_{n \to \infty} Y_n = y) = \prb(X = y)$ is $\prb(X = y) = 1/2$ for $y = 0, 1$ and otherwise $\prb(X = y) = 0$.
    \item Suppose there is a number $m$ such that $\prb(\lim_{n \to \infty} Y_n = m) = 1$, i.e., $\prb(X = m) = 1$. From part (a), $m$ cannot be 1 because $\prb(X = 1) = 1/2 < 1$. Now $m$ is not 1. $\prb(\lim_{n \to \infty} Y_n = m) = \prb(X = m) \leqslant \prb(X \neq 1) = 1 - \prb(X = 1) = 1/2$. Hence, there is no $m$ satisfying $\prb(\lim_{n \to \infty} Y_n = m) = 1$.
    \item In the law of large numbers, the independence of random variables $X_1, \ldots, X_n$ is assumed. If the independence condition is dropped, then the law of large numbers may not hold even though each variable is identically distributed.
\end{enumerate}
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:4.3.13}
Generate i.i.d.\ $X_1, \ldots, X_n$ distributed Exponential$(5)$ with $n$ large (take $n = 10^5$ if possible). Plot the values $M_1, M_2, \ldots, M_n$. To what value are they converging? How quickly?
\end{exercise}

\begin{solution}
The sample means are converging to $1/5 = 0.2$.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig4_3_13.pdf}
    \caption{Sample means converging to 0.2 for Exercise 4.3.13}
    %\label{fig:sample-means-4313}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:4.3.14}
Generate i.i.d.\ $X_1, \ldots, X_n$ distributed Poisson$(7)$ with $n$ large (take $n = 10^5$ if possible). Plot the values $M_1, M_2, \ldots, M_n$. To what value are they converging? How quickly?
\end{exercise}

\begin{solution}
The sample means are converging (slowly) to 7.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig4_3_14.pdf}
    \caption{Sample means converging to 7 for Exercise 4.3.14}
    %\label{fig:sample-means-4314}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:4.3.15}
Generate i.i.d.\ $X_1, X_2, \ldots, X_n$ distributed $\mathrm{N}(4, 3)$ with $n$ large (take $n = 10^5$ if possible). Plot the values $M_1, M_2, \ldots, M_n$. To what value are they converging? How quickly?
\end{exercise}

\begin{solution}
The sample means are converging (slowly) to $-4$.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig4_3_15.pdf}
    \caption{Sample means converging to $-4$ for Exercise 4.3.15}
    %\label{fig:sample-means-4315}
\end{figure}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:4.3.16}
Suppose for each positive integer $k$, there are random variables $W_k, X_{k,1}, X_{k,2}, \ldots$ such that $\prb(\lim_{n \to \infty} X_{k,n} = W_k) = 1$. Prove that $\prb(\lim_{n \to \infty} X_{k,n} = W_k \text{ for all } k) = 1$.
\end{exercise}

\begin{solution}
By countable subadditivity, $\prb(\lim_{n \to \infty} X_{n,k} = W_k \text{ for all } k) = 1 - \prb(\exists k; \lim_{n \to \infty} X_{n,k} \neq W_k) \geqslant 1 - \sum_k \prb(\lim_{n \to \infty} X_{n,k} \neq W_k) = 1 - \sum_k 0 = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.3.17}
Prove that $X_n \xrightarrow{\text{a.s.}} 0$ if and only if $|X_n| \xrightarrow{\text{a.s.}} 0$.
\end{exercise}

\begin{solution}
Note that $x_n \to 0$ if and only if for all $c > 0$, $|x_n - 0| < c$ for all but finitely many $n$. But ``$|x_n - 0| < c$'' is the same as ``$|x_n| < c$'' is the same as ``$||x_n| - 0| < c$.'' Hence, $x_n \to 0$ if and only if $|x_n| \to 0$. Thus, $\prb(X_n \to 0) = \prb(|X_n| \to 0)$. Therefore, $X_n \to 0$ with probability 1 if and only if $|X_n| \to 0$ with probability 1.
\end{solution}

\begin{exercise}
\label{exer:4.3.18}
Prove or disprove that $X_n \xrightarrow{\text{a.s.}} 5$ if and only if $|X_n| \xrightarrow{\text{a.s.}} 5$.
\end{exercise}

\begin{solution}
This is false. For example, if $X_n = -5$ for all $n$, then $\prb(X_n \to 5) = 0$ but $\prb(|X_n| \to 5) = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.3.19}
Suppose $X_n \xrightarrow{\text{a.s.}} X$, and $Y_n \xrightarrow{\text{a.s.}} Y$. Let $Z_n = X_n + Y_n$ and $Z = X + Y$. Prove that $Z_n \xrightarrow{\text{a.s.}} Z$.
\end{exercise}

\begin{solution}
Let $A$ be the event $X_n \to X$ and $B$ be the event $Y_n \to Y$. Then, $X_n \xrightarrow{a.s.} X$ and $Y_n \xrightarrow{a.s.} Y$ imply $\prb(A) = \prb(B) = 1$. Let $C$ be the event $Z_n \to Z$. On $A \cap B$, $X_n \to X$ and $Y_n \to Y$. Hence, $Z_n = X_n + Y_n \to X + Y = Z$. Thus, $A \cap B \subset C$. The probability of $A \cap B$ is $\prb(A \cap B) = \prb(A) + \prb(B) - \prb(A \cup B) = 1 + 1 - 1 = 1$. Therefore, $Z_n \xrightarrow{a.s.} Z$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:4.3.20}
Suppose for each real number $r \in [0,1]$, there are random variables $W_r, X_{r,1}, X_{r,2}, \ldots$ such that $\prb(\lim_{n \to \infty} X_{r,n} = W_r) = 1$. Prove or disprove that we must have $\prb(\lim_{n \to \infty} X_{n,r} = W_r \text{ for all } r \in [0,1]) = 1$.
\end{exercise}

\begin{solution}
This is false. For example, let $U \sim \text{Uniform}[0, 1]$. Let $W_r = 0$ for all $r$ and let $X_r = 0$ if $U \neq r$, with $X_r = 1$ if $U = r$. Then $\prb(X_r \to 0) = \prb(U \neq r) = 1$, but $\prb(X_r \to 0 \text{ for all } r) = \prb(U \notin [0, 1]) = 0$.
\end{solution}

\begin{exercise}
\label{exer:4.3.21}
Give an example of random variables $X_1, X_2, \ldots$ such that $X_n$ converges to $0$ with probability 1, but $\expc(|X_n|) \geqslant 1$ for all $n$.
\end{exercise}

\begin{solution}
Let $S = \{1, 2, 3, \ldots\}$, with $\prb(s) = 2^{-s}$. Let $X_n(s) = 2^n$ for $s = n$, otherwise $X_n(s) = 0$. Then $\expc(X_n) = (2^n)(2^{-n}) = 1$. However, $X_n(s) = 0$ whenever $n > s$, so $\prb(X_n \to 0) = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.3.22}
Suppose $X_n \xrightarrow{\text{a.s.}} X$, and $f$ is a continuous function. Prove that $f(X_n) \xrightarrow{\text{a.s.}} f(X)$.
\end{exercise}

\begin{solution}
The continuity of the function $f$ implies $f(x_n) \to f(x)$ whenever $x_n \to x$. Let $A$ be the event $f(X_n) \to f(X)$ and $B$ be the event $X_n \to X$, i.e., $X_n \to X$ on $B$. Hence, on $B$, $X_n \to X$ implies $f(X_n) \to f(X)$. Thus, $B \subset A$. Then, $\prb(\lim_{n \to \infty} f(X_n) = f(X)) = \prb(A) \geqslant \prb(B) = 1$. Therefore, $f(X_n) \xrightarrow{a.s.} f(X)$.
\end{solution}


\section{Convergence in Distribution}
\label{sec:4.4}

There is yet another notion of convergence of a sequence of random variables that is important in applications of probability and statistics.

\begin{definition}
\label{def:4.4.1}
Let $X, X_1, X_2, \ldots$ be random variables. Then we say that the sequence $\{X_n\}$ \emph{converges in distribution} to $X$ if for all $x \in \mathbb{R}^1$ such that $\prb(X = x) = 0$, we have $\lim_{n \to \infty} \prb(X_n \leqslant x) = \prb(X \leqslant x)$, and we write $X_n \xrightarrow{D} X$.
\end{definition}

Intuitively, $\{X_n\}$ converges in distribution to $X$ if for large $n$, the distribution of $X_n$ is close to that of $X$. The importance of this, as we will see, is that often the distribution of $X_n$ is difficult to work with, while that of $X$ is much simpler. With $\{X_n\}$ converging in distribution to $X$, however, we can approximate the distribution of $X_n$ by that of $X$.

\begin{example}
\label{ex:4.4.1}
Suppose $\prb(X_n = 1) = 1/n$, and $\prb(X_n = 0) = 1 - 1/n$. Let $X = 0$ so that $\prb(X = 0) = 1$. Then,
\[
\prb(X_n \leqslant x) = \begin{cases}
0 & x < 0 \\
1 - 1/n & 0 \leqslant x < 1 \\
1 & 1 \leqslant x
\end{cases}
\to \prb(X \leqslant x) = \begin{cases}
0 & x < 0 \\
1 & 0 \leqslant x
\end{cases}
\]
as $n \to \infty$. As $\prb(X_n \leqslant x) \to \prb(X \leqslant x)$ for every $x$, and in particular at all $x$ where $\prb(X = x) = 0$, we have that $\{X_n\}$ converges in distribution to $X$. Intuitively, as $n \to \infty$, it is more and more likely that $X_n$ will equal $0$.
\end{example}

\begin{example}
\label{ex:4.4.2}
Suppose $\prb(X_n = 1) = 1/2 + 1/n$, and $\prb(X_n = 0) = 1/2 - 1/n$. Suppose further that $\prb(X = 0) = \prb(X = 1) = 1/2$. Then $\{X_n\}$ converges in distribution to $X$ because $\prb(X_n \leqslant 1) \to 1/2$ and $\prb(X_n \leqslant 0) \to 1/2$ as $n \to \infty$.
\end{example}

\begin{example}
\label{ex:4.4.3}
Let $X \sim \text{Uniform}[0,1]$, and let $\prb(X_n = i/n) = 1/n$ for $i = 1, 2, \ldots, n$. Then $X$ is absolutely continuous, while $X_n$ is discrete. On the other hand, for any $0 < x < 1$, we have $\prb(X \leqslant x) = x$, and letting $\lfloor x \rfloor$ denote the greatest integer less than or equal to $x$, we have
\[
\prb(X_n \leqslant x) = \frac{\lfloor nx \rfloor}{n}.
\]
Hence, $|\prb(X_n \leqslant x) - \prb(X \leqslant x)| \leqslant 1/n$ for all $n$. Because $\lim_{n \to \infty} 1/n = 0$, we do indeed have $X_n \to X$ in distribution.
\end{example}

\begin{example}
\label{ex:4.4.4}
Suppose $X_1, X_2, \ldots$ are i.i.d.\ with finite mean $\mu$, and $M_n = (X_1 + \cdots + X_n)/n$. Then the weak law of large numbers says that for any $\epsilon > 0$ we have
\[
\prb(M_n \leqslant \mu - \epsilon) \to 0 \quad \text{and} \quad \prb(M_n \leqslant \mu + \epsilon) \to 1
\]
as $n \to \infty$. It follows that $\lim_{n \to \infty} \prb(M_n \leqslant x) = \prb(M \leqslant x)$ for any $x \neq \mu$, where $M$ is the constant random variable $M = \mu$. Hence, $M_n \to M$ in distribution. Note that it is not necessarily the case that $\prb(M_n \leqslant \mu) \to \prb(M \leqslant \mu) = 1$. However, this does not contradict the definition of convergence in distribution because $\prb(M = \mu) \neq 0$, so we do not need to worry about the case $x = \mu$.
\end{example}

\begin{example}[Poisson Approximation to the Binomial]
\label{ex:4.4.5}
Suppose $X_n \sim \text{Binomial}(n, \lambda/n)$ and $X \sim \text{Poisson}(\lambda)$. We have seen in Example~\ref{ex:2.3.6} that
\[
\prb(X_n = j) = \binom{n}{j} \left(\frac{\lambda}{n}\right)^j \left(1 - \frac{\lambda}{n}\right)^{n-j} \to e^{-\lambda} \frac{\lambda^j}{j!}
\]
as $n \to \infty$. This implies that $F_{X_n}(x) \to F_X(x)$ at every point $x \in \{0, 1, 2, \ldots\}$, and these are precisely the points for which $\prb(X = x) = 0$. Therefore, $\{X_n\}$ converges in distribution to $X$. (Indeed, this was our original motivation for the Poisson distribution.)
\end{example}

Many more examples of convergence in distribution are given by the central limit theorem, discussed in the next section. We first pause to consider the relationship of convergence in distribution to our previous notions of convergence.

\begin{theorem}
\label{thm:4.4.1}
If $X_n \xrightarrow{P} X$, then $X_n \xrightarrow{D} X$.
\end{theorem}

\begin{proof}
  See Section~\ref{sec:4.7} for the proof of this result.
\end{proof}

The converse to Theorem~\ref{thm:4.4.1} is false. Indeed, the fact that $\{X_n\}$ converges in distribution to $X$ says nothing about the underlying relationship between $X_n$ and $X$; it says only something about their distributions. The following example illustrates this.

\begin{example}
\label{ex:4.4.6}
Suppose $X, X_1, X_2, \ldots$ are i.i.d., each equal to $\pm 1$ with probability $1/2$ each. In this case, $\prb(X_n \leqslant x) = \prb(X \leqslant x)$ for all $n$ and for all $x \in \mathbb{R}^1$, so of course $\{X_n\}$ converges in distribution to $X$. On the other hand, because $X$ and $X_n$ are independent,
\[
\prb(|X - X_n| \geqslant 2) = \frac{1}{2}
\]
for all $n$, which does not go to $0$ as $n \to \infty$. Hence, $\{X_n\}$ does not converge to $X$ in probability (or with probability 1). So we can have convergence in distribution without having convergence in probability or convergence with probability 1.
\end{example}

The following result, stated without proof, indicates how moment-generating functions can be used to check for convergence in distribution. (This generalizes Theorem~3.4.6.)

\begin{theorem}
\label{thm:4.4.2}
Let $X$ be a random variable, such that for some $s_0 > 0$, we have $m_X(s) < \infty$ whenever $|s| < s_0$. If $Z_1, Z_2, \ldots$ is a sequence of random variables with $m_{Z_n}(s) < \infty$ and $\lim_{n \to \infty} m_{Z_n}(s) = m_X(s)$ for all $|s| < s_0$, then $\{Z_n\}$ converges to $X$ in distribution.
\end{theorem}

We will make use of this result to prove one of the most famous theorems of probability --- the central limit theorem.

Finally, we note that combining Theorem~\ref{thm:4.4.1} with Theorem~\ref{thm:4.3.1} reveals the following.

\begin{corollary}
\label{cor:4.4.1}
If $X_n \to X$ with probability 1, then $X_n \xrightarrow{D} X$.
\end{corollary}

\subsection{The Central Limit Theorem}
\label{ssec:4.4.1}

We now present the central limit theorem, one of the most important results in all of probability theory. Intuitively, it says that a large sum of i.i.d.\ random variables, properly normalized, will always have approximately a normal distribution. This shows that the normal distribution is extremely fundamental in probability and statistics --- even though its density function is complicated and its cumulative distribution function is intractable.

Suppose $X_1, X_2, \ldots$ is an i.i.d.\ sequence of random variables each having finite mean $\mu$ and finite variance $\sigma^2$. Let $S_n = X_1 + \cdots + X_n$ be the sample sum and $M_n = S_n/n$ be the sample mean. The central limit theorem is concerned with the distribution of the random variable
\[
Z_n = \frac{S_n - n\mu}{\sqrt{n}\,\sigma} = \frac{M_n - \mu}{\sigma/\sqrt{n}} = \frac{\sqrt{n}(M_n - \mu)}{\sigma},
\]
where $\sigma = \sqrt{\sigma^2}$. We know $\expc(M_n) = \mu$ and $\var(M_n) = \sigma^2/n$, which implies that $\expc(Z_n) = 0$ and $\var(Z_n) = 1$. The variable $Z_n$ is thus obtained from the sample mean (or sample sum) by subtracting its mean and dividing by its standard deviation. This transformation is referred to as \emph{standardizing} a random variable, so that it has mean $0$ and variance $1$. Therefore, $Z_n$ is the standardized version of the sample mean (sample sum).

Note that the distribution of $Z_n$ shares two characteristics with the $\mathrm{N}(0,1)$ distribution, namely, it has mean $0$ and variance $1$. The central limit theorem shows that there is an even stronger relationship.

\begin{theorem}[The central limit theorem]
\label{thm:4.4.3}
Let $X_1, X_2, \ldots$ be i.i.d.\ with finite mean $\mu$ and finite variance $\sigma^2$. Let $Z \sim \mathrm{N}(0,1)$. Then as $n \to \infty$, the sequence $\{Z_n\}$ converges in distribution to $Z$, i.e., $Z_n \xrightarrow{D} Z$.
\end{theorem}

\begin{proof}
  See Section~\ref{sec:4.7} for the proof of this result.
\end{proof}

The central limit theorem is so important that we shall restate its conclusions in several different ways.

\begin{corollary}
\label{cor:4.4.2}
For each fixed $x \in \mathbb{R}^1$, $\lim_{n \to \infty} \prb(Z_n \leqslant x) = \Phi(x)$, where $\Phi$ is the cumulative distribution function for the standard normal distribution.
\end{corollary}

We can write this as follows.

\begin{corollary}
\label{cor:4.4.3}
For each fixed $x \in \mathbb{R}^1$,
\[
\lim_{n \to \infty} \prb(S_n \leqslant n\mu + x\sqrt{n}\,\sigma) = \Phi(x) \quad \text{and} \quad \lim_{n \to \infty} \prb(M_n \leqslant \mu + x\sigma/\sqrt{n}) = \Phi(x).
\]
In particular, $S_n$ is approximately equal to $n\mu$, with deviations from this value of order $\sqrt{n}$, and $M_n$ is approximately equal to $\mu$, with deviations from this value of order $1/\sqrt{n}$.
\end{corollary}

We note that it is not essential in the central limit theorem to divide by $\sigma$, in which case the theorem asserts instead that $(S_n - n\mu)/\sqrt{n}$ (or $\sqrt{n}(M_n - \mu)$) converges in distribution to the $\mathrm{N}(0, \sigma^2)$ distribution. That is, the limiting distribution will still be normal but will have variance $\sigma^2$ instead of variance $1$.

Similarly, instead of dividing by exactly $\sigma$, it suffices to divide by any quantity $\sigma_n$, provided $\sigma_n \xrightarrow{\text{a.s.}} \sigma$. A simple modification of the proof of Theorem~\ref{thm:4.4.2} leads to the following result.

\begin{corollary}
\label{cor:4.4.4}
If
\[
Z_n^* = \frac{S_n - n\mu}{\sqrt{n}\,\sigma_n} = \frac{M_n - \mu}{\sigma_n/\sqrt{n}} = \frac{\sqrt{n}(M_n - \mu)}{\sigma_n}
\]
and $\lim_{n \to \infty} \sigma_n \xrightarrow{\text{a.s.}} \sigma$, then $Z_n^* \xrightarrow{D} Z$ as $n \to \infty$.
\end{corollary}

To illustrate the central limit theorem, we consider a simulation experiment.

\begin{example}[The Central Limit Theorem Illustrated in a Simulation]
\label{ex:4.4.7}
Suppose we generate a sample $X_1, \ldots, X_n$ from the Uniform$[0,1]$ density. Note that the Uniform$[0,1]$ density is completely unlike a normal density. An easy calculation shows that when $X \sim \text{Uniform}[0,1]$, then $\expc(X) = 1/2$ and $\var(X) = 1/12$.

Now suppose we are interested in the distribution of the sample average $M_n = S_n/n = (X_1 + \cdots + X_n)/n$ for various choices of $n$. The central limit theorem tells us that
\[
Z_n = \frac{S_n - n/2}{\sqrt{n/12}} = \frac{\sqrt{n}(M_n - 1/2)}{\sqrt{1/12}}
\]
converges in distribution to an $\mathrm{N}(0,1)$ distribution. But how large does $n$ have to be for this approximation to be accurate?

To assess this, we ran a Monte Carlo simulation experiment. In Figure~\ref{fig:4.4.1}, we have plotted a density histogram of $N = 10^5$ values from the $\mathrm{N}(0,1)$ distribution based on 800 subintervals of $[-4, 4]$, each of length $l = 0.01$. Density histograms are more extensively discussed in Section~\ref{ssec:5.4.3}, but for now we note that above each interval we have plotted the proportion of sampled values that fell in the interval, divided by the length of the interval. As we increase $N$ and decrease $l$, these histograms will look more and more like the density of the distribution from which we are sampling. Indeed, Figure~\ref{fig:4.4.1} looks very much like an $\mathrm{N}(0,1)$ density, as it should.

In Figure~\ref{fig:4.4.2}, we have plotted a density histogram (using the same values of $N$ and $l$) of $Z_1$. Note that $Z_1 \sim \text{Uniform}[-\sqrt{12}/2, \sqrt{12}/2]$ and indeed the histogram does look like a uniform density. Figure~\ref{fig:4.4.3} presents a density histogram of $Z_2$, which still looks very nonnormal --- but note that the histogram of $Z_3$ in Figure~\ref{fig:4.4.4} is beginning to look more like a normal distribution. The histogram of $Z_{10}$ in Figure~\ref{fig:4.4.5} looks very normal. In fact, the proportion of $Z_{10}$ values in $(-\infty, 1.96]$ for this histogram, equals $0.9759$, while the exact proportion for an $\mathrm{N}(0,1)$ distribution is $0.9750$.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_4_1.pdf}
  \caption{Density histogram of $10^5$ standard normal values.}
  \label{fig:4.4.1}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_4_2.pdf}
  \caption{Density histogram for $10^5$ values of $Z_1$ in Example~\ref{ex:4.4.7}.}
  \label{fig:4.4.2}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_4_3.pdf}
  \caption{Density histogram for $10^5$ values of $Z_2$ in Example~\ref{ex:4.4.7}.}
  \label{fig:4.4.3}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_4_4.pdf}
  \caption{Density histogram for $10^5$ values of $Z_3$ in Example~\ref{ex:4.4.7}.}
  \label{fig:4.4.4}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_4_5.pdf}
  \caption{Density histogram for $10^5$ values of $Z_{10}$ in Example~\ref{ex:4.4.7}.}
  \label{fig:4.4.5}
\end{figure}

So in this example, the central limit theorem has taken effect very quickly, even though we are sampling from a very nonnormal distribution. As it turns out, it is primarily the tails of a distribution that determine how large $n$ has to be for the central limit theorem approximation to be accurate. When a distribution has tails no heavier than a normal distribution, we can expect the approximation to be quite accurate for relatively small sample sizes.

We consider some further applications of the central limit theorem.

\begin{example}
\label{ex:4.4.8}
For example, suppose $X_1, X_2, \ldots$ are i.i.d.\ random variables, each with the Poisson$(5)$ distribution. Recall that this implies that $\expc(X_i) = 5$ and $\sigma^2 = \var(X_i) = 5$. Hence, for each fixed $x \in \mathbb{R}^1$, we have
\[
\prb(S_n \leqslant 5n + x\sqrt{5n}) \to \Phi(x)
\]
as $n \to \infty$.
\end{example}

\begin{example}[Normal Approximation to the Binomial Distribution]
\label{ex:4.4.9}
Suppose $X_1, X_2, \ldots$ are i.i.d.\ random variables, each with the Bernoulli$(\theta)$ distribution. Recall that this implies that $\expc(X_i) = \theta$ and $\var(X_i) = \theta(1 - \theta)$. Hence, for each fixed $x \in \mathbb{R}^1$, we have
\begin{equation}
\label{eq:4.4.1}
\prb(S_n \leqslant n\theta + x\sqrt{n\theta(1 - \theta)}) \to \Phi(x)
\end{equation}
as $n \to \infty$.

But now note that we have previously shown that $Y_n = S_n \sim \text{Binomial}(n, \theta)$. So \eqref{eq:4.4.1} implies that whenever we have a random variable $Y_n \sim \text{Binomial}(n, \theta)$, then
\begin{equation}
\label{eq:4.4.2}
\prb(Y_n \leqslant y) = P\left(\frac{Y_n - n\theta}{\sqrt{n\theta(1 - \theta)}} \leqslant \frac{y - n\theta}{\sqrt{n\theta(1 - \theta)}}\right) \approx \Phi\left(\frac{y - n\theta}{\sqrt{n\theta(1 - \theta)}}\right)
\end{equation}
for large $n$.

Note that we are approximating a discrete distribution by a continuous distribution here. Reflecting this, a small improvement is often made to \eqref{eq:4.4.2} when $y$ is a nonnegative integer. Instead, we use
\[
\prb(Y_n \leqslant y) \approx \Phi\left(\frac{y + 0.5 - n\theta}{\sqrt{n\theta(1 - \theta)}}\right).
\]
Adding $0.5$ to $y$ is called the \emph{correction for continuity}. In effect, this allocates all the relevant normal probability in the interval $(y - 0.5, y + 0.5)$ to the nonnegative integer $y$. This has been shown to improve the approximation \eqref{eq:4.4.2}.
\end{example}

\begin{example}[Approximating Probabilities Using the Central Limit Theorem]
\label{ex:4.4.10}
While there are tables for the binomial distribution (Table~D.6), we often have to compute binomial probabilities for situations the tables do not cover. We can always use statistical software for this; in fact, such software makes use of the normal approximation we derived from the central limit theorem.

For example, suppose that we have a biased coin, where the probability of getting a head on a single toss is $\theta = 0.6$. We will toss the coin $n = 1000$ times and then calculate the probability of getting at least 550 heads and no more than 625 heads. If $Y$ denotes the number of heads obtained in the 1000 tosses, we have that $Y \sim \text{Binomial}(1000, 0.6)$, so
\begin{align*}
\expc(Y) &= 1000 \times 0.6 = 600, \\
\var(Y) &= 1000 \times 0.6 \times 0.4 = 240.
\end{align*}
Therefore, using the correction for continuity and Table~D.2,
\begin{align*}
\prb(550 \leqslant Y \leqslant 625) &= \prb(550 - 0.5 \leqslant Y \leqslant 625 + 0.5) \\
&\approx P\left(\frac{549.5 - 600}{\sqrt{240}} \leqslant \frac{Y - 600}{\sqrt{240}} \leqslant \frac{625.5 - 600}{\sqrt{240}}\right) \\
&\approx \prb(-3.2598 \leqslant \frac{Y - 600}{\sqrt{240}} \leqslant 1.646) \\
&\approx \Phi(1.65) - \Phi(-3.26) \approx 0.9505 - 0.0006 = 0.9499.
\end{align*}
Note that it would be impossible to compute this probability using the formulas for the binomial distribution.
\end{example}

One of the most important uses of the central limit theorem is that it leads to a method for assessing the error in an average when this is estimating or approximating some quantity of interest.

\subsection{The Central Limit Theorem and Assessing Error}
\label{ssec:4.4.2}

Suppose $X_1, X_2, \ldots$ is an i.i.d.\ sequence of random variables, each with finite mean $\mu$ and finite variance $\sigma^2$, and we are using the sample average $M_n$ to approximate the mean $\mu$. This situation arises commonly in many computational (see Section~\ref{sec:4.5}) and statistical (see Chapter~\ref{ch:6}) problems. In such a context, we can generate the $X_i$ but we do not know the value of $\mu$.

If we approximate $\mu$ by $M_n$, then a natural question to ask is: How much error is there in the approximation? The central limit theorem tells us that
\[
\Phi(3) - \Phi(-3) = \lim_{n \to \infty} P\left(-3 < \frac{M_n - \mu}{\sigma/\sqrt{n}} < 3\right) = \lim_{n \to \infty} P\left(M_n - 3\frac{\sigma}{\sqrt{n}} < \mu < M_n + 3\frac{\sigma}{\sqrt{n}}\right).
\]
Using Table~D.2 (or statistical software), we have that $\Phi(3) - \Phi(-3) = 0.9987 - (1 - 0.9987) = 0.9974$. So, for large $n$, we have that the interval
\[
\left(M_n - 3\sigma/\sqrt{n},\, M_n + 3\sigma/\sqrt{n}\right)
\]
contains the unknown value of $\mu$ with virtual certainty (actually with probability about $0.9974$). Therefore, the half-length $3\sigma/\sqrt{n}$ of this interval gives us an assessment of the error in the approximation $M_n$. Note that $\var(M_n) = \sigma^2/n$, so the half-length of the interval equals 3 standard deviations of the estimate $M_n$.

Because we do not know $\mu$, it is extremely unlikely that we will know $\sigma$ (as its definition uses $\mu$). But if we can find a consistent estimate $\sigma_n$ of $\sigma$, then we can use Corollary~\ref{cor:4.4.4} instead to construct such an interval.

As it turns out, the correct choice of $\sigma_n$ depends on what we know about the distribution we are sampling from (see Chapter~\ref{ch:6} for more discussion of this). For example, if $X_1 \sim \text{Bernoulli}(\theta)$, then $\theta = \mu$ and $\sigma^2 = \var(X_1) = \theta(1 - \theta)$. By the strong law of large numbers (Theorem~\ref{thm:4.3.2}), $M_n \xrightarrow{\text{a.s.}} \theta$ and thus
\[
\sigma_n = \sqrt{M_n(1 - M_n)} \xrightarrow{\text{a.s.}} \sqrt{\theta(1 - \theta)} = \sigma.
\]
Then, using the same argument as above, we have that, for large $n$, the interval
\begin{equation}
\label{eq:4.4.3}
\left(M_n - 3\sqrt{M_n(1 - M_n)/n},\, M_n + 3\sqrt{M_n(1 - M_n)/n}\right)
\end{equation}
contains the true value of $\theta$ with virtual certainty (again, with probability about $0.9974$). The half-length of \eqref{eq:4.4.3} is a measure of the accuracy of the estimate $M_n$ --- notice that this can be computed from the values $X_1, \ldots, X_n$. We refer to the quantity $\sqrt{M_n(1 - M_n)/n}$ as the \emph{standard error} of the estimate $M_n$.

For a general random variable $X_1$, let
\begin{align*}
\sigma_n^2 &= \frac{1}{n-1} \sum_{i=1}^{n} (X_i - M_n)^2 = \frac{1}{n-1} \left(\sum_{i=1}^{n} X_i^2 - 2M_n \sum_{i=1}^{n} X_i + nM_n^2\right) \\
&= \frac{n}{n-1} \left(\frac{1}{n} \sum_{i=1}^{n} X_i^2 - 2M_n^2 + M_n^2\right) = \frac{n}{n-1} \left(\frac{1}{n} \sum_{i=1}^{n} X_i^2 - M_n^2\right).
\end{align*}
By the strong law of large numbers, we have that $M_n \xrightarrow{\text{a.s.}} \mu$ and
\[
\frac{1}{n} \sum_{i=1}^{n} X_i^2 \xrightarrow{\text{a.s.}} \expc(X_1^2) = \sigma^2 + \mu^2.
\]
Because $n/(n-1) \to 1$ and $M_n^2 \xrightarrow{\text{a.s.}} \mu^2$ as well, we conclude that $\sigma_n^2 \xrightarrow{\text{a.s.}} \sigma^2$. This implies that $\sigma_n \xrightarrow{\text{a.s.}} \sigma$; hence $\sigma_n$ is consistent for $\sigma$. It is common to call $\sigma_n^2$ the \emph{sample variance} of the sample $X_1, \ldots, X_n$. When the sample size $n$ is fixed, we will often denote this estimate of the variance by $S^2$.

Again, using the above argument, we have that, for large $n$, the interval
\begin{equation}
\label{eq:4.4.4}
\left(M_n - 3\sigma_n/\sqrt{n},\, M_n + 3\sigma_n/\sqrt{n}\right) = \left(M_n - 3S/\sqrt{n},\, M_n + 3S/\sqrt{n}\right)
\end{equation}
contains the true value of $\mu$ with virtual certainty (also with probability about $0.9974$). Therefore, the half-length is a measure of the accuracy of the estimate $M_n$ --- notice that this can be computed from the values $X_1, \ldots, X_n$. The quantity $S/\sqrt{n}$ is referred to as the \emph{standard error} of the estimate $M_n$.

We will make use of these estimates of the error in approximations in the following section.

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:4.4}}
\begin{itemize}
\item A sequence $\{X_n\}$ of random variables converges in distribution to $Y$ if, for all $y \in \mathbb{R}^1$ with $\prb(Y = y) = 0$, we have $\lim_{n \to \infty} F_{X_n}(y) = F_Y(y)$, i.e., $\lim_{n \to \infty} \prb(X_n \leqslant y) = \prb(Y \leqslant y)$.
\item If $\{X_n\}$ converges to $Y$ in probability (or with probability 1), then $\{X_n\}$ converges to $Y$ in distribution.
\item The very important central limit theorem says that if $\{X_n\}$ are i.i.d.\ with finite mean $\mu$ and variance $\sigma^2$, then the random variables $Z_n = (S_n - n\mu)/(\sqrt{n}\,\sigma)$ converge in distribution to a standard normal distribution.
\item The central limit theorem allows us to approximate various distributions by normal distributions, which is helpful in simulation experiments and in many other contexts. Table~D.2 (or any statistical software package) provides values for the cumulative distribution function of a standard normal.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:4.4.1}
Suppose $\prb(X_n = i/n) = (i + 3n)/(6n)$ for $i = 1, 2, 3$. Suppose also that $\prb(X = i) = 1/3$ for $i = 1, 2, 3$. Prove that $\{X_n\}$ converges in distribution to $X$.
\end{exercise}

\begin{solution}
Here $\lim_{n \to \infty} \prb(X_n = i) = 1/3 = \prb(X = i)$ for $i = 1, 2, 3$, so $\lim_{n \to \infty} \prb(X_n \leqslant x) = \prb(X \leqslant x)$ for all $x$, so $X_n \to X$ in distribution.
\end{solution}

\begin{exercise}
\label{exer:4.4.2}
Suppose $\prb(Y_n = k) = (1/2)^{n+1}(1/2)^{k+1}$ for $k = 0, 1, \ldots, n$. Let $Y \sim \text{Geometric}(1/2)$. Prove that $\{Y_n\}$ converges in distribution to $Y$.
\end{exercise}

\begin{solution}
We have that $\lim_{n \to \infty} \prb(Y_n = k) = 1/2^{k+1}$ for $k = 0, 1, \ldots$ so $\lim_{n \to \infty} \prb(Y_n \leqslant y) = \prb(Y \leqslant y)$.
\end{solution}

\begin{exercise}
\label{exer:4.4.3}
Let $Z_n$ have density $(n+1)x^n$ for $0 \leqslant x \leqslant 1$, and $0$ otherwise. Let $Z = 1$. Prove that $\{Z_n\}$ converges in distribution to $Z$.
\end{exercise}

\begin{solution}
Here $\prb(Z_n \leqslant 1) = 1$, and for $0 \leqslant z < 1$, $\prb(Z_n \leqslant z) = \int_0^z (n + 1)x^n \, \mathrm{d}x = z^{n+1} \to 0$ as $n \to \infty$. Also, $\prb(Z \leqslant z) = 1$ for $z \geqslant 1$, and 0 for $z < 1$. Hence, $\lim_{n \to \infty} \prb(Z_n \leqslant z) = \prb(Z \leqslant z)$ for all $z$, so $Z_n \to Z$ in distribution.
\end{solution}

\begin{exercise}
\label{exer:4.4.4}
Let $W_n$ have density
\[
\frac{1 + x/n}{1 + 1/(2n)}
\]
for $0 \leqslant x \leqslant 1$, and $0$ otherwise. Let $W \sim \text{Uniform}[0,1]$. Prove that $\{W_n\}$ converges in distribution to $W$.
\end{exercise}

\begin{solution}
For $0 < w < 1$, $\prb(W_n \leqslant w) = \int_0^w (1 + x/n)/(1 + 1/2n) \, \mathrm{d}x = (w + w^2/2n)/(1 + 1/2n) \to w$ as $n \to \infty$. Also, $\prb(W \leqslant w) = w$. Hence, $\lim_{n \to \infty} \prb(W_n \leqslant w) = \prb(W \leqslant w)$ for all $w$, so $W_n \to W$ in distribution.
\end{solution}

\begin{exercise}
\label{exer:4.4.5}
Let $Y_1, Y_2, \ldots$ be i.i.d.\ with distribution Exponential$(3)$. Use the central limit theorem and Table~D.2 (or software) to estimate the probability $P\bigl(\sum_{i=1}^{1600} Y_i \leqslant 540\bigr)$.
\end{exercise}

\begin{solution}
Let $S = Y_1 + Y_2 + \cdots + Y_{1600}$. Then $S$ has mean $1600/3$ and variance $1600/9$. Hence,
\begin{align*}
    \prb(S \leqslant 540) &= \prb((S - 1600/3)/\sqrt{1600/9} \leqslant (540 - 1600/3)/\sqrt{1600/9}) \\
    &\approx \Phi((540 - 1600/3)/(40/3)) = \Phi(1/2) = 1 - \Phi(-1/2) = 1 - \Phi(-0.5) \\
    &\approx 1 - 0.3085 = 0.6915.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:4.4.6}
Let $Z_1, Z_2, \ldots$ be i.i.d.\ with distribution Uniform$[-20, 10]$. Use the central limit theorem and Table~D.2 (or software) to estimate the probability $P\bigl(\sum_{i=1}^{900} Z_i \geqslant -4470\bigr)$.
\end{exercise}

\begin{solution}
Let $S = Z_1 + Z_2 + \cdots + Z_{900}$. Then $S$ has mean $900(-5) = -4500$ and variance $900(30^2/12)$. Hence,
\begin{align*}
    \prb(S \geqslant -4470) &= \prb((S - (-4500))/\sqrt{900(30^2/12)} \geqslant (-4470 - (-4500))/\sqrt{900(30^2/12)}) \\
    &\approx 1 - \Phi((-4470 - (-4500))/\sqrt{900(30^2/12)}) \\
    &\approx 1 - \Phi(0.11547) = \Phi(-0.11547) \approx \Phi(-0.12) = 0.4522.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:4.4.7}
Let $X_1, X_2, \ldots$ be i.i.d.\ with distribution Geometric$(1/4)$. Use the central limit theorem and Table~D.2 (or software) to estimate the probability $P\bigl(\sum_{i=1}^{800} X_i \leqslant 2450\bigr)$.
\end{exercise}

\begin{solution}
Let $S = X_1 + X_2 + \cdots + X_{800}$. Then $S$ has mean $800(1 - 1/4)/(1/4) = 2400$ and variance $800(1 - 1/4)/(1/4)^2 = 9600$. Hence,
\begin{align*}
    \prb(S \geqslant 2450) &= \prb((S - 2400)/\sqrt{9600} \geqslant (2450 - 2400)/\sqrt{9600}) \\
    &\approx 1 - \Phi((2450 - 2400)/\sqrt{9600}) \approx 1 - \Phi(0.51) = \Phi(-0.51) = 0.3050.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:4.4.8}
Suppose $X_n \sim \mathrm{N}(0, 1/n)$, i.e., $X_n$ has a normal distribution with mean $0$ and variance $1/n$. Does the sequence $\{X_n\}$ converge in distribution to some random variable? If yes, what is the distribution of the random variable?
\end{exercise}

\begin{solution}
Yes, $\{X_n\}$ converges in distribution to 0, since for $x < 0$, $\prb(X_n \leqslant x) = \Phi(\sqrt{n} x) \to 0$, while for $x > 0$, $\prb(X_n \leqslant x) = \Phi(\sqrt{n} x) \to 1$.
\end{solution}

\begin{exercise}
\label{exer:4.4.9}
Suppose $\prb(X_n = i/n) = 2i/(n(n+1))$ for $i = 1, 2, 3, \ldots, n$. Let $Z$ have density function given by $f(z) = 2z$ for $0 \leqslant z \leqslant 1$, otherwise $f(z) = 0$.
\begin{enumerate}[(a)]
\item Compute $\prb(Z \leqslant y)$ for $0 \leqslant y \leqslant 1$.
\item Compute $\prb(X_n \leqslant m/n)$ for some integer $1 \leqslant m \leqslant n$. (Hint: Remember that $\sum_{i=1}^{m} i = m(m+1)/2$.)
\item Compute $\prb(X_n \leqslant y)$ for $0 \leqslant y \leqslant 1$.
\item Prove that $X_n \xrightarrow{D} Z$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item For $0 < y < 1$, $\prb(Z \leqslant y) = \int_0^y 2x \, \mathrm{d}x = x^2 \big|_{x=0}^{x=y} = y^2$.
    \item For $1 \leqslant m \leqslant n$, $\prb(X_n \leqslant m/n) = \sum_{i=1}^{m} \prb(X_n = i/n) = \sum_{i=1}^{m} 2i/n(n + 1) = m(m + 1)/[n(n + 1)]$.
    \item For $0 < y < 1$, let $m = \lfloor ny \rfloor$, the biggest integer not greater than $ny$. Since there is no integer in $(m, ny)$, $\prb(m/n < X_n < y) \leqslant \prb(m/n < X_n < (m + 1)/n) = 0$. Thus, $\prb(X_n \leqslant y) = \prb(X_n \leqslant m/n) + \prb(m/n < X_n < y) = \prb(X_n \leqslant \lfloor ny \rfloor/n) = m(m + 1)/[n(n + 1)]$ where $m = \lfloor ny \rfloor$.
    \item Let $m_n = \lfloor ny \rfloor$. Then, $m_n/n \leqslant ny/n = y$ and $m_n/n \geqslant (ny - 1)/n = y - 1/n \to y$. Hence, $m_n/n \to y$ as $n \to \infty$. In part (c), $\prb(X_n \leqslant y) = \prb(X_n \leqslant m_n/n) = m_n(m_n + 1)/[n(n + 1)] = (m_n/n)((m_n/n) + (1/n))/(1 + 1/n) \to y^2$ as $n \to \infty$. Therefore, $X_n \xrightarrow{D} Z$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.4.10}
Suppose $\prb(Y_n \leqslant y) = 1 - e^{-2ny/(n+1)}$ for all $y \geqslant 0$. Prove that $Y_n \xrightarrow{D} Y$ where $Y \sim \text{Exponential}(\lambda)$ for some $\lambda > 0$, and compute $\lambda$.
\end{exercise}

\begin{solution}
Note that the cdf of $\text{Exponential}(\lambda)$ is $F(x) = 1 - e^{-\lambda x}$ for $x > 0$ otherwise $F(x) = 0$. For $y > 0$, the cdf of $Y_n$ converges to $F_{Y_n}(y) = \prb(Y_n \leqslant y) = 1 - e^{-ny/(n+1)} \to 1 - e^{-y}$ as $n \to \infty$. Hence, $Y_n \xrightarrow{D} \text{Exponential}(1)$. Therefore, $\lambda = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.4.11}
Suppose $\prb(Z_n \leqslant z) = 1 - \bigl(1 - \frac{3z}{n}\bigr)^n$ for all $0 \leqslant z \leqslant n/3$. Prove that $Z_n \xrightarrow{D} Z$ where $Z \sim \text{Exponential}(\lambda)$ for some $\lambda > 0$, and compute $\lambda$. (Hint: Recall from calculus that $\lim_{n \to \infty} (1 + c/n)^n = e^c$ for any real number $c$.)
\end{exercise}

\begin{solution}
Note that the cdf of $\text{Exponential}(\lambda)$ is $F(x) = 1 - e^{-\lambda x}$ for $x > 0$ otherwise $F(x) = 0$. For $z > 0$, the cdf of $Z_n$ converges to $F_{Z_n}(z) = \prb(Z_n \leqslant z) = 1 - (1 - 3z/n)^n \to 1 - e^{-3z}$ as $n \to \infty$. Hence, $Z_n \xrightarrow{D} \text{Exponential}(3)$. Therefore, $\lambda = 3$.
\end{solution}

\begin{exercise}
\label{exer:4.4.12}
Suppose the service time, in minutes, at a bank has the Exponential distribution with $\lambda = 1/2$. Use the central limit theorem to estimate the probability that the average service time of the first $n$ customers is less than $2.5$ minutes, when:
\begin{enumerate}[(a)]
\item $n = 16$.
\item $n = 36$.
\item $n = 100$.
\end{enumerate}
\end{exercise}

\begin{solution}
The Exponential distribution has mean $= 1/\lambda = 2$ and variance $= 1/\lambda^2 = 4$. So, if $M$ is the sample mean of $n$ customers, then $M \approx N(2, 4/n)$, so $Z \equiv (M - 2)/\sqrt{4/n} = \sqrt{n}(M - 2)/2 \approx N(0, 1)$, so $\prb(M < 2.5) = \prb(\sqrt{n}(M - 2)/2 < \sqrt{n}(2.5 - 2)/2) = \prb(Z < \sqrt{n}/4)$. Using Table D.2, we see that if $n = 16$, this equals $\prb(Z < 4/4) = \prb(Z < 1) = 1 - \prb(Z < -1) = 1 - 0.1587 = 0.8413$. If $n = 36$, this equals $\prb(Z < 6/4) = \prb(Z < 1.5) = 0.9332$. If $n = 100$, this equals $\prb(Z < 10/4) = \prb(Z < 2.5) = 0.9938$. (It becomes more and more certain, as $n$ increases.)
\end{solution}

\begin{exercise}
\label{exer:4.4.13}
Suppose the number of kilograms of a metal alloy produced by a factory each week is uniformly distributed between 20 and 30. Use the central limit theorem to estimate the probability that next year's output will be less than 1280 kilograms. (Assume that a year contains precisely 52 weeks.)
\end{exercise}

\begin{solution}
The weekly output has mean $(20 + 30)/2 = 25$, and variance $(30 - 20)^2/12 = 8.33$. So, the yearly output, $Y$, is approximately normally distributed with mean $52 \times 25 = 1300$, and variance $52 \times 8.33 = 433$, and standard deviation $\sqrt{433} = 20.8$. So, $\prb(Y < 1280) = \prb((Y - 1300)/20.8 < (1280 - 1300)/20.8) = \prb((Y - 1300)/20.8 < -0.96) = 0.1685$ (using Table D.2), i.e.\ the probability is about 17\%.
\end{solution}

\begin{exercise}
\label{exer:4.4.14}
Suppose the time, in days, until a component fails has the Gamma distribution with $\alpha = 5$ and $\lambda = 1/10$. When a component fails, it is immediately replaced by a new component. Use the central limit theorem to estimate the probability that 40 components will together be sufficient to last at least 6 years. (Assume that a year contains precisely $365.25$ days.)
\end{exercise}

\begin{solution}
The Gamma distribution has mean $\alpha/\theta = 50$, and variance $\alpha/\theta^2 = 500$. So, the duration of 40 components, $X$, is approximately normally distributed with mean $40 \times 50 = 2000$, and variance $40 \times 500 = 20000$, and standard deviation $\sqrt{20000} = 141$. So, the probability that 40 components will not last for 6 years is $\prb(X < 6 \times 365.25) = \prb(X < 2191.5) = \prb((X - 2000)/141 < (2191.5 - 2000)/141) = \prb((X - 2000)/141 < 1.36) = 0.9131$ (using Table D.2). So, the probability that they will last for 6 years is $1 - 0.9131 = 0.0869$, or about 8.7\%.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:4.4.15}
Generate $N$ samples $X_1, X_2, \ldots, X_{20} \sim \text{Exponential}(3)$ for $N$ large ($N = 10^4$, if possible). Use these samples to estimate the probability $\prb(1/6 \leqslant M_{20} \leqslant 1/2)$. How does your answer compare to what the central limit theorem gives as an approximation?
\end{exercise}

\begin{solution}
Using R we obtain the following.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_samples <- 10000
sample_size <- 20

# Generate samples from Exponential(1/3)
samples <- matrix(rexp(n_samples * sample_size, rate = 3), 
                  nrow = n_samples, ncol = sample_size)
means <- rowMeans(samples)

# Proportion between 1/6 and 1/2
k1 <- mean(means >= 1/6 & means <= 1/2)
cat("Proportion:", k1, "\n")
\end{minted}
\caption{CLT approximation for exponential (sol04\_ex4415.R)}
\label{lst:sol04_ex4415}
\end{listing}

\noindent Sample output:
\begin{verbatim}
Proportion: 0.9746
\end{verbatim}

And so we record 97.4\% of the averages between 1/6 and 1/2. The central limit theorem gives the approximation
\begin{align*}
    \prb(1/6 \leqslant M_{20} \leqslant 1/2) &= \prb\left(\frac{\sqrt{20}(1/6 - 1/3)}{1/3} \leqslant \frac{\sqrt{20}(M_{20} - 1/3)}{1/3} \leqslant \frac{\sqrt{20}(1/2 - 1/3)}{1/3}\right) \\
    &= \prb\left(-2.2361 \leqslant \frac{\sqrt{20}(M_{20} - 1/3)}{1/3} \leqslant 2.2361\right) \approx \prb(-2.2361 \leqslant Z \leqslant 2.2361) \\
    &= \Phi(2.2361) - \Phi(-2.2361) = 0.9873 - 0.0127 = 0.9746
\end{align*}
and this is close to the observed proportion.
\end{solution}

\begin{exercise}
\label{exer:4.4.16}
Generate $N$ samples $X_1, X_2, \ldots, X_{30} \sim \text{Uniform}[-20, 10]$ for $N$ large ($N = 10^4$, if possible). Use these samples to estimate the probability $\prb(M_{30} \geqslant -5)$. How does your answer compare to what the central limit theorem gives as an approximation?
\end{exercise}

\begin{solution}
Using R we obtain the following.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_samples <- 10000
sample_size <- 30

# Generate samples from Uniform(-20, 10)
samples <- matrix(runif(n_samples * sample_size, min = -20, max = 10), 
                  nrow = n_samples, ncol = sample_size)
means <- rowMeans(samples)

# Proportion less than or equal to -5
k1 <- mean(means <= -5)
cat("Proportion:", k1, "\n")
\end{minted}
\caption{CLT approximation for uniform (sol04\_ex4416.R)}
\label{lst:sol04_ex4416}
\end{listing}

\noindent Sample output:
\begin{verbatim}
Proportion: 0.5005
\end{verbatim}

And so we record 50.0\% of the averages less than $-5$. The central limit theorem gives the approximation
\[
    \prb(M_{30} \leqslant -5) = \prb\left(\frac{\sqrt{30}(M_{30} + 5)}{30\sqrt{1/12}} \leqslant \frac{\sqrt{30}(-5 + 5)}{30\sqrt{1/12}}\right) = \prb\left(\frac{\sqrt{30}(M_{30} + 5)}{30\sqrt{1/12}} \leqslant 0\right) \approx \prb(Z \leqslant 0) = \Phi(0) = 0.5
\]
and this is close to the observed proportion.
\end{solution}

\begin{exercise}
\label{exer:4.4.17}
Generate $N$ samples $X_1, X_2, \ldots, X_{20} \sim \text{Geometric}(1/4)$ for $N$ large ($N = 10^4$, if possible). Use these samples to estimate the probability $\prb(2.5 \leqslant M_{20} \leqslant 3.3)$. How does your answer compare to what the central limit theorem gives as an approximation?
\end{exercise}

\begin{solution}
Using R we obtain the following.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_samples <- 10000
sample_size <- 20

# Generate Geometric(1/4) samples using inverse transform
# P(X = k) = (1-p)^(k-1) * p for k = 1, 2, ...
# Using ceiling(log(U)/log(1-p)) for Geometric starting at 1
u <- matrix(runif(n_samples * sample_size), 
            nrow = n_samples, ncol = sample_size)
samples <- ceiling(log(u) / log(0.75))

means <- rowMeans(samples)

# Proportion between 2.5 and 3.3
k1 <- mean(means >= 2.5 & means <= 3.3)
cat("Proportion:", k1, "\n")
\end{minted}
\caption{CLT approximation for geometric (sol04\_ex4417.R)}
\label{lst:sol04_ex4417}
\end{listing}

\noindent Sample output:
\begin{verbatim}
Proportion: 0.1837
\end{verbatim}

And so we record 18.4\% of the averages between 2.5 and 3.3. The central limit theorem gives the approximation (mean of $\text{Geometric}(1/4)$ is $(1 - 0.25)/0.25 = 3$ and variance is $(1 - 0.25)/(0.25)^2 = 12.0$)
\begin{align*}
    \prb(2.5 \leqslant M_{20} \leqslant 3.3) &= \prb\left(\frac{\sqrt{20}(2.5 - 3)}{\sqrt{12}} \leqslant \frac{\sqrt{20}(M_{20} - 3)}{\sqrt{12}} \leqslant \frac{\sqrt{20}(3.3 - 3)}{\sqrt{12}}\right) \\
    &= \prb\left(-0.6455 \leqslant \frac{\sqrt{20}(M_{20} - 3)}{\sqrt{12}} \leqslant 0.3873\right) \approx \prb(-0.6455 \leqslant Z \leqslant 0.3873) \\
    &= \Phi(0.3873) - \Phi(-0.6455) = 0.6507 - 0.2593 = 0.3914
\end{align*}
and this is not close to the observed proportion because the $\text{Geometric}(1/4)$ is a skewed distribution.
\end{solution}

\begin{exercise}
\label{exer:4.4.18}
Generate $N$ samples $X_1, X_2, \ldots, X_{20}$ from the distribution of $\log Z$ where $Z \sim \text{Gamma}(4, 1)$ for $N$ large ($N = 10^4$, if possible). Use these samples to construct a density histogram of the values of $M_{20}$. Comment on the shape of this graph.
\end{exercise}

\begin{solution}
Using R we obtain the following. Note that the histogram looks a lot like a normal density.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_samples <- 10000
sample_size <- 20

# Generate samples from Gamma(4, 1) and take log
samples <- matrix(rgamma(n_samples * sample_size, shape = 4, rate = 1), 
                  nrow = n_samples, ncol = sample_size)
log_samples <- log(samples)
means <- rowMeans(log_samples)

# Create histogram
hist(means, breaks = 30, probability = TRUE, 
     main = "Histogram of Sample Means of log(Gamma(4,1))",
     xlab = "Sample Mean", col = "lightblue")
\end{minted}
\caption{Histogram of log-gamma sample means (sol04\_ex4418.R)}
\label{lst:sol04_ex4418}
\end{listing}

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig4_4_18.pdf}
    \caption{Histogram of sample means of $\log(\text{Gamma}(4, 1))$ for Exercise 4.4.18}
    %\label{fig:histogram-4418}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:4.4.19}
Generate $N$ samples $X_1, X_2, \ldots, X_{20}$ from the Binomial$(10, 0.01)$ distribution for $N$ large ($N = 10^4$, if possible). Use these samples to construct a density histogram of the values of $M_{20}$. Comment on the shape of this graph.
\end{exercise}

\begin{solution}
Using R we obtain the following. Note that the histogram does not look a lot like a normal density. So a larger sample size is required for the CLT approximation to apply.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_samples <- 10000
sample_size <- 20

# Generate samples from Binomial(10, 0.01)
samples <- matrix(rbinom(n_samples * sample_size, size = 10, prob = 0.01), 
                  nrow = n_samples, ncol = sample_size)
means <- rowMeans(samples)

# Create histogram
hist(means, breaks = 30, probability = TRUE, 
     main = "Histogram of Sample Means of Binomial(10, 0.01)",
     xlab = "Sample Mean", col = "lightblue")
\end{minted}
\caption{Histogram of binomial sample means (sol04\_ex4419.R)}
\label{lst:sol04_ex4419}
\end{listing}

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig4_4_19.pdf}
    \caption{Histogram of sample means of $\text{Binomial}(10, 0.01)$ for Exercise 4.4.19}
    %\label{fig:histogram-4419}
\end{figure}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:4.4.20}
Let $a_1, a_2, \ldots$ be any sequence of nonnegative real numbers with $\sum_i a_i = 1$. Suppose $\prb(X = i) = a_i$ for every positive integer $i$. Construct a sequence $\{X_n\}$ of absolutely continuous random variables, such that $X_n \to X$ in distribution.
\end{exercise}

\begin{solution}
For example, let $Z_{j,n} \sim \text{Normal}(j, 1/n)$, and let $\prb(Y = i) = a_i$ for positive integers $i$, with $Y$ independent of the $\{Z_j\}$. Then let $X_n = Z_{Y,n}$, i.e., $X_n = Z_{j,n}$ whenever $Y = j$. Then $X_n$ is absolutely continuous since each $Z_{j,n}$ is. Also, if $\prb(X = x) = 0$, then $\prb(X_n \leqslant x) = \sum_i a_i \prb(Z_{i,n} \leqslant x) \to \sum_{i < x} a_i$ as $n \to \infty$, so $X_n \to X$ in distribution.
\end{solution}

\begin{exercise}
\label{exer:4.4.21}
Let $f : [0,1] \to (0, \infty)$ be a continuous positive function such that $\int_0^1 f(x)\,\mathrm{d}x = 1$. Consider random variables $X$ and $X_n$ such that $\prb(a \leqslant X \leqslant b) = \int_a^b f(x)\,\mathrm{d}x$ for $a < b$ and
\[
P\left(X_n = \frac{i}{n}\right) = \frac{f(i/n)}{\sum_{j=1}^{n} f(j/n)}
\]
for $i = 1, 2, 3, \ldots, n$. Prove that $X_n \to X$ in distribution.
\end{exercise}

\begin{solution}
Here $\prb(X_n \leqslant x) = \{\sum_{i < nx} f(i/n)\}/\{\sum_{i=1}^{n} f(i/n)\}$. We recognize this as a Riemann sum (from Calculus) for the integral $\int_0^x f(x) \, \mathrm{d}x$. Hence, since $f$ is continuous, $\prb(X_n \leqslant x) \to \int_0^x f(x) \, \mathrm{d}x = \prb(X \leqslant x)$, so $X_n \to X$ in distribution.
\end{solution}

\begin{exercise}
\label{exer:4.4.22}
Suppose that $Y_i = X_i^3$ and that $X_1, \ldots, X_n$ is a sample from an $\mathrm{N}(0,1)$ distribution. Indicate how you would approximate the probability $\prb(M_n \leqslant m)$ where $M_n = (Y_1 + \cdots + Y_n)/n$.
\end{exercise}

\begin{solution}
We have that $\expc(X^3) = 0$, so (putting $y = x^2/2$, $x = \sqrt{2y}$, $\mathrm{d}x = \mathrm{d}y/\sqrt{2y}$)
\begin{align*}
    \var(X^3) &= \expc(X^6) = \int_{-\infty}^{\infty} x^6 \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \, \mathrm{d}x = 2 \int_0^{\infty} x^6 \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \, \mathrm{d}x \\
    &= \frac{2}{\sqrt{2\pi}} \int_0^{\infty} (\sqrt{2y})^5 e^{-y} \, \mathrm{d}y = \frac{2^3}{\sqrt{\pi}} \int_0^{\infty} y^{5/2} e^{-y} \, \mathrm{d}y = \frac{2^3}{\sqrt{\pi}} \Gamma(7/2) \\
    &= \frac{2^3}{\sqrt{\pi}} \cdot \frac{5}{2} \cdot \frac{3}{2} \cdot \frac{1}{2} \cdot \Gamma(1/2) = 15.
\end{align*}
Therefore,
\[
    \prb(M_n \leqslant m) = \prb\left(\frac{\sqrt{n}(M_n - 0)}{\sqrt{15}} \leqslant \frac{\sqrt{n}(m - 0)}{\sqrt{15}}\right) \approx \prb\left(Z \leqslant \frac{\sqrt{n}(m - 0)}{\sqrt{15}}\right)
\]
where $Z \sim N(0, 1)$.
\end{solution}

\begin{exercise}
\label{exer:4.4.23}
Suppose $Y_i = \cos(2\pi U_i)$ and $U_1, \ldots, U_n$ is a sample from the Uniform$[0,1]$ distribution. Indicate how you would approximate the probability $\prb(M_n \leqslant m)$, where $M_n = (Y_1 + \cdots + Y_n)/n$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \expc(Y) &= \int_0^1 \cos(2\pi u) \, \mathrm{d}u = \frac{\sin(2\pi u)}{2\pi} \bigg|_0^1 = 0 \\
    \var(Y) &= \expc(Y^2) = \int_0^1 \cos^2(2\pi u) \, \mathrm{d}u = \int_0^1 \frac{1 + \cos(4\pi u)}{2} \, \mathrm{d}u = \frac{1}{4} + \frac{1}{2} \frac{\sin(4\pi u)}{4\pi} \bigg|_0^1 = \frac{1}{4}.
\end{align*}
Therefore,
\[
    \prb(M_n \leqslant m) = \prb\left(\frac{\sqrt{n}(M_n - 0)}{\sqrt{1/4}} \leqslant \frac{\sqrt{n}(m - 0)}{\sqrt{1/4}}\right) \approx \prb\left(Z \leqslant \frac{\sqrt{n}(m - 0)}{\sqrt{1/4}}\right)
\]
where $Z \sim N(0, 1)$.
\end{solution}

\subsection*{Computer Problems}

\begin{exercise}
\label{exer:4.4.24}
Suppose that $Y = X^3$ and $X \sim \mathrm{N}(0,1)$. By generating a large sample ($n = 10^4$, if possible) from the distribution of $Y$, approximate the probability $\prb(Y \leqslant 1)$ and assess the error in your approximation. Compute this probability exactly and compare it with your approximation.
\end{exercise}

\begin{solution}
Using R we obtain the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 10000

# Generate standard normal samples
x <- rnorm(n, mean = 0, sd = 1)
y <- x^3
indicator <- as.numeric(y <= 1)

k1 <- mean(indicator)
k2 <- 3 * sqrt(k1 * (1 - k1) / n)
k3 <- k1 - k2
k4 <- k1 + k2

cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Monte Carlo estimation of $\prb(Y \leqslant 1)$ (sol04\_ex4424.R)}
\label{lst:sol04_ex4424}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.837500
K3: 0.826433
K4: 0.848567
\end{verbatim}

So our estimate of $\prb(Y \leqslant 1)$ is 0.837500, and the true value of this quantity lies in the interval $(0.826433, 0.848567)$ with virtual certainty. So we know the value of $\prb(Y \leqslant 1)$ with considerable accuracy. This probability can be evaluated exactly as $\prb(Y \leqslant 1) = \prb(X^3 \leqslant 1) = \prb(X \leqslant 1) = \Phi(1) = 0.8413$.
\end{solution}

\begin{exercise}
\label{exer:4.4.25}
Suppose that $Y = X^3$ and $X \sim \mathrm{N}(0,1)$. By generating a large sample ($n = 10^4$, if possible) from the distribution of $Y$, approximate the expectation $\expc(\cos(X^3))$, and assess the error in your approximation.
\end{exercise}

\begin{solution}
Using R we obtain the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 10000

# Generate standard normal samples
x <- rnorm(n, mean = 0, sd = 1)
y <- cos(x^3)

k1 <- mean(y)
k2 <- sd(y) / sqrt(n)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2

cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Monte Carlo estimation of $\expc(Y)$ (sol04\_ex4425.R)}
\label{lst:sol04_ex4425}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.588037
K3: 0.569203
K4: 0.606872
\end{verbatim}

So our estimate of $\expc(Y)$ is 0.588037 and the true value of this quantity lies in the interval $(0.569203, 0.606872)$ with virtual certainty.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:4.4.26}
Suppose $X_n \to C$ in distribution, where $C$ is a constant. Prove that $X_n \to C$ in probability. (This proves that if $X$ is constant, then the converse to Theorem~\ref{thm:4.4.1} does hold, even though it does not hold for general $X$.)
\end{exercise}

\begin{solution}
If $X_n \to C$ in distribution, then $\prb(X_n \leqslant x) \to 0$ for $x < C$, and $\prb(X_n \leqslant x) \to 1$ for $x > C$. Then for all $c > 0$, $\prb(|X_n - C| \geqslant c) = \prb(X_n \geqslant C + c) + \prb(X_n \leqslant C - c) = 1 - \prb(X_n < C + c) + \prb(X_n \leqslant C - c) \to 1 - 1 + 0 = 0$ as $n \to \infty$. Hence, $X_n \to C$ in probability.
\end{solution}


\section{Monte Carlo Approximations}
\label{sec:4.5}

The laws of large numbers say that if $X_1, X_2, \ldots$ is an i.i.d.\ sequence of random variables with mean $\mu$, and
\[
M_n = \frac{X_1 + \cdots + X_n}{n},
\]
then for large $n$ we will have $M_n \approx \mu$.

Suppose now that $\mu$ is unknown. Then, as discussed in Section~\ref{ssec:4.4.2}, it is possible to change perspective and use $M_n$ (for large $n$) as an estimator or approximation of $\mu$. Any time we approximate or estimate a quantity, we must also say something about how much error is in the estimate. Of course, we cannot say what this error is exactly, as that would require knowing the exact value of $\mu$. In Section~\ref{ssec:4.4.2}, however, we showed how the central limit theorem leads to a very natural approach to assessing this error, using three times the standard error of the estimate. We consider some examples.

\begin{example}
\label{ex:4.5.1}
Consider flipping a sequence of identical coins, each of which has probability $\theta$ of coming up heads, but where $\theta$ is unknown. Let $M_n$ again be the fraction of the first $n$ coins that are heads. Then we know that for large $n$, it is very likely that $M_n$ is very close to $\theta$. Hence, we can use $M_n$ to estimate $\theta$. Furthermore, the discussion in Section~\ref{ssec:4.4.2} indicates that \eqref{eq:4.4.3} is the relevant interval to quote when assessing the accuracy of the estimate $M_n$.
\end{example}

\begin{example}
\label{ex:4.5.2}
Suppose we believe a certain medicine lowers blood pressure, but we do not know by how much. We would like to know the mean amount $\mu$, by which this medicine lowers blood pressure.

Suppose we observe $n$ patients (chosen at random so they are i.i.d.), where patient $i$ has blood pressure $B_i$ before taking the medicine and blood pressure $A_i$ afterwards. Let $X_i = B_i - A_i$. Then
\[
M_n = \frac{1}{n} \sum_{i=1}^{n} (B_i - A_i)
\]
is the average amount of blood pressure decrease. (Note that $B_i - A_i$ may be negative for some patients, and it is important to also include those negative terms in the sum.) Then for large $n$, the value of $M_n$ is a good estimate of $\expc(X_i) = \mu$. Furthermore, the discussion in Section~\ref{ssec:4.4.2} indicates that \eqref{eq:4.4.4} is the relevant interval to quote when assessing the accuracy of the estimate $M_n$.
\end{example}

Such estimators can also be used to estimate purely mathematical quantities that do not involve any experimental data (such as coins or medical patients) but that are too difficult to compute directly. In this case, such estimators are called \emph{Monte Carlo approximations} (named after the gambling casino in the principality of Monaco because they introduce randomness to solve nonrandom problems).

\begin{example}
\label{ex:4.5.3}
Suppose we wish to evaluate
\[
I = \int_0^1 \cos(x^2) \sin(x^4) \,\mathrm{d}x.
\]
This integral cannot easily be solved exactly. But it can be approximately computed using a Monte Carlo approximation, as follows. We note that
\[
I = \expc\bigl(\cos(U^2) \sin(U^4)\bigr),
\]
where $U \sim \text{Uniform}[0,1]$. Hence, for large $n$, the integral $I$ is approximately equal to $M_n = (T_1 + \cdots + T_n)/n$, where $T_i = \cos(U_i^2) \sin(U_i^4)$, and where $U_1, U_2, \ldots$ are i.i.d.\ Uniform$[0,1]$.

Putting this all together, we obtain an algorithm for approximating the integral $I$, as follows.
\begin{enumerate}[1.]
\item Select a large positive integer $n$.
\item Obtain $U_i \sim \text{Uniform}[0,1]$, independently for $i = 1, 2, \ldots, n$.
\item Set $T_i = \cos(U_i^2) \sin(U_i^4)$, for $i = 1, 2, \ldots, n$.
\item Estimate $I$ by $M_n = (T_1 + \cdots + T_n)/n$.
\end{enumerate}

For large enough $n$, this algorithm will provide a good estimate of the integral $I$. For example, the following table records the estimates $M_n$ and the intervals \eqref{eq:4.4.4} based on samples of Uniform$[0,1]$ variables for various choices of $n$:

\begin{center}
\begin{tabular}{lll}
$n$ & $M_n$ & $(M_n - 3S/\sqrt{n},\, M_n + 3S/\sqrt{n})$ \\
\hline
$10^3$ & 0.145294 & (0.130071, 0.160518) \\
$10^4$ & 0.138850 & (0.134105, 0.143595) \\
$10^5$ & 0.139484 & (0.137974, 0.140993)
\end{tabular}
\end{center}

From this we can see that the value of $I$ is approximately $0.139484$ and the true value is almost certainly in the interval $(0.137974, 0.140993)$. Notice how the lengths of the intervals decrease as we increase $n$. In fact, it can be shown that the exact value is $I = 0.139567$, so our approximation is excellent.
\end{example}

\begin{example}
\label{ex:4.5.4}
Suppose we want to evaluate the integral
\[
I = \int_0^{\infty} 25x^2 \cos(x^2) e^{-25x} \,\mathrm{d}x.
\]
This integral cannot easily be solved exactly, but it can also be approximately computed using a Monte Carlo approximation, as follows.

We note first that $I = \expc(X^2 \cos(X^2))$, where $X \sim \text{Exponential}(25)$. Hence, for large $n$, the integral $I$ is approximately equal to $M_n = (T_1 + \cdots + T_n)/n$, where $T_i = X_i^2 \cos(X_i^2)$, with $X_1, X_2, \ldots$ i.i.d.\ Exponential$(25)$.

Now, we know from Section~\ref{sec:2.10} that we can simulate $X \sim \text{Exponential}(25)$ by setting $X = -\ln(U)/25$, where $U \sim \text{Uniform}[0,1]$. Hence, putting this all together, we obtain an algorithm for approximating the integral $I$, as follows.
\begin{enumerate}[1.]
\item Select a large positive integer $n$.
\item Obtain $U_i \sim \text{Uniform}[0,1]$, independently for $i = 1, 2, \ldots, n$.
\item Set $X_i = -\ln(U_i)/25$, for $i = 1, 2, \ldots, n$.
\item Set $T_i = X_i^2 \cos(X_i^2)$, for $i = 1, 2, \ldots, n$.
\item Estimate $I$ by $M_n = (T_1 + \cdots + T_n)/n$.
\end{enumerate}

For large enough $n$, this algorithm will provide a good estimate of the integral $I$. For example, the following table records the estimates $M_n$ and the intervals \eqref{eq:4.4.4} based on samples of Exponential$(25)$ variables for various choices of $n$:

\begin{center}
\begin{tabular}{lll}
$n$ & $M_n$ & $(M_n - 3S/\sqrt{n},\, M_n + 3S/\sqrt{n})$ \\
\hline
$10^3$ & $3.33846 \times 10^{-3}$ & $(2.63370 \times 10^{-3},\, 4.04321 \times 10^{-3})$ \\
$10^4$ & $3.29933 \times 10^{-3}$ & $(3.06646 \times 10^{-3},\, 3.53220 \times 10^{-3})$ \\
$10^5$ & $3.20629 \times 10^{-3}$ & $(3.13759 \times 10^{-3},\, 3.27499 \times 10^{-3})$
\end{tabular}
\end{center}

From this we can see that the value of $I$ is approximately $3.20629 \times 10^{-3}$ and that the true value is almost certainly in the interval $(3.13759 \times 10^{-3},\, 3.27499 \times 10^{-3})$.
\end{example}

\begin{example}
\label{ex:4.5.5}
Suppose we want to evaluate the sum
\[
S = \sum_{j=0}^{\infty} j^{2.375} \left(\frac{j}{5}\right)^j.
\]
Though this is very difficult to compute directly, it can be approximately computed using a Monte Carlo approximation.

Let us rewrite the sum as
\[
S = \frac{5}{4} \sum_{j=0}^{\infty} j^{2.37} \cdot \frac{4}{5} \cdot \left(\frac{1}{4/5}\right)^j = \frac{5}{4} \expc(X^{2.37}),
\]
where $X \sim \text{Geometric}(4/5)$.

Now, we know from Section~\ref{sec:2.10} that we can simulate $X \sim \text{Geometric}(4/5)$ by setting $X = \lfloor \ln(1 - U) / \ln(1 - 4/5) \rfloor$ or, equivalently, $X = \lfloor \ln(U) / \ln(1 - 4/5) \rfloor$, where $U \sim \text{Uniform}[0,1]$ and where $\lfloor \cdot \rfloor$ means to round down to the next integer value. Hence, we obtain an algorithm for approximating the sum $S$, as follows.
\begin{enumerate}[1.]
\item Select a large positive integer $n$.
\item Obtain $U_i \sim \text{Uniform}[0,1]$, independently for $i = 1, 2, \ldots, n$.
\item Set $X_i = \lfloor \ln(U_i) / \ln(1 - 4/5) \rfloor$, for $i = 1, 2, \ldots, n$.
\item Set $T_i = X_i^{2.37}$, for $i = 1, 2, \ldots, n$.
\item Estimate $S$ by $M_n = (5/4)(T_1 + \cdots + T_n)/n$.
\end{enumerate}

For large enough $n$, this algorithm will provide a good estimate of the sum $S$. For example, the following table records the estimates $M_n$ and the intervals \eqref{eq:4.4.4} based on samples of Geometric$(4/5)$ variables for various choices of $n$:

\begin{center}
\begin{tabular}{lll}
$n$ & $M_n$ & $(M_n - 3S/\sqrt{n},\, M_n + 3S/\sqrt{n})$ \\
\hline
$10^3$ & $4.66773 \times 10^{-4}$ & $(4.47078 \times 10^{-4},\, 4.86468 \times 10^{-4})$ \\
$10^4$ & $4.73538 \times 10^{-4}$ & $(4.67490 \times 10^{-4},\, 4.79586 \times 10^{-4})$ \\
$10^5$ & $4.69377 \times 10^{-4}$ & $(4.67436 \times 10^{-4},\, 4.71318 \times 10^{-4})$
\end{tabular}
\end{center}

From this we can see that the value of $S$ is approximately $4.69377 \times 10^{-4}$ and that the true value is almost certainly in the interval $(4.67436 \times 10^{-4},\, 4.71318 \times 10^{-4})$.
\end{example}

Note that when using a Monte Carlo approximation, it is not necessary that the range of an integral or sum be the entire range of the corresponding random variable, as follows.

\begin{example}
\label{ex:4.5.6}
Suppose we want to evaluate the integral
\[
J = \int_0^{\infty} \sin(x) \, e^{-x^2/2} \,\mathrm{d}x.
\]
Again, this is extremely difficult to evaluate exactly.

Here
\[
J = \sqrt{2\pi} \, \expc\bigl(\sin(X) \cdot \indc_{(X \geqslant 0)}\bigr),
\]
where $X \sim \mathrm{N}(0,1)$ and $\indc_{(X \geqslant 0)}$ is the indicator function of the event $\{X \geqslant 0\}$. We know from Section~\ref{sec:2.10} that we can simulate $X \sim \mathrm{N}(0,1)$ by setting
\[
X = \sqrt{-2\log(1 - U)} \cos(2\pi V),
\]
where $U$ and $V$ are i.i.d.\ Uniform$[0,1]$. Hence, we obtain the following algorithm for approximating the integral $J$.
\begin{enumerate}[1.]
\item Select a large positive integer $n$.
\item Obtain $U_i, V_i \sim \text{Uniform}[0,1]$, independently for $i = 1, 2, \ldots, n$.
\item Set $X_i = \sqrt{-2\log(1 - U_i)} \cos(2\pi V_i)$, for $i = 1, 2, \ldots, n$.
\item Set $T_i = \sin(X_i) \cdot \indc_{(X_i \geqslant 0)}$, for $i = 1, 2, \ldots, n$. (That is, set $T_i = \sin(X_i)$ if $X_i \geqslant 0$, otherwise set $T_i = 0$.)
\item Estimate $J$ by $M_n = \sqrt{2\pi}(T_1 + \cdots + T_n)/n$.
\end{enumerate}

For large enough $n$, this algorithm will again provide a good estimate of the integral $I$. For example, the following table records the estimates $M_n$ and the intervals \eqref{eq:4.4.4} based on samples of $\mathrm{N}(0,1)$ variables for various choices of $n$:

\begin{center}
\begin{tabular}{lll}
$n$ & $M_n$ & $(M_n - 3S/\sqrt{n},\, M_n + 3S/\sqrt{n})$ \\
\hline
$10^3$ & 0.744037 & (0.657294, 0.830779) \\
$10^4$ & 0.733945 & (0.706658, 0.761233) \\
$10^5$ & 0.722753 & (0.714108, 0.731398)
\end{tabular}
\end{center}

From this we can see that the value of $J$ is approximately $0.722753$ and that the true value is almost certainly in the interval $(0.714108, 0.731398)$.
\end{example}

Now we consider an important problem for statistical applications of probability theory.

\begin{example}[Approximating Sampling Distributions Using Monte Carlo]
\label{ex:4.5.7}
Suppose $X_1, X_2, \ldots, X_n$ is an i.i.d.\ sequence from the probability measure $P$. We want to find the distribution of a new random variable $Y = h(X_1, X_2, \ldots, X_n)$ for some function $h$. Provided we can generate from $P$, then Monte Carlo methods give us a way to approximate this distribution.

Denoting the cumulative distribution function of $Y$ by $F_Y$, we have
\[
F_Y(y) = \prb(Y \leqslant y) = \expc_P[\indc_{(-\infty, y]}(Y)] = \expc[\indc_{(-\infty, y]}(h(X_1, X_2, \ldots, X_n))].
\]
So $F_Y(y)$ can be expressed as the expectation of the random variable
\[
\indc_{(-\infty, y]}(h(X_1, X_2, \ldots, X_n))
\]
based on sampling from $P$.

To estimate this, we generate $N$ samples of size $n$
\[
(X_{i,1}, X_{i,2}, \ldots, X_{i,n})
\]
for $i = 1, \ldots, N$ from $P$ (note $N$ is the Monte Carlo sample size and can be varied, whereas the sample size $n$ is fixed here) and then calculate the proportion of values $h(X_{i,1}, X_{i,2}, \ldots, X_{i,n}) \leqslant y$. The estimate $M_N$ is then given by
\[
\hat{F}_Y(y) = \frac{1}{N} \sum_{i=1}^{N} \indc_{(-\infty, y]}(h(X_{i,1}, X_{i,2}, \ldots, X_{i,n})).
\]
By the laws of large numbers, this converges to $F_Y(y)$ as $N \to \infty$. To evaluate the error in this approximation, we use \eqref{eq:4.4.3}, which now takes the form
\[
\bigl(\hat{F}_Y(y) - 3\sqrt{\hat{F}_Y(y)(1 - \hat{F}_Y(y))/n},\, \hat{F}_Y(y) + 3\sqrt{\hat{F}_Y(y)(1 - \hat{F}_Y(y))/n}\bigr).
\]

We presented an application of this in Example~\ref{ex:4.4.7}. Note that if the base of a rectangle in the histogram of Figure~\ref{fig:4.4.2} is given by $(a, b]$, then the height of this rectangle equals the proportion of values that fell in $(a, b]$ times $1/(b - a)$. This can be expressed as $(\hat{F}_Y(b) - \hat{F}_Y(a))/(b - a)$, which converges to $(F_Y(b) - F_Y(a))/(b - a)$ as $N \to \infty$. This proves that the areas of the rectangles in the histogram converge to $F_Y(b) - F_Y(a)$ as $N \to \infty$.

More generally, we can approximate an expectation $\expc(g(Y))$ using the average
\[
\frac{1}{N} \sum_{i=1}^{N} g(h(X_{i,1}, X_{i,2}, \ldots, X_{i,n})).
\]
By the laws of large numbers, this average converges to $\expc(g(Y))$ as $N \to \infty$.
\end{example}

Typically, there is more than one possible Monte Carlo algorithm for estimating a quantity of interest. For example, suppose we want to approximate the integral $\int_a^b g(x) \,\mathrm{d}x$, where we assume this integral is finite. Let $f$ be a density on the interval $(a, b)$, such that $f(x) > 0$ for every $x \in (a, b)$, and suppose we have a convenient algorithm for generating $X_1, X_2, \ldots$ i.i.d.\ with distribution given by $f$. We have that
\[
\int_a^b g(x) \,\mathrm{d}x = \int_a^b \frac{g(x)}{f(x)} f(x) \,\mathrm{d}x = \expc\left[\frac{g(X)}{f(X)}\right],
\]
when $X$ is distributed with density $f$. So we can estimate $\int_a^b g(x) \,\mathrm{d}x$ by
\[
M_n(f) = \frac{1}{n} \sum_{i=1}^{n} \frac{g(X_i)}{f(X_i)} = \frac{1}{n} \sum_{i=1}^{n} T_i,
\]
where $T_i = g(X_i)/f(X_i)$. In effect, this is what we did in Example~\ref{ex:4.5.3} ($f$ is the Uniform$[0,1]$ density), in Example~\ref{ex:4.5.4} ($f$ is the Exponential$(25)$ density), and in Example~\ref{ex:4.5.6} ($f$ is the $\mathrm{N}(0,1)$ density). But note that there are many other possible choices. In Example~\ref{ex:4.5.3}, we could have taken $f$ to be any beta density. In Example~\ref{ex:4.5.4}, we could have taken $f$ to be any gamma density, and similarly in Example~\ref{ex:4.5.6}. Most statistical computer packages have commands for generating from these distributions. In a given problem, what is the best one to use?

In such a case, we would naturally use the algorithm that was most efficient. For the algorithms we have been discussing here, this means that if, based on a sample of $n$, algorithm 1 leads to an estimate with standard error $\sigma_1/\sqrt{n}$, and algorithm 2 leads to an estimate with standard error $\sigma_2/\sqrt{n}$, then algorithm 1 is more efficient than algorithm 2 whenever $\sigma_1 < \sigma_2$. Naturally, we would prefer algorithm 1 because the intervals \eqref{eq:4.4.3} or \eqref{eq:4.4.4} will tend to be shorter for algorithm 1 for the same sample size. Actually, a more refined comparison of efficiency would also take into account the total amount of computer time used by each algorithm, but we will ignore this aspect of the problem here. See Problem~\ref{exer:4.5.21} for more discussion of efficiency and the choice of algorithm in the context of the integration problem.

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:4.5}}
\begin{itemize}
\item An unknown quantity can be approximately computed using a Monte Carlo approximation, whereby independent replications of a random experiment (usually on a computer) are averaged to estimate the quantity.
\item Monte Carlo approximations can be used to approximate complicated sums, integrals, and sampling distributions, all by choosing the random experiment appropriately.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:4.5.1}
Describe a Monte Carlo approximation of $\int_{-\infty}^{\infty} \cos^2(x) \, e^{-x^2/2} \,\mathrm{d}x$.
\end{exercise}

\begin{solution}
This integral equals $\sqrt{2\pi}\expc(\cos^2(Z))$, where $Z \sim N(0, 1)$. Hence, let $\{U_i\}$ be i.i.d.\ $\sim \text{Uniform}[0, 1]$ for $1 \leqslant i \leqslant 2n$. Let $Z_i = \sqrt{2\ln(1/U_{2i-1})} \cos(2\pi U_{2i})$, so that $Z_i \sim N(0, 1)$. Then let $I = (\sqrt{2\pi}/n) \sum_{i=1}^{n} \cos^2(Z_i)$. For large $n$, $I$ is a good approximation to the integral.
\end{solution}

\begin{exercise}
\label{exer:4.5.2}
Describe a Monte Carlo approximation of $\sum_{j=0}^{m} j^6 \binom{m}{j} (2/3)^j (1/3)^{m-j}$. (Hint: Remember the Binomial$(m, 2/3)$ distribution.)
\end{exercise}

\begin{solution}
Note that this sum equals $\expc(Z^6)$, where $Z \sim \text{Bernoulli}(2/3)$. Hence, let $\{U_{ij}\}$ be i.i.d.\ $\text{Uniform}[0, 1]$ for $1 \leqslant i \leqslant n$ and $1 \leqslant j \leqslant m$. Let $B_{ij} = 1$ if $U_{ij} < 2/3$, otherwise $B_{ij} = 0$, so that $B_{ij} \sim \text{Bernoulli}(2/3)$. Let $Z_i = B_{i1} + B_{i2} + \cdots + B_{im}$, so that $Z_i \sim \text{Binomial}(m, 2/3)$. Then let $S = (1/n) \sum_{i=1}^{n} (Z_i)^6$. For large $n$, $S$ is a good approximation to the sum.
\end{solution}

\begin{exercise}
\label{exer:4.5.3}
Describe a Monte Carlo approximation of $\int_0^{\infty} e^{-5x} 14x^2 \,\mathrm{d}x$. (Hint: Remember the Exponential$(5)$ distribution.)
\end{exercise}

\begin{solution}
This integral equals $(1/5)\expc(e^{-14Z^2})$, where $Z \sim \text{Exponential}(5)$. Hence, let $\{U_i\}$ be i.i.d.\ $\sim \text{Uniform}[0, 1]$ for $1 \leqslant i \leqslant n$. Let $Z_i = \ln(1/U_i)/5$, so that $Z_i \sim \text{Exponential}(5)$. Then let $I = (1/5n) \sum_{i=1}^{n} e^{-14Z_i^2}$. For large $n$, $I$ is a good approximation to the integral.
\end{solution}

\begin{exercise}
\label{exer:4.5.4}
Suppose $X_1, X_2, \ldots$ are i.i.d.\ with distribution Poisson$(\lambda)$, where $\lambda$ is unknown. Consider $M_n = (X_1 + X_2 + \cdots + X_n)/n$ as an estimate of $\lambda$. Suppose we know that $\lambda \leqslant 10$. How large must $n$ be to guarantee that $M_n$ will be within $0.1$ of the true value of $\lambda$ with virtual certainty, i.e., when $\sigma$ is 3 standard deviations smaller than $0.1$?
\end{exercise}

\begin{solution}
$M_n$ has mean $\lambda$ and variance $\lambda/n$. So the interval $M_n \pm 3\sqrt{\lambda/n}$ will contain the true value of $\lambda$ with virtual certainty. But this implies that $M_n \pm 3\sqrt{10/n}$ will contain the true value of $\lambda$ with virtual certainty. Therefore, the error criterion will be satisfied whenever $3\sqrt{10/n} \leqslant 0.1$ or $n \geqslant 9(10)/(0.1)^2 = 9000.0$.
\end{solution}

\begin{exercise}
\label{exer:4.5.5}
Describe a Monte Carlo approximation of $\sum_{j=0}^{\infty} \sin(j^2) \cdot 5^j / j!$. Assume you have available an algorithm for generating from the Poisson$(5)$ distribution.
\end{exercise}

\begin{solution}
This sum is approximately equal to $e^5 \expc(\sin(Z^2))$, where $Z \sim \text{Poisson}(5)$. Hence, let $Z_1, Z_2, \ldots, Z_n$ be i.i.d.\ with distribution $\text{Poisson}(5)$ (perhaps generated using computer software). Then let $S = (e^5/n) \sum_{i=1}^{n} \sin(Z_i^2)$. For large $n$, $S$ is a good approximation to the sum.
\end{solution}

\begin{exercise}
\label{exer:4.5.6}
Describe a Monte Carlo approximation of $\int_0^{10} e^{-x^4} \,\mathrm{d}x$. (Hint: Remember the Uniform$[0,10]$ distribution.)
\end{exercise}

\begin{solution}
This integral is equal to $10\expc(e^{-Z^4})$, where $Z \sim \text{Uniform}[0, 10]$. Hence, let $\{U_i\}$ be i.i.d.\ $\sim \text{Uniform}[0, 1]$ for $1 \leqslant i \leqslant n$. Let $Z_i = 10U_i$, so that $Z_i \sim \text{Uniform}[0, 10]$. Then let $I = (10/n) \sum_{i=1}^{n} e^{-Z_i^4}$. For large $n$, $I$ is a good approximation to the integral.
\end{solution}

\begin{exercise}
\label{exer:4.5.7}
Suppose we repeat a certain experiment 2000 times and obtain a sample average of 5 and a standard error of 17. In terms of this, specify an interval that is virtually certain to contain the experiment's (unknown) true mean $\mu$.
\end{exercise}

\begin{solution}
We treat $(M_n - 3S_n/\sqrt{n}, M_n + 3S_n/\sqrt{n})$ as a virtually certain interval. Hence, $(-5 - 3 \cdot 17/\sqrt{2000}, -5 + 3 \cdot 17/\sqrt{2000}) = (-6.1404, -3.8596)$ is a virtually certain interval to contain the true mean $\mu$.
\end{solution}

\begin{exercise}
\label{exer:4.5.8}
Suppose we repeat a certain experiment 400 times and get i.i.d.\ response values $X_1, X_2, \ldots, X_{400}$. Suppose we compute that the sample average is $M_{400} = 6$ and furthermore that $\sum_{i=1}^{400} X_i^2 = 15{,}400$. In terms of this:
\begin{enumerate}[(a)]
\item Compute the standard error $\sigma_n$.
\item Specify an interval that is virtually certain to contain the (unknown) true mean of the $X_i$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The standard error is $\sigma_n = \left(\frac{1}{n-1}\left(\sum_{i=1}^{n}(X_i - \bar{X})^2\right)\right)^{1/2} = ((15400 - 62 \times 400)/399)^{1/2} = 1.5831$.
    \item Since $M_n = 6$ and $S_n = 1.5831$, a virtually certain interval to contain the true mean $\mu$ is given by $(M_n - 3S_n/\sqrt{n}, M_n + 3S_n/\sqrt{n}) = (5.7625, 6.2375)$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.5.9}
Suppose a certain experiment has probability $\theta$ of success, where $0 < \theta < 1$ but $\theta$ is unknown. Suppose we repeat the experiment 1000 times, of which 400 are successes and 600 are failures. Compute an interval of values that are virtually certain to contain $\theta$.
\end{exercise}

\begin{solution}
The computation is similar to Example \ref{ex:4.5.7}. Since $M_n = 400/1000 = 0.4$, the interval
\[
    (M_n - 3\sqrt{M_n(1 - M_n)/n}, M_n + 3\sqrt{M_n(1 - M_n)/n}) = (0.3535, 0.4465)
\]
is a virtually certain interval to contain $\theta$.
\end{solution}

\begin{exercise}
\label{exer:4.5.10}
Suppose a certain experiment has probability $\theta$ of success, where $0 < \theta < 1$ but $\theta$ is unknown. Suppose we repeat the experiment $n$ times, and let $Y$ be the fraction of successes.
\begin{enumerate}[(a)]
\item In terms of $\theta$, what is $\var(Y)$?
\item For what value of $\theta$ is $\var(Y)$ the largest?
\item What is this largest possible value of $\var(Y)$?
\item Compute the smallest integer $n$ such that we can be sure that $\var(Y) \leqslant 0.01$, regardless of the value of $\theta$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Since each experiment follows $\text{Bernoulli}(\theta)$ distribution, the total number of successes among the $n$ experiments has a $\text{Binomial}(n, \theta)$ distribution. Thus, $T = nY \sim \text{Binomial}(n, \theta)$. By noting that $\var(T) = n\theta(1 - \theta)$, we have $\var(Y) = \var(T/n) = \var(T)/n^2 = \theta(1 - \theta)/n$.
    \item Since $\var(Y) = n^{-1}(\theta - \theta^2) = n^{-1}(1/4 - (\theta - 1/2)^2)$, the variance of $Y$ has the maximum $1/(4n)$ at $\theta = 1/2$.
    \item In part (b), $1/(4n)$ is the largest possible value of $\var(Y)$ when $\theta = 1/2$.
    \item We find the smallest $n$ such that $\max_{0 < \theta < 1} \var(Y) < 0.01$. Since $\max_{0 < \theta < 1} \var(Y) = 1/(4n)$, the inequality becomes $1/(4n) < 0.01$. It solves $n > 1/(0.04) = 25$. Hence, $n = 26$ is the smallest integer satisfying $\var(Y) < 0.01$ for all $0 < \theta < 1$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.5.11}
Suppose $X$ and $Y$ are random variables with joint density given by $f_{X,Y}(x, y) = C \cdot g(x, y)$ for $0 < x < y < 1$ (with $f_{X,Y}(x, y) = 0$ for other $(x, y)$), for appropriate constant $C$, where
\[
g(x, y) = x^2 y^3 \sin(xy) \cos(x + y) \exp(-x^2 y).
\]
\begin{enumerate}[(a)]
\item Explain why
\[
\expc(X) = \frac{\int_0^1 \int_0^1 x \cdot f_{X,Y}(x, y) \,\mathrm{d}x \,\mathrm{d}y}{\int_0^1 \int_0^1 f_{X,Y}(x, y) \,\mathrm{d}x \,\mathrm{d}y} = \frac{\int_0^1 \int_0^1 x \cdot g(x, y) \,\mathrm{d}x \,\mathrm{d}y}{\int_0^1 \int_0^1 g(x, y) \,\mathrm{d}x \,\mathrm{d}y}.
\]
\item Describe a Monte Carlo algorithm to approximately compute $\expc(X)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The constant $C$ must satisfy $\int_{\mathbb{R}} \int_{\mathbb{R}} f(x, y) \, \mathrm{d}x \, \mathrm{d}y = 1$ to make $f$ a density. This equation gives
    \[
        1 = \int_0^1 \int_0^1 Cg(x, y) \, \mathrm{d}x \, \mathrm{d}y = C \int_0^1 \int_0^1 g(x, y) \, \mathrm{d}x \, \mathrm{d}y.
    \]
    Thus, $C = \left[\int_0^1 \int_0^1 g(x, y) \, \mathrm{d}x \, \mathrm{d}y\right]^{-1}$. Hence, the expectation of $X$ is
    \[
        \expc(X) = \int_0^1 \int_0^1 x f_{X,Y}(x, y) \, \mathrm{d}x \, \mathrm{d}y = C \int_0^1 \int_0^1 xg(x, y) \, \mathrm{d}x \, \mathrm{d}y = \frac{\int_0^1 \int_0^1 xg(x, y) \, \mathrm{d}x \, \mathrm{d}y}{\int_0^1 \int_0^1 g(x, y) \, \mathrm{d}x \, \mathrm{d}y}.
    \]
    \item We approximate denominator and numerator at the same time. We generate $X_i$'s from a density proportional to $x^2$ and $Y_i$'s from a density proportional to $y^3$. Since $\int_0^x u^{p-1} \, \mathrm{d}u = x^p/p$ for $0 < x < 1$ and $p > 1$, the densities are $f_X(x) = 3x^2$ for $0 < x < 1$, otherwise $f_X(x) = 0$, and $f_Y(y) = 4y^3$ for $0 < y < 1$ and otherwise $f_Y(y) = 0$. Using the inverse cdf functions $F_X^{-1}(u) = u^{1/3}$ and $F_Y^{-1}(v) = v^{1/4}$, random variables $X_i$'s and $Y_i$'s are generated. A Monte Carlo algorithm to approximate $\expc(X)$ is described below.
    \begin{enumerate}[(1)]
        \item Select a large positive integer $n$.
        \item Obtain $U_i, V_i \sim \text{Uniform}[0, 1]$, independently for $i = 1, \ldots, n$.
        \item Set $X_i = (U_i)^{1/3}$ and $Y_i = (V_i)^{1/4}$ for $i = 1, \ldots, n$.
        \item Set $D_i = \sin(X_i Y_i) \cos(\sqrt{X_i Y_i}) \exp(X_i^2 + Y_i)/12$ and $N_i = X_i \cdot D_i$ for $i = 1, \ldots, n$.
        \item Estimate $\expc(X)$ by $M_n = \bar{N}/\bar{D} = (N_1 + \cdots + N_n)/(D_1 + \cdots + D_n)$.
    \end{enumerate}
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.5.12}
Let $g(x, y) = \cos(xy)$, and consider the integral $I = \int_0^5 \int_0^4 g(x, y) \,\mathrm{d}y \,\mathrm{d}x$.
\begin{enumerate}[(a)]
\item Prove that $I = 20 \, \expc[g(X, Y)]$, where $X \sim \text{Uniform}[0,5]$ and $Y \sim \text{Uniform}[0,4]$.
\item Use part (a) to describe a Monte Carlo algorithm to approximately compute $I$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The density function of $X$ and $Y$ are $\indc_{[0,5]}(x)/5$ and $\indc_{[0,4]}(y)/4$. Hence,
    \[
        I = \int_0^5 \int_0^4 g(x, y) \, \mathrm{d}y \, \mathrm{d}x = \int_0^5 \int_0^4 20g(x, y) \frac{\indc_{[0,4]}(y)}{4} \, \mathrm{d}y \frac{\indc_{[0,5]}(x)}{5} \, \mathrm{d}x = \expc[20g(X, Y)].
    \]
    \item A Monte Carlo algorithm to approximate $I = 20\expc[g(X, Y)]$ is described below.
    \begin{enumerate}[(1)]
        \item Select a large positive integer $n$.
        \item Obtain $U_i, V_i \sim \text{Uniform}[0, 1]$, independently for $i = 1, \ldots, n$.
        \item Set $X_i = 5U_i$ and $Y_i = 4V_i$ for $i = 1, \ldots, n$.
        \item Set $T_i = g(X_i, Y_i)$ for $i = 1, \ldots, n$.
        \item Estimate $I$ by $M_n = 20\bar{T} = 20(T_1 + \cdots + T_n)/n$.
    \end{enumerate}
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.5.13}
Consider the integral $J = \int_0^1 \int_0^{\infty} h(x, y) \,\mathrm{d}y \,\mathrm{d}x$, where
\[
h(x, y) = e^{-y^2} \cos(xy).
\]
\begin{enumerate}[(a)]
\item Prove that $J = \expc[e^Y h(X, Y)]$, where $X \sim \text{Uniform}[0,1]$ and $Y \sim \text{Exponential}(1)$.
\item Use part (a) to describe a Monte Carlo algorithm to approximately compute $J$.
\item If $X \sim \text{Uniform}[0,1]$ and $Y \sim \text{Exponential}(5)$, then prove that
\[
J = (1/5) \, \expc[e^{5Y} h(X, Y)].
\]
\item Use part (c) to describe a Monte Carlo algorithm to approximately compute $J$.
\item Explain how you might use a computer to determine which is better, the algorithm in part (b) or the algorithm in part (d).
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The density of $X$ and $Y$ are $\indc_{[0,1]}(x)$ and $\indc_{[0,\infty)}(y)e^{-y}$. The integration $J$ becomes
    \[
        J = \int_0^1 \int_0^{\infty} h(x, y) \, \mathrm{d}y \, \mathrm{d}x = \int_0^1 \int_0^{\infty} e^y h(x, y) \indc_{[0,\infty)}(y) e^{-y} \, \mathrm{d}y \, \indc_{[0,1]}(x) \, \mathrm{d}x = \expc[e^Y h(X, Y)].
    \]
    \item A Monte Carlo algorithm to approximate $J$ is given below.
    \begin{enumerate}[(1)]
        \item Select a large positive integer $n$.
        \item Obtain $U_i, V_i \sim \text{Uniform}[0, 1]$, independently for $i = 1, \ldots, n$.
        \item Set $X_i = U_i$ and $Y_i = -\ln V_i$ for $i = 1, \ldots, n$.
        \item Set $T_i = e^{Y_i} h(X_i, Y_i)$ for $i = 1, \ldots, n$.
        \item Estimate $J$ by $M_n = \bar{T} = (T_1 + \cdots + T_n)/n$.
    \end{enumerate}
    \item The density of $\text{Exponential}(5)$ is $\indc_{[0,\infty)}(y) 5e^{-5y}$. The integration $J$ becomes
    \[
        J = \int_0^1 \int_0^{\infty} e^{5y} h(x, y) \indc_{[0,\infty)}(y) 5e^{-5y} \, \mathrm{d}y \, \indc_{[0,1]}(x) \, \mathrm{d}x = \expc[e^{5Y} h(X, Y)].
    \]
    \item A Monte Carlo algorithm to approximate $J$ is given below.
    \begin{enumerate}[(1)]
        \item Select a large positive integer $n$.
        \item Obtain $U_i, V_i \sim \text{Uniform}[0, 1]$, independently for $i = 1, \ldots, n$.
        \item Set $X_i = U_i$ and $Y_i = -5^{-1} \ln V_i$ for $i = 1, \ldots, n$.
        \item Set $W_i = e^{5Y_i} h(X_i, Y_i)$ for $i = 1, \ldots, n$.
        \item Estimate $J$ by $M_n = \bar{W} = (W_1 + \cdots + W_n)/n$.
    \end{enumerate}
    \item Both Monte Carlo algorithms in parts (b) and (d) converge to $J$. Between them, we would prefer the algorithm that converges faster than the other. Hence, the algorithm having smaller variance is better. Thus, compute the sample variances $\hat{\sigma}_T^2$ and $\hat{\sigma}_W^2$ of $T_1, \ldots, T_n$ and $W_1, \ldots, W_n$. Then, compare $\hat{\sigma}_T^2$ and $\hat{\sigma}_W^2$.
\end{enumerate}
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:4.5.14}
Use a Monte Carlo algorithm to approximate $\int_0^1 \cos(x^3) \sin(x^4) \,\mathrm{d}x$ based on a large sample (take $n = 10^5$, if possible). Assess the error in the approximation.
\end{exercise}

\begin{solution}
Using R we obtain the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 100000

u <- runif(n, 0, 1)
y <- cos(u^3) * sin(u^4)

k1 <- mean(y)
k2 <- sd(y) / sqrt(n)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2

cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Monte Carlo integration of $\cos(x^3)\sin(x^4)$ (sol04\_ex4514.R)}
\label{lst:sol04_ex4514}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.147770
K3: 0.146163
K4: 0.149378
\end{verbatim}

So the estimate is 0.147770, and the true value of the integral lies in $(0.146163, 0.149378)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:4.5.15}
Use a Monte Carlo algorithm to approximate $\int_0^{\infty} 25 \cos(x^4) e^{-25x} \,\mathrm{d}x$ based on a large sample (take $n = 10^5$, if possible). Assess the error in the approximation.
\end{exercise}

\begin{solution}
Using R we obtain the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 100000

# Generate Exponential(4) samples (rate = 4, mean = 0.25)
x <- rexp(n, rate = 4)
y <- cos(x^4)

k1 <- mean(y)
k2 <- sd(y) / sqrt(n)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2

cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Monte Carlo integration with exponential sampling (sol04\_ex4515.R)}
\label{lst:sol04_ex4515}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.973201
K3: 0.971596
K4: 0.974806
\end{verbatim}

So the estimate is 0.973201, and the true value of the integral lies in $(0.971596, 0.974806)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:4.5.16}
Use a Monte Carlo algorithm to approximate $\sum_{j=0}^{\infty} j^{2.355} / j!$ based on a large sample (take $n = 10^5$, if possible). Assess the error in the approximation.
\end{exercise}

\begin{solution}
Using R we obtain the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 100000

u <- runif(n, 0, 1)
# Generate Geometric(1/5) using inverse transform
x <- floor(log(u) / log(4/5))
y <- (5/4) * (x^2 + 3)^(-5)

k1 <- mean(y)
k2 <- sd(y) / sqrt(n)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2

cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Monte Carlo sum with geometric distribution (sol04\_ex4516.R)}
\label{lst:sol04_ex4516}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.00124565
K3: 0.00122658
K4: 0.00126473
\end{verbatim}

So the estimate is 0.00124565, and the true value of the integral lies in $(0.00122658, 0.00126473)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:4.5.17}
Suppose $X \sim \mathrm{N}(0,1)$. Use a Monte Carlo algorithm to approximate $\prb(X^2 - 3X + 2 > 0)$ based on a large sample (take $n = 10^5$, if possible). Assess the error in the approximation.
\end{exercise}

\begin{solution}
Using R we obtain the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 100000

x <- rnorm(n, mean = 0, sd = 1)
y <- x^2 - 3*x + 2
indicator <- as.numeric(y >= 0)

k1 <- mean(indicator)
k2 <- sqrt(k1 * (1 - k1)) / sqrt(n)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2

cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Monte Carlo probability estimation (sol04\_ex4517.R)}
\label{lst:sol04_ex4517}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.863930
K3: 0.860677
K4: 0.867183
\end{verbatim}

So the estimate is 0.863930, and the true value of the integral lies in $(0.860677, 0.867183)$ with virtual certainty.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:4.5.18}
Suppose that $X_1, X_2, \ldots$ are i.i.d.\ Bernoulli$(\theta)$ where $\theta$ is unknown. Determine a lower bound on $n$ so that the probability that the estimate $M_n$ will be within $\epsilon$ of the unknown value of $\theta$ is about $0.9974$. This allows us to run simulations with high confidence that the error in the approximation quoted is less than some prescribed value $\epsilon$. (Hint: Use the fact that $x(1 - x) \leqslant 1/4$ for all $x \in [0,1]$.)
\end{exercise}

\begin{solution}
This requires that we determine $n$ so that $3\sqrt{M_n(1 - M_n)/n} \leqslant \delta$. We have that $3\sqrt{M_n(1 - M_n)/n} \leqslant 3\sqrt{(1/2)(1 - 1/2)/n} \leqslant \delta$ if and only if $n \geqslant 9/(4\delta^2)$.
\end{solution}

\begin{exercise}
\label{exer:4.5.19}
Suppose that $X_1, X_2, \ldots$ are i.i.d.\ with unknown mean $\mu$ and unknown variance $\sigma^2$. Suppose we know, however, that $\sigma^2 \leqslant \sigma_0^2$, where $\sigma_0^2$ is a known value. Determine a lower bound on $n$ so that the probability that the estimate $M_n$ will be within $\epsilon$ of the unknown value of $\mu$ is about $0.9974$. This allows us to run simulations with high confidence that the error in the approximation quoted is less than some prescribed value $\epsilon$.
\end{exercise}

\begin{solution}
This requires that we determine $n$ so that $3\sigma_0/\sqrt{n} \leqslant \delta$ or $n \geqslant 9\sigma_0^2/\delta^2$.
\end{solution}

\begin{exercise}
\label{exer:4.5.20}
Suppose $X_1, X_2, \ldots$ are i.i.d.\ with distribution Uniform$[0, \theta]$, where $\theta$ is unknown, and consider $Z_n = \frac{n+1}{n} X_{(n)}$ as an estimate of $\theta$ (see Section~\ref{ssec:2.8.4} on order statistics).
\begin{enumerate}[(a)]
\item Prove that $\expc(Z_n) = \theta$ and compute $\var(Z_n)$.
\item Use Chebyshev's inequality to show that $Z_n$ converges in probability to $\theta$.
\item Show that $\expc(2M_n) = \theta$ and compare $M_n$ and $Z_n$ with respect to their efficiencies as estimators of $\theta$. Which would you use to estimate $\theta$ and why?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Here for $0 \leqslant z \leqslant \theta$, $\prb(Z_n \leqslant z) = (z/\theta)^n$. Hence, $X_{(n)}$ has density function $f(z) = nz^{n-1}\theta^{-n}$. Then
    \[
        \expc(X_{(n)}) = \int_0^{\theta} z \cdot nz^{n-1}\theta^{-n} \, \mathrm{d}z = n\theta^{-n} \int_0^{\theta} z^n \, \mathrm{d}z = n\theta^{-n} \frac{\theta^{n+1}}{n+1} = \frac{n\theta}{n+1}
    \]
    and so $\expc(Z_n) = \theta$. Then
    \begin{align*}
        \var(Z_n) &= \expc(Z_n^2) - \theta^2 = \left(\frac{n+1}{n}\right)^2 \int_0^{\theta} z^2 \cdot nz^{n-1}\theta^{-n} \, \mathrm{d}z - \theta^2 \\
        &= n\theta^{-n} \left(\frac{n+1}{n}\right)^2 \int_0^{\theta} z^{n+1} \, \mathrm{d}z - \theta^2 = \frac{n}{n+2} \left(\frac{n+1}{n}\right)^2 \theta^{-n} \theta^{n+2} - \theta^2 \\
        &= \left(\frac{(n+1)^2}{n(n+2)} - 1\right)\theta^2 = \frac{\theta^2}{n(n+2)}.
    \end{align*}
    \item By Chebyshev's inequality we have that $\prb(|Z_n - \theta| \geqslant c) \leqslant \theta^2/(c^2 n(n+2)) \to 0$ as $n \to \infty$.
    \item We have that $\expc(2M_n) = \theta$ and $\var(2M_n) = 4\theta^2/(12n) = \theta^2/(3n)$. Now $n(n+2) \geqslant 3n$ for every $n$, so $\var(2M_n) \geqslant \var(Z_n)$. This implies that the estimator $Z_n$ will be more accurate as the intervals given by the estimator plus/minus three standard deviations will be shorter.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.5.21}
(Importance sampling) Suppose we want to approximate the integral $\int_a^b g(x) \,\mathrm{d}x$, where we assume this integral is finite. Let $f$ be a density on the interval $(a, b)$ such that $f(x) > 0$ for every $x \in (a, b)$ and is such that we have a convenient algorithm for generating $X_1, X_2, \ldots$ i.i.d.\ with distribution given by $f$.
\begin{enumerate}[(a)]
\item Prove that
\[
M_n(f) = \frac{1}{n} \sum_{i=1}^{n} \frac{g(X_i)}{f(X_i)} \xrightarrow{\text{a.s.}} \int_a^b g(x) \,\mathrm{d}x.
\]
(We refer to $f$ as an \emph{importance sampler} and note this shows that every $f$ satisfying the above conditions, provides a consistent estimator $M_n(f)$ of $\int_a^b g(x) \,\mathrm{d}x$.)
\item Prove that
\[
\var(M_n(f)) = \frac{1}{n} \left[\int_a^b \frac{g^2(x)}{f(x)} \,\mathrm{d}x - \left(\int_a^b g(x) \,\mathrm{d}x\right)^2\right].
\]
\item Suppose that $g(x) = h(x) f(x)$, where $f$ is as described above. Show that importance sampling with respect to $f$ leads to the estimator
\[
M_n(f) = \frac{1}{n} \sum_{i=1}^{n} h(X_i).
\]
\item Show that if there exists $c$ such that $g(x) = c f(x)$ for all $x \in (a, b)$, then $\var(M_n(f)) = 0$.
\item Determine the standard error of $M_n(f)$ and indicate how you would use this to assess the error in the approximation $M_n(f)$ when $\var(M_n(f)) \neq 0$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item When $X \sim f$ we have that $\expc\left(\frac{g(X)}{f(X)}\right) = \int_a^b \frac{g(x)}{f(x)} f(x) \, \mathrm{d}x = \int_a^b g(x) \, \mathrm{d}x$, so $\expc(M_n(f)) = \int_a^b g(x) \, \mathrm{d}x$.
    \item When $X \sim f$ then $\expc\left(\left(\frac{g(X)}{f(X)}\right)^2\right) = \int_a^b \frac{g^2(x)}{f^2(x)} f(x) \, \mathrm{d}x = \int_a^b \frac{g^2(x)}{f(x)} \, \mathrm{d}x$, so
    \[
        \var(M_n(f)) = \frac{1}{n} \left\{\expc\left(\left(\frac{g(X)}{f(X)}\right)^2\right) - \left(\expc\left(\frac{g(X)}{f(X)}\right)\right)^2\right\} = \frac{1}{n} \left\{\int_a^b \frac{g^2(x)}{f(x)} \, \mathrm{d}x - \left(\int_a^b g(x) \, \mathrm{d}x\right)^2\right\}
    \]
    \item[(d)] Put $q(x) = |g(x)|/\int_a^b |g(x)| \, \mathrm{d}x$. We have that
    \begin{align*}
        \var(M_n(f)) &= \frac{1}{n} \left\{\int_a^b \frac{g^2(x)}{f(x)} \, \mathrm{d}x - \left(\int_a^b g(x) \, \mathrm{d}x\right)^2\right\} \\
        &= \frac{1}{n} \left(\int_a^b |g(x)| \, \mathrm{d}x\right)^2 \left\{\int_a^b \frac{q^2(x)}{f(x)} \, \mathrm{d}x - \frac{\left(\int_a^b g(x) \, \mathrm{d}x\right)^2}{\left(\int_a^b |g(x)| \, \mathrm{d}x\right)^2}\right\}
    \end{align*}
    and $\int_a^b \frac{q^2(x)}{f(x)} \, \mathrm{d}x = \int_a^b \frac{(q(x) - f(x))^2}{f(x)} \, \mathrm{d}x + 1$, and this is minimized by taking $f = q$ and the minimum variance is $\left(\int_a^b |g(x)| \, \mathrm{d}x\right)^2 - \left(\int_a^b g(x) \, \mathrm{d}x\right)^2$. This is 0 when $g$ is nonnegative.
    
    The optimum importance sampler is not feasible because it requires that we be able to compute $\int_a^b |g(x)| \, \mathrm{d}x$, which is typically at least as hard to evaluate as the original integral.
    \item[(e)] We have that $\int_a^b (g^2(x)/f(x)) \, \mathrm{d}x \leqslant \int_a^b (|g(x)| cf(x)/f(x)) \, \mathrm{d}x = c \int_a^b |g(x)| \, \mathrm{d}x < \infty$.
    \item[(f)] The standard error of $M_n(f)$ is given by
    \[
        S = \left\{\frac{1}{n-1} \left(\sum_{i=1}^{n} \frac{g^2(X_i)}{f^2(X_i)} - n\left(\frac{1}{n} \sum_{i=1}^{n} \frac{g(X_i)}{f(X_i)}\right)^2\right)\right\}^{1/2}
    \]
    divided by $\sqrt{n}$. The CLT then implies that the true value of the integral is in the interval $M_n(f) \pm 3S/\sqrt{n}$ with virtual certainty when $n$ is large.
\end{enumerate}
\end{solution}

\subsection*{Computer Problems}

\begin{exercise}
\label{exer:4.5.22}
Use a Monte Carlo algorithm to approximate $\prb(X^3 + Y^3 \leqslant 3)$, where $X \sim \mathrm{N}(1, 2)$ independently of $Y \sim \text{Gamma}(1, 1)$, based on a large sample (take $n = 10^5$, if possible). Assess the error in the approximation. How large does $n$ have to be to guarantee the estimate is within $0.01$ of the true value with virtual certainty? (Hint: Problem~\ref{exer:4.5.18}.)
\end{exercise}

\begin{solution}
Using R we obtain the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 100000

# Generate X ~ Normal(1, 4) and Y ~ Gamma(1, 1)
x <- rnorm(n, mean = 1, sd = 2)
y <- rgamma(n, shape = 1, rate = 1)

x3 <- x^3
y3 <- y^3
z <- x3 + y3

indicator <- as.numeric(z <= 3)
k1 <- mean(indicator)
k2 <- sqrt(k1 * (1 - k1)) / sqrt(n)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2

cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Monte Carlo probability for sum of cubes (sol04\_ex4522.R)}
\label{lst:sol04_ex4522}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.451620
K3: 0.446899
K4: 0.456341
\end{verbatim}

So the estimate is 0.451620, and the true value of the probability lies in $(0.446899, 0.456341)$ with virtual certainty.

By Problem \ref{exer:4.5.18} we must have $n \geqslant 9/(4(0.01)^2) = 22500.0$.
\end{solution}

\begin{exercise}
\label{exer:4.5.23}
Use a Monte Carlo algorithm to approximate $\expc(X^3 + Y^3)$, where $X \sim \mathrm{N}(1, 2)$ independently of $Y \sim \text{Gamma}(1, 1)$, based on a large sample (take $n = 10^5$, if possible). Assess the error in the approximation.
\end{exercise}

\begin{solution}
Using R we obtain the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 100000

# Generate X ~ Normal(1, 4) and Y ~ Gamma(1, 1)
x <- rnorm(n, mean = 1, sd = 2)
y <- rgamma(n, shape = 1, rate = 1)

x3 <- x^3
y3 <- y^3
z <- x3 + y3

k1 <- mean(z)
k2 <- sd(z) / sqrt(n)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2

cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Monte Carlo expectation for sum of cubes (sol04\_ex4523.R)}
\label{lst:sol04_ex4523}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 18.9665
K3: 18.5143
K4: 19.4186
\end{verbatim}

So the estimate is 18.9665, and the true value of the expectation lies in $(18.5143, 19.4186)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:4.5.24}
For the integral of Exercise~\ref{exer:4.5.3}, compare the efficiencies of the algorithm based on generating from an Exponential$(5)$ distribution with that based on generating from an $\mathrm{N}(0, 1/7)$ distribution.
\end{exercise}

\begin{solution}
Using R we obtain the following results for the algorithm based on generating from the $\text{Exponential}(5)$ distribution.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 100000

# Algorithm based on Exponential(5)
x <- rexp(n, rate = 5)
y <- (exp(-14 * x * x)) / 4

k1 <- mean(y)
k2 <- sd(y) / sqrt(n)

cat("Exponential(5) algorithm:\n")
cat("K1:", k1, "\n")
cat("K2:", k2, "\n")
\end{minted}
\caption{Monte Carlo with Exponential(5) sampling (sol04\_ex4524a.R)}
\label{lst:sol04_ex4524a}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.159487
K2: 0.000274517
\end{verbatim}

Using R we obtain the following results for the algorithm based on generating from the $N(0, 1/7)$ distribution.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n <- 100000

# Algorithm based on N(0, 1/7)
sigma <- sqrt(1/7)
k_const <- sigma * sqrt(2 * pi)

x <- rnorm(n, mean = 0, sd = sigma)
y <- k_const * exp(-5 * x)
y <- y * (x > 0)  # Indicator for x > 0

k1 <- mean(y)
k2 <- sd(y) / sqrt(n)

cat("N(0, 1/7) algorithm:\n")
cat("K1:", k1, "\n")
cat("K2:", k2, "\n")
\end{minted}
\caption{Monte Carlo with Normal sampling (sol04\_ex4524b.R)}
\label{lst:sol04_ex4524b}
\end{listing}

\noindent Sample output:
\begin{verbatim}
K1: 0.165965
K2: 0.000788127
\end{verbatim}

Notice that while the estimates 0.159487 and 0.165965 are similar, the standard error for the $\text{Exponential}(5)$ algorithm is 0.000274517 and the standard error for the $N(0, 1/7)$ algorithm is 0.000788127. So the $\text{Exponential}(5)$ algorithm is substantially more accurate.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:4.5.25}
(Buffon's needle) Suppose you drop a needle at random onto a large sheet of lined paper. Assume the distance between the lines is exactly equal to the length of the needle.
\begin{enumerate}[(a)]
\item Prove that the probability that the needle lands touching a line is equal to $2/\pi$. (Hint: Let $D$ be the distance from the higher end of the needle to the line just below it, and let $A$ be the angle the needle makes with that line. Then what are the distributions of $D$ and $A$? Under what conditions on $D$ and $A$ will the needle be touching a line?)
\item Explain how this experiment could be used to obtain a Monte Carlo approximation for the value of $\pi$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Let $D$ and $A$ be as in the hint, and let $L$ be the distance between the lines. Then $D \sim \text{Uniform}[0, L]$, and $A \sim \text{Uniform}[0, \pi]$. Also, the needle will touch the line just below it if and only if $L\sin(A) \geqslant D$. This happens with probability
    \begin{align*}
        (1/L) \int_0^L (1/\pi) \int_0^{\pi} \indc_{L\sin(A) \geqslant D} \, \mathrm{d}A \, \mathrm{d}D &= (1/\pi) \int_0^{\pi} (1/L) \int_0^L \indc_{D \leqslant L\sin(A)} \, \mathrm{d}D \, \mathrm{d}A \\
        &= (1/\pi) \int_0^{\pi} (1/L)(L\sin(A)) \, \mathrm{d}A = (1/\pi) \int_0^{\pi} \sin(A) \, \mathrm{d}A \\
        &= (1/\pi)[-\cos(\pi) + \cos(0)] = 2/\pi.
    \end{align*}
    \item Repeat the experiment a large number $N$ of times. Let $M$ be the number of times the needle is touching a line. Then by the strong law of large numbers, for large $N$, we should have $M/N \approx 2/\pi$, so that $\pi \approx 2N/M$. Hence, for large $N$, the quantity $2N/M$ is a good Monte Carlo estimate of $\pi$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.5.26}
(Optimal importance sampling) Consider importance sampling as described in Problem~\ref{exer:4.5.21}.
\begin{enumerate}[(a)]
\item Prove that $\var(M_n(f))$ is minimized by taking
\[
f(x) = \frac{|g(x)|}{\int_a^b |g(x)| \,\mathrm{d}x}.
\]
Calculate the minimum variance and show that the minimum variance is $0$ when $g(x) \geqslant 0$ for all $x \in (a, b)$.
\item Why is this optimal importance sampler typically not feasible? (The optimal importance sampler does indicate, however, that in our search for an efficient importance sampler, we look for an $f$ that is large when $|g|$ is large and small when $|g|$ is small.)
\end{enumerate}
\end{exercise}

\begin{solution}
Let $I = \int_a^b g(x) \, \mathrm{d}x$ and $J = \int_a^b |g(x)| \, \mathrm{d}x$.
\begin{enumerate}[(a)]
  \item In Problem \ref{exer:4.5.21}, we have shown that $\var(M_n(f)) = n^{-1}\left[\int_a^b g^2(x)/f(x) \, \mathrm{d}x - I^2\right]$. Hence, the minimizer of the variance $\var(M_n(f))$ also minimizes $\int_a^b g^2(x)/f(x) \, \mathrm{d}x$. Define a density $h$ by $h(x) = |g(x)|/J$. Then, $g^2(x) = J^2 h^2(x)$ and
    \[
        \int_a^b \frac{g^2(x)}{f(x)} \, \mathrm{d}x = \int_a^b \frac{J^2 \cdot h^2(x)}{f(x)} \, \mathrm{d}x = J^2 \int_a^b \left(\frac{h(x)}{f(x)} - 1\right)^2 f(x) \, \mathrm{d}x + J^2.
    \]
    Hence, the variance of $M_n(f)$ is minimized when $f(x) = h(x) = |g(x)|/\int_a^b |g(y)| \, \mathrm{d}y$.
    
    If $g(x) \geqslant 0$ on $(a, b)$ or $g(x) \leqslant 0$ on $(a, b)$, then $|I| = |\int_a^b g(x) \, \mathrm{d}x| = \int_a^b |g(x)| \, \mathrm{d}x = J$. Hence, the minimum variance of $M_n(f)$ becomes $\var(M_n(h)) = n^{-1}(J^2 - I^2) = 0$.
    \item Suppose $g(x) \geqslant 0$ on $(a, b)$. Since it contains the target value, the optimal importance sampler given by $f(x) = g(x)/I$ is unrealistic where $I = \int_a^b g(x) \, \mathrm{d}x$ is the target value.
\end{enumerate}
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:4.5.27}
An integral like $\int_0^{\infty} x^2 \cos(x^2) e^{-x} \,\mathrm{d}x$ can be approximately computed using a numerical integration computer package (e.g., using Simpson's rule). What are some advantages and disadvantages of using a Monte Carlo approximation instead of a numerical integration package?
\end{exercise}

\begin{exercise}
\label{exer:4.5.28}
Carry out the Buffon's needle Monte Carlo experiment, described in Challenge~\ref{exer:4.5.25}, by repeating the experiment at least 20 times. Present the estimate of $\pi$ so obtained. How close is it to the true value of $\pi$? What could be done to make the estimate more accurate?
\end{exercise}


\section{Normal Distribution Theory}
\label{sec:4.6}

Because of the central limit theorem (Theorem~\ref{thm:4.4.3}), the normal distribution plays an extremely important role in statistical theory. For this reason, we shall consider a number of important properties and distributions related to the normal distribution. These properties and distributions will be very important for the statistical theory in later chapters of this book.

We already know that if $X_1 \sim \mathrm{N}(\mu_1, \sigma_1^2)$ independent of $X_2 \sim \mathrm{N}(\mu_2, \sigma_2^2)$, then $cX_1 + d \sim \mathrm{N}(c\mu_1 + d, c^2\sigma_1^2)$ (see Exercise~\ref{exer:2.6.3}) and $X_1 + X_2 \sim \mathrm{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$ (see Problem~2.9.14). Combining these facts and using induction, we have the following result.

\begin{theorem}
\label{thm:4.6.1}
Suppose $X_i \sim \mathrm{N}(\mu_i, \sigma_i^2)$ for $i = 1, 2, \ldots, n$ and that they are independent random variables. Let $Y = \sum_i a_i X_i + b$ for some constants $a_i$ and $b$. Then
\[
Y \sim \mathrm{N}\left(\sum_i a_i \mu_i + b,\, \sum_i a_i^2 \sigma_i^2\right).
\]
\end{theorem}

This immediately implies the following.

\begin{corollary}
\label{cor:4.6.1}
Suppose $X_i \sim \mathrm{N}(\mu, \sigma^2)$ for $i = 1, 2, \ldots, n$ and that they are independent random variables. If $\bar{X} = (X_1 + \cdots + X_n)/n$, then $\bar{X} \sim \mathrm{N}(\mu, \sigma^2/n)$.
\end{corollary}

A more subtle property of normal distributions is the following.

\begin{theorem}
\label{thm:4.6.2}
Suppose $X_i \sim \mathrm{N}(\mu_i, \sigma_i^2)$ for $i = 1, 2, \ldots, n$ and also that the $X_i$ are independent. Let $U = \sum_{i=1}^{n} a_i X_i$ and $V = \sum_{i=1}^{n} b_i X_i$ for some constants $a_i$ and $b_i$. Then $\cor(U, V) = \sum_i a_i b_i \sigma_i^2$. Furthermore, $\cor(U, V) = 0$ if and only if $U$ and $V$ are independent.
\end{theorem}

\begin{proof}
The formula for $\cor(U, V)$ follows immediately from the linearity of covariance (Theorem~3.3.2) because we have
\begin{align*}
\cor(U, V) &= \cor\left(\sum_{i=1}^{n} a_i X_i,\, \sum_{j=1}^{n} b_j X_j\right) = \sum_{i=1}^{n} \sum_{j=1}^{n} a_i b_j \cor(X_i, X_j) \\
&= \sum_{i=1}^{n} a_i b_i \cor(X_i, X_i) = \sum_{i=1}^{n} a_i b_i \var(X_i) = \sum_{i=1}^{n} a_i b_i \sigma_i^2
\end{align*}
(note that $\cor(X_i, X_j) = 0$ for $i \neq j$, by independence). Also, if $U$ and $V$ are independent, then we must have $\cor(U, V) = 0$ by Corollary~3.3.2.

It remains to prove that, if $\cor(U, V) = 0$, then $U$ and $V$ are independent. This involves a two-dimensional change of variable, as discussed in the advanced Section~\ref{ssec:2.9.2}, so we refer the reader to Section~\ref{sec:4.7} for this part of the proof.
\end{proof}

Theorem~\ref{thm:4.6.2} says that, for the special case of linear combinations of independent normal distributions, if $\cor(U, V) = 0$ then $U$ and $V$ are independent. However, it is important to remember that this property is not true in general, and there are random variables $X$ and $Y$ such that $\cor(X, Y) = 0$ even though $X$ and $Y$ are not independent (see Example~\ref{ex:3.3.10}). Furthermore, this property is not even true of normal distributions in general (see Problem~\ref{exer:4.6.13}).

Note that using linear algebra, we can write the equations $U = \sum_{i=1}^{n} a_i X_i$ and $V = \sum_{i=1}^{n} b_i X_i$ of Theorem~\ref{thm:4.6.2} in matrix form as
\begin{equation}
\label{eq:4.6.1}
\begin{pmatrix} U \\ V \end{pmatrix} = A \begin{pmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{pmatrix},
\end{equation}
where
\[
A = \begin{pmatrix} a_1 & a_2 & \cdots & a_n \\ b_1 & b_2 & \cdots & b_n \end{pmatrix}.
\]
Furthermore, the rows of $A$ are orthogonal if and only if $\sum_i a_i b_i = 0$. Now, in the case $\sigma_i = 1$ for all $i$, we have that $\cor(U, V) = \sum_i a_i b_i$. Hence, if $\sigma_i = 1$ for all $i$, then Theorem~\ref{thm:4.6.2} can be interpreted as saying that if $U$ and $V$ are given by \eqref{eq:4.6.1}, then $U$ and $V$ are independent if and only if the rows of $A$ are orthogonal. Linear algebra is used extensively in more advanced treatments of these ideas.

\subsection{The Chi-Squared Distribution}
\label{ssec:4.6.1}

We now introduce another distribution, related to the normal distribution.

\begin{definition}
\label{def:4.6.1}
The \emph{chi-squared distribution with $n$ degrees of freedom} (or chi-squared$(n)$ or $\chi^2(n)$) is the distribution of the random variable
\[
Z = X_1^2 + X_2^2 + \cdots + X_n^2,
\]
where $X_1, \ldots, X_n$ are i.i.d., each with the standard normal distribution $\mathrm{N}(0, 1)$.
\end{definition}

Most statistical packages have built-in routines for the evaluation of chi-squared probabilities (also see Table~D.3 in Appendix~D).

One property of the chi-squared distribution is easy.

\begin{theorem}
\label{thm:4.6.3}
If $Z \sim \chi^2(n)$, then $\expc(Z) = n$.
\end{theorem}

\begin{proof}
Write $Z = X_1^2 + X_2^2 + \cdots + X_n^2$, where $X_i$ are i.i.d.\ $\mathrm{N}(0, 1)$. Then $\expc(X_i^2) = 1$. It follows by linearity that $\expc(Z) = 1 + \cdots + 1 = n$.
\end{proof}

The density function of the chi-squared distribution is a bit harder to obtain. We begin with the case $n = 1$.

\begin{theorem}
\label{thm:4.6.4}
Let $Z \sim \chi^2(1)$. Then
\[
f_Z(z) = \frac{1}{\sqrt{2\pi z}} e^{-z/2} = \frac{(1/2)^{1/2}}{\Gamma(1/2)} z^{1/2 - 1} e^{-z/2}
\]
for $z > 0$, with $f_Z(z) = 0$ for $z \leqslant 0$. That is, $Z \sim \text{Gamma}(1/2, 1/2)$ (using $\Gamma(1/2) = \sqrt{\pi}$).
\end{theorem}

\begin{proof}
Because $Z \sim \chi^2(1)$, we can write $Z = X^2$ where $X \sim \mathrm{N}(0, 1)$. We then compute that, for $z > 0$,
\[
\int_{-\infty}^{z} f_Z(s) \,\mathrm{d}s = \prb(Z \leqslant z) = \prb(X^2 \leqslant z) = \prb(-\sqrt{z} \leqslant X \leqslant \sqrt{z}).
\]
But because $X \sim \mathrm{N}(0, 1)$ with density function $\phi(s) = (2\pi)^{-1/2} e^{-s^2/2}$, we can rewrite this as
\[
\int_{-\infty}^{z} f_Z(s) \,\mathrm{d}s = \int_{-\sqrt{z}}^{\sqrt{z}} \phi(s) \,\mathrm{d}s = \int_0^{\sqrt{z}} \phi(s) \,\mathrm{d}s + \int_{-\sqrt{z}}^0 \phi(s) \,\mathrm{d}s.
\]
Because this is true for all $z > 0$, we can differentiate with respect to $z$ (using the fundamental theorem of calculus and the chain rule) to obtain
\[
f_Z(z) = \frac{1}{2\sqrt{z}} \phi(\sqrt{z}) + \frac{1}{2\sqrt{z}} \phi(-\sqrt{z}) = \frac{1}{\sqrt{z}} \phi(\sqrt{z}) = \frac{1}{\sqrt{2\pi z}} e^{-z/2},
\]
as claimed.
\end{proof}

In Figure~\ref{fig:4.6.1}, we have plotted the $\chi^2(1)$ density. Note that the density becomes infinite at $0$.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_6_1.pdf}
  \caption{Plot of the $\chi^2(1)$ density.}
  \label{fig:4.6.1}
\end{figure}

\begin{theorem}
\label{thm:4.6.5}
Let $Z \sim \chi^2(n)$. Then $Z \sim \text{Gamma}(n/2, 1/2)$. That is,
\[
f_Z(z) = \frac{1}{2^{n/2} \Gamma(n/2)} z^{n/2 - 1} e^{-z/2}
\]
for $z > 0$, with $f_Z(z) = 0$ for $z \leqslant 0$.
\end{theorem}

\begin{proof}
Because $Z \sim \chi^2(n)$, we can write $Z = X_1^2 + X_2^2 + \cdots + X_n^2$, where the $X_i$ are i.i.d.\ $\mathrm{N}(0, 1)$. But this means that $X_i^2$ are i.i.d.\ $\chi^2(1)$. Hence, by Theorem~\ref{thm:4.6.4}, we have $X_i^2$ i.i.d.\ Gamma$(1/2, 1/2)$ for $i = 1, 2, \ldots, n$. Therefore, $Z$ is the sum of $n$ independent random variables, each having distribution Gamma$(1/2, 1/2)$.

Now by Appendix~C (see Problem~3.4.20), the moment-generating function of a Gamma$(\alpha, \lambda)$ random variable is given by $m(s) = (\lambda/(\lambda - s))^\alpha$ for $s < \lambda$. Putting $\alpha = 1/2$ and $\lambda = 1/2$ and applying Theorem~3.4.5, the variable $Y = X_1^2 + X_2^2 + \cdots + X_n^2$ has moment-generating function given by
\[
m_Y(s) = \prod_{i=1}^{n} m_{X_i^2}(s) = \prod_{i=1}^{n} \left(\frac{1/2}{1/2 - s}\right)^{1/2} = \left(\frac{1/2}{1/2 - s}\right)^{n/2}
\]
for $s < 1/2$. We recognize this as the moment-generating function of the Gamma$(n/2, 1/2)$ distribution. Therefore, by Theorem~3.4.6, we have that $X_1^2 + X_2^2 + \cdots + X_n^2 \sim \text{Gamma}(n/2, 1/2)$, as claimed.
\end{proof}

This result can also be obtained using Problem~2.9.15 and induction.

Note that the $\chi^2(2)$ density is the same as the Exponential$(2)$ density. In Figure~\ref{fig:4.6.2}, we have plotted several $\chi^2$ densities. Observe that the $\chi^2$ are asymmetric and skewed to the right. As the degrees of freedom increase, the central mass of probability moves to the right.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_6_2.pdf}
  \caption{Plot of the $\chi^2(3)$ (solid line) and the $\chi^2(7)$ (dashed line) density functions.}
  \label{fig:4.6.2}
\end{figure}

One application of the chi-squared distribution is the following.

\begin{theorem}
\label{thm:4.6.6}
Let $X_1, \ldots, X_n$ be i.i.d.\ $\mathrm{N}(\mu, \sigma^2)$. Put
\[
\bar{X} = \frac{1}{n}(X_1 + \cdots + X_n) \quad \text{and} \quad S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2.
\]
Then $(n-1)S^2/\sigma^2 \sim \chi^2(n-1)$ and furthermore, $S^2$ and $\bar{X}$ are independent.
\end{theorem}

\begin{proof}
See Section~\ref{sec:4.7} for the proof of this result.
\end{proof}

Because the $\chi^2(n-1)$ distribution has mean $n - 1$, we obtain the following.

\begin{corollary}
\label{cor:4.6.2}
$\expc(S^2) = \sigma^2$.
\end{corollary}

\begin{proof}
Theorems~\ref{thm:4.6.6} and~\ref{thm:4.6.3} imply that $\expc((n-1)S^2/\sigma^2) = n - 1$ and that $\expc(S^2) = \sigma^2$.
\end{proof}

Theorem~\ref{thm:4.6.6} will find extensive use in Chapter~\ref{ch:6}. For example, this result, together with Corollary~\ref{cor:4.6.1}, gives us the joint sampling distribution of the sample mean $\bar{X}$ and the sample variance $S^2$ when we are sampling from an $\mathrm{N}(\mu, \sigma^2)$ distribution. If we do not know $\mu$, then $\bar{X}$ is a natural estimator of this quantity and, similarly, $S^2$ is a natural estimator of $\sigma^2$ when it is unknown. Interestingly, we divide by $n - 1$ in $S^2$ rather than $n$ precisely because we want $\expc(S^2) = \sigma^2$ to hold, as in Corollary~\ref{cor:4.6.2}. Actually, this property does not depend on sampling from a normal distribution. It can be shown that anytime $X_1, \ldots, X_n$ is a sample from a distribution with variance $\sigma^2$, then $\expc(S^2) = \sigma^2$.

\subsection{The \texorpdfstring{$t$}{t} Distribution}
\label{ssec:4.6.2}

The $t$ distribution also has many statistical applications.

\begin{definition}
\label{def:4.6.2}
The \emph{$t$ distribution with $n$ degrees of freedom} (or Student$(n)$ or $t(n)$), is the distribution of the random variable
\[
Z = \frac{X}{\sqrt{(X_1^2 + X_2^2 + \cdots + X_n^2)/n}},
\]
where $X, X_1, \ldots, X_n$ are i.i.d., each with the standard normal distribution $\mathrm{N}(0, 1)$. (Equivalently, $Z = X/\sqrt{Y/n}$, where $Y \sim \chi^2(n)$.)
\end{definition}

Most statistical packages have built-in routines for the evaluation of $t(n)$ probabilities (also see Table~D.4 in Appendix~D).

The density of the $t(n)$ distribution is given by the following result.

\begin{theorem}
\label{thm:4.6.7}
Let $U \sim t(n)$. Then
\[
f_U(u) = \frac{\Gamma\bigl(\frac{n+1}{2}\bigr)}{\Gamma\bigl(\frac{n}{2}\bigr)} \left(1 + \frac{u^2}{n}\right)^{-(n+1)/2} \frac{1}{\sqrt{n\pi}}
\]
for all $u \in \mathbb{R}^1$.
\end{theorem}

\begin{proof}
For the proof of this result, see Section~\ref{sec:4.7}.
\end{proof}

The following result shows that, when $n$ is large, the $t(n)$ distribution is very similar to the $\mathrm{N}(0, 1)$ distribution.

\begin{theorem}
\label{thm:4.6.8}
As $n \to \infty$, the $t(n)$ distribution converges in distribution to a standard normal distribution.
\end{theorem}

\begin{proof}
Let $Z_1, \ldots, Z_n, Z$ be i.i.d.\ $\mathrm{N}(0, 1)$. As $n \to \infty$, by the strong law of large numbers, $(Z_1^2 + \cdots + Z_n^2)/n$ converges with probability 1 to the constant $1$. Hence, the distribution of
\begin{equation}
\label{eq:4.6.2}
\frac{Z}{\sqrt{(Z_1^2 + \cdots + Z_n^2)/n}}
\end{equation}
converges to the distribution of $Z$, which is the standard normal distribution. By Definition~\ref{def:4.6.2}, we have that \eqref{eq:4.6.2} is distributed $t(n)$.
\end{proof}

In Figure~\ref{fig:4.6.3}, we have plotted several $t$ densities. Notice that the densities of the $t$ distributions are symmetric about $0$ and look like the standard normal density.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_6_3.pdf}
  \caption{Plot of the $t(1)$ (solid line) and the $t(30)$ (dashed line) density functions.}
  \label{fig:4.6.3}
\end{figure}

The $t(n)$ distribution has longer tails than the $\mathrm{N}(0, 1)$ distribution. For example, the $t(1)$ distribution (also known as the \emph{Cauchy distribution}) has $0.9366$ of its probability in the interval $(-10, 10)$, whereas the $\mathrm{N}(0, 1)$ distribution has all of its probability there (at least to four decimal places). The $t(30)$ and the $\mathrm{N}(0, 1)$ densities are very similar.

\subsection{The \texorpdfstring{$F$}{F} Distribution}
\label{ssec:4.6.3}

Finally, we consider the $F$ distribution.

\begin{definition}
\label{def:4.6.3}
The \emph{$F$ distribution with $m$ and $n$ degrees of freedom} (or $F(m, n)$) is the distribution of the random variable
\[
Z = \frac{(X_1^2 + X_2^2 + \cdots + X_m^2)/m}{(Y_1^2 + Y_2^2 + \cdots + Y_n^2)/n},
\]
where $X_1, \ldots, X_m, Y_1, \ldots, Y_n$ are i.i.d., each with the standard normal distribution. (Equivalently, $Z = (X/m)/(Y/n)$, where $X \sim \chi^2(m)$ and $Y \sim \chi^2(n)$.)
\end{definition}

Most statistical packages have built-in routines for the evaluation of $F(m, n)$ probabilities (also see Table~D.5 in Appendix~D).

The density of the $F(m, n)$ distribution is given by the following result.

\begin{theorem}
\label{thm:4.6.9}
Let $U \sim F(m, n)$. Then
\[
f_U(u) = \frac{\Gamma\bigl(\frac{m+n}{2}\bigr)}{\Gamma\bigl(\frac{m}{2}\bigr) \Gamma\bigl(\frac{n}{2}\bigr)} \left(\frac{m}{n}\right)^{m/2} u^{m/2 - 1} \left(1 + \frac{m}{n} u\right)^{-(m+n)/2} \frac{m}{n}
\]
for $u > 0$, with $f_U(u) = 0$ for $u \leqslant 0$.
\end{theorem}

\begin{proof}
For the proof of this result, see Section~\ref{sec:4.7}.
\end{proof}

In Figure~\ref{fig:4.6.4}, we have plotted several $F(m, n)$ densities. Notice that these densities are skewed to the right.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig4_6_4.pdf}
  \caption{Plot of the $F(2, 1)$ (solid line) and the $F(3, 10)$ (dashed line) density functions.}
  \label{fig:4.6.4}
\end{figure}

The following results are useful when it is necessary to carry out computations with the $F(m, n)$ distribution.

\begin{theorem}
\label{thm:4.6.10}
If $Z \sim F(m, n)$, then $1/Z \sim F(n, m)$.
\end{theorem}

\begin{proof}
Using Definition~\ref{def:4.6.3}, we have
\[
\frac{1}{Z} = \frac{(Y_1^2 + Y_2^2 + \cdots + Y_n^2)/n}{(X_1^2 + X_2^2 + \cdots + X_m^2)/m},
\]
and the result is immediate from the definition.
\end{proof}

Therefore, if $Z \sim F(m, n)$, then $\prb(Z \leqslant z) = \prb(1/Z \geqslant 1/z) = 1 - \prb(1/Z \leqslant 1/z)$, and $\prb(1/Z \leqslant 1/z)$ is the cdf of the $F(n, m)$ distribution evaluated at $1/z$.

In many statistical applications, $n$ can be very large. The following result then gives a useful approximation for that case.

\begin{theorem}
\label{thm:4.6.11}
If $Z_n \sim F(m, n)$, then $mZ_n$ converges in distribution to a $\chi^2(m)$ distribution as $n \to \infty$.
\end{theorem}

\begin{proof}
Using Definition~\ref{def:4.6.3}, we have
\[
mZ = \frac{X_1^2 + X_2^2 + \cdots + X_m^2}{(Y_1^2 + Y_2^2 + \cdots + Y_n^2)/n}.
\]
By Definition~\ref{def:4.6.1}, $X_1^2 + \cdots + X_m^2 \sim \chi^2(m)$. By Theorem~\ref{thm:4.6.3}, $\expc(Y_i^2) = 1$, so the strong law of large numbers implies that $(Y_1^2 + Y_2^2 + \cdots + Y_n^2)/n$ converges almost surely to $1$. This establishes the result.
\end{proof}

Finally, Definitions~\ref{def:4.6.2} and~\ref{def:4.6.3} immediately give the following result.

\begin{theorem}
\label{thm:4.6.12}
If $Z \sim t(n)$, then $Z^2 \sim F(1, n)$.
\end{theorem}

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:4.6}}
\begin{itemize}
\item Linear combinations of independent normal random variables are also normal, with appropriate mean and variance.
\item Two linear combinations of the same collection of independent normal random variables are independent if and only if their covariance equals $0$.
\item The chi-squared distribution with $n$ degrees of freedom is the distribution corresponding to a sum of squares of $n$ i.i.d.\ standard normal random variables. It has mean $n$. It is equal to the Gamma$(n/2, 1/2)$ distribution.
\item The $t$ distribution with $n$ degrees of freedom is the distribution corresponding to a standard normal random variable, divided by the square-root of $1/n$ times an independent chi-squared random variable with $n$ degrees of freedom. Its density function was presented. As $n \to \infty$, it converges in distribution to a standard normal distribution.
\item The $F$ distribution with $m$ and $n$ degrees of freedom is the distribution corresponding to $m/n$ times a chi-squared distribution with $m$ degrees of freedom, divided by an independent chi-squared distribution with $n$ degrees of freedom. Its density function was presented. If $t$ has a $t(n)$ distribution, then $t^2$ is distributed $F(1, n)$.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:4.6.1}
Let $X_1 \sim \mathrm{N}(3, 2^2)$ and $X_2 \sim \mathrm{N}(8, 5^2)$ be independent. Let $U = X_1 + 5X_2$ and $V = 6X_1 + CX_2$, where $C$ is a constant.
\begin{enumerate}[(a)]
\item What are the distributions of $U$ and $V$?
\item What value of $C$ makes $U$ and $V$ be independent?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $U \sim N(1(3) - 5(-8), 1^2(2^2) + 5^2(5^2)) = N(44, 629)$. $V \sim N(-6(3) + C(-8), 6^2(2^2) + C^2(5^2)) = N(-18 - 8C, 144 + 25C^2)$.
    \item $\cov(U, V) = (1)(-6)(2^2) + (-5)(C)(5^2) = -24 - 125C$. Hence, $U$ and $V$ are independent if and only if $\cov(U, V) = 0$ if and only if $C = -24/125$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.6.2}
Let $X \sim \mathrm{N}(3, 5)$ and $Y \sim \mathrm{N}(7, 2)$ be independent.
\begin{enumerate}[(a)]
\item What is the distribution of $Z = 4X + Y - 3$?
\item What is the covariance of $X$ and $Z$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item $Z \sim \text{Normal}(4(3) - (1/3)(-7), 4^2(5) + (1/3)^2(2)) = \text{Normal}(43/3, 722/9)$.
    \item $\cov(X, Z) = 4\var(X) = 20$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.6.3}
Let $X \sim \mathrm{N}(3, 5)$ and $Y \sim \mathrm{N}(7, 2)$ be independent. Find values of $C_1 \neq 0$, $C_2$, $C_3 \neq 0$, $C_4$, $C_5$ so that $C_1(X - C_2)^2 + C_3(Y - C_4)^2 \sim \chi^2(C_5)$.
\end{exercise}

\begin{solution}
$C_1 = 1/\sqrt{5}$. $C_2 = -3$. $C_3 = 1/\sqrt{2}$. $C_4 = 7$. $C_5 = 2$.
\end{solution}

\begin{exercise}
\label{exer:4.6.4}
Let $X \sim \chi^2(n)$ and $Y \sim \mathrm{N}(0, 1)$ be independent. Prove that $X + Y^2 \sim \chi^2(n+1)$.
\end{exercise}

\begin{solution}
Since $X \sim \chi^2(n)$, we can find $Z_1, \ldots, Z_n \sim N(0, 1)$, which are i.i.d., with $X = (Z_1)^2 + \cdots + (Z_n)^2$. Then $X + Y^2 = (Z_1)^2 + \cdots + (Z_n)^2 + Y^2 \sim \chi^2(n + 1)$ since it is the sum of squares of $n + 1$ independent standard normal random variables.
\end{solution}

\begin{exercise}
\label{exer:4.6.5}
Let $X \sim \chi^2(n)$ and $Y \sim \chi^2(m)$ be independent. Prove that $X + Y \sim \chi^2(n+m)$.
\end{exercise}

\begin{solution}
Since $X \sim \chi^2(n)$ and $Y \sim \chi^2(m)$, we can find $Z_1, \ldots, Z_n, W_1, \ldots, W_m \sim N(0, 1)$, which are i.i.d., with $X = (Z_1)^2 + \cdots + (Z_n)^2$ and $Y = (W_1)^2 + \cdots + (W_m)^2$. Then $X + Y = (Z_1)^2 + \cdots + (Z_n)^2 + (W_1)^2 + \cdots + (W_m)^2 \sim \chi^2(n + m)$ since it is the sum of squares of $n + m$ independent standard normal random variables.
\end{solution}

\begin{exercise}
\label{exer:4.6.6}
Let $X_1, X_2, \ldots, X_{4n}$ be i.i.d.\ with distribution $\mathrm{N}(0, 1)$. Find a value of $C$ such that
\[
C \cdot \frac{X_1^2 + X_2^2 + \cdots + X_n^2}{X_{n+1}^2 + X_{n+2}^2 + \cdots + X_{4n}^2} \sim F(n, 3n).
\]
\end{exercise}

\begin{solution}
$C = (1/n)/(1/3n) = 3$.
\end{solution}

\begin{exercise}
\label{exer:4.6.7}
Let $X_1, X_2, \ldots, X_{n+1}$ be i.i.d.\ with distribution $\mathrm{N}(0, 1)$. Find a value of $C$ such that
\[
C \cdot \frac{X_1}{\sqrt{X_2^2 + \cdots + X_n^2 + X_{n+1}^2}} \sim t(n).
\]
\end{exercise}

\begin{solution}
$C = 1/(1/\sqrt{n}) = \sqrt{n}$.
\end{solution}

\begin{exercise}
\label{exer:4.6.8}
Let $X \sim \mathrm{N}(3, 5)$ and $Y \sim \mathrm{N}(7, 2)$ be independent. Find values of $C_1$, $C_2$, $C_3$, $C_4$, $C_5$, $C_6$ so that
\[
\frac{C_1(X - C_2)/C_3}{\sqrt{(Y - C_4)^2/C_5}} \sim t(C_6).
\]
\end{exercise}

\begin{solution}
$C_1 = \sqrt{2/5}$. $C_2 = -3$. $C_3 = 1$. $C_4 = 7$. $C_5 = 1$. $C_6 = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.6.9}
Let $X \sim \mathrm{N}(3, 5)$ and $Y \sim \mathrm{N}(7, 2)$ be independent. Find values of $C_1$, $C_2$, $C_3$, $C_4$, $C_5$, $C_6$, $C_7$ so that
\[
\frac{C_1(X - C_2)^2/C_3}{(Y - C_4)^2/C_5} \sim F(C_6, C_7).
\]
\end{exercise}

\begin{solution}
$C_1 = 2/5$. $C_2 = -3$. $C_3 = 2$. $C_4 = 7$. $C_5 = 2$. $C_6 = 1$. $C_7 = 1$.
\end{solution}

\begin{exercise}
\label{exer:4.6.10}
Let $X_1, X_2, \ldots, X_{100}$ be independent, each with the standard normal distribution.
\begin{enumerate}[(a)]
\item Compute the distribution of $X_1^2$.
\item Compute the distribution of $X_3^2 + X_5^2$.
\item Compute the distribution of $X_{10}/\sqrt{[X_{20}^2 + X_{30}^2 + X_{40}^2]/3}$.
\item Compute the distribution of $3X_{10}^2/[X_{20}^2 + X_{30}^2 + X_{40}^2]$.
\item Compute the distribution of
\[
\frac{30}{70} \cdot \frac{X_1^2 + X_2^2 + \cdots + X_{70}^2}{X_{71}^2 + X_{72}^2 + \cdots + X_{100}^2}.
\]
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Since $X_1$ has a standard normal distribution, $(X_1)^2$ has a chi-squared distribution with 1 degree of freedom.
    \item Here $(X_3)^2$ and $(X_5)^2$ each have a chi-squared distribution with 1 degree of freedom, and they are independent, so their sum has a chi-squared distribution with 2 degrees of freedom.
    \item Here $(X_{20})^2 + (X_{30})^2 + (X_{40})^2$ has a chi-squared distribution with 3 degrees of freedom, and $X_{10}$ is standard normal, and they are independent, so $X_{10}/\sqrt{[(X_{20})^2 + (X_{30})^2 + (X_{40})^2]/3}$ has a $t$ distribution with 3 degrees of freedom.
    \item Here $(X_{10})^2$ has a chi-squared distribution with 1 degree of freedom, and $(X_{20})^2 + (X_{30})^2 + (X_{40})^2$ has a chi-squared distribution with 3 degrees of freedom, and they are independent, so $(X_{10})^2/[((X_{20})^2 + (X_{30})^2 + (X_{40})^2)/3] = 3(X_{10})^2/[(X_{20})^2 + (X_{30})^2 + (X_{40})^2]$ has an $F$ distribution with 1 and 3 degrees of freedom.
    \item Here $(X_1)^2 + (X_2)^2 + \cdots + (X_{70})^2$ has a chi-squared distribution with 70 degrees of freedom, and $(X_{71})^2 + (X_{72})^2 + \cdots + (X_{100})^2$ has a chi-squared distribution with 30 degrees of freedom, and they are independent, so $\frac{[(X_1)^2 + (X_2)^2 + \cdots + (X_{70})^2]/70}{[(X_{71})^2 + (X_{72})^2 + \cdots + (X_{100})^2]/30} = \frac{30}{70} \frac{(X_1)^2 + (X_2)^2 + \cdots + (X_{70})^2}{(X_{71})^2 + (X_{72})^2 + \cdots + (X_{100})^2}$ has an $F$ distribution with 70 and 30 degrees of freedom.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.6.11}
Let $X_1, X_2, \ldots, X_{61}$ be independent, each distributed as $\mathrm{N}(\mu, \sigma^2)$. Set $\bar{X} = \frac{1}{61}(X_1 + X_2 + \cdots + X_{61})$ and
\[
S^2 = \frac{1}{60}\bigl[(X_1 - \bar{X})^2 + (X_2 - \bar{X})^2 + \cdots + (X_{61} - \bar{X})^2\bigr]
\]
as usual.
\begin{enumerate}[(a)]
\item For what values of $K$ and $m$ is it true that the quantity $Y = K(\bar{X} - \mu)/S^2$ has a $t$ distribution with $m$ degrees of freedom?
\item With $K$ as in part (a), find $y$ such that $\prb(Y \leqslant y) = 0.05$.
\item For what values of $a$ and $b$ and $c$ is it true that the quantity $W = a(\bar{X} - \mu)^2/S^2$ has an $F$ distribution with $b$ and $c$ degrees of freedom?
\item For those values of $a$ and $b$ and $c$, find a quantity $\omega$ so that $\prb(W \leqslant \omega) = 0.05$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item We know that $(n - 1)S^2/\sigma^2$ has a chi-squared distribution with $n - 1$ degrees of freedom. Also, $\bar{X} - \mu$ has a normal distribution with mean 0 and variance $\sigma^2/n$, so $\sqrt{n/\sigma^2}(\bar{X} - \mu)$ has a standard normal distribution. Hence, $[\sqrt{n/\sigma^2}(\bar{X} - \mu)]/\sqrt{(n-1)S^2/\sigma^2(n-1)} = [\sqrt{n}(\bar{X} - \mu)]/\sqrt{S^2}$ has a $t$ distribution with $n - 1$ degrees of freedom. Hence, $m = n - 1 = 60$, and $K = \sqrt{n} = \sqrt{61} = 7.81$.
    \item According to text Table D.4, since $Y$ has a $t$ distribution with 60 degrees of freedom, $\prb(Y \leqslant 1.671) = 0.95$, so $\prb(Y \geqslant 1.671) = 0.05$, so $y = 1.671$.
    \item Here $\sqrt{n/\sigma^2}(\bar{X} - \mu)$ has a standard normal distribution, and $(n - 1)S^2/\sigma^2$ has a chi-squared distribution with $n - 1$ degrees of freedom, and they are independent. Hence, the quantity $W = \frac{[\sqrt{n/\sigma^2}(\bar{X} - \mu)]^2/1}{(n-1)S^2/\sigma^2/(n-1)} = n(\bar{X} - \mu)^2/S^2$ has an $F$ distribution with 1 and $n - 1$ degrees of freedom. Hence, $a = n = 61$, and $b = 1$, and $c = n - 1 = 60$.
    \item According to text Table D.5, $\prb(W \leqslant 4.00) = 0.95$, so $\prb(W \geqslant 4.00) = 0.05$, so $w = 4.00$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.6.12}
Suppose the core temperature (in degrees celsius, when used intensively) of the latest Dell desktop computer is normally distributed with mean 40 and standard deviation 5, while for the latest Compaq it is normally distributed with mean 45 and standard deviation 8. Suppose we measure the Dell temperature 20 times (on separate days) and obtain measurements $D_1, D_2, \ldots, D_{20}$, and we also measure the Compaq temperature 30 times and obtain measurements $C_1, C_2, \ldots, C_{30}$.
\begin{enumerate}[(a)]
\item Compute the distribution of $\bar{D} = (D_1 + \cdots + D_{20})/20$.
\item Compute the distribution of $\bar{C} = (C_1 + \cdots + C_{30})/30$.
\item Compute the distribution of $Z = \bar{C} - \bar{D}$.
\item Compute $\prb(\bar{C} < \bar{D})$.
\item Let $U = (D_1 - \bar{D})^2 + (D_2 - \bar{D})^2 + \cdots + (D_{20} - \bar{D})^2$. What is $\prb(U \leqslant 633.25)$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Since $D_i \sim N(40, 5^2)$, $\bar{D} \sim N(40, 5^2/20) = N(40, 1.25)$, a normal distribution with mean 40 and variance 1.25.
    \item Since $C_i \sim N(45, 8^2)$, $\bar{C} \sim N(45, 8^2/30) = N(45, 2.13)$, a normal distribution with mean 45 and variance 2.13.
    \item Since $\bar{C} \sim N(45, 2.13)$ and $\bar{D} \sim N(40, 1.25)$, independent, it follows that $Z \equiv \bar{C} - \bar{D} \sim N(45 - 40, 2.13 + 1.25) = N(5, 3.38)$.
    \item $\prb(\bar{C} < \bar{D}) = \prb(Z < 0) = \prb((Z - 5)/\sqrt{3.38} < (0 - 5)/\sqrt{3.38}) = \prb((Z - 5)/\sqrt{3.38} < -2.72) = 0.0033$ (using text Table D.2).
    \item Here $D_i \sim N(40, 5^2)$, so $(n - 1)S^2/\sigma^2 = U/5^2$ has a chi-squared distribution with $n - 1 = 19$ degrees of freedom. Hence, $\prb(U > 633.25) = \prb((U/5^2) > (633.25/5^2)) = \prb((U/5^2) > 25.33) = 1 - \prb((U/5^2) \leqslant 25.33) = 1 - 0.85 = 0.15$, using text Table D.3.
\end{enumerate}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:4.6.13}
Let $X \sim \mathrm{N}(0, 1)$, and let $\prb(Y = 1) = \prb(Y = -1) = 1/2$. Assume $X$ and $Y$ are independent. Let $Z = XY$.
\begin{enumerate}[(a)]
\item Prove that $Z \sim \mathrm{N}(0, 1)$.
\item Prove that $\cor(X, Z) = 0$.
\item Prove directly that $X$ and $Z$ are not independent.
\item Why does this not contradict Theorem~\ref{thm:4.6.2}?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Note that $\prb(X \leqslant z) = \prb(X \geqslant -z) = \prb(-X \leqslant z) = \Phi(z)$. Hence, $\prb(Z \leqslant z) = \prb(XY \leqslant z) = \prb(XY \leqslant z, Y = 1) + \prb(XY \leqslant z, Y = -1) = \prb(X \leqslant z, Y = 1) + \prb(-X \leqslant z, Y = -1) = \prb(X \leqslant z)\prb(Y = 1) + \prb(-X \leqslant z)\prb(Y = -1) = \Phi(z)\prb(Y = 1) + \Phi(z)\prb(Y = -1) = \Phi(z)$, so $Z \sim N(0, 1)$.
    \item $\cov(X, Z) = \expc(XZ) = \expc(X(XY)) = \expc(X^2)\expc(Y) = (1)(0) = 0$.
    \item For example, $\prb(X < -10, Z < -10) = \prb(X < -10, Y = 1) = \Phi(-10)/2$, while $\prb(X < -10)\prb(Z < -10) = \Phi(-10)^2 \neq \Phi(-10)/2$, so $X$ and $Z$ are not independent.
    \item Here $X$ and $Z$ do not arise as linear combinations of the same collection of independent normal random variables.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:4.6.14}
Let $Z \sim t(n)$. Prove that $\prb(Z \leqslant x) = \prb(Z \geqslant -x)$ for $x \in \mathbb{R}^1$, namely, prove that the $t(n)$ distribution is symmetric about $0$.
\end{exercise}

\begin{solution}
We see that $f_Z(-z) = \Gamma((n+1)/2)(1 + (-z)^2/n)^{-(n+1)/2}/\Gamma(n/2)\sqrt{\pi n} = \Gamma((n+1)/2)(1 + z^2/n)^{-(n+1)/2}/\Gamma(n/2)\sqrt{\pi n} = f_Z(z)$. Then using the substitution $s = -t$, we have $\prb(Z < -x) = \int_{-\infty}^{-x} f_Z(t) \, \mathrm{d}t = -\int_{\infty}^{x} f_Z(-s)(-\mathrm{d}s) = \int_x^{\infty} f_Z(s) \, \mathrm{d}s = \prb(Z > x)$.
\end{solution}

\begin{exercise}
\label{exer:4.6.15}
Let $X_n \sim F(n, 2n)$ for $n = 1, 2, 3, \ldots$. Prove that $X_n \to 1$ in probability and with probability 1.
\end{exercise}

\begin{solution}
If $X_n \sim F(n, 2n)$, then we can find $X_1, \ldots, X_{3n}$ i.i.d.\ $\sim N(0, 1)$, with $X_n = (((X_1)^2 + \cdots + (X_n)^2)/n)/(((X_{n+1})^2 + \cdots + (X_{3n})^2)/2n)$. But as $n \to \infty$, by the strong law of large numbers, since $\expc((X_i)^2) = 1$, $((X_1)^2 + \cdots + (X_n)^2)/n \to 1$ and $((X_{n+1})^2 + \cdots + (X_{3n})^2)/2n \to 1$ with probability 1. Hence, $X_n \to 1/1 = 1$ with probability 1, and hence also in probability.
\end{solution}

\begin{exercise}
\label{exer:4.6.16}
(The general chi-squared distribution) Prove that for $\alpha > 0$ the function
\[
f(z) = \frac{1}{2^{\alpha/2} \Gamma(\alpha/2)} z^{\alpha/2 - 1} e^{-z/2}
\]
defines a probability distribution on $(0, \infty)$. This distribution is known as the $\chi^2(\alpha)$ distribution, i.e., it generalizes the distribution in Section~\ref{ssec:4.6.1} by allowing the degrees of freedom to be an arbitrary positive real number. (Hint: The $\chi^2(\alpha)$ distribution is the same as a Gamma$(\alpha/2, 1/2)$ distribution.)
\end{exercise}

\begin{solution}
The $\text{Gamma}(\alpha/2, 1/2)$ distribution has density function $g(x) = (1/2)^{\alpha/2} x^{\alpha/2 - 1} e^{-x/2}/\Gamma(\alpha/2)$. By inspection, $g(z) = f(z)$, i.e., the $\chi^2(\alpha)$ distribution corresponds to the $\text{Gamma}(\alpha/2, 1/2)$ distribution and is thus a well-defined probability distribution on $(0, \infty)$.
\end{solution}

\begin{exercise}
\label{exer:4.6.17}
(MV) (The general $t$ distribution) Prove that for $\alpha > 0$ the function
\[
f(u) = \frac{\Gamma\bigl(\frac{1 + \alpha}{2}\bigr)}{\Gamma\bigl(\frac{\alpha}{2}\bigr)} \left(1 + \frac{u^2}{\alpha}\right)^{-(1 + \alpha)/2} \frac{1}{\sqrt{\alpha\pi}}
\]
defines a probability distribution on $\mathbb{R}$ by showing that the random variable
\[
U = \frac{X}{\sqrt{Y/\alpha}}
\]
has this density when $X \sim \mathrm{N}(0, 1)$ independent of $Y \sim \chi^2(\alpha)$, as in Problem~\ref{exer:4.6.16}. This distribution is known as the $t(\alpha)$ distribution, i.e., it generalizes the distribution in Section~\ref{ssec:4.6.2} by allowing the degrees of freedom to be an arbitrary positive real number. (Hint: The proof is virtually identical to that of Theorem~\ref{thm:4.6.7}.)
\end{exercise}

\begin{solution}
Just replace $n$ by $\alpha$ throughout in the proof of Theorem \ref{thm:4.6.7}.
\end{solution}

\begin{exercise}
\label{exer:4.6.18}
(MV) (The general $F$ distribution) Prove that for $\alpha > 0$, $\beta > 0$ the function
\[
f(u) = \frac{\Gamma\bigl(\frac{\alpha + \beta}{2}\bigr)}{\Gamma\bigl(\frac{\alpha}{2}\bigr) \Gamma\bigl(\frac{\beta}{2}\bigr)} \left(\frac{\alpha}{\beta}\right)^{\alpha/2} u^{\alpha/2 - 1} \left(1 + \frac{\alpha}{\beta} u\right)^{-(\alpha + \beta)/2}
\]
defines a probability distribution on $(0, \infty)$ by showing that the random variable
\[
U = \frac{X/\alpha}{Y/\beta}
\]
has this density whenever $X \sim \chi^2(\alpha)$ independent of $Y \sim \chi^2(\beta)$, as in Problem~\ref{exer:4.6.16}. This distribution is known as the $F(\alpha, \beta)$ distribution, i.e., it generalizes the distribution in Section~\ref{ssec:4.6.3} by allowing the numerator and denominator degrees of freedom to be arbitrary positive real numbers. (Hint: The proof is virtually identical to that of Theorem~\ref{thm:4.6.9}).
\end{exercise}

\begin{solution}
Just replace $n$ by $\alpha$ throughout in the proof of Theorem \ref{thm:4.6.9}.
\end{solution}

\begin{exercise}
\label{exer:4.6.19}
Prove that when $X \sim t(\alpha)$ as defined in Problem~\ref{exer:4.6.17}, and $\alpha > 1$, then $\expc(X) = 0$. Further prove that when $\alpha > 2$, $\var(X) = \alpha/(\alpha - 2)$. You can assume the existence of these integrals --- see Challenge~\ref{exer:4.6.21}. (Hint: To evaluate the second moment, use $Y = X^2 \sim F(1, \alpha)$ as defined in Problem~\ref{exer:4.6.18}.)
\end{exercise}

\begin{solution}
When $\alpha > 1$, we have that
\[
    \expc(X) = \int_{-\infty}^{\infty} x \left(1 + \frac{x^2}{\alpha}\right)^{-\frac{\alpha+1}{2}} \mathrm{d}x = -\frac{2\alpha}{2(\alpha - 1)} \left(1 + \frac{x^2}{\alpha}\right)^{-\frac{\alpha-1}{2}} \bigg|_{-\infty}^{\infty} = 0.
\]
When $\alpha > 2$ we can write (using $X \sim t(\alpha)$ implies that $Y = X^2 \sim F(1, \alpha)$)
\begin{align*}
    \expc(X^2) &= \frac{\Gamma\left(\frac{1+\alpha}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{\alpha}{2}\right)\alpha} \int_0^{\infty} u^{1/2} (1 + u)^{-\frac{\alpha+1}{2}} \, \mathrm{d}u \\
    &= \frac{\Gamma\left(\frac{1+\alpha}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{\alpha}{2}\right)\alpha} \int_0^{\infty} u^{3/2 - 1} (1 + u)^{-\frac{3+\alpha-2}{2}} \, \mathrm{d}u \\
    &= \frac{\Gamma\left(\frac{1+\alpha}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{\alpha}{2}\right)\alpha} \int_0^{\infty} \left(\frac{3}{\alpha - 2}v\right)^{3/2 - 1} \left(1 + \frac{3}{\alpha - 2}v\right)^{-\frac{3+\alpha-2}{2}} \frac{3}{\alpha - 2} \, \mathrm{d}v \\
    &= \frac{\Gamma\left(\frac{1+\alpha}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{\alpha}{2}\right)} \frac{\Gamma\left(\frac{3}{2}\right)\Gamma\left(\frac{\alpha-2}{2}\right)}{\Gamma\left(\frac{1+\alpha}{2}\right)\alpha} = \frac{1/2}{\alpha/2 - 1} \alpha = \frac{\alpha}{\alpha - 2}.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:4.6.20}
Prove that when $X \sim F(\alpha, \beta)$, then $\expc(X) = \beta/(\beta - 2)$ when $\beta > 2$, and
\[
\var(X) = \frac{2\beta^2(\alpha + \beta - 2)}{\alpha(\beta - 2)^2(\beta - 4)}
\]
when $\beta > 4$.
\end{exercise}

\begin{solution}
Making the transformation $v = (\alpha(\beta - 2)/\beta(\alpha + 2))u$, we have that
\begin{align*}
    \expc(X) &= \frac{\Gamma\left(\frac{\alpha+\beta}{2}\right)}{\Gamma\left(\frac{\alpha}{2}\right)\Gamma\left(\frac{\beta}{2}\right)} \frac{\beta}{\alpha} \int_0^{\infty} \left(\frac{\alpha}{\beta}u\right)^{\frac{\alpha+2}{2} - 1} \left(1 + \frac{\alpha}{\beta}u\right)^{-\frac{\alpha+\beta}{2}} \frac{\alpha}{\beta} \, \mathrm{d}u \\
    &= \frac{\Gamma\left(\frac{\alpha+\beta}{2}\right)}{\Gamma\left(\frac{\alpha}{2}\right)\Gamma\left(\frac{\beta}{2}\right)} \frac{\beta}{\alpha} \int_0^{\infty} \left(\frac{\alpha + 2}{\beta - 2}v\right)^{\frac{\alpha+2}{2} - 1} \left(1 + \frac{\alpha + 2}{\beta - 2}v\right)^{-\frac{\alpha+\beta}{2}} \frac{\alpha + 2}{\beta - 2} \, \mathrm{d}v \\
    &= \frac{\Gamma\left(\frac{\alpha+\beta}{2}\right)}{\Gamma\left(\frac{\alpha}{2}\right)\Gamma\left(\frac{\beta}{2}\right)} \frac{\beta}{\alpha} \frac{\Gamma\left(\frac{\alpha+2}{2}\right)\Gamma\left(\frac{\beta-2}{2}\right)}{\Gamma\left(\frac{\alpha+\beta}{2}\right)} = \frac{\beta}{\alpha} \frac{\alpha/2}{(\beta - 2)/2} = \frac{\beta}{\beta - 2},
\end{align*}
\begin{align*}
    \expc(X^2) &= \frac{\Gamma\left(\frac{\alpha+\beta}{2}\right)}{\Gamma\left(\frac{\alpha}{2}\right)\Gamma\left(\frac{\beta}{2}\right)} \left(\frac{\beta}{\alpha}\right)^2 \int_0^{\infty} \left(\frac{\alpha}{\beta}u\right)^{\frac{\alpha+4}{2} - 1} \left(1 + \frac{\alpha}{\beta}u\right)^{-\frac{\alpha+\beta}{2}} \frac{\alpha}{\beta} \, \mathrm{d}u \\
    &= \frac{\Gamma\left(\frac{\alpha+\beta}{2}\right)}{\Gamma\left(\frac{\alpha}{2}\right)\Gamma\left(\frac{\beta}{2}\right)} \left(\frac{\beta}{\alpha}\right)^2 \frac{\Gamma\left(\frac{\alpha+4}{2}\right)\Gamma\left(\frac{\beta-4}{2}\right)}{\Gamma\left(\frac{\alpha+\beta}{2}\right)} = \left(\frac{\beta}{\alpha}\right)^2 \frac{(\alpha/2)((\alpha/2) + 1)}{((\beta - 2)/2)((\beta - 4)/2)}.
\end{align*}
Therefore,
\[
    \var(X) = \left(\frac{\beta}{\alpha}\right)^2 \frac{(\alpha)(\alpha + 2)}{(\beta - 2)(\beta - 4)} - \frac{\beta^2}{(\beta - 2)^2} = \frac{\beta^2}{\alpha(\beta - 2)^2(\beta - 4)} \{(\alpha + 2)(\beta - 2) - \alpha(\beta - 4)\} = \frac{2\beta^2(\alpha + \beta - 2)}{\alpha(\beta - 2)^2(\beta - 4)}.
\]
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:4.6.21}
Following Problem~\ref{exer:4.6.19}, prove that the mean of $X$ does not exist whenever $0 < \alpha \leqslant 1$. Further prove that the variance of $X$ does not exist whenever $0 < \alpha \leqslant 1$ and is infinite when $1 < \alpha \leqslant 2$.
\end{exercise}

\begin{solution}
Suppose that $0 < \alpha \leqslant 1$. We have that
\[
    \int_0^{\infty} x \left(1 + \frac{x^2}{\alpha}\right)^{-\frac{\alpha+1}{2}} \mathrm{d}x \geqslant \int_0^{\infty} x \left(1 + \frac{x^2}{\alpha}\right)^{-1} \mathrm{d}x = \frac{\alpha}{2} \ln\left(1 + \frac{x^2}{\alpha}\right) \bigg|_0^{\infty} = \infty
\]
and, similarly, $\int_{-\infty}^{0} x \left(1 + \frac{x^2}{\alpha}\right)^{-\frac{\alpha+1}{2}} \mathrm{d}x = -\infty$, which implies that the mean does not exist.

Consider now the second moment. Since $X \sim t(\alpha)$ implies that $Y = X^2 \sim F(1, \alpha)$, we have that
\[
    \expc(X^2) = \frac{\Gamma\left(\frac{1+\alpha}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{\alpha}{2}\right)} \int_0^{\infty} \left(\frac{y}{\alpha}\right)^{1/2} \left(1 + \frac{y}{\alpha}\right)^{-\frac{\alpha+1}{2}} \mathrm{d}y = \frac{\Gamma\left(\frac{1+\alpha}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{\alpha}{2}\right)\alpha} \int_0^{\infty} u^{1/2} (1 + u)^{-\frac{\alpha+1}{2}} \, \mathrm{d}u.
\]
Now if $0 < \alpha \leqslant 2$, we have that
\[
    \int_0^{\infty} u^{1/2} (1 + u)^{-\frac{\alpha+1}{2}} \, \mathrm{d}u \geqslant \int_0^{\infty} u^{1/2} (1 + u)^{-3/2} \, \mathrm{d}u.
\]
Since $\lim_{u \to \infty} u^{1/2}(1 + u)^{-1/2} = 1$, we have that $u^{1/2}(1 + u)^{-1/2} \geqslant 1 - c$ for a specified $c > 0$ whenever $u > c_W$. Therefore,
\[
    \int_0^{\infty} u^{1/2} (1 + u)^{-3/2} \, \mathrm{d}u \geqslant \int_{c_W}^{\infty} u^{1/2} (1 + u)^{-3/2} \, \mathrm{d}u \geqslant (1 - c) \int_{c_W}^{\infty} (1 + u)^{-1} \, \mathrm{d}u = (1 - c) \ln(1 + u) \big|_{c_W}^{\infty} = \infty.
\]
Obviously, the variance is undefined when the mean does not exist, as when $0 < \alpha \leqslant 1$, and the above shows that it is infinite when $1 < \alpha \leqslant 2$.
\end{solution}

\begin{exercise}
\label{exer:4.6.22}
Prove the identity \eqref{eq:4.7.1} in Section~\ref{sec:4.7}, which arises as part of the proof of Theorem~\ref{thm:4.6.6}.
\end{exercise}

\begin{solution}
We use induction on $n$. For $n = 1$ both sides are 0, so the equation holds. Assume now that it holds for some value of $n$. We shall prove that it holds for $n + 1$. Multiplying through by $\sigma^2$, it suffices to take $\sigma = 1$. Let $\bar{X}_m = (1/m)(X_1 + \cdots + X_m)$ for any $m$. Then $\bar{X}_{n+1} = (n\bar{X}_n + X_{n+1})/(n+1)$, so that $\bar{X}_{n+1} - \bar{X}_n = (X_{n+1} - \bar{X}_n)/(n+1)$. Hence, for $1 \leqslant i \leqslant n$, $(X_i - \bar{X}_{n+1})^2 = (X_i - \bar{X}_n + (\bar{X}_n - \bar{X}_{n+1}))^2 = (X_i - \bar{X}_n)^2 + (\bar{X}_n - \bar{X}_{n+1})^2 + 2(X_i - \bar{X}_n)(\bar{X}_n - \bar{X}_{n+1})$.

Now, $\sum_{i=1}^{n}(X_i - \bar{X}_n)^2$ equals the right-hand side of (4.7.1) by the induction assumption. Also, $\sum_{i=1}^{n}(\bar{X}_n - \bar{X}_{n+1})^2 = n(\bar{X}_n - \bar{X}_{n+1})^2 = (n/(n+1)^2)(\bar{X}_n - X_{n+1})^2$. Also, $\sum_{i=1}^{n}(X_i - \bar{X}_n)(\bar{X}_n - \bar{X}_{n+1}) = (\bar{X}_n - \bar{X}_{n+1})\sum_{i=1}^{n}(X_i - \bar{X}_n) = 0$ by definition of $\bar{X}_n$. Hence, $\sum_{i=1}^{n+1}(X_i - \bar{X}_n)^2$ equals the right-hand side of (4.7.1) plus $n(\bar{X}_n - X_{n+1})^2$ plus $(X_{n+1} - \bar{X}_{n+1})^2$. But $(n/(n+1)^2)(\bar{X}_n - X_{n+1})^2 + (X_{n+1} - \bar{X}_{n+1})^2 = (n/(n+1)^2)((1/n)(X_1 + \cdots + X_n) - X_{n+1})^2 + (X_{n+1} - (1/(n+1))(X_1 + \cdots + X_{n+1}))^2 = (1/n(n+1)^2)((X_1 + \cdots + X_n) - nX_{n+1})^2 + (1/(n+1))^2(X_1 + \cdots + X_n + X_{n+1} - (n+1)X_{n+1})^2 = (1/n(n+1)^2)((X_1 + \cdots + X_n) - nX_{n+1})^2 + (1/(n+1))^2(X_1 + \cdots + X_n - nX_{n+1})^2 = ((1/n(n+1)^2) + (1/(n+1))^2)(X_1 + \cdots + X_n - nX_{n+1})^2 = (1/n(n+1))(X_1 + \cdots + X_n - nX_{n+1})^2 = ((X_1 + \cdots + X_n - nX_{n+1})/\sqrt{n(n+1)})^2$. Hence, $\sum_{i=1}^{n+1}(X_i - \bar{X}_n)^2$ equals the right-hand side of (4.7.1) plus $((X_1 + \cdots + X_n - nX_{n+1})/\sqrt{n(n+1)})^2$. The result follows by induction.
\end{solution}


\section{Further Proofs (Advanced)}
\label{sec:4.7}

\subsection*{Proof of Theorem~\ref{thm:4.3.1}}

We want to prove the following result. Let $Z, Z_1, Z_2, \ldots$ be random variables. Suppose $Z_n \to Z$ with probability 1. Then $Z_n \to Z$ in probability. That is, if a sequence of random variables converges almost surely, then it converges in probability to the same limit.

Assume $\prb(Z_n \to Z) = 1$. Fix $\epsilon > 0$, and let $A_n = \{s : |Z_m - Z| > \epsilon \text{ for some } m \geqslant n\}$. Then $\{A_n\}$ is a decreasing sequence of events. Furthermore, if $s \notin \bigcap_{n=1}^{\infty} A_n$, then $Z_n(s) \to Z(s)$ as $n \to \infty$. Hence,
\[
P\left(\bigcap_{n=1}^{\infty} A_n\right) \leqslant \prb(Z_n \not\to Z) = 0.
\]
By continuity of probabilities, we have $\lim_{n \to \infty} \prb(A_n) = \prb(\bigcap_{n=1}^{\infty} A_n) = 0$. Hence,
\[
\prb(|Z_n - Z| > \epsilon) \leqslant \prb(A_n) \to 0 \quad \text{as } n \to \infty.
\]
Because this is true for any $\epsilon > 0$, we see that $Z_n \to Z$ in probability.

\subsection*{Proof of Theorem~\ref{thm:4.4.1}}

We show that if $X_n \xrightarrow{P} X$, then $X_n \xrightarrow{D} X$.

Suppose $X_n \to X$ in probability and that $\prb(X = x) = 0$. We wish to show that $\lim_{n \to \infty} \prb(X_n \leqslant x) = \prb(X \leqslant x)$.

Choose any $\epsilon > 0$. Now, if $X_n \leqslant x$ then we must have either $X \leqslant x + \epsilon$ or $|X - X_n| > \epsilon$. Hence, by subadditivity,
\[
\prb(X_n \leqslant x) \leqslant \prb(X \leqslant x + \epsilon) + \prb(|X - X_n| > \epsilon).
\]
Replacing $x$ by $x - \epsilon$ in this equation, we see also that
\[
\prb(X \leqslant x - \epsilon) \leqslant \prb(X_n \leqslant x) + \prb(|X - X_n| > \epsilon).
\]
Rearranging and combining these two inequalities, we have
\[
\prb(X \leqslant x - \epsilon) - \prb(|X - X_n| > \epsilon) \leqslant \prb(X_n \leqslant x) \leqslant \prb(X \leqslant x + \epsilon) + \prb(|X - X_n| > \epsilon).
\]
This is the key.

We next let $n \to \infty$. Because $X_n \to X$ in probability, we know that
\[
\lim_{n \to \infty} \prb(|X - X_n| > \epsilon) = 0.
\]
This means that $\lim_{n \to \infty} \prb(X_n \leqslant x)$ is ``sandwiched'' between $\prb(X \leqslant x - \epsilon)$ and $\prb(X \leqslant x + \epsilon)$.

We then let $\epsilon \to 0$. By continuity of probabilities,
\[
\lim_{\epsilon \to 0} \prb(X \leqslant x - \epsilon) = \prb(X < x) \quad \text{and} \quad \lim_{\epsilon \to 0} \prb(X \leqslant x + \epsilon) = \prb(X \leqslant x).
\]
This means that $\lim_{n \to \infty} \prb(X_n \leqslant x)$ is ``sandwiched'' between $\prb(X < x)$ and $\prb(X \leqslant x)$.

But because $\prb(X = x) = 0$, we must have $\prb(X < x) = \prb(X \leqslant x)$. Hence, $\lim_{n \to \infty} \prb(X_n \leqslant x) = \prb(X \leqslant x)$, as required.

\subsection*{Proof of Theorem~\ref{thm:4.4.3} (The central limit theorem)}

We must prove the following. Let $X_1, X_2, \ldots$ be i.i.d.\ with finite mean $\mu$ and finite variance $\sigma^2$. Let $Z \sim \mathrm{N}(0, 1)$. Set $S_n = X_1 + \cdots + X_n$, and
\[
Z_n = \frac{S_n - n\mu}{\sqrt{n}\,\sigma^2}.
\]
Then as $n \to \infty$, the sequence $\{Z_n\}$ converges in distribution to the $Z$, i.e., $Z_n \xrightarrow{D} Z$.

Recall that the standard normal distribution has moment-generating function given by $m_Z(s) = \exp(s^2/2)$.

We shall now assume that $m_{Z_n}(s)$ is finite for $|s| < s_0$ for some $s_0 > 0$. (This assumption can be eliminated by using characteristic functions instead of moment-generating functions.) Assuming this, we will prove that for each real number $s$, we have $\lim_{n \to \infty} m_{Z_n}(s) = m_Z(s)$, where $m_{Z_n}(s)$ is the moment-generating function of $Z_n$. It then follows from Theorem~\ref{thm:4.4.2} that $Z_n$ converges to $Z$ in distribution.

To proceed, let $Y_i = (X_i - \mu)/\sigma$. Then $\expc(Y_i) = 0$ and $\expc(Y_i^2) = \var(Y_i) = 1$. Also, we have
\[
Z_n = \frac{1}{\sqrt{n}}(Y_1 + \cdots + Y_n).
\]
Let $m_Y(s) = \expc(e^{sY_i})$ be the moment-generating function of $Y_i$ (which is the same for all $i$, because they are i.i.d.). Then using independence, we compute that
\begin{align*}
\lim_{n \to \infty} m_{Z_n}(s) &= \lim_{n \to \infty} \expc(e^{s Z_n}) = \lim_{n \to \infty} \expc(e^{s(Y_1 + \cdots + Y_n)/\sqrt{n}}) \\
&= \lim_{n \to \infty} \expc(e^{sY_1/\sqrt{n}} e^{sY_2/\sqrt{n}} \cdots e^{sY_n/\sqrt{n}}) \\
&= \lim_{n \to \infty} \expc(e^{sY_1/\sqrt{n}}) \expc(e^{sY_2/\sqrt{n}}) \cdots \expc(e^{sY_n/\sqrt{n}}) \\
&= \lim_{n \to \infty} m_Y(s/\sqrt{n}) \cdot m_Y(s/\sqrt{n}) \cdots m_Y(s/\sqrt{n}) \\
&= \lim_{n \to \infty} \bigl[m_Y(s/\sqrt{n})\bigr]^n.
\end{align*}

Now, we know from Theorem~3.5.3 that $m_Y(0) = \expc(e^0) = 1$. Also, $m_Y'(0) = \expc(Y_i) = 0$ and $m_Y''(0) = \expc(Y_i^2) = 1$. But then expanding $m_Y(s)$ in a Taylor series around $s = 0$, we see that
\[
m_Y(s) = 1 + 0 \cdot s + \frac{1}{2!} s^2 + o(s^2) = 1 + s^2/2 + o(s^2),
\]
where $o(s^2)$ stands for a quantity that, as $s \to 0$, goes to $0$ faster than $s^2$ does --- namely, $o(s^2)/s \to 0$ as $s \to 0$. This means that
\[
m_Y(s/\sqrt{n}) = 1 + (s/\sqrt{n})^2/2 + o((s/\sqrt{n})^2) = 1 + s^2/(2n) + o(1/n),
\]
where now $o(1/n)$ stands for a quantity that, as $n \to \infty$, goes to $0$ faster than $1/n$ does.

Finally, we recall from calculus that, for any real number $c$, $\lim_{n \to \infty} (1 + c/n)^n = e^c$. It follows from this and the above that
\[
\lim_{n \to \infty} \bigl[m_Y(s^2/(2n))\bigr]^n = \lim_{n \to \infty} \bigl[1 + s^2/(2n)\bigr]^n = e^{s^2/2}.
\]
That is, $\lim_{n \to \infty} m_{Z_n}(s) = e^{s^2/2}$, as claimed.

\subsection*{Proof of Theorem~\ref{thm:4.6.2}}

We prove the following. Suppose $X_i \sim \mathrm{N}(\mu_i, \sigma_i^2)$ for $i = 1, 2, \ldots, n$ and also that the $X_i$ are independent. Let $U = \sum_{i=1}^{n} a_i X_i$ and $V = \sum_{i=1}^{n} b_i X_i$, for some constants $a_i$ and $b_i$. Then $\cor(U, V) = \sum_i a_i b_i \sigma_i^2$. Furthermore, $\cor(U, V) = 0$ if and only if $U$ and $V$ are independent.

It was proved in Section~\ref{sec:4.6} that $\cor(U, V) = \sum_i a_i b_i \sigma_i^2$ and that $\cor(U, V) = 0$ if $U$ and $V$ are independent. It remains to prove that, if $\cor(U, V) = 0$, then $U$ and $V$ are independent. For simplicity, we take $n = 2$ and $\mu_1 = \mu_2 = 0$ and $\sigma_1^2 = \sigma_2^2 = 1$; the general case is similar but messier. We therefore have
\[
U = a_1 X_1 + a_2 X_2 \quad \text{and} \quad V = b_1 X_1 + b_2 X_2.
\]
The Jacobian derivative of this transformation is
\[
J(x_1, x_2) = \left|\frac{\partial U}{\partial X_1} \frac{\partial V}{\partial X_2} - \frac{\partial V}{\partial X_1} \frac{\partial U}{\partial X_2}\right| = |a_1 b_2 - b_1 a_2|.
\]
Inverting the transformation gives
\[
X_1 = \frac{b_2 U - a_2 V}{a_1 b_2 - b_1 a_2} \quad \text{and} \quad X_2 = \frac{a_1 V - b_1 U}{a_1 b_2 - b_1 a_2}.
\]
Also,
\[
f_{X_1, X_2}(x_1, x_2) = \frac{1}{2\pi} e^{-(x_1^2 + x_2^2)/2}.
\]
Hence, from the multidimensional change of variable theorem (Theorem~2.9.2), we have
\begin{align*}
f_{U, V}(u, v) &= f_{X_1, X_2}\left(\frac{b_2 u - a_2 v}{a_1 b_2 - b_1 a_2},\, \frac{a_1 v - b_1 u}{a_1 b_2 - b_1 a_2}\right) \cdot |J(x_1, x_2)|^{-1} \\
&= \frac{1}{2\pi} \exp\left(-\frac{(b_2 u - a_2 v)^2 + (a_1 v - b_1 u)^2}{2(a_1 b_2 - b_1 a_2)^2}\right) \cdot \frac{1}{|a_1 b_2 - b_1 a_2|}.
\end{align*}
But
\[
(b_2 u - a_2 v)^2 + (a_1 v - b_1 u)^2 = (b_1^2 + b_2^2)u^2 + (a_1^2 + a_2^2)v^2 - 2(a_1 b_1 + a_2 b_2)uv,
\]
and $\cor(U, V) = a_1 b_1 + a_2 b_2$. Hence, if $\cor(U, V) = 0$, then
\[
(b_2 u - a_2 v)^2 + (a_1 v - b_1 u)^2 = (b_1^2 + b_2^2)u^2 + (a_1^2 + a_2^2)v^2
\]
and
\begin{align*}
f_{U, V}(u, v) &= \frac{\exp\bigl(-(b_1^2 + b_2^2)u^2/(2(a_1 b_2 - b_1 a_2)^2)\bigr) \exp\bigl(-(a_1^2 + a_2^2)v^2/(2(a_1 b_2 - b_1 a_2)^2)\bigr)}{2\pi |a_1 b_2 - b_1 a_2|}.
\end{align*}
It follows that we can factor $f_{U, V}(u, v)$ as a function of $u$ times a function of $v$. But this implies (see Problem~2.8.19) that $U$ and $V$ are independent.

\subsection*{Proof of Theorem~\ref{thm:4.6.6}}

We want to prove that when $X_1, \ldots, X_n$ are i.i.d.\ $\mathrm{N}(\mu, \sigma^2)$ and
\[
\bar{X} = \frac{1}{n}(X_1 + \cdots + X_n) \quad \text{and} \quad S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2,
\]
then $(n-1)S^2/\sigma^2 \sim \chi^2(n-1)$ and, furthermore, that $S^2$ and $\bar{X}$ are independent.

We have
\[
\frac{n-1}{\sigma^2} S^2 = \sum_{i=1}^{n} \left(\frac{X_i - \bar{X}}{\sigma}\right)^2.
\]
We rewrite this expression as (see Challenge~\ref{exer:4.6.22})
\begin{align}
\label{eq:4.7.1}
\frac{n-1}{\sigma^2} S^2 &= \left[\frac{X_1 - X_2}{\sqrt{2}}\right]^2 + \left[\frac{X_1 + X_2 - 2X_3}{\sqrt{2 \cdot 3}}\right]^2 + \left[\frac{X_1 + X_2 + X_3 - 3X_4}{\sqrt{3 \cdot 4}}\right]^2 \notag \\
&\qquad + \cdots + \left[\frac{X_1 + X_2 + \cdots + X_{n-1} - (n-1)X_n}{\sqrt{(n-1) \cdot n}}\right]^2.
\end{align}

Now, by Theorem~\ref{thm:4.6.1}, each of the $n - 1$ expressions within brackets in \eqref{eq:4.7.1} has the standard normal distribution. Furthermore, by Theorem~\ref{thm:4.6.2}, the expressions within brackets in \eqref{eq:4.7.1} are all independent of one another and are also all independent of $\bar{X}$.

It follows that $(n-1)S^2/\sigma^2$ is independent of $\bar{X}$. It also follows, by the definition of the chi-squared distribution, that $(n-1)S^2/\sigma^2 \sim \chi^2(n-1)$.

\subsection*{Proof of Theorem~\ref{thm:4.6.7}}

We want to show that when $U \sim t(n)$, then
\[
f_U(u) = \frac{\Gamma\bigl(\frac{n+1}{2}\bigr)}{\Gamma\bigl(\frac{n}{2}\bigr)} \left(1 + \frac{u^2}{n}\right)^{-(n+1)/2} \frac{1}{\sqrt{n\pi}}
\]
for all $u \in \mathbb{R}^1$.

Because $U \sim t(n)$, we can write $U = X/\sqrt{Y/n}$, where $X$ and $Y$ are independent with $X \sim \mathrm{N}(0, 1)$ and $Y \sim \chi^2(n)$. It follows that $X$ and $Y$ have joint density given by
\[
f_{X, Y}(x, y) = \frac{e^{-x^2/2} y^{n/2 - 1} e^{-y/2}}{\sqrt{2\pi} \cdot 2^{n/2} \Gamma(n/2)}
\]
when $y > 0$ (with $f_{X, Y}(x, y) = 0$ for $y \leqslant 0$).

Let $V = \sqrt{Y}$. We shall use the multivariate change of variables formula (Theorem~2.9.2) to compute the joint density $f_{U, V}(u, v)$ of $U$ and $V$. Because $U = X/\sqrt{Y/n}$ and $V = \sqrt{Y}$, it follows that $X = UV/\sqrt{n}$ and $Y = V^2$. We compute the Jacobian term as
\[
J(x, y) = \left|\det \begin{pmatrix} \partial u / \partial x & \partial u / \partial y \\ 0 & 1 \end{pmatrix}\right| = \left|\det \begin{pmatrix} 1/\sqrt{y/n} & \cdots \\ 0 & 1 \end{pmatrix}\right| = \frac{1}{\sqrt{y/n}}.
\]
Hence,
\begin{align*}
f_{U, V}(u, v) &= f_{X, Y}(uv/\sqrt{n}, v^2) \cdot |J|^{-1}(uv/\sqrt{n}, v^2) \\
&= \frac{e^{-u^2 v^2/(2n)} (v^2)^{n/2 - 1} e^{-v^2/2}}{\sqrt{2\pi} \cdot 2^{n/2} \Gamma(n/2)} \cdot \frac{v}{\sqrt{n}} \\
&= \frac{1}{\sqrt{n\pi}} \cdot \frac{1}{2^{(n+1)/2} \Gamma(n/2)} v^{n-1} e^{-v^2(1 + u^2/n)/2}
\end{align*}
for $v > 0$ (with $f_{U, V}(u, v) = 0$ for $v \leqslant 0$).

Finally, we compute the marginal density of $U$:
\begin{align*}
f_U(u) &= \int_0^{\infty} f_{U, V}(u, v) \,\mathrm{d}v \\
&= \frac{1}{\sqrt{n\pi}} \cdot \frac{1}{2^{(n+1)/2} \Gamma(n/2)} \int_0^{\infty} v^{n-1} e^{-v^2(1 + u^2/n)/2} \,\mathrm{d}v \\
&= \frac{1}{\sqrt{n\pi}} \cdot \frac{(1 + u^2/n)^{-n/2}}{2^{(n+1)/2} \Gamma(n/2)} \int_0^{\infty} w^{n-1} e^{-w^2/2} \,\mathrm{d}w \\
&= \frac{\Gamma\bigl(\frac{n+1}{2}\bigr)}{\Gamma\bigl(\frac{n}{2}\bigr)} \left(1 + \frac{u^2}{n}\right)^{-(n+1)/2} \frac{1}{\sqrt{n\pi}},
\end{align*}
where we have made the substitution $w = v\sqrt{1 + u^2/n}$ to get the third equality and then used the definition of the gamma function to obtain the result.

\subsection*{Proof of Theorem~\ref{thm:4.6.9}}

We want to show that when $U \sim F(m, n)$, then
\[
f_U(u) = \frac{\Gamma\bigl(\frac{m+n}{2}\bigr)}{\Gamma\bigl(\frac{m}{2}\bigr) \Gamma\bigl(\frac{n}{2}\bigr)} \left(\frac{m}{n}\right)^{m/2} u^{m/2 - 1} \left(1 + \frac{m}{n} u\right)^{-(m+n)/2} \frac{m}{n}
\]
for $u > 0$, with $f_U(u) = 0$ for $u \leqslant 0$.

Because $U \sim F(n, m)$, we can write $U = (X/m)/(Y/n)$, where $X$ and $Y$ are independent with $X \sim \chi^2(m)$ and $Y \sim \chi^2(n)$. It follows that $X$ and $Y$ have joint density given by
\[
f_{X, Y}(x, y) = \frac{x^{m/2 - 1} e^{-x/2} y^{n/2 - 1} e^{-y/2}}{2^{m/2} \Gamma(m/2) \cdot 2^{n/2} \Gamma(n/2)}
\]
when $x, y > 0$ (with $f_{X, Y}(x, y) = 0$ for $x \leqslant 0$ or $y \leqslant 0$).

Let $V = Y$ and use the multivariate change of variables formula (Theorem~2.9.2) to compute the joint density $f_{U, V}(u, v)$ of $U$ and $V$. Because $U = (X/m)/(Y/n)$ and $V = Y$, it follows that $X = (m/n)UV$ and $Y = V$. We compute the Jacobian term as
\[
J(x, y) = \left|\det \begin{pmatrix} \partial u / \partial x & \partial u / \partial y \\ 0 & 1 \end{pmatrix}\right| = \frac{n}{my}.
\]
Hence,
\begin{align*}
f_{U, V}(u, v) &= f_{X, Y}((m/n)uv, v) \cdot |J|^{-1}((m/n)uv, v) \\
&= \frac{((m/n)uv)^{m/2 - 1} e^{-(m/n)uv/2} v^{n/2 - 1} e^{-v/2}}{2^{m/2} \Gamma(m/2) \cdot 2^{n/2} \Gamma(n/2)} \cdot \frac{m}{n} v \\
&= \frac{1}{\Gamma(m/2) \Gamma(n/2)} \left(\frac{m}{n}\right)^{m/2} u^{m/2 - 1} \cdot \frac{1}{2^{(m+n)/2}} v^{(m+n)/2 - 1} e^{-v(1 + mu/n)/2}
\end{align*}
for $u, v > 0$ (with $f_{U, V}(u, v) = 0$ for $u \leqslant 0$ or $v \leqslant 0$).

Finally, we compute the marginal density of $U$ as
\begin{align*}
f_U(u) &= \int_0^{\infty} f_{U, V}(u, v) \,\mathrm{d}v \\
&= \frac{1}{\Gamma(m/2) \Gamma(n/2)} \left(\frac{m}{n}\right)^{m/2} u^{m/2 - 1} \cdot \frac{1}{2^{(m+n)/2}} \int_0^{\infty} v^{(m+n)/2 - 1} e^{-v(1 + mu/n)/2} \,\mathrm{d}v \\
&= \frac{1}{\Gamma(m/2) \Gamma(n/2)} \left(\frac{m}{n}\right)^{m/2} u^{m/2 - 1} \left(1 + \frac{m}{n} u\right)^{-(n+m)/2} \cdot \frac{m}{n} \int_0^{\infty} w^{(m+n)/2 - 1} e^{-w} \,\mathrm{d}w \\
&= \frac{\Gamma\bigl(\frac{m+n}{2}\bigr)}{\Gamma\bigl(\frac{m}{2}\bigr) \Gamma\bigl(\frac{n}{2}\bigr)} \left(\frac{m}{n}\right)^{m/2} u^{m/2 - 1} \left(1 + \frac{m}{n} u\right)^{-(n+m)/2} \cdot \frac{m}{n},
\end{align*}
where we have used the substitution $w = v(1 + mu/n)/2$ to get the third equality, and the final result follows from the definition of the gamma function
