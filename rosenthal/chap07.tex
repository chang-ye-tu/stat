\chapter{Bayesian Inference}
\label{ch:7}

\noindent\textbf{CHAPTER OUTLINE}
\begin{itemize}
\item Section 1 \quad The Prior and Posterior Distributions
\item Section 2 \quad Inferences Based on the Posterior
\item Section 3 \quad Bayesian Computations
\item Section 4 \quad Choosing Priors
\item Section 5 \quad Further Proofs (Advanced)
\end{itemize}

In Chapter~\ref{ch:5}, we introduced the basic concepts of inference. At the heart of the theory of inference is the concept of the statistical model $\{f_\theta : \theta \in \Omega\}$ that describes the statistician's uncertainty about how the observed data were produced. Chapter~\ref{ch:6} dealt with the analysis of this uncertainty based on the model and the data alone. In some cases, this seemed quite successful, but we note that we only dealt with some of the simpler contexts there.

If we accept the principle that, to be amenable to analysis, all uncertainties need to be described by probabilities, then the prescription of a model alone is incomplete, as this does not tell us how to make probability statements about the unknown true value of $\theta$. In this chapter, we complete the description so that all uncertainties are described by probabilities. This leads to a probability distribution for $\theta$ and, in essence, we are in the situation of Section~\ref{sec:5.2}, with the parameter now playing the role of the unobserved response. This is the Bayesian approach to inference.

Many statisticians prefer to develop statistical theory without the additional ingredients necessary for a full probability description of the unknowns. In part, this is motivated by the desire to avoid the prescription of the additional model ingredients necessary for the Bayesian formulation. Of course, we would prefer to have our statistical analysis proceed based on the fewest and weakest model assumptions possible. For example, in Section~\ref{sec:6.4}, we introduced distribution-free methods. A price is paid for this weakening, however, and this typically manifests itself in ambiguities about how inference should proceed. The Bayesian formulation in essence removes the ambiguity, but at the price of a more involved model.

The Bayesian approach to inference is sometimes presented as antagonistic to methods that are based on repeated sampling properties (often referred to as frequentist methods), as discussed, for example, in Chapter~\ref{ch:6}. The approach taken in this text, however, is that the Bayesian model arises naturally from the statistician assuming more ingredients for the model. It is up to the statistician to decide what ingredients can be justified and then use appropriate methods. We must be wary of all model assumptions, because using inappropriate ones may invalidate our inferences. Model checking will be taken up in Chapter~\ref{ch:9}.

\section{The Prior and Posterior Distributions}
\label{sec:7.1}

The Bayesian model for inference contains the statistical model $\{f_\theta : \theta \in \Omega\}$ for the data $s \in S$ and adds to this the prior probability measure $\pi$ for $\theta$. The prior describes the statistician's beliefs about the true value of the parameter a priori, i.e., before observing the data. For example, if $\theta \in [0, 1]$ and $\theta$ equals the probability of getting a head on the toss of a coin, then the prior density plotted in Figure~\ref{fig:7.1.1} indicates that the statistician has some belief that the true value of $\theta$ is around 0.5. But this information is not very precise.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_1_1.pdf}
  \caption{A fairly diffuse prior on $[0,1]$.}
  \label{fig:7.1.1}
\end{figure}

On the other hand, the prior density plotted in Figure~\ref{fig:7.1.2} indicates that the statistician has very precise information about the true value of $\theta$. In fact, if the statistician knows nothing about the true value of $\theta$, then using the uniform distribution on $[0, 1]$ might be appropriate.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_1_2.pdf}
  \caption{A fairly precise prior on $[0,1]$.}
  \label{fig:7.1.2}
\end{figure}

It is important to remember that the probabilities prescribed by the prior represent beliefs. They do not in general correspond to long-run frequencies, although they could in certain circumstances. A natural question to ask is: Where do these beliefs come from in an application? An easy answer is to say that they come from previous experience with the random system under investigation or perhaps with related systems. To be honest, however, this is rarely the case, and one has to admit that the prior, as well as the statistical model, is often a somewhat arbitrary construction used to drive the statistician's investigations. This raises the issue as to whether or not the inferences derived have any relevance to the practical context, if the model ingredients suffer from this arbitrariness. This is where the concept of model checking comes into play, a topic we will discuss in Chapter~\ref{ch:9}. At this point, we will assume that all the ingredients make sense, but remember that in an application, these must be checked if the inferences taken are to be practically meaningful.

We note that the ingredients of the Bayesian formulation for inference prescribe a marginal distribution for $\theta$, namely, the prior $\pi$, and a set of conditional distributions for the data $s$ given $\theta$, namely, $\{f_\theta : \theta \in \Omega\}$. By the law of total probability (Theorems~\ref{thm:2.3.1} and~\ref{thm:2.8.1}), these ingredients specify a joint distribution for $(s, \theta)$, namely,
\[
f_\theta(s) \, \pi(\theta),
\]
where $\pi(\theta)$ denotes the probability or density function associated with $\pi$. When the prior distribution is absolutely continuous, the marginal distribution for $s$ is given by
\[
m(s) = \int_\Omega f_\theta(s) \, \pi(\theta) \, \mathrm{d}\theta
\]
and is referred to as the prior predictive distribution of the data. When the prior distribution of $\theta$ is discrete, we replace (as usual) the integral by a sum.

If we did not observe any data, then the prior predictive distribution is the relevant distribution for making probability statements about the unknown value of $s$. Similarly, the prior $\pi$ is the relevant distribution to use in making probability statements about $\theta$ before we observe $s$. Inference about these unobserved quantities then proceeds as described in Section~\ref{sec:5.2}.

Recall now the principle of conditional probability; namely, $\prb(A)$ is replaced by $\prb(A \mid C)$ after we are told that $C$ is true. Therefore, after observing the data, the relevant distribution to use in making probability statements about $\theta$ is the conditional distribution of $\theta$ given $s$. We denote this conditional probability measure by $\pi(\cdot \mid s)$ and refer to it as the posterior distribution of $\theta$. Note that the density (or probability function) of the posterior is obtained immediately by taking the joint density $f_\theta(s) \, \pi(\theta)$ of $(s, \theta)$ and dividing it by the marginal $m(s)$ of $s$.

\begin{definition}
\label{def:7.1.1}
The posterior distribution of $\theta$ is the conditional distribution of $\theta$, given $s$. The posterior density, or posterior probability function (whichever is relevant), is given by
\begin{equation}
\label{eq:7.1.1}
\pi(\theta \mid s) = \frac{f_\theta(s) \, \pi(\theta)}{m(s)}.
\end{equation}
\end{definition}

Sometimes this use of conditional probability is referred to as an application of Bayes' theorem (Theorem~\ref{thm:1.5.2}). This is because we can think of a value of $\theta$ being selected first according to $\pi$, and then $s$ is generated from $f_\theta$. We then want to make probability statements about the first stage, having observed the outcome of the second stage. It is important to remember, however, that choosing to use the posterior distribution for probability statements about $\theta$ is an axiom, or principle, not a theorem.

We note that in \eqref{eq:7.1.1} the prior predictive of the data $s$ plays the role of the inverse normalizing constant for the posterior density. By this we mean that the posterior density of $\theta$ is proportional to $f_\theta(s) \, \pi(\theta)$, as a function of $\theta$; to convert this into a proper density function, we need only divide by $m(s)$. In many examples, we do not need to compute the inverse normalizing constant. This is because we recognize the functional form, as a function of $\theta$, of the posterior from the expression $f_\theta(s) \, \pi(\theta)$ and so immediately deduce the posterior probability distribution of $\theta$. Also, there are Monte Carlo methods, such as those discussed in Chapter~\ref{ch:4}, that allow us to sample from $\pi(\cdot \mid s)$ without knowing $m(s)$ (also see Section~\ref{sec:7.3}).

We consider some applications of Bayesian inference.

\begin{example}[Bernoulli Model]
\label{ex:7.1.1}
Suppose that we observe a sample $x_1, \ldots, x_n$ from the Bernoulli$(\theta)$ distribution with $\theta \in [0, 1]$ unknown. For the prior, we take $\pi(\theta)$ to be equal to a Beta$(\alpha, \beta)$ density (see Problem~\ref{exer:2.4.16}). Then the posterior of $\theta$ is proportional to the likelihood
\[
\prod_{i=1}^{n} \theta^{x_i} (1 - \theta)^{1 - x_i} = \theta^{n\bar{x}} (1 - \theta)^{n(1 - \bar{x})}
\]
times the prior
\[
\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}.
\]
This product is proportional to
\[
\theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1}.
\]
We recognize this as the unnormalized density of a Beta$(n\bar{x} + \alpha, n(1 - \bar{x}) + \beta)$ distribution. So in this example, we did not need to compute $m(x_1, \ldots, x_n)$ to obtain the posterior.

As a specific case, suppose that we observe $n\bar{x} = 10$ in a sample of $n = 40$ and $\alpha = \beta = 1$, i.e., we have a uniform prior on $\theta$. Then the posterior of $\theta$ is given by the Beta$(11, 31)$ distribution. We plot the posterior density in Figure~\ref{fig:7.1.3} as well as the prior.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_1_3.pdf}
  \caption{Prior (dashed line) and posterior densities (solid line) in Example~\ref{ex:7.1.1}.}
  \label{fig:7.1.3}
\end{figure}

The spread of the posterior distribution gives us some idea of the precision of any probability statements we make about $\theta$. Note how much information the data have added, as reflected in the graphs of the prior and posterior densities.
\end{example}

\begin{example}[Location Normal Model]
\label{ex:7.1.2}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma_0^2)$ distribution, where $\mu \in R^1$ is unknown and $\sigma_0^2$ is known. The likelihood function is then given by
\[
L(\mu \mid x_1, \ldots, x_n) = \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right\}.
\]
Suppose we take the prior distribution of $\mu$ to be an $N(\mu_0, \tau_0^2)$ for some specified choice of $\mu_0$ and $\tau_0^2$. The posterior density of $\mu$ is then proportional to
\begin{align}
&\exp\left\{-\frac{1}{2\tau_0^2}(\mu - \mu_0)^2\right\} \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right\} \notag\\
&= \exp\left\{-\frac{1}{2}\left(\frac{1}{\tau_0^2}(\mu^2 - 2\mu\mu_0) + \frac{n}{\sigma_0^2}(\bar{x}^2 - 2\bar{x}\mu)\right)\right\} \notag\\
&= \exp\left\{-\frac{1}{2}\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)\mu^2 + 2\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right)\mu\right\} \notag\\
&= \exp\left\{-\frac{\left(\tau_0^{-2} + n\sigma_0^{-2}\right)\left(\bar{x} - \mu\right)^2}{2}\right\} \cdot \exp\left\{-\frac{1}{2} \cdot \frac{(\mu_0 - \bar{x})^2}{\tau_0^2 + \sigma_0^2/n}\right\} \notag\\
&\propto \exp\left\{-\frac{1}{2}\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)\left(\mu - \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right)\right)^2\right\}.\label{eq:7.1.2}
\end{align}
We immediately recognize this, as a function of $\mu$, as being proportional to the density of an
\[
N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right), \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right)
\]
distribution.

Notice that the posterior mean is a weighted average of the prior mean $\mu_0$ and the sample mean $\bar{x}$, with weights
\[
\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \frac{1}{\tau_0^2} \quad \text{and} \quad \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \frac{n}{\sigma_0^2},
\]
respectively. This implies that the posterior mean lies between the prior mean and the sample mean.

Furthermore, the posterior variance is smaller than the variance of the sample mean. So if the information expressed by the prior is accurate, inferences about $\mu$ based on the posterior will be more accurate than those based on the sample mean alone. Note that the more diffuse the prior is --- namely, the larger $\tau_0^2$ is --- the less influence the prior has. For example, when $n = 20$ and $\sigma_0^2 = 1 = \tau_0^2$, then the ratio of the posterior variance to the sample mean variance is $20/21 = 0.95$. So there has been a 5\% improvement due to the use of prior information.

For example, suppose that $\sigma_0^2 = 1$, $\mu_0 = 0$, $\tau_0^2 = 2$, and that for $n = 10$ we observe $\bar{x} = 1.2$. Then the prior is an $N(0, 2)$ distribution, while the posterior is an
\[
N\left(\left(\frac{1}{2} + \frac{10}{1}\right)^{-1}\left(\frac{0}{2} + \frac{10}{1} \cdot 1.2\right), \left(\frac{1}{2} + \frac{10}{1}\right)^{-1}\right) = N(1.1429, 9.523\,8 \times 10^{-2})
\]
distribution. These densities are plotted in Figure~\ref{fig:7.1.4}. Notice that the posterior is quite concentrated compared to the prior, so we have learned a lot from the data.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_1_4.pdf}
  \caption{Plot of the $N(0, 2)$ prior (dashed line) and the $N(1.1429, 9.523\,8 \times 10^{-2})$ posterior (solid line) in Example~\ref{ex:7.1.2}.}
  \label{fig:7.1.4}
\end{figure}
\end{example}

\begin{example}[Multinomial Model]
\label{ex:7.1.3}
Suppose we have a categorical response $s$ that takes $k$ possible values, say, $s \in S = \{1, \ldots, k\}$. For example, suppose we have a bowl containing chips labelled one of $1, \ldots, k$. A proportion $\theta_i$ of the chips are labelled $i$, and we randomly draw a chip, observing its label.

When the $\theta_i$ are unknown, the statistical model is given by
\[
\{p_{\theta_1, \ldots, \theta_k} : (\theta_1, \ldots, \theta_k) \in \Omega\},
\]
where $p_{\theta_1, \ldots, \theta_k}(i) = \prb(s = i) = \theta_i$ and
\[
\Omega = \{(\theta_1, \ldots, \theta_k) : 0 \leqslant \theta_i \leqslant 1, \, i = 1, \ldots, k, \text{ and } \theta_1 + \cdots + \theta_k = 1\}.
\]
Note that the parameter space is really only $(k-1)$-dimensional because, for example, $\theta_k = 1 - \theta_1 - \cdots - \theta_{k-1}$, namely, once we have determined $k - 1$ of the $\theta_i$, the remaining value is specified.

Now suppose we observe a sample $s_1, \ldots, s_n$ from this model. Let the frequency (count) of the $i$th category in the sample be denoted by $x_i$. Then, from Example~\ref{ex:2.8.5}, we see that the likelihood is given by
\[
L(\theta_1, \ldots, \theta_k \mid s_1, \ldots, s_n) = \theta_1^{x_1} \theta_2^{x_2} \cdots \theta_k^{x_k}.
\]
For the prior we assume that $(\theta_1, \ldots, \theta_{k-1}) \sim \text{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_k)$ with density (see Problem~\ref{exer:2.7.13}) given by
\begin{equation}
\label{eq:7.1.3}
\pi(\theta_1, \ldots, \theta_k) = \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} \theta_1^{\alpha_1 - 1} \theta_2^{\alpha_2 - 1} \cdots \theta_k^{\alpha_k - 1}
\end{equation}
for $(\theta_1, \ldots, \theta_k) \in \Omega$ (recall that $\theta_k = 1 - \theta_1 - \cdots - \theta_{k-1}$). The $\alpha_i$ are nonnegative constants chosen by the statistician to reflect her beliefs about the unknown value of $(\theta_1, \ldots, \theta_k)$. The choice $\alpha_1 = \alpha_2 = \cdots = \alpha_k = 1$ corresponds to a uniform distribution, as then \eqref{eq:7.1.3} is constant on $\Omega$.

The posterior density of $(\theta_1, \ldots, \theta_{k-1})$ is then proportional to
\[
\theta_1^{x_1 + \alpha_1 - 1} \theta_2^{x_2 + \alpha_2 - 1} \cdots \theta_k^{x_k + \alpha_k - 1}
\]
for $(\theta_1, \ldots, \theta_k) \in \Omega$. From \eqref{eq:7.1.3}, we immediately deduce that the posterior distribution of $(\theta_1, \ldots, \theta_{k-1})$ is Dirichlet$(x_1 + \alpha_1, x_2 + \alpha_2, \ldots, x_k + \alpha_k)$.
\end{example}

\begin{example}[Location-Scale Normal Model]
\label{ex:7.1.4}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution, where $\mu \in R^1$ and $\sigma > 0$ are unknown. The likelihood function is then given by
\[
L(\mu, \sigma^2 \mid x_1, \ldots, x_n) = (2\pi\sigma^2)^{-n/2} \exp\left\{-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2\right\} \exp\left\{-\frac{n-1}{2\sigma^2}s^2\right\}.
\]
Suppose we put the following prior on $(\mu, \sigma^2)$. First, we specify that
\[
\mu \mid \sigma^2 \sim N(\mu_0, \tau_0^2 \sigma^2),
\]
i.e., the conditional prior distribution of $\mu$ given $\sigma^2$ is normal with mean $\mu_0$ and variance $\tau_0^2 \sigma^2$. Then we specify the marginal prior distribution of $\sigma^2$ as
\begin{equation}
\label{eq:7.1.4}
1/\sigma^2 \sim \text{Gamma}(\alpha_0, \beta_0).
\end{equation}
Sometimes \eqref{eq:7.1.4} is referred to by saying that $\sigma^2$ is distributed inverse Gamma. The values $\mu_0$, $\tau_0^2$, $\alpha_0$, and $\beta_0$ are selected by the statistician to reflect his prior beliefs.

From this, we can deduce (see Section~\ref{sec:7.5} for the full derivation) that the posterior distribution of $(\mu, \sigma^2)$ is given by
\begin{equation}
\label{eq:7.1.5}
\mu \mid \sigma^2, x_1, \ldots, x_n \sim N\left(\mu(x), (n + \tau_0^{-2})^{-1} \sigma^2\right)
\end{equation}
and
\begin{equation}
\label{eq:7.1.6}
1/\sigma^2 \mid x_1, \ldots, x_n \sim \text{Gamma}(\alpha_0 + n/2, \beta(x))
\end{equation}
where
\begin{equation}
\label{eq:7.1.7}
\mu(x) = (n + \tau_0^{-2})^{-1}(\mu_0 \tau_0^{-2} + n\bar{x})
\end{equation}
and
\begin{equation}
\label{eq:7.1.8}
\beta(x) = \beta_0 + \frac{n-1}{2}s^2 + \frac{1}{2} \cdot \frac{n(\bar{x} - \mu_0)^2}{1 + n\tau_0^2}.
\end{equation}

To generate a value $(\mu, \sigma^2)$ from the posterior, we can make use of the method of composition (see Problem~\ref{exer:2.10.13}) by first generating $\sigma^2$ using \eqref{eq:7.1.6} and then using \eqref{eq:7.1.5} to generate $\mu$. We will discuss this further in Section~\ref{sec:7.3}.

Notice that as $\tau_0 \to \infty$, i.e., as the prior on $\mu$ becomes increasingly diffuse, the conditional posterior distribution of $\mu$ given $\sigma^2$ converges in distribution to an $N(\bar{x}, \sigma^2/n)$ distribution because
\begin{equation}
\label{eq:7.1.9}
\mu(x) \to \bar{x}
\end{equation}
and
\begin{equation}
\label{eq:7.1.10}
(n + \tau_0^{-2})^{-1} \to \frac{1}{n}.
\end{equation}
Furthermore, as $\tau_0 \to \infty$ and $\alpha_0, \beta_0 \to 0$, the marginal posterior of $1/\sigma^2$ converges in distribution to a Gamma$(0 + n/2, (n-1)s^2/2)$ distribution because
\begin{equation}
\label{eq:7.1.11}
\beta(x) \to (n-1)s^2/2.
\end{equation}

Actually, it does not really seem to make sense to let $\tau_0 \to \infty$ and $\alpha_0, \beta_0 \to 0$ in the prior distribution of $(\mu, \sigma^2)$, as the prior does not converge to a proper probability distribution. The idea here, however, is that we think of taking $\tau_0$ large and $\alpha_0, \beta_0$ small, so that the posterior inferences are approximately those obtained from the limiting posterior. There is still a need to choose $\tau_0$, however, even in the diffuse case, as the limiting inferences are dependent on this quantity.
\end{example}

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:7.1}}

\begin{itemize}
\item Bayesian inference adds the prior probability distribution $\pi$ to the sampling model for the data as an additional ingredient to be used in determining inferences about the unknown value of the parameter.

\item Having observed the data, the principle of conditional probability leads to the posterior distribution of the parameter as the basis for inference.

\item Inference about marginal parameters is handled by marginalizing the full posterior.
\end{itemize}

\subsection*{EXERCISES}

\begin{exercise}
\label{exer:7.1.1}
Suppose that $S = \{1, 2\}$, $\Omega = \{1, 2, 3\}$, and the class of probability distributions for the response $s$ is given by the following table.
\begin{center}
\begin{tabular}{c|cc}
 & $s = 1$ & $s = 2$ \\
\hline
$f_1(s)$ & $1/2$ & $1/2$ \\
$f_2(s)$ & $1/3$ & $2/3$ \\
$f_3(s)$ & $3/4$ & $1/4$
\end{tabular}
\end{center}
If we use the prior given by the table
\begin{center}
\begin{tabular}{c|ccc}
$\theta$ & $1$ & $2$ & $3$ \\
\hline
$\pi(\theta)$ & $1/5$ & $2/5$ & $2/5$
\end{tabular}
\end{center}
then determine the posterior distribution of $\theta$ for each possible sample of size 2.
\end{exercise}

\begin{solution}
First, we compute $m(s)$ as follows.
\[
    m(s) = \sum_{\theta=1}^{3} \pi(\theta) f_\theta(s) = \begin{cases}
        \dfrac{1}{5} \cdot \dfrac{1}{2} + \dfrac{2}{5} \cdot \dfrac{1}{3} + \dfrac{2}{5} \cdot \dfrac{3}{4} = \dfrac{8}{15} & s = 1, \\[10pt]
        \dfrac{1}{5} \cdot \dfrac{1}{2} + \dfrac{2}{5} \cdot \dfrac{2}{3} + \dfrac{2}{5} \cdot \dfrac{1}{4} = \dfrac{7}{15} & s = 2.
    \end{cases}
\]
The posterior distribution of $\theta$ is then given by
\begin{center}
\begin{tabular}{c|ccc}
$\theta$ & 1 & 2 & 3 \\
\hline
$\pi(\theta \mid s = 1)$ & $3/16$ & $1/4$ & $9/16$ \\
$\pi(\theta \mid s = 2)$ & $3/14$ & $4/7$ & $3/14$
\end{tabular}
\end{center}
\end{solution}

\begin{exercise}
\label{exer:7.1.2}
In Example~\ref{ex:7.1.1}, determine the posterior mean and variance of $\theta$.
\end{exercise}

\begin{solution}
Since the posterior distribution of $\theta$ is $\text{Beta}(n\bar{x} + \alpha, n(1 - \bar{x}) + \beta)$ we have that
\begin{align*}
    \expc(\theta \mid x_1, \ldots, x_n) &= \int_0^1 \theta \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \int_0^1 \theta^{n\bar{x} + \alpha} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \cdot \frac{\Gamma(n\bar{x} + \alpha + 1) \Gamma(n(1 - \bar{x}) + \beta)}{\Gamma(n + \alpha + \beta + 1)} \\
    &= \frac{n\bar{x} + \alpha}{n + \alpha + \beta}.
\end{align*}
and
\begin{align*}
    \expc(\theta^2 \mid x_1, \ldots, x_n) &= \int_0^1 \theta^2 \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \int_0^1 \theta^{n\bar{x} + \alpha + 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \cdot \frac{\Gamma(n\bar{x} + \alpha + 2) \Gamma(n(1 - \bar{x}) + \beta)}{\Gamma(n + \alpha + \beta + 2)} \\
    &= \frac{(n\bar{x} + \alpha)(n\bar{x} + \alpha + 1)}{(n + \alpha + \beta)(n + \alpha + \beta + 1)},
\end{align*}
so
\[
    \var(\theta \mid x_1, \ldots, x_n) = \frac{(n\bar{x} + \alpha)(n\bar{x} + \alpha + 1)}{(n + \alpha + \beta)(n + \alpha + \beta + 1)} - \left(\frac{n\bar{x} + \alpha}{n + \alpha + \beta}\right)^2 = \frac{(n\bar{x} + \alpha)(n(1 - \bar{x}) + \beta)}{(n + \alpha + \beta)^2(n + \alpha + \beta + 1)}.
\]
\end{solution}

\begin{exercise}
\label{exer:7.1.3}
In Example~\ref{ex:7.1.2}, what is the posterior probability that $\mu$ is positive, given that $n = 10$, $\bar{x} = 1$, when $\sigma_0^2 = 1$, $\mu_0 = 0$, and $\tau_0^2 = 10$? Compare this with the prior probability of this event.
\end{exercise}

\begin{solution}
First, the prior distribution of $\theta$ is $N(0, 10)$, therefore, the prior probability that $\theta$ is positive is 0.5. Next, the posterior distribution of $\theta$ is
\[
    N\left(\left(\frac{1}{10} + \frac{10}{1}\right)^{-1} \left(\frac{10}{1}\right), \left(\frac{1}{10} + \frac{10}{1}\right)^{-1}\right) = N(0.99010, 9.9010 \times 10^{-2}).
\]
Therefore, the posterior probability that $\theta > 0$ is
\[
    1 - \Phi\left((0 - 0.99010)/\sqrt{9.9010 \times 10^{-2}}\right) = 1 - \Phi(-3.1466) = 1 - 0.0008 = 0.9992.
\]
\end{solution}

\begin{exercise}
\label{exer:7.1.4}
Suppose that $x_1, \ldots, x_n$ is a sample from a Poisson$(\lambda)$ distribution with $\lambda > 0$ unknown. If we use the prior distribution for $\lambda$ given by the Gamma$(\alpha, \beta)$ distribution, then determine the posterior distribution of $\lambda$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\lambda \mid x_1, \ldots, x_n) = e^{-n\lambda} \lambda^{n\bar{x}} / \prod(x_i!)$. The prior distribution has density given by $\beta^\alpha \lambda^{\alpha - 1} e^{-\beta\lambda} / \Gamma(\alpha)$. The posterior density of $\lambda$ is then proportional to $\beta^\alpha \lambda^{n\bar{x} + \alpha - 1} e^{-\lambda(n + \beta)} / \Gamma(\alpha) \prod(x_i!)$, and we recognize this as being proportional to the density of a $\text{Gamma}(n\bar{x} + \alpha, n + \beta)$ distribution.
\end{solution}

\begin{exercise}
\label{exer:7.1.5}
Suppose that $x_1, \ldots, x_n$ is a sample from a Uniform$[0, \theta]$ distribution with $\theta > 0$ unknown. If the prior distribution of $\theta$ is Gamma$(\alpha, \beta)$, then obtain the form of the posterior density of $\theta$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{-n} \indc_{[x_{(n)}, \infty)}(\theta)$. The prior distribution is the same as in the previous exercise. The posterior distribution of $\theta$ is then given by
\[
    \pi(\theta \mid x_1, \ldots, x_n) \propto \theta^{\alpha - n - 1} e^{-\beta\theta} \indc_{[x_{(n)}, \infty)}(\theta) \bigg/ \int_{x_{(n)}}^{\infty} \theta^{\alpha - n - 1} e^{-\beta\theta} \, \mathrm{d}\theta.
\]
\end{solution}

\begin{exercise}
\label{exer:7.1.6}
Find the posterior mean and variance of $\theta_i$ in Example~\ref{ex:7.1.3} when $k = 3$. (Hint: See Problems~\ref{exer:3.2.16} and~\ref{exer:3.3.20}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:3.2.23} the posterior mean of $\theta_i$ is
\[
    \frac{f_i + \alpha_i}{f_1 + \alpha_1 + f_2 + \alpha_2 + f_3 + \alpha_3} = \frac{f_i + \alpha_i}{n + \alpha_1 + \alpha_2 + \alpha_3}
\]
and the posterior variance of $\theta_i$ is given by
\[
    \frac{(f_i + \alpha_i)(f_1 + \alpha_1 + f_2 + \alpha_2 + f_3 + \alpha_3 - f_i - \alpha_i)}{(n + \alpha_1 + \alpha_2 + \alpha_3)^2(n + \alpha_1 + \alpha_2 + \alpha_3 + 1)}.
\]
\end{solution}

\begin{exercise}
\label{exer:7.1.7}
Suppose we have a sample
\begin{align*}
&6.56 \quad 6.39 \quad 3.30 \quad 3.03 \quad 5.31 \quad 5.62 \quad 5.10 \quad 2.45 \quad 8.24 \quad 3.71 \\
&4.14 \quad 2.80 \quad 7.43 \quad 6.82 \quad 4.75 \quad 4.09 \quad 7.95 \quad 5.84 \quad 8.44 \quad 9.36
\end{align*}
from an $N(\mu, \sigma^2)$ distribution and we determine that a prior specified by $\mu \mid \sigma^2 \sim N(3, 4\sigma^2)$, $1/\sigma^2 \sim \text{Gamma}(1, 1)$ is appropriate. Determine the posterior distribution of $(\mu, 1/\sigma^2)$.
\end{exercise}

\begin{solution}
From the sample, we have $\bar{x} = 5.567$. Also, $\mu_0 = 3$, $\tau_0^2 = 4$ and $\alpha_0 = \beta_0 = 1$. Hence, the posterior distributions are given by $\mu \mid \sigma^2, x_1, \ldots, x_n \sim N(5.5353, \frac{4}{81}\sigma^2)$ and $1/\sigma^2 \mid x_1, \ldots, x_n \sim \text{Gamma}(11, 41.737)$.
\end{solution}

\begin{exercise}
\label{exer:7.1.8}
Suppose that the prior probability of $\theta$ being in a set $A$ is $0.25$ and the posterior probability of $\theta$ being in $A$ is $0.80$.
\begin{enumerate}[(a)]
\item Explain what effect the data have had on your beliefs concerning the true value of $\theta$ being in $A$.
\item Explain why a posterior probability is more relevant to report than is a prior probability.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The belief of $\theta$ being in $A$ was 0.25 before observing data, and is increased to 0.80 after observing data. Hence, the belief ratio of $\theta$ being $A$ after observing data to before observing data is $0.80/0.25 = 3.2$. In other words, the posterior belief of $A$ to the prior belief is increased 3.2 times.
    \item A prior distribution is determined based on the background knowledge. Thus, a prior probability is not based on the data observed. Given the joint probability model for the parameter and data, the principle of conditional probability requires that any probabilities that we quote after observing the data must be posterior probabilities.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.1.9}
Suppose you toss a coin and put a Uniform$[0.4, 0.6]$ prior on $\theta$, the probability of getting a head on a single toss.
\begin{enumerate}[(a)]
\item If you toss the coin $n$ times and obtain $n$ heads, then determine the posterior density of $\theta$.
\item Suppose the true value of $\theta$ is, in fact, $0.99$. Will the posterior distribution of $\theta$ ever put any probability mass around $\theta = 0.99$ for any sample of $n$?
\item What do you conclude from part (b) about how you should choose a prior $\pi$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The prior predictive density is $m(n) = \int_0^1 \binom{n}{n} \theta^n (1 - \theta)^{n - n} \cdot 5 \indc_{[0.4, 0.6]}(\theta) \, \mathrm{d}\theta = 5 \int_{0.4}^{0.6} \theta^n \, \mathrm{d}\theta = 5(0.6^{n+1} - 0.4^{n+1})/(n + 1)$. The posterior density is $\pi(\theta \mid n) = \theta^n \cdot 5 \indc_{[0.4, 0.6]}(\theta) / m(n) = (n + 1) \theta^n \indc_{[0.4, 0.6]}(\theta) / (0.6^{n+1} - 0.4^{n+1})$.
    \item For any $c \in (0, 0.01)$,
    \[
        \Pi([0.99 - c, 0.99 + c] \mid n) = \int_{0.99 - c}^{0.99 + c} (n + 1) \theta^n \indc_{[0.4, 0.6]}(\theta) / (0.6^{n+1} - 0.4^{n+1}) \, \mathrm{d}\theta = 0.
    \]
    Hence, the posterior will not put any probability mass around $\theta = 0.99$.
    \item If you exclude a parameter value by forcing the prior to be 0 at that value, the posterior can never be positive no matter what data is obtained. To avoid this the prior must be greater than 0 on any parameter values that we believe are possible.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.1.10}
Suppose that for statistical model $\{f_\theta : \theta \in \Omega \subset R^1\}$, we assign the prior density $\pi(\theta)$. Now suppose that we reparameterize the model via the function $\psi = \Psi(\theta)$, where $\Psi : R^1 \to R^1$ is differentiable and strictly increasing.
\begin{enumerate}[(a)]
\item Determine the prior density of $\psi$.
\item Show that $m(x)$ is the same whether we parameterize the model by $\theta$ or by $\psi$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item Let $\Psi'(\theta) = \mathrm{d}\Psi(\theta)/\mathrm{d}\theta$ be the differential of $\Psi$ at $\theta$. Since $\Psi$ is increasing, $\Psi'$ is always positive. By Theorem \ref{thm:2.6.2},
    \[
        \pi_\Psi(\psi) = \pi(\Psi^{-1}(\psi)) / |\Psi'(\Psi^{-1}(\psi))| = \pi(\Psi^{-1}(\psi)) / \Psi'(\Psi^{-1}(\psi)).
    \]
    \item Let $m_\Psi(x)$ be the prior predictive density with respect to the $\psi$ parametrization.
    \begin{align*}
        m_\Psi(x) &= \int_{R^1} f_{\Psi^{-1}(\psi)}(x) \pi(\Psi^{-1}(\psi)) / \Psi'(\Psi^{-1}(\psi)) \, \mathrm{d}\psi \\
        &= \int_{R^1} f_\theta(x) (\pi(\theta) / \Psi'(\theta)) \left|\frac{\mathrm{d}\psi}{\mathrm{d}\theta}(\theta)\right| \mathrm{d}\theta \\
        &= \int_{R^1} f_\theta(x) (\pi(\theta) / \Psi'(\theta)) |\Psi'(\theta)| \, \mathrm{d}\theta \\
        &= \int_{R^1} f_\theta(x) \pi(\theta) \, \mathrm{d}\theta \\
        &= m(x).
    \end{align*}
    Hence, the prior predictive distribution is independent of any reparameterization.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.1.11}
Suppose that for statistical model $\{f_\theta : \theta \in \Omega\}$, where $\Omega = \{-2, -1, 0, 1, 2, 3\}$, we assign the prior probability function $\pi(\theta)$, which is uniform on $\Omega$. Now suppose we are interested primarily in making inferences about $\psi = \theta^2$.
\begin{enumerate}[(a)]
\item Determine the prior probability distribution of $\psi$. Is this distribution uniform?
\item A uniform prior distribution is sometimes used to express complete ignorance about the value of a parameter. Does complete ignorance about the value of a parameter imply complete ignorance about a function of a parameter? Explain.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Since $\theta$ is uniformly distributed on $\Omega = \{-2, -1, 0, 1, 2, 3\}$, $\Pi(|\theta| = 0) = \Pi(\theta = 0) = 1/6$, $\Pi(|\theta| = 1) = \Pi(\theta = 1 \text{ or } \theta = -1) = 1/3$, $\Pi(|\theta| = 2) = \Pi(\theta = 2 \text{ or } \theta = -2) = 1/3$ and $\Pi(|\theta| = 3) = \Pi(\theta = 3) = 1/6$. Hence, $|\theta|$ is not uniformly distributed on $\{0, 1, 2, 3\}$.
    \item If $\Psi$ is not 1-1 then logically we may have greater prior belief in some values of $\psi = \Psi(\theta)$ than others. For example, in part (a) it makes sense that we have less prior belief in $\Psi(\theta) = 0$ because only one value of $\theta$ is mapped to 0 while two values are mapped to each of the other possible values for $\Psi$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.1.12}
Suppose that for statistical model $\{f_\theta : \theta \in [0, 1]\}$, we assign the prior density $\pi(\theta)$, which is uniform on $[0, 1]$. Now suppose we are interested primarily in making inferences about $\psi = \theta^2$.
\begin{enumerate}[(a)]
\item Determine the prior density of $\psi = \theta^2$. Is this distribution uniform?
\item A uniform prior distribution is sometimes used to express complete ignorance about the value of a parameter. Does complete ignorance about the value of a parameter imply complete ignorance about a function of a parameter? Explain.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item Let $\Psi(\theta) = \theta^2$. Then, $\Psi'(\theta) = 2\theta$ and $\Psi^{-1}(\psi) = \psi^{1/2}$. By Theorem \ref{thm:2.6.2},
    \[
        \pi_\Psi(\psi) = \pi(\Psi^{-1}(\psi)) / \Psi'(\Psi^{-1}(\psi)) = 0.5 \psi^{-1/2}.
    \]
    Thus, $\pi_\Psi$ is not uniform on $[0, 1]$.
    \item As we can see in part (a), complete ignorance is not achieved for an arbitrary function of a parameter, at least when we demand that a distribution be uniform to reflect ignorance. Notice, however, that $\Psi$ is 1-1 and the change from a uniform distribution for $\theta$ to a nonuniform distribution for $\psi$ is caused by the change of variable factor $\psi^{-1/2}$ which reflects how the transformation $\Psi$ is changing lengths ($\Psi$ shortens lengths more severely for intervals near 0.)
\end{enumerate}
\end{solution}

\subsection*{COMPUTER EXERCISES}

\begin{exercise}
\label{exer:7.1.13}
In Example~\ref{ex:7.1.2}, when $\mu_0 = 2$, $\tau_0^2 = 1$, $\sigma_0^2 = 1$, $n = 20$, and $\bar{x} = 8.2$, generate a sample of $10^4$ (or as large as possible) from the posterior distribution of $\mu$ and estimate the posterior probability that the coefficient of variation $\sigma_0/\mu$ is greater than $0.125$, i.e., the posterior probability that $\sigma_0/\mu > 0.125$. Estimate the error in your approximation.
\end{exercise}

\begin{solution}
The posterior distribution is
\[
    N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right), \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right) = N\left(\left(\frac{1}{1} + \frac{20}{1}\right)^{-1} \left(\frac{2}{1} + \frac{20}{1} \cdot 8.2\right), \left(\frac{1}{1} + \frac{20}{1}\right)^{-1}\right) = N(7.9048, 4.7619 \times 10^{-2}).
\]
Then using R the simulation proceeds as follows.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_sim <- 10000
c1 <- rnorm(n_sim, mean = 7.90480, sd = sqrt(4.7619e-2))
c2 <- 1 / c1
c3 <- as.numeric(c2 > 0.125)
k1 <- mean(c3)
k2 <- sqrt(k1 * (1 - k1)) / sqrt(n_sim)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2
cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Simulation for posterior probability of coefficient of variation (prob7113.R)}
\label{lst:prob7113}
\end{listing}

So the estimate of the posterior probability that the coefficient of variation is greater than 0.125 is 0.683900, and the true value is in the interval $(0.669951, 0.697849)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:7.1.14}
In Example~\ref{ex:7.1.2}, when $\mu_0 = 2$, $\tau_0^2 = 1$, $\sigma_0^2 = 1$, $n = 20$, and $\bar{x} = 8.2$, generate a sample of $10^4$ (or as large as possible) from the posterior distribution of $\mu$ and estimate the posterior expectation of the coefficient of variation $\sigma_0/\mu$. Estimate the error in your approximation.
\end{exercise}

\begin{solution}
The posterior distribution is
\[
    N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right), \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right) = N\left(\left(\frac{1}{1} + \frac{20}{1}\right)^{-1} \left(\frac{2}{1} + \frac{20}{1} \cdot 8.2\right), \left(\frac{1}{1} + \frac{20}{1}\right)^{-1}\right) = N(7.9048, 4.7619 \times 10^{-2}).
\]
Then using R the simulation proceeds as follows.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
n_sim <- 10000
c1 <- rnorm(n_sim, mean = 7.90480, sd = sqrt(4.7619e-2))
c2 <- 1 / c1
k1 <- mean(c2)
k2 <- sd(c2) / sqrt(n_sim)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2
cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Simulation for posterior expectation of coefficient of variation (prob7114.R)}
\label{lst:prob7114}
\end{listing}

So the estimate of the posterior expectation of the coefficient of variation is 0.126677, and the true value is in the interval $(0.126572, 0.126783)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:7.1.15}
In Example~\ref{ex:7.1.1}, plot the prior and posterior densities on the same graph and compare them when $n = 30$, $\bar{x} = 0.73$, $\alpha = 3$, and $\beta = 3$. (Hint: Calculate the logarithm of the posterior density and then exponentiate this. You will need the loggamma function defined by $\ln \Gamma(\alpha)$ for $\alpha > 0$.)
\end{exercise}

\begin{solution}
The prior density is given by
\[
    \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1} = \frac{\Gamma(6)}{\Gamma(3) \Gamma(3)} \theta^2 (1 - \theta)^2 = \frac{5!}{2^2} \theta^2 (1 - \theta)^2
\]
and is plotted below (thick line). The posterior density is given by
\begin{align*}
    &\frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \\
    &= \frac{\Gamma(30 + 3 + 3)}{\Gamma(30(0.73) + 3) \Gamma(30(1 - 0.73) + 3)} \theta^{30(0.73) + 3 - 1} (1 - \theta)^{30(1 - 0.73) + 3 - 1} \\
    &= \frac{\Gamma(36)}{\Gamma(24.9) \Gamma(11.1)} \theta^{23.9} (1 - \theta)^{10.1}
\end{align*}
and is plotted below (thin line). The posterior density has shifted to the right and is more concentrated.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig7115.pdf}
    \caption{Prior (thick line) and posterior (thin line) densities for Exercise 7.1.15}
    %\label{fig:prior-posterior-beta}
\end{figure}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:7.1.16}
Suppose the prior of a real-valued parameter $\theta$ is given by the $N(0, \tau^2)$ distribution. Show that this distribution does not converge to a probability distribution as $\tau \to \infty$. (Hint: Consider the limits of the distribution functions.)
\end{exercise}

\begin{solution}
Suppose that $X_\tau \sim N(\mu_0, \tau^2)$. Then $\prb(X_\tau < x) = \Phi((x - \mu_0)/\tau) \to \Phi(0) = 1/2$ for every $x$ and this is not a distribution function.
\end{solution}

\begin{exercise}
\label{exer:7.1.17}
Suppose that $x_1, \ldots, x_n$ is a sample from $\{f_\theta : \theta \in \Omega\}$ and that we have a prior $\pi(\theta)$. Show that if we observe a further sample $x_{n+1}, \ldots, x_{n+m}$, then the posterior you obtain from using the posterior $\pi(\theta \mid x_1, \ldots, x_n)$ as a prior, and then conditioning on $x_{n+1}, \ldots, x_{n+m}$, is the same as the posterior obtained using the prior $\pi(\theta)$ and conditioning on $x_1, \ldots, x_n, x_{n+1}, \ldots, x_{n+m}$. This is the Bayesian updating property.
\end{exercise}

\begin{solution}
First, observe that the posterior density of $\theta$ given $x_1, \ldots, x_n$ is $\pi(\theta \mid x_1, \ldots, x_n) \propto \pi(\theta) \prod_{i=1}^{n} f_\theta(x_i)$. Using this as the prior density to obtain the posterior density of $\theta$ given $x_{n+1}, \ldots, x_{n+m}$, we get $\pi(\theta, x_1, \ldots, x_n \mid x_{n+1}, \ldots, x_{n+m}) \propto \pi(\theta) \prod_{i=1}^{n} f_\theta(x_i) \prod_{i=n+1}^{m+n} f_\theta(x_i)$, and this is the same as the posterior density of $\theta$ given $x_1, \ldots, x_n, x_{n+1}, \ldots, x_{n+m}$.
\end{solution}

\begin{exercise}
\label{exer:7.1.18}
In Example~\ref{ex:7.1.1}, determine $m(x)$. If you were asked to generate a value from this distribution, how would you do it? (Hint: For the generation part, use the theorem of total probability.)
\end{exercise}

\begin{solution}
The joint density of $(\theta, x_1, \ldots, x_n)$ is given by
\[
    \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1}
\]
and integrating out $\theta$ gives the marginal probability function for $(x_1, \ldots, x_n)$ as
\[
    m(x_1, \ldots, x_n) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \cdot \frac{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)}{\Gamma(\alpha + \beta + n)} \quad \text{for } (x_1, \ldots, x_n) \in \{0, 1\}^n.
\]
To generate from this distribution we can first generate $\theta \sim \text{Beta}(\alpha, \beta)$ and then generate $x_1, \ldots, x_n$ i.i.d.\ from the $\text{Bernoulli}(\theta)$ distribution.
\end{solution}

\begin{exercise}
\label{exer:7.1.19}
Prove that the posterior distribution depends on the data only through the value of a sufficient statistic.
\end{exercise}

\begin{solution}
First, note that if $T$ is a sufficient statistic, then, by the factorization theorem (Theorem \ref{thm:6.1.1}), the density (or probability function) for the model factors as $f_\theta(s) = h(s) g_\theta(T(s))$. The posterior density of $\theta$ is then given by
\[
    \pi(\theta \mid s) = \frac{\pi(\theta) h(s) g_\theta(T(s))}{\int_\Omega \pi(\theta) h(s) g_\theta(T(s)) \, \mathrm{d}\theta} = \frac{\pi(\theta) g_\theta(T(s))}{\int_\Omega \pi(\theta) g_\theta(T(s)) \, \mathrm{d}\theta}
\]
and this depends on the data only through the value of $T(s)$.
\end{solution}

\subsection*{Computer Problems}

\begin{exercise}
\label{exer:7.1.20}
For the data of Exercise~\ref{exer:7.1.7}, plot the prior and posterior densities of $\sigma^2$ over $(0, 10]$ on the same graph and compare them. (Hint: Evaluate the logarithms of the densities first and then plot the exponential of these values.)
\end{exercise}

\begin{solution}
The prior $\text{Gamma}(1, 1)$ density of $x = 1/\sigma^2$ is $(1/\Gamma(1)) x^{1-1} e^{-x} = e^{-x}$ for $x > 0$. Making the transformation $x \to y = 1/x$, the prior density of $\sigma^2$ is $x^{-2} e^{-1/x}$ for $x > 0$.

The posterior density of $1/\sigma^2$ is
\[
    \frac{41.737}{\Gamma(11)} (41.737 x)^{11-1} e^{-41.737 x} = \frac{41.737}{10!} (41.737 x)^{10} e^{-41.737 x}
\]
for $x > 0$. Making the transformation $x \to y = 1/x$, the posterior density of $\sigma^2$ is $(41.737)^{11} x^{-12} e^{-41.737/x} / 10!$. Plotting these we see that the posterior of $\sigma^2$ (thin line) is much more diffuse than the prior (thick line).

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig7120.pdf}
    \caption{Prior (thick line) and posterior (thin line) densities for $\sigma^2$ in Problem 7.1.20}
    %\label{fig:prior-posterior-sigma2}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:7.1.21}
In Example~\ref{ex:7.1.4}, when $\mu_0 = 0$, $\tau_0^2 = 1$, $\alpha_0 = 2$, $\beta_0 = 1$, $n = 20$, $\bar{x} = 8.2$, and $s^2 = 2.1$, generate a sample of $10^4$ (or as large as is feasible) from the posterior distribution of $(\mu, \sigma^2)$ and estimate the posterior probability that $\mu > 2$. Estimate the error in your approximation.
\end{exercise}

\begin{solution}
We have that $\mu \mid \sigma^2, x_1, \ldots, x_n \sim N(7.8095, (4.7619 \times 10^{-2}) \sigma^2)$ and $1/\sigma^2 \mid x_1, \ldots, x_n \sim \text{Gamma}(12, 52.969)$ since
\[
    \left(n + \frac{1}{\tau_0^2}\right)^{-1} = \left(20 + \frac{1}{1}\right)^{-1} = 4.7619 \times 10^{-2}
\]
\[
    \mu_x = \left(n + \frac{1}{\tau_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + n\bar{x}\right) = (4.7619 \times 10^{-2}) \left(\frac{0}{1} + 20(8.2)\right) = 7.8095
\]
and
\begin{align*}
    \beta_x &= \beta_0 + \frac{n}{2} \bar{x}^2 + \frac{\mu_0^2}{2\tau_0^2} + \frac{n - 1}{2} s^2 - \frac{1}{2} \left(n + \frac{1}{\tau_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + n\bar{x}\right)^2 \\
    &= 1 + \frac{20}{2}(8.2)^2 + \frac{0}{2} + \frac{20 - 1}{2}(2.1) - \frac{1}{2} \left(20 + \frac{1}{1}\right)^{-1} \left(\frac{0}{1} + 20(8.2)\right)^2 \\
    &= 52.969.
\end{align*}
Using R we obtained the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
k1 <- 1 / 52.969
cat("K1:", k1, "\n")

n_sim <- 10000
c1 <- rgamma(n_sim, shape = 12, rate = 52.969)
c2 <- 1 / sqrt(c1)
c3 <- as.numeric(c2 > 2)
k1 <- mean(c3)
k2 <- sqrt(k1 * (1 - k1)) / sqrt(n_sim)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2
cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Simulation for posterior probability that $\sigma > 2$ (prob7121.R)}
\label{lst:prob7121}
\end{listing}

So the estimate of the posterior probability that $\sigma > 2$ is 0.671800, and the true value is in the interval $(0.657713, 0.685887)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:7.1.22}
In Example~\ref{ex:7.1.4}, when $\mu_0 = 0$, $\tau_0^2 = 1$, $\alpha_0 = 2$, $\beta_0 = 1$, $n = 20$, $\bar{x} = 8.2$, and $s^2 = 2.1$, generate a sample of $10^4$ (or as large as is feasible) from the posterior distribution of $(\mu, \sigma^2)$ and estimate the posterior expectation of $\mu$. Estimate the error in your approximation.
\end{exercise}

\begin{solution}
We use the distribution determined in 7.1.16. Using R we obtained the following results.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
k1 <- 1 / 52.969
cat("K1:", k1, "\n")

n_sim <- 10000
c1 <- rgamma(n_sim, shape = 12, rate = 52.969)
c2 <- 1 / sqrt(c1)
k1 <- mean(c2)
k2 <- sd(c2) / sqrt(n_sim)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2
cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Simulation for posterior expectation of $\sigma$ (prob7122.R)}
\label{lst:prob7122}
\end{listing}

So the estimate of the posterior expectation of $\sigma$ is 2.17083 and the true value is in the interval $(2.16107, 2.18059)$ with virtual certainty.
\end{solution}

\subsection*{DISCUSSION TOPICS}

\begin{exercise}
\label{exer:7.1.23}
One of the objections raised concerning Bayesian inference methodology is that it is subjective in nature. Comment on this and the role of subjectivity in scientific investigations.
\end{exercise}

\begin{exercise}
\label{exer:7.1.24}
Two statisticians are asked to analyze a data set $x$ produced by a system under study. Statistician I chooses to use a sampling model $\{f_\theta : \theta \in \Omega\}$ and prior $\pi_I$, while statistician II chooses to use a sampling model $\{g_\theta : \theta \in \Omega\}$ and prior $\pi_{II}$. Comment on the fact that these ingredients can be completely different and so the subsequent analyses completely different. What is the relevance of this for the role of subjectivity in scientific analyses of data?
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inferences Based on the Posterior}
\label{sec:7.2}

In Section~\ref{sec:7.1}, we determined the posterior distribution of $\theta$ as a fundamental object of Bayesian inference. In essence, the principle of conditional probability asserts that the posterior distribution $\pi(\cdot \mid s)$ contains all the relevant information in the sampling model $\{f_\theta : \theta \in \Omega\}$, the prior $\pi$, and the data $s$ about the unknown true value of $\theta$. While this is a major step forward, it does not completely tell us how to make the types of inferences we discussed in Section~\ref{ssec:5.5.3}.

In particular, we must specify how to compute estimates, credible regions, and carry out hypothesis assessment --- which is what we will do in this section. It turns out that there are often several plausible ways of proceeding, but they all have the common characteristic that they are based on the posterior.

In general, we are interested in specifying inferences about a real-valued characteristic of interest $\psi = \Psi(\theta)$. One of the great advantages of the Bayesian approach is that inferences about $\psi$ are determined in the same way as inferences about the full parameter $\theta$, but with the marginal posterior distribution for $\psi$ replacing the full posterior. This situation can be compared with the likelihood methods of Chapter~\ref{ch:6}, where it is not always entirely clear how we should proceed to determine inferences about $\psi$ based upon the likelihood. Still, we have paid a price for this in requiring the addition of another model ingredient, namely, the prior.

So we need to determine the posterior distribution of $\psi$. This can be a difficult task in general, even if we have a closed-form expression for $\pi(\theta \mid s)$. When the posterior distribution of $\psi$ is discrete, the posterior probability function of $\psi$ is given by
\[
\pi(\psi_0 \mid s) = \sum_{\theta : \Psi(\theta) = \psi_0} \pi(\theta \mid s).
\]
When the posterior distribution of $\psi$ is absolutely continuous, we can often find a complementing function $\eta = H(\theta)$ so that $h(\theta) = (\Psi(\theta), H(\theta))$ is 1--1, and such that the methods of Section~\ref{ssec:2.9.2} can be applied. Then, denoting the inverse of this transformation by $h^{-1}(\psi, \eta)$, the methods of Section~\ref{ssec:2.9.2} show that the marginal posterior distribution of $\psi$ has density given by
\begin{equation}
\label{eq:7.2.1}
\pi(\psi_0 \mid s) = \int \pi(h^{-1}(\psi_0, \eta) \mid s) \, |J(h^{-1}(\psi_0, \eta))|^{-1} \, \mathrm{d}\eta,
\end{equation}
where $J$ denotes the Jacobian derivative of this transformation (see Problem~\ref{exer:7.2.35}).

Evaluating \eqref{eq:7.2.1} can be difficult, and we will generally avoid doing so here. An example illustrates how we can sometimes avoid directly implementing \eqref{eq:7.2.1} and still obtain the marginal posterior distribution of $\psi$.

\begin{example}[Location-Scale Normal Model]
\label{ex:7.2.1}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution, where $\mu \in R^1$ and $\sigma > 0$ are unknown, and we use the prior given in Example~\ref{ex:7.1.4}. The posterior distribution for $(\mu, \sigma^2)$ is then given by \eqref{eq:7.1.5} and \eqref{eq:7.1.6}.

Suppose we are primarily interested in $\psi = \sigma^2 = \Psi(\mu, \sigma^2)$. We see immediately that the marginal posterior of $\sigma^2$ is prescribed by \eqref{eq:7.1.6} and thus have no further work to do, unless we want a form for the marginal posterior density of $\sigma^2$. We can use the methods of Section~\ref{sec:2.6} for this (see Exercise~\ref{exer:7.2.4}).

If we want the marginal posterior distribution of $\psi = \mu = \Psi(\mu, \sigma^2)$, then things are not quite so simple because \eqref{eq:7.1.5} only prescribes the conditional posterior distribution of $\mu$ given $\sigma^2$. We can, however, avoid the necessity to implement \eqref{eq:7.2.1}. Note that \eqref{eq:7.1.5} implies that
\[
Z = \frac{\mu - \mu(x)}{(n + \tau_0^{-2})^{-1/2} \sigma} \mid \sigma^2, x_1, \ldots, x_n \sim N(0, 1),
\]
where $\mu(x)$ is given in \eqref{eq:7.1.7}. Because this distribution does not involve $\sigma^2$, the posterior distribution of $Z$ is independent of the posterior distribution of $\sigma^2$. Now if $X \sim \text{Gamma}(\alpha, \beta)$, then $Y = 2\beta X \sim \text{Gamma}(\alpha, 1/2) = \chi^2(2\alpha)$ (see Problem~\ref{exer:4.6.16} for the definition of the general chi-squared distribution) and so, from \eqref{eq:7.1.6},
\[
\frac{2\beta(x)}{\sigma^2} \mid x_1, \ldots, x_n \sim \chi^2(2\alpha_0 + n).
\]
where $\beta(x)$ is given in \eqref{eq:7.1.8}. Therefore (using Problem~\ref{exer:4.6.14}), as we are dividing an $N(0, 1)$ variable by the square root of an independent $\chi^2(2\alpha_0 + n)$ random variable divided by its degrees of freedom, we conclude that the posterior distribution of
\[
T = \frac{Z}{\sqrt{2\beta(x)/\sigma^2/(2\alpha_0 + n)}} = \frac{\mu - \mu(x)}{\sqrt{2\beta(x)/(2\alpha_0 + n)(n + \tau_0^{-2})^{-1}}}
\]
is $t(2\alpha_0 + n)$. Equivalently, we can say the posterior distribution of $\mu$ is the same as
\[
\mu(x) + \sqrt{\frac{1}{2\alpha_0 + n} \cdot \frac{2\beta(x)}{n + \tau_0^{-2}}} \, T,
\]
where $T \sim t(2\alpha_0 + n)$. By \eqref{eq:7.1.9}, \eqref{eq:7.1.10}, and \eqref{eq:7.1.11}, we have that the posterior distribution of $\mu$ converges to the distribution of
\[
\bar{x} + \sqrt{\frac{n-1}{2\alpha_0 + n} \cdot \frac{s}{\sqrt{n}}} \, T
\]
as $\tau_0 \to \infty$ and $\alpha_0, \beta_0 \to 0$.
\end{example}

In other cases, we cannot avoid the use of \eqref{eq:7.2.1} if we want the marginal posterior density of $\psi$. For example, suppose we are interested in the posterior distribution of the coefficient of variation $\psi = \sigma/\mu$ (we exclude the line given by $\mu = 0$ from the parameter space)
\[
\psi = \Psi(\mu, \sigma^2) = \sigma^{1/2}/\mu = (\sigma^2)^{1/2}/\mu.
\]
Then a complementing function to $\psi$ is given by
\[
\eta = H(\mu, \sigma^2) = \sigma^2
\]
and it can be shown (see Section~\ref{sec:7.5}) that
\[
|J| = |\mu|/\sigma = \psi^{-1}\sigma^{-2}.
\]
If we let $\pi_1(\mu \mid \sigma^2, x_1, \ldots, x_n)$ and $\pi(\sigma^2 \mid x_1, \ldots, x_n)$ denote the posterior densities of $\mu$ given $\sigma^2$, and the posterior density of $\sigma^2$, respectively, then, from \eqref{eq:7.2.1}, the marginal density of $\psi$ is given by
\begin{equation}
\label{eq:7.2.2}
\int_0^\infty \pi_1(\eta^{1/2}/\psi \mid \eta, x_1, \ldots, x_n) \, \pi(\eta \mid x_1, \ldots, x_n) \, \psi^{-1} \eta^{-2} \, \mathrm{d}\eta.
\end{equation}

Without writing this out (see Problem~\ref{exer:7.2.22}), we note that we are left with a rather messy integral to evaluate.

In some cases, integrals such as \eqref{eq:7.2.2} can be evaluated in closed form; in other cases, they cannot. While it is convenient to have a closed form for a density, often this is not necessary, as we can use Monte Carlo methods to approximate posterior probabilities and expectations of interest. We will return to this in Section~\ref{sec:7.3}. We should always remember that our goal, in implementing Bayesian inference methods, is not to find the marginal posterior densities of quantities of interest, but rather to have a computational algorithm that allows us to implement our inferences.

Under fairly weak conditions, it can be shown that the posterior distribution of $\psi$ converges, as the sample size increases, to a distribution degenerate at the true value. This is very satisfying, as it indicates that Bayesian inference methods are consistent.

\subsection{Estimation}
\label{ssec:7.2.1}

Suppose now that we want to calculate an estimate of a characteristic of interest $\psi$. We base this on the posterior distribution of this quantity. There are several different approaches to this problem.

Perhaps the most natural estimate is to obtain the posterior density (or probability function when relevant) of $\psi$ and use the posterior mode $\hat{\psi}$, i.e., the point where the posterior probability or density function of $\psi$ takes its maximum. In the discrete case, this is the value of $\psi$ with the greatest posterior probability; in the continuous case, it is the value that has the greatest amount of posterior probability in short intervals containing it.

To calculate the posterior mode, we need to maximize $\pi(\psi \mid s)$ as a function of $\psi$. Note that it is equivalent to maximize $m(s) \, \pi(\psi \mid s)$ so that we do not need to compute the inverse normalizing constant to implement this. In fact, we can conveniently choose to maximize any function that is a 1--1 increasing function of $\pi(\psi \mid s)$ and get the same answer. In general, $\pi(\psi \mid s)$ may not have a unique mode, but typically there is only one.

An alternative estimate is commonly used and has a natural interpretation. This is given by the posterior mean
\[
\expc(\psi \mid s),
\]
whenever this exists. When the posterior distribution of $\psi$ is symmetrical about its mode, and the expectation exists, then the posterior expectation is the same as the posterior mode; otherwise, these estimates will be different. If we want the estimate to reflect where the central mass of probability lies, then in cases where $\pi(\psi \mid s)$ is highly skewed, perhaps the mode is a better choice than the mean. We will see in Chapter~\ref{ch:8}, however, that there are other ways of justifying the posterior mean as an estimate.

We now consider some examples.

\begin{example}[Bernoulli Model]
\label{ex:7.2.2}
Suppose we observe a sample $x_1, \ldots, x_n$ from the Bernoulli$(\theta)$ distribution with $\theta \in [0, 1]$ unknown and we place a Beta$(\alpha, \beta)$ prior on $\theta$. In Example~\ref{ex:7.1.1}, we determined the posterior distribution of $\theta$ to be Beta$(n\bar{x} + \alpha, n(1 - \bar{x}) + \beta)$. Let us suppose that the characteristic of interest is $\psi = \theta$.

The posterior expectation of $\theta$ is given by
\begin{align*}
\expc(\theta \mid x_1, \ldots, x_n) &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1-\bar{x}) + \beta)} \int_0^1 \theta \cdot \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1-\bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
&= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1-\bar{x}) + \beta)} \int_0^1 \theta^{n\bar{x} + \alpha} (1 - \theta)^{n(1-\bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
&= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1-\bar{x}) + \beta)} \cdot \frac{\Gamma(n\bar{x} + \alpha + 1) \Gamma(n(1-\bar{x}) + \beta)}{\Gamma(n + \alpha + \beta + 1)} \\
&= \frac{n\bar{x} + \alpha}{n + \alpha + \beta}.
\end{align*}
When we have a uniform prior, i.e., $\alpha = \beta = 1$, the posterior expectation is given by
\[
\expc(\theta \mid x) = \frac{n\bar{x} + 1}{n + 2}.
\]

To determine the posterior mode, we need to maximize
\[
\ln(\theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1-\bar{x}) + \beta - 1}) = (n\bar{x} + \alpha - 1) \ln \theta + (n(1-\bar{x}) + \beta - 1) \ln(1 - \theta).
\]
This function has first derivative
\[
\frac{n\bar{x} + \alpha - 1}{\theta} - \frac{n(1-\bar{x}) + \beta - 1}{1 - \theta}
\]
and second derivative
\[
-\frac{n\bar{x} + \alpha - 1}{\theta^2} - \frac{n(1-\bar{x}) + \beta - 1}{(1 - \theta)^2}.
\]
Setting the first derivative equal to 0 and solving gives the solution
\[
\hat{\theta} = \frac{n\bar{x} + \alpha - 1}{n + \alpha + \beta - 2}.
\]
Now, if $\alpha > 1$, $\beta > 1$, we see that the second derivative is always negative, and so $\hat{\theta}$ is the unique posterior mode. The restriction on the choice of $\alpha > 1$, $\beta > 1$ implies that the prior has a mode in $(0, 1)$ rather than at 0 or 1. Note that when $\alpha = \beta = 1$, namely, when we put a uniform prior on $\theta$, the posterior mode is $\bar{x}$. This is the same as the maximum likelihood estimate (MLE).

The posterior is highly skewed whenever $n\bar{x} + \alpha$ and $n(1 - \bar{x}) + \beta$ are far apart (plot Beta densities to see this). Thus, in such a case, we might consider the posterior mode as a more sensible estimate of $\theta$. Note that when $n$ is large, the mode and the mean will be very close together and in fact very close to the MLE $\bar{x}$.
\end{example}

\begin{example}[Location Normal Model]
\label{ex:7.2.3}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma_0^2)$ distribution, where $\mu \in R^1$ is unknown and $\sigma_0^2$ is known, and we take the prior distribution on $\mu$ to be $N(\mu_0, \tau_0^2)$. Let us suppose, that the characteristic of interest is $\psi = \mu$.

In Example~\ref{ex:7.1.2} we showed that the posterior distribution of $\mu$ is given by the
\[
N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right), \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right)
\]
distribution. Because this distribution is symmetric about its mode, and the mean exists, the posterior mode and mean agree and equal
\[
\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right).
\]
This is a weighted average of the prior mean and the sample mean and lies between these two values.

When $n$ is large, we see that this estimator is approximately equal to the sample mean $\bar{x}$, which we also know to be the MLE for this situation. Furthermore, when we take the prior to be very diffuse, namely, when $\tau_0^2$ is very large, then again this estimator is close to the sample mean.

Also observe that the ratio of the sampling variance of $\bar{x}$ to the posterior variance of $\mu$ is
\[
\frac{\sigma_0^2/n}{\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}} = \frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2} \cdot \frac{\sigma_0^2}{n}
\]
is always greater than 1. The closer $\tau_0^2$ is to 0, the larger this ratio is. Furthermore, as $\tau_0^2 \to 0$, the Bayesian estimate converges to $\mu_0$.

If we are pretty confident that the population mean $\mu$ is close to the prior mean $\mu_0$, we will take $\tau_0^2$ small so that the bias in the Bayesian estimate will be small and its variance will be much smaller than the sampling variance of $\bar{x}$. In such a situation, the Bayesian estimator improves on accuracy over the sample mean. Of course, if we are not very confident that $\mu$ is close to the prior mean $\mu_0$, then we choose a large value for $\tau_0^2$, and the Bayesian estimator is basically the MLE.
\end{example}

\begin{example}[Multinomial Model]
\label{ex:7.2.4}
Suppose we have a sample $s_1, \ldots, s_n$ from the model discussed in Example~\ref{ex:7.1.3} and we place a Dirichlet$(\alpha_1, \alpha_2, \ldots, \alpha_k)$ distribution on $(\theta_1, \ldots, \theta_{k-1})$. The posterior distribution of $(\theta_1, \ldots, \theta_{k-1})$ is then
\[
\text{Dirichlet}(x_1 + \alpha_1, x_2 + \alpha_2, \ldots, x_k + \alpha_k),
\]
where $x_i$ is the number of responses in the $i$th category.

Now suppose we are interested in estimating $\psi = \theta_1 = \Psi(\theta_1, \ldots, \theta_k)$, the probability that a response is in the first category. It can be shown (see Problem~\ref{exer:7.2.25}) that, if $(\theta_1, \ldots, \theta_{k-1})$ is distributed Dirichlet$(\alpha_1, \alpha_2, \ldots, \alpha_k)$, then $\theta_i$ is distributed
\[
\text{Dirichlet}(\alpha_i, \alpha_{-i}) = \text{Beta}(\alpha_i, \alpha_{-i}),
\]
where $\alpha_{-i} = \alpha_1 + \alpha_2 + \cdots + \alpha_k - \alpha_i$. This result implies that the marginal posterior distribution of $\theta_1$ is
\[
\text{Beta}(x_1 + \alpha_1, x_2 + \cdots + x_k + \alpha_2 + \cdots + \alpha_k).
\]

Then, assuming that each $\alpha_i > 1$ and using the argument in Example~\ref{ex:7.2.2} and $x_1 + \cdots + x_k = n$, the marginal posterior mode of $\theta_1$ is
\[
\hat{\theta}_1 = \frac{x_1 + \alpha_1 - 1}{n - 2 + \alpha_1 + \cdots + \alpha_k}.
\]
When the prior is the uniform, namely, $\alpha_1 = \cdots = \alpha_k = 1$, then
\[
\hat{\theta}_1 = \frac{x_1}{n + k - 2}.
\]
As in Example~\ref{ex:7.2.2}, we compute the posterior expectation to be
\[
\expc(\theta_1 \mid x) = \frac{x_1 + \alpha_1}{n + \alpha_1 + \cdots + \alpha_k}.
\]
The posterior distribution is highly skewed whenever $x_1 + \alpha_1$ and $x_2 + \cdots + x_k + \alpha_2 + \cdots + \alpha_k$ are far apart.

From Problem~\ref{exer:7.2.26}, we have that the plug-in MLE of $\theta_1$ is $x_1/n$. When $n$ is large, the Bayesian estimates are close to this value, so there is no conflict between the estimates. Notice, however, that when the prior is uniform, then $\alpha_1 + \cdots + \alpha_k = k$, hence the plug-in MLE and the Bayesian estimates will be quite different when $k$ is large relative to $n$. In fact, the posterior mode will always be smaller than the plug-in MLE when $k > 2$ and $x_1 > 0$. This is a situation in which the Bayesian and frequentist approaches to inference differ.

At this point, the decision about which estimate to use is left with the practitioner, as theory does not seem to provide a clear answer. We can be comforted by the fact that the estimates will not differ by much in many contexts of practical importance.
\end{example}

\begin{example}[Location-Scale Normal Model]
\label{ex:7.2.5}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution, where $\mu \in R^1$ and $\sigma > 0$ are unknown, and we use the prior given in Example~\ref{ex:7.1.4}. Let us suppose that the characteristic of interest is $\psi = \mu = \Psi(\mu, \sigma^2)$.

In Example~\ref{ex:7.2.1}, we derived the marginal posterior distribution of $\mu$ to be the same as the distribution of
\[
\mu(x) + \sqrt{\frac{1}{2\alpha_0 + n} \cdot \frac{2\beta(x)}{n + \tau_0^{-2}}} \, T,
\]
where $T \sim t(n + 2\alpha_0)$. This is a $t(n + 2\alpha_0)$ distribution relocated to have its mode at $\mu(x)$ and rescaled by the factor
\[
\sqrt{\frac{1}{2\alpha_0 + n} \cdot \frac{2\beta(x)}{n + \tau_0^{-2}}}.
\]
So the marginal posterior mode of $\mu$ is
\[
\mu(x) = (n + \tau_0^{-2})^{-1}(\mu_0 \tau_0^{-2} + n\bar{x}).
\]
Because a $t$ distribution is symmetric about its mode, this is also the posterior mean of $\mu$, provided that $n + 2\alpha_0 > 1$, as a $t(\nu)$ distribution has a mean only when $\nu > 1$ (see Problem~\ref{exer:4.6.16}). This will always be the case as the sample size $n \geqslant 1$. Again, $\mu(x)$ is a weighted average of the prior mean $\mu_0$ and the sample average $\bar{x}$.

The marginal posterior mode and expectation can also be obtained for $\psi = \sigma = \Psi(\mu, \sigma^2)$. These computations are left to the reader (see Exercise~\ref{exer:7.2.4}).
\end{example}

One issue that we have not yet addressed is how we will assess the accuracy of Bayesian estimates. Naturally, this is based on the posterior distribution and how concentrated it is about the estimate being used. In the case of the posterior mean, this means that we compute the posterior variance as a measure of spread for the posterior distribution of $\psi$ about its mean. For the posterior mode, we will discuss this issue further in Section~\ref{ssec:7.2.3}.

\begin{example}[Posterior Variances]
\label{ex:7.2.6}
In Example~\ref{ex:7.2.2}, the posterior variance of $\theta$ is given by (see Exercise~\ref{exer:7.2.6})
\[
\frac{(n\bar{x} + \alpha)(n(1-\bar{x}) + \beta)}{(n + \alpha + \beta)^2 (n + \alpha + \beta + 1)}.
\]
Notice that the posterior variance converges to 0 as $n \to \infty$.

In Example~\ref{ex:7.2.3}, the posterior variance is given by $\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}$. Notice that the posterior variance converges to 0 as $\tau_0^2 \to 0$ and converges to $\sigma_0^2/n$, the sampling variance of $\bar{x}$, as $\tau_0^2 \to \infty$.

In Example~\ref{ex:7.2.4}, the posterior variance of $\theta_1$ is given by (see Exercise~\ref{exer:7.2.7})
\[
\frac{(x_1 + \alpha_1)(x_2 + \cdots + x_k + \alpha_2 + \cdots + \alpha_k)}{(n + \alpha_1 + \cdots + \alpha_k)^2 (n + \alpha_1 + \cdots + \alpha_k + 1)}.
\]
Notice that the posterior variance converges to 0 as $n \to \infty$.

In Example~\ref{ex:7.2.5}, the posterior variance of $\mu$ is given by (see Problem~\ref{exer:7.2.28})
\[
\frac{1}{n + \tau_0^{-2}} \cdot \frac{2\beta(x)}{n + 2\alpha_0} \cdot \frac{n + 2\alpha_0}{n + 2\alpha_0 - 2} = \frac{2\beta(x)}{n + \tau_0^{-2}} \cdot \frac{1}{n + 2\alpha_0 - 2},
\]
provided $n + 2\alpha_0 > 2$ because the variance of a $t(\nu)$ distribution is $\nu/(\nu - 2)$ when $\nu > 2$ (see Problem~\ref{exer:4.6.16}). Notice that the posterior variance goes to 0 as $n \to \infty$.
\end{example}

\subsection{Credible Intervals}
\label{ssec:7.2.2}

A credible interval, for a real-valued parameter $\psi$, is an interval $C(s) = [l(s), u(s)]$ that we believe will contain the true value of $\psi$. As with the sampling theory approach, we specify a probability $\gamma$ and then find an interval $C(s)$ satisfying
\begin{equation}
\label{eq:7.2.3}
\pi(\psi \in C(s) \mid s) = \pi(\psi : l(s) \leqslant \psi \leqslant u(s) \mid s) \geqslant \gamma.
\end{equation}
We then refer to $C(s)$ as a $\gamma$-credible interval for $\psi$.

Naturally, we try to find a $\gamma$-credible interval $C(s)$ so that $\pi(\psi \in C(s) \mid s)$ is as close to $\gamma$ as possible, and such that $C(s)$ is as short as possible. This leads to the consideration of highest posterior density (HPD) intervals, which are of the form
\[
C(s) = \{\psi : \pi(\psi \mid s) \geqslant c\},
\]
where $\pi(\psi \mid s)$ is the marginal posterior density of $\psi$ and where $c$ is chosen as large as possible so that \eqref{eq:7.2.3} is satisfied. In Figure~\ref{fig:7.2.1}, we have plotted an example of an HPD interval for a given value of $c$.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_2_1.pdf}
  \caption{An HPD interval $C(s) = [l(s), u(s)] = \{\psi : \pi(\psi \mid s) \geqslant c\}$.}
  \label{fig:7.2.1}
\end{figure}

Clearly, $C(s)$ contains the mode whenever $c \leqslant \max_\psi \pi(\psi \mid s)$. We can take the length of an HPD interval as a measure of the accuracy of the mode of $\pi(\psi \mid s)$ as an estimator of $\psi$. The length of a $0.95$-credible interval for $\psi$ will serve the same purpose as the margin of error does with confidence intervals.

Consider now some applications of the concept of credible interval.

\begin{example}[Location Normal Model]
\label{ex:7.2.7}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma_0^2)$ distribution, where $\mu \in R^1$ is unknown and $\sigma_0^2$ is known, and we take the prior distribution on $\mu$ to be $N(\mu_0, \tau_0^2)$. In Example~\ref{ex:7.1.2}, we showed that the posterior distribution of $\mu$ is given by the
\[
N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right), \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right)
\]
distribution. Since this distribution is symmetric about its mode (also mean) $\hat{\mu}$, a shortest $\gamma$-HPD interval is of the form
\[
\hat{\mu} \pm \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2} c,
\]
where $c$ is such that
\begin{align*}
&\pi\left(\hat{\mu} - \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2} c \leqslant \mu \leqslant \hat{\mu} + \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2} c \,\bigg|\, x_1, \ldots, x_n\right) \\
&= \pi\left(-c \leqslant \frac{\mu - \hat{\mu}}{\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2}} \leqslant c \,\bigg|\, x_1, \ldots, x_n\right) = \gamma.
\end{align*}
Since
\[
\frac{\mu - \hat{\mu}}{\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2}} \mid x_1, \ldots, x_n \sim N(0, 1),
\]
we have $\Phi(c) - \Phi(-c) = \gamma$, where $\Phi$ is the standard normal cumulative distribution function (cdf). This immediately implies that $c = z_{(1+\gamma)/2}$ and the $\gamma$-HPD interval is given by
\[
\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right) \pm \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2} z_{(1+\gamma)/2}.
\]

Note that as $\tau_0^2 \to \infty$, namely, as the prior becomes increasingly diffuse, this interval converges to the interval
\[
\bar{x} \pm \frac{\sigma_0}{\sqrt{n}} z_{(1+\gamma)/2},
\]
which is also the $\gamma$-confidence interval derived in Chapter~\ref{ch:6} for this problem. So under a diffuse normal prior, the Bayesian and frequentist approaches agree.
\end{example}

\begin{example}[Location-Scale Normal Model]
\label{ex:7.2.8}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution, where $\mu \in R^1$ and $\sigma > 0$ are unknown, and we use the prior given in Example~\ref{ex:7.1.4}. In Example~\ref{ex:7.2.1}, we derived the marginal posterior distribution of $\mu$ to be the same as
\[
\mu(x) + \sqrt{\frac{1}{2\alpha_0 + n} \cdot \frac{2\beta(x)}{n + \tau_0^{-2}}} \, T,
\]
where $T \sim t(2\alpha_0 + n)$. Because this distribution is symmetric about its mode $\mu(x)$, a $\gamma$-HPD interval is of the form
\[
\mu(x) \pm \sqrt{\frac{1}{2\alpha_0 + n} \cdot \frac{2\beta(x)}{n + \tau_0^{-2}}} \, c,
\]
where $c$ satisfies
\begin{align*}
&\pi\left(\mu(x) - \sqrt{\frac{1}{2\alpha_0 + n} \cdot \frac{2\beta(x)}{n + \tau_0^{-2}}} \, c \leqslant \mu \leqslant \mu(x) + \sqrt{\frac{1}{2\alpha_0 + n} \cdot \frac{2\beta(x)}{n + \tau_0^{-2}}} \, c \,\bigg|\, x_1, \ldots, x_n\right) \\
&= \pi\left(-c \leqslant \frac{\mu - \mu(x)}{\sqrt{2\beta(x)/(2\alpha_0 + n)(n + \tau_0^{-2})^{-1}}} \leqslant c \,\bigg|\, x_1, \ldots, x_n\right) \\
&= G_{2\alpha_0 + n}(c) - G_{2\alpha_0 + n}(-c) = \gamma.
\end{align*}
Here, $G_{2\alpha_0 + n}$ is the $t(2\alpha_0 + n)$ cdf, and therefore $c = t_{(1+\gamma)/2}(2\alpha_0 + n)$.

Using \eqref{eq:7.1.9}, \eqref{eq:7.1.10}, and \eqref{eq:7.1.11} we have that this interval converges to the interval
\[
\bar{x} \pm \sqrt{\frac{n-1}{2\alpha_0 + n}} \cdot \frac{s}{\sqrt{n}} \, t_{(1+\gamma)/2}(n + 2\alpha_0)
\]
as $\tau_0 \to \infty$ and $\alpha_0, \beta_0 \to 0$. Note that this is a little different from the $\gamma$-confidence interval we obtained for $\mu$ in Example~\ref{ex:6.3.8}, but when $\alpha_0/n$ is small, they are virtually identical.
\end{example}

In the examples we have considered so far, we could obtain closed-form expressions for the HPD intervals. In general, this is not the case. In such situations, we have to resort to numerical methods to obtain the HPD intervals, but we do not pursue this topic further here.

There are other methods of deriving credible intervals. For example, a common method of obtaining a $\gamma$-credible interval for $\psi$ is to take the interval $[\psi_l, \psi_r]$, where $\psi_l$ is a $(1 - \gamma)/2$ quantile for the posterior distribution of $\psi$ and $\psi_r$ is a $1 - (1 - \gamma)/2$ quantile for this distribution. Alternatively, we could form one-sided intervals. These credible intervals avoid the more extensive computations that may be needed for HPD intervals.

\subsection{Hypothesis Testing and Bayes Factors}
\label{ssec:7.2.3}

Suppose now that we want to assess the evidence in the observed data concerning the hypothesis $H_0 : \psi(\theta) = \psi_0$. It seems clear how we should assess this, namely, compute the posterior probability
\begin{equation}
\label{eq:7.2.4}
\pi(\psi(\theta) = \psi_0 \mid s).
\end{equation}
If this is small, then conclude that we have evidence against $H_0$. We will see further justification for this approach in Chapter~\ref{ch:8}.

\begin{example}
\label{ex:7.2.9}
Suppose we want to assess the evidence concerning whether or not $\theta \in A$. If we let $\psi = \indc_A(\theta)$, then we are assessing the hypothesis $H_0 : \psi = 1$ and
\[
\pi(\psi = 1 \mid s) = \pi(\theta \in A \mid s).
\]
So in this case, we simply compute the posterior probability that $\theta \in A$.
\end{example}

There can be a problem, however, with using \eqref{eq:7.2.4} to assess a hypothesis. For when the prior distribution of $\psi$ is absolutely continuous, then $\pi(\psi = \psi_0 \mid s) = 0$ for all data $s$. Therefore, we would always find evidence against $H_0$ no matter what is observed, which does not make sense. In general, if the value $\psi_0$ is assigned small prior probability, then it can happen that this value also has a small posterior probability no matter what data are observed.

To avoid this problem, there is an alternative approach to hypothesis assessment that is sometimes used. Recall that, if $\psi_0$ is a surprising value for the posterior distribution of $\psi$, then this is evidence that $H_0$ is false. The value $\psi_0$ is surprising whenever it occurs in a region of low probability for the posterior distribution of $\psi$. A region of low probability will correspond to a region where the posterior density $\pi(\psi \mid s)$ is relatively low. So, one possible method for assessing this is by computing the (Bayesian) P-value
\begin{equation}
\label{eq:7.2.5}
\pi(\psi : \pi(\psi \mid s) \leqslant \pi(\psi_0 \mid s) \mid s).
\end{equation}
Note that when $\pi(\psi \mid s)$ is unimodal, \eqref{eq:7.2.5} corresponds to computing a tail probability.

If the probability \eqref{eq:7.2.5} is small, then $\psi_0$ is surprising, at least with respect to our posterior beliefs. When we decide to reject $H_0$ whenever the P-value is less than $1 - \gamma$, then this approach is equivalent to computing a $\gamma$-HPD region for $\psi$ and rejecting $H_0$ whenever $\psi_0$ is not in the region.

\begin{example}[Example~\ref{ex:7.2.9} continued]
\label{ex:7.2.10}
Applying the P-value approach to this problem, we see that $\psi = \indc_A(\theta)$ has posterior given by the Bernoulli$(\pi(A \mid s))$ distribution. Therefore, $\pi(\psi \mid s)$ is defined by
\[
\pi(0 \mid s) = 1 - \pi(A \mid s) = \pi(A^c \mid s) \quad \text{and} \quad \pi(1 \mid s) = \pi(A \mid s).
\]
Now $\psi_0 = 1$, so
\[
\{\psi : \pi(\psi \mid s) \leqslant \pi(1 \mid s)\} = \{\psi : \indc_A(\psi \mid s) \leqslant \pi(A \mid s)\} = 
\begin{cases}
\{1\} & \text{if } \pi(A \mid s) \leqslant \pi(A^c \mid s), \\
\{0, 1\} & \text{if } \pi(A \mid s) \geqslant \pi(A^c \mid s).
\end{cases}
\]
Therefore, \eqref{eq:7.2.5} becomes
\[
\pi(\psi : \pi(\psi \mid s) \leqslant \pi(1 \mid s) \mid s) = 
\begin{cases}
\pi(A \mid s) & \text{if } \pi(A \mid s) \leqslant \pi(A^c \mid s), \\
\pi(A \mid s) + \pi(A^c \mid s) = 1 & \text{if } \pi(A \mid s) \geqslant \pi(A^c \mid s),
\end{cases}
\]
so again we have evidence against $H_0$ whenever $\pi(A \mid s)$ is small.
\end{example}

We see from Examples~\ref{ex:7.2.9} and~\ref{ex:7.2.10} that computing the P-value \eqref{eq:7.2.5} is essentially equivalent to using \eqref{eq:7.2.4}, whenever the marginal parameter $\psi$ takes only two values. This is not the case whenever $\psi$ takes more than two values, however, and the statistician has to decide which method is more appropriate in such a context.

As previously noted, when the prior distribution of $\psi$ is absolutely continuous, then \eqref{eq:7.2.4} is always 0, no matter what data are observed. As the following example illustrates, there is also a difficulty with using \eqref{eq:7.2.5} in such a situation.

\begin{example}
\label{ex:7.2.11}
Suppose that the posterior distribution of $\theta$ is Beta$(2, 1)$, i.e., $\pi(\theta \mid s) = 2\theta$ when $0 \leqslant \theta \leqslant 1$, and we want to assess $H_0 : \theta = 3/4$. Then $\pi(\theta \mid s) \leqslant \pi(3/4 \mid s)$ if and only if $\theta \leqslant 3/4$, and \eqref{eq:7.2.5} is given by
\[
\int_0^{3/4} 2\theta \, \mathrm{d}\theta = 9/16.
\]

On the other hand, suppose we make a 1--1 transformation to $\psi = \theta^2$, so that the hypothesis is now $H_0 : \psi = 9/16$. The posterior distribution of $\psi$ is Beta$(1, 1)$. Since the posterior density of $\psi$ is constant, this implies that the posterior density at every possible value is less than or equal to the posterior density evaluated at $9/16$. Therefore, \eqref{eq:7.2.5} equals 1, and we would never find evidence against $H_0$ using this parameterization.

This example shows that our assessment of $H_0$ via \eqref{eq:7.2.5} depends on the parameterization used, which does not seem appropriate.
\end{example}

The difficulty in using \eqref{eq:7.2.5}, as demonstrated in Example~\ref{ex:7.2.11}, only occurs with continuous posterior distributions. So, to avoid this problem, it is often recommended that the hypothesis to be tested always be assigned a positive prior probability. As demonstrated in Example~\ref{ex:7.2.10}, the approach via \eqref{eq:7.2.5} is then essentially equivalent to using \eqref{eq:7.2.4} to assess $H_0$.

In problems where it seems natural to use continuous priors, this is accomplished by taking the prior $\pi$ to be a mixture of probability distributions, as discussed in Section~\ref{ssec:2.5.4}, namely, the prior distribution equals
\[
\pi = p\pi_1 + (1 - p)\pi_2,
\]
where $\pi_1(\{\psi_0\}) = 1$ and $\pi_2(\{\psi_0\}) = 0$, i.e., $\pi_1$ is degenerate at $\psi_0$ and $\pi_2$ is continuous at $\psi_0$. Then
\[
\pi(\psi = \psi_0) = p\pi_1(\psi = \psi_0) + (1 - p)\pi_2(\psi = \psi_0) = p \cdot 1 + 0 = p
\]
is the prior probability that $H_0$ is true.

The prior predictive for the data $s$ is then given by
\[
m(s) = pm_1(s) + (1 - p)m_2(s),
\]
where $m_i$ is the prior predictive obtained via prior $\pi_i$ (see Problem~\ref{exer:7.2.34}). This implies (see Problem~\ref{exer:7.2.34}) that the posterior probability measure for $\psi$ when using the prior $\pi$ is
\begin{equation}
\label{eq:7.2.6}
\pi(A \mid s) = \frac{pm_1(s)}{pm_1(s) + (1-p)m_2(s)} \pi_1(A \mid s) + \frac{(1-p)m_2(s)}{pm_1(s) + (1-p)m_2(s)} \pi_2(A \mid s),
\end{equation}
where $\pi_i(\cdot \mid s)$ is the posterior measure obtained via the prior $\pi_i$. Note that this a mixture of the posterior probability measures $\pi_1(\cdot \mid s)$ and $\pi_2(\cdot \mid s)$ with mixture probabilities
\[
\frac{pm_1(s)}{pm_1(s) + (1-p)m_2(s)} \quad \text{and} \quad \frac{(1-p)m_2(s)}{pm_1(s) + (1-p)m_2(s)}.
\]
Now $\pi_1(\cdot \mid s)$ is degenerate at $\psi_0$ (if the prior is degenerate at a point then the posterior must be degenerate at that point too) and $\pi_2(\cdot \mid s)$ is continuous at $\psi_0$. Therefore,
\begin{equation}
\label{eq:7.2.7}
\pi(\psi = \psi_0 \mid s) = \frac{pm_1(s)}{pm_1(s) + (1-p)m_2(s)},
\end{equation}
and we use this probability to assess $H_0$.

The following example illustrates this approach.

\begin{example}[Location Normal Model]
\label{ex:7.2.12}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma_0^2)$ distribution, where $\mu \in R^1$ is unknown and $\sigma_0^2$ is known, and we want to assess the hypothesis $H_0 : \mu = \mu_0$. As in Example~\ref{ex:7.1.2}, we will take the prior for $\mu$ to be an $N(\mu_0, \tau_0^2)$ distribution. Given that we are assessing whether or not $\mu = \mu_0$, it seems reasonable to place the mode of the prior at the hypothesized value. The choice of the hyperparameter $\tau_0^2$ then reflects the degree of our prior belief that $H_0$ is true. We let $\pi_2$ denote this prior probability measure, i.e., $\pi_2$ is the $N(\mu_0, \tau_0^2)$ probability measure.

If we use $\pi_2$ as our prior, then, as shown in Example~\ref{ex:7.1.2}, the posterior distribution of $\mu$ is absolutely continuous. This implies that \eqref{eq:7.2.4} is 0. So, following the preceding discussion, we consider instead the prior $\pi = p\pi_1 + (1 - p)\pi_2$ obtained by mixing $\pi_2$ with a probability measure $\pi_1$ degenerate at $\mu_0$. Then $\pi(\mu = \mu_0) = p$. As shown in Example~\ref{ex:7.1.2}, under $\pi_2$ the posterior distribution of $\mu$ is
\[
N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right), \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right),
\]
while the posterior under $\pi_1$ is the distribution degenerate at $\mu_0$. We now need to evaluate \eqref{eq:7.2.7}, and we will do this in Example~\ref{ex:7.2.13}.
\end{example}

\subsubsection*{Bayes Factors}

Bayes factors comprise another method of hypothesis assessment and are defined in terms of odds.

\begin{definition}
\label{def:7.2.1}
In a probability model with sample space $S$ and probability measure $P$, the odds in favor of event $A \subset S$ is defined to be $P(A)/P(A^c)$, namely, the ratio of the probability of $A$ to the probability of $A^c$.
\end{definition}

Obviously, large values of the odds in favor of $A$ indicate a strong belief that $A$ is true. Odds represent another way of presenting probabilities that are convenient in certain contexts, e.g., horse racing. Bayes factors compare posterior odds with prior odds.

\begin{definition}
\label{def:7.2.2}
The Bayes factor $BF_{H_0}$ in favor of the hypothesis $H_0 : \psi(\theta) = \psi_0$ is defined, whenever the prior probability of $H_0$ is not 0 or 1, to be the ratio of the posterior odds in favor of $H_0$ to the prior odds in favor of $H_0$, or
\begin{equation}
\label{eq:7.2.8}
BF_{H_0} = \frac{\pi(\psi = \psi_0 \mid s)}{1 - \pi(\psi = \psi_0 \mid s)} \bigg/ \frac{\pi(\psi = \psi_0)}{1 - \pi(\psi = \psi_0)}.
\end{equation}
\end{definition}

So the Bayes factor in favor of $H_0$ is measuring the degree to which the data have changed the odds in favor of the hypothesis. If $BF_{H_0}$ is small, then the data are providing evidence against $H_0$ and evidence in favor of $H_0$ when $BF_{H_0}$ is large.

There is a relationship between the posterior probability of $H_0$ being true and $BF_{H_0}$. From \eqref{eq:7.2.8}, we obtain
\begin{equation}
\label{eq:7.2.9}
\pi(\psi = \psi_0 \mid s) = \frac{r \cdot BF_{H_0}}{1 + r \cdot BF_{H_0}},
\end{equation}
where
\[
r = \frac{\pi(\psi = \psi_0)}{1 - \pi(\psi = \psi_0)}
\]
is the prior odds in favor of $H_0$. So, when $BF_{H_0}$ is small, then $\pi(\psi = \psi_0 \mid s)$ is small and conversely.

One reason for using Bayes factors to assess hypotheses is the following result. This establishes a connection with likelihood ratios.

\begin{theorem}
\label{thm:7.2.1}
If the prior $\pi$ is a mixture $p\pi_1 + (1 - p)\pi_2$, where $\pi_1(A) = 1$, $\pi_2(A^C) = 1$, and we want to assess the hypothesis $H_0 : \theta \in A$, then
\[
BF_{H_0} = m_1(s)/m_2(s),
\]
where $m_i$ is the prior predictive of the data under $\pi_i$.
\end{theorem}

\begin{proof}
Recall that, if a prior concentrates all of its probability on a set, then the posterior concentrates all of its probability on this set, too. Then using \eqref{eq:7.2.6}, we have
\[
BF_{H_0} = \frac{\pi(\theta \in A \mid s)}{1 - \pi(\theta \in A \mid s)} \bigg/ \frac{\pi(\theta \in A)}{1 - \pi(\theta \in A)} = \frac{pm_1(s)}{(1-p)m_2(s)} \bigg/ \frac{p}{1-p} = \frac{m_1(s)}{m_2(s)}.
\]
\end{proof}

Interestingly, Theorem~\ref{thm:7.2.1} indicates that the Bayes factor is independent of $p$. We note, however, that it is not immediately clear how to interpret the value of $BF_{H_0}$. In particular, how large does $BF_{H_0}$ have to be to provide strong evidence in favor of $H_0$? One approach to this problem is to use \eqref{eq:7.2.9}, as this gives the posterior probability of $H_0$, which is directly interpretable. So we can calibrate the Bayes factor. Note, however, that this requires the specification of $p$.

\begin{example}[Location Normal Model (Example~\ref{ex:7.2.12} continued)]
\label{ex:7.2.13}
We now compute the prior predictive under $\pi_2$. We have that the joint density of $(x_1, \ldots, x_n, \mu)$ given $\mu$ equals
\[
(2\pi\sigma_0^2)^{-n/2} \exp\left\{-\frac{n-1}{2\sigma_0^2}s^2\right\} \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right\}
\]
and so
\begin{align*}
m_2(x_1, \ldots, x_n) &= (2\pi\sigma_0^2)^{-n/2} \exp\left\{-\frac{n-1}{2\sigma_0^2}s^2\right\} \int_{-\infty}^{\infty} \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right\} \\
&\qquad \times (2\pi\tau_0^2)^{-1/2} \exp\left\{-\frac{1}{2\tau_0^2}(\mu - \mu_0)^2\right\} \, \mathrm{d}\mu \\
&= (2\pi\sigma_0^2)^{-n/2} \exp\left\{-\frac{n-1}{2\sigma_0^2}s^2\right\} \\
&\qquad \times \frac{1}{\sqrt{2\pi\tau_0^2}} \int_{-\infty}^{\infty} \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right\} \exp\left\{-\frac{1}{2\tau_0^2}(\mu - \mu_0)^2\right\} \, \mathrm{d}\mu.
\end{align*}
Then using \eqref{eq:7.1.2}, we have
\begin{align}
&\frac{1}{\sqrt{2\pi\tau_0^2}} \int_{-\infty}^{\infty} \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right\} \exp\left\{-\frac{1}{2\tau_0^2}(\mu - \mu_0)^2\right\} \, \mathrm{d}\mu \notag \\
&= \frac{1}{\sqrt{2\pi\tau_0^2}} \exp\left\{-\frac{1}{2}\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right)^2\right\} \notag \\
&\qquad \times \exp\left\{\frac{1}{2} \cdot \frac{(\mu_0 - \bar{x})^2}{\tau_0^2 + \sigma_0^2/n}\right\} \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2}. \label{eq:7.2.10}
\end{align}
Therefore,
\begin{align*}
m_2(x_1, \ldots, x_n) &= (2\pi\sigma_0^2)^{-n/2} \exp\left\{-\frac{n-1}{2\sigma_0^2}s^2\right\} \frac{1}{\sqrt{2\pi\tau_0^2}} \\
&\qquad \times \exp\left\{-\frac{1}{2}\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2}\bar{x}\right)^2\right\} \\
&\qquad \times \exp\left\{\frac{1}{2} \cdot \frac{(\mu_0 - \bar{x})^2}{\tau_0^2 + \sigma_0^2/n}\right\} \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2}.
\end{align*}

Because $\pi_1$ is degenerate at $\mu_0$, it is immediate that the prior predictive under $\pi_1$ is given by
\[
m_1(x_1, \ldots, x_n) = (2\pi\sigma_0^2)^{-n/2} \exp\left\{-\frac{n-1}{2\sigma_0^2}s^2\right\} \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu_0)^2\right\}.
\]
Therefore, $BF_{H_0}$ equals
\[
\exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu_0)^2\right\}
\]
divided by \eqref{eq:7.2.10}.

For example, suppose that $\mu_0 = 0$, $\sigma_0^2 = 2$, $\tau_0^2 = 1$, $n = 10$, and $\bar{x} = 0.2$. Then
\[
\exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu_0)^2\right\} = \exp\left\{-\frac{10}{2}(0.2)^2\right\} = 0.81873,
\]
while \eqref{eq:7.2.10} equals
\[
\frac{1}{\sqrt{2\pi}} \exp\left\{-\frac{1}{2}\left(\frac{1}{2} + 10\right)^{-1}(10 \cdot 0.2)^2\right\} \exp\left\{\frac{(10 \cdot 0.2)^2}{2 \cdot 10}\right\} \left(\frac{1}{2} + 10\right)^{-1/2} = 0.21615.
\]
So
\[
BF_{H_0} = \frac{0.81873}{0.21615} = 3.7878,
\]
which gives some evidence in favor of $H_0 : \mu = 0$. If we suppose that $p = 1/2$, so that we are completely indifferent between $H_0$ being true and not being true, then $r = 1$ and \eqref{eq:7.2.9} gives
\[
\pi(\mu = \mu_0 \mid x_1, \ldots, x_n) = \frac{3.7878}{1 + 3.7878} = 0.79114,
\]
indicating a large degree of support for $H_0$.
\end{example}

\subsection{Prediction}
\label{ssec:7.2.4}

Prediction problems arise when we have an unobserved response value $t$ in a sample space $T$ and observed response $s \in S$. Furthermore, we have the statistical model $\{\prb_\theta : \theta \in \Omega\}$ for $s$ and the conditional statistical model $\{Q_{\theta,s} : \theta \in \Omega\}$ for $t$ given $s$. We assume that both models have the same true value of $\theta$. The objective is to construct a prediction $\hat{t}(s) \in T$ of the unobserved value $t$ based on the observed data $s$. The value of $t$ could be unknown simply because it represents a future outcome.

If we denote the conditional density or probability function (whichever is relevant) of $t$ by $q_{\theta,s}$, the joint distribution of $(s, t, \theta)$ is given by
\[
q_{\theta,s}(t) f_\theta(s) \pi(\theta).
\]
Then, once we have observed $s$ (assume here that the distributions of $\theta$ and $t$ are absolutely continuous; if not, we replace integrals by sums), the conditional density of $(t, \theta)$, given $s$, is
\[
\frac{q_{\theta,s}(t) f_\theta(s) \pi(\theta)}{\int_\Omega \int_T q_{\theta,s}(t) f_\theta(s) \pi(\theta) \, \mathrm{d}t \, \mathrm{d}\theta} = \frac{q_{\theta,s}(t) f_\theta(s) \pi(\theta)}{\int_\Omega f_\theta(s) \pi(\theta) \, \mathrm{d}\theta} = \frac{q_{\theta,s}(t) f_\theta(s) \pi(\theta)}{m(s)}.
\]
Then the marginal posterior distribution of $t$, known as the posterior predictive of $t$, is
\[
q(t \mid s) = \int_\Omega \frac{q_{\theta,s}(t) f_\theta(s) \pi(\theta)}{m(s)} \, \mathrm{d}\theta = \int_\Omega q_{\theta,s}(t) \pi(\theta \mid s) \, \mathrm{d}\theta.
\]
Notice that the posterior predictive of $t$ is obtained by averaging the conditional density of $t$ given $(s, \theta)$ with respect to the posterior distribution of $\theta$.

Now that we have obtained the posterior predictive distribution of $t$, we can use it to select an estimate of the unobserved value. Again, we could choose the posterior mode $\hat{t}$ or the posterior expectation $\expc(t \mid x) = \int_T t \, q(t \mid s) \, \mathrm{d}t$ as our prediction, whichever is deemed most relevant.

\begin{example}[Bernoulli Model]
\label{ex:7.2.14}
Suppose we want to predict the next independent outcome $X_{n+1}$ having observed a sample $x_1, \ldots, x_n$ from the Bernoulli$(\theta)$ and Beta$(\alpha, \beta)$. Here, the future observation is independent of the observed data. The posterior predictive probability function of $X_{n+1}$ at $t$ is then given by
\begin{align*}
q(t \mid x_1, \ldots, x_n) &= \int_0^1 \theta^t (1 - \theta)^{1-t} \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1-\bar{x}) + \beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1-\bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
&= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1-\bar{x}) + \beta)} \int_0^1 \theta^{n\bar{x} + t + \alpha - 1} (1 - \theta)^{n(1-\bar{x}) + (1-t) + \beta - 1} \, \mathrm{d}\theta \\
&= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1-\bar{x}) + \beta)} \cdot \frac{\Gamma(n\bar{x} + t + \alpha) \Gamma(n(1-\bar{x}) + (1-t) + \beta)}{\Gamma(n + \alpha + \beta + 1)} \\
&= \begin{cases}
\dfrac{n\bar{x} + \alpha}{n + \alpha + \beta} & t = 1, \\[10pt]
\dfrac{n(1-\bar{x}) + \beta}{n + \alpha + \beta} & t = 0,
\end{cases}
\end{align*}
which is the probability function of a Bernoulli$\left(\dfrac{n\bar{x} + \alpha}{n + \alpha + \beta}\right)$ distribution.

Using the posterior mode as the predictor, i.e., maximizing $q(t \mid x_1, \ldots, x_n)$ for $t$, leads to the prediction
\[
\hat{t} = \begin{cases}
1 & \text{if } \dfrac{n\bar{x} + \alpha}{n + \alpha + \beta} \geqslant \dfrac{n(1-\bar{x}) + \beta}{n + \alpha + \beta}, \\[10pt]
0 & \text{otherwise}.
\end{cases}
\]
The posterior expectation predictor is given by
\[
\expc(t \mid x_1, \ldots, x_n) = \frac{n\bar{x} + \alpha}{n + \alpha + \beta}.
\]
Note that the posterior mode takes a value in $\{0, 1\}$, and the future $X_{n+1}$ will be in this set, too. The posterior mean can be any value in $[0, 1]$.
\end{example}

\begin{example}[Location Normal Model]
\label{ex:7.2.15}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma_0^2)$ distribution, where $\mu \in R^1$ is unknown and $\sigma_0^2$ is known, and we use the prior given in Example~\ref{ex:7.1.2}. Suppose we want to predict a future observation $X_{n+1}$, but this time $X_{n+1}$ is from the
\begin{equation}
\label{eq:7.2.11}
N\left(\bar{x}, \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} + \sigma_0^2\right)
\end{equation}
distribution. So, in this case, the future observation is not independent of the observed data, but it is independent of the parameter. A simple calculation (see Exercise~\ref{exer:7.2.9}) shows that \eqref{eq:7.2.11} is the posterior predictive distribution of $t$, and so we would predict $t$ by $\bar{x}$, as this is both the posterior mode and mean.
\end{example}

We can also construct a $\gamma$-prediction region $C(s)$ for a future value $t$ from the model $\{q_{\theta,s} : \theta \in \Omega\}$. A $\gamma$-prediction region for $t$ satisfies $Q(C(s) \mid s) \geqslant \gamma$, where $Q(\cdot \mid s)$ is the posterior predictive measure for $t$. One approach to constructing $C(s)$ is to apply the HPD concept to $q(t \mid s)$. We illustrate this via several examples.

\begin{example}[Bernoulli Model (Example~\ref{ex:7.2.14} continued)]
\label{ex:7.2.16}
Suppose we want a $\gamma$-prediction region for a future value $X_{n+1}$. In Example~\ref{ex:7.2.14}, we derived the posterior predictive distribution of $X_{n+1}$ to be
\[
\text{Bernoulli}\left(\frac{n\bar{x} + \alpha}{n + \alpha + \beta}\right).
\]
Accordingly, a $\gamma$-prediction region for $t$, derived via the HPD concept, is given by
\[
C(x_1, \ldots, x_n) = \begin{cases}
\{0, 1\} & \text{if } \max\left(\dfrac{n\bar{x} + \alpha}{n + \alpha + \beta}, \dfrac{n(1-\bar{x}) + \beta}{n + \alpha + \beta}\right) < \gamma, \\[10pt]
\{1\} & \text{if } \max\left(\dfrac{n\bar{x} + \alpha}{n + \alpha + \beta}, \dfrac{n(1-\bar{x}) + \beta}{n + \alpha + \beta}\right) = \dfrac{n\bar{x} + \alpha}{n + \alpha + \beta} \geqslant \gamma, \\[10pt]
\{0\} & \text{if } \max\left(\dfrac{n\bar{x} + \alpha}{n + \alpha + \beta}, \dfrac{n(1-\bar{x}) + \beta}{n + \alpha + \beta}\right) = \dfrac{n(1-\bar{x}) + \beta}{n + \alpha + \beta} \geqslant \gamma.
\end{cases}
\]
We see that this predictive region contains just the mode or encompasses all possible values for $X_{n+1}$. In the latter case, this is not an informative inference.
\end{example}

\begin{example}[Location Normal Model (Example~\ref{ex:7.2.15} continued)]
\label{ex:7.2.17}
Suppose we want a $\gamma$-prediction interval for a future observation $X_{n+1}$ from a
\[
N\left(\bar{x}, \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} + \sigma_0^2\right)
\]
distribution. As this is also the posterior predictive distribution of $X_{n+1}$ and is symmetric about $\bar{x}$, a $\gamma$-prediction interval for $X_{n+1}$, derived via the HPD concept, is given by
\[
\bar{x} \pm \left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} + \sigma_0^2\right)^{1/2} z_{(1+\gamma)/2}.
\]
\end{example}

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:7.2}}

\begin{itemize}
\item Based on the posterior distribution of a parameter, we can obtain estimates of the parameter (posterior modes or means), construct credible intervals for the parameter (HPD intervals), and assess hypotheses about the parameter (posterior probability of the hypothesis, Bayesian P-values, Bayes factors).

\item A new type of inference was discussed in this section, namely, prediction problems where we are concerned with predicting an unobserved value from a sampling model.
\end{itemize}

\subsection*{EXERCISES}

\begin{exercise}
\label{exer:7.2.1}
For the model discussed in Example~\ref{ex:7.1.1}, derive the posterior mean of $\theta^m$, where $m > 0$.
\end{exercise}

\begin{solution}
Recall that for the model discussed in Example \ref{ex:7.1.1}, the posterior distribution of $\theta$ was $\text{Beta}(n\bar{x} + \alpha, n(1 - \bar{x}) + \beta)$. The posterior density is then given by
\[
    \pi_{\theta \mid x_1, \ldots, x_n} = \frac{\Gamma(\alpha + \beta + n)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1}.
\]
The posterior mean is given by
\begin{align*}
    \expc(\theta^m \mid x_1, \ldots, x_n) &= \int_0^1 \frac{\Gamma(\alpha + \beta + n)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \theta^{n\bar{x} + \alpha + m - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(\alpha + \beta + n) \Gamma(n\bar{x} + \alpha + m)}{\Gamma(n\bar{x} + \alpha) \Gamma(\alpha + \beta + n + m)}.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:7.2.2}
For the model discussed in Example~\ref{ex:7.1.2}, determine the posterior distribution of the third quartile $\mu + \sigma_0 z_{0.75}$. Determine the posterior mode and the posterior expectation of $\psi$.
\end{exercise}

\begin{solution}
Recall that for the model discussed in Example \ref{ex:7.1.2} the posterior distribution of $\mu$ is
\[
    N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right), \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right).
\]
By Exercise \ref{exer:2.6.3}, the posterior distribution of the third quartile $\Psi = \mu + \sigma_0 z_{0.75}$ is
\[
    N\left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right) + \sigma_0 z_{0.75}, \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1}\right).
\]
Since the normal distribution is symmetric about its mode and the mean exists, the posterior mode and mean agree and given by
\[
    \hat{\psi} = \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right) + \sigma_0 z_{0.75}.
\]
\end{solution}

\begin{exercise}
\label{exer:7.2.3}
In Example~\ref{ex:7.2.1}, determine the posterior expectation and mode of $\sigma = \sigma^{1/2}$.
\end{exercise}

\begin{solution}
Recall that the posterior distribution of $\sigma^2$ in Example \ref{ex:7.2.1} is inverse $\text{Gamma}(\alpha_0 + n/2, \beta_x)$, where $\beta_x$ is given by (7.1.8). The posterior mean is then given by $\expc(1/\sigma^2 \mid x_1, \ldots, x_n) = (\alpha_0 + n/2) / \beta_x$. To find the posterior mode we need only maximize $\ln(y^{\alpha_0 + n/2 - 1} \exp(-\beta_x y)) = (\alpha_0 + n/2 - 1) \ln y - \beta_x y$. This has first derivative given by $(\alpha_0 + n/2 - 1)/y - \beta_x$ and second derivative $-(\alpha_0 + n/2 - 1)/y^2$. Setting the first derivative equal to 0 and solving gives the solution $1/\hat{\sigma}^2 = (\alpha_0 + n/2 - 1)/\beta_x$. The second derivative at this value is negative so this is the unique mode.
\end{solution}

\begin{exercise}
\label{exer:7.2.4}
In Example~\ref{ex:7.2.1}, determine the posterior expectation and mode of $\sigma^2$. (Hint: You will need the posterior density of $\sigma^2$ to determine the mode.)
\end{exercise}

\begin{solution}
Recall that the posterior distribution of $\sigma^2$ in Example \ref{ex:7.2.1} is inverse $\text{Gamma}(\alpha_0 + n/2, \beta_x)$, where $\beta_x$ is given by (7.1.8). The posterior mean is then given by
\begin{align*}
    \expc(\sigma^2 \mid x_1, \ldots, x_n) &= \int_0^\infty \frac{1}{y} \frac{\beta_x^{\alpha_0 + n/2}}{\Gamma(\alpha_0 + n/2)} y^{\alpha_0 + n/2 - 1} e^{-\beta_x y} \, \mathrm{d}y \\
    &= \frac{\beta_x^{\alpha_0 + n/2}}{\Gamma(\alpha_0 + n/2)} \int_0^\infty y^{\alpha_0 + n/2 - 2} e^{-\beta_x y} \, \mathrm{d}y \\
    &= \frac{\beta_x^{\alpha_0 + n/2}}{\Gamma(\alpha_0 + n/2)} \cdot \frac{\Gamma(\alpha_0 + n/2 - 1)}{\beta_x^{\alpha_0 + n/2 - 1}} \int_0^\infty \frac{1}{\Gamma(\alpha_0 + n/2 - 1)} y^{\alpha_0 + n/2 - 2} e^{-y} \, \mathrm{d}y \\
    &= \frac{\beta_x}{\alpha_0 + n/2 - 1}.
\end{align*}
By Theorem \ref{thm:2.6.2} the posterior density of $\sigma^2$ is given by $\pi(\sigma^2 \mid x_1, \ldots, x_n) = (\Gamma(\alpha_0 + n/2))^{-1} (\beta_x)^{\alpha_0 + n/2} (\sigma^2)^{-(\alpha_0 + n/2 + 1)} \exp(-\beta_x / \sigma^2)$. Then to find the posterior mode we need only maximize $\ln(y^{-(\alpha_0 + n/2 + 1)} \exp(-\beta_x / y)) = -(\alpha_0 + n/2 + 1) \ln y - \beta_x / y$. This has first derivative given by $-(\alpha_0 + n/2 + 1)/y + \beta_x / y^2$ and second derivative $(\alpha_0 + n/2 + 1)/y^2 - 2\beta_x / y^3$. Setting the first derivative equal to 0 and solving gives the solution $\hat{\sigma}^2 = \beta_x / (\alpha_0 + n/2 + 1)$. The second derivative at this value is $(\alpha_0 + n/2 + 1)^2 / \beta_x^2 - 2(\alpha_0 + n/2 + 1)^3 / \beta_x^2 = (\alpha_0 + n/2 + 1)^2 (-1 - 2\alpha_0 - n) / \beta_x^2 < 0$, so this is the unique mode.
\end{solution}

\begin{exercise}
\label{exer:7.2.5}
Carry out the calculations to verify the posterior mode and posterior expectation of $\theta_1$ in Example~\ref{ex:7.2.4}.
\end{exercise}

\begin{solution}
Recall that in Example \ref{ex:7.2.4} the marginal posterior distribution of $\theta_1$ is $\text{Beta}(f_1 + \alpha_1, f_2 + \cdots + f_k + \alpha_2 + \cdots + \alpha_k)$. The posterior mean is then given by
\begin{align*}
    \expc(\theta_1 \mid x_1, \ldots, x_n) &= \int_0^1 \theta_1 \frac{\Gamma(n + \sum_{i=1}^{k} \alpha_i)}{\Gamma(f_1 + \alpha_1) \Gamma(\sum_{i=2}^{k} (f_i + \alpha_i))} (\theta_1)^{f_1 + \alpha_1 - 1} (1 - \theta_1)^{\sum_{i=2}^{k} (f_i + \alpha_i) - 1} \, \mathrm{d}\theta_1 \\
    &= \frac{\Gamma(n + \sum_{i=1}^{k} \alpha_i)}{\Gamma(f_1 + \alpha_1) \Gamma(\sum_{i=2}^{k} (f_i + \alpha_i))} \int_0^1 (\theta_1)^{f_1 + \alpha_1} (1 - \theta_1)^{\sum_{i=2}^{k} (f_i + \alpha_i) - 1} \, \mathrm{d}\theta_1 \\
    &= \frac{\Gamma(n + \sum_{i=1}^{k} \alpha_i) \Gamma(f_1 + \alpha_1 + 1)}{\Gamma(f_1 + \alpha_1) \Gamma(n + \sum_{i=1}^{k} \alpha_i + 1)} = \frac{f_1 + \alpha_1}{n + \sum_{i=1}^{k} \alpha_i}.
\end{align*}
To find the posterior mode we need to maximize
\[
    \ln\left((\theta_1)^{f_1 + \alpha_1 - 1} (1 - \theta_1)^{\sum_{i=2}^{k} (f_i + \alpha_i) - 1}\right) = (f_1 + \alpha_1 - 1) \ln(\theta_1) + \left(\sum_{i=2}^{k} (f_i + \alpha_i) - 1\right) \ln(1 - \theta_1).
\]
This has first derivative given by $(f_1 + \alpha_1 - 1)/\theta_1 - (\sum_{i=2}^{k} (f_i + \alpha_i) - 1)/(1 - \theta_1)$ and second derivative $-(f_1 + \alpha_1 - 1)/\theta_1^2 - (\sum_{i=2}^{k} (f_i + \alpha_i) - 1)/(1 - \theta_1)^2$. Note that this is always negative when $\alpha_i \geqslant 1$. Setting the first derivative equal to 0 and solving gives the solution $\hat{\theta}_1 = (f_1 + \alpha_1 - 1)/(n + \sum_{i=1}^{k} \alpha_i - 2)$. Since the second derivative at this value is negative, $\hat{\theta}_1$ is the unique posterior mode.
\end{solution}

\begin{exercise}
\label{exer:7.2.6}
Establish that the variance of the $\theta$ in Example~\ref{ex:7.2.2} is as given in Example~\ref{ex:7.2.6}. Prove that this goes to 0 as $n \to \infty$.
\end{exercise}

\begin{solution}
Recall that the posterior distribution of $\theta$ in Example \ref{ex:7.2.2} is $\text{Beta}(n\bar{x} + \alpha, n(1 - \bar{x}) + \beta)$. To find the posterior variance we need only to find the second moment as follows.
\begin{align*}
    \expc(\theta^2 \mid x_1, \ldots, x_n) &= \int_0^1 \theta^2 \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \int_0^1 \theta^{n\bar{x} + \alpha + 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \cdot \frac{\Gamma(n\bar{x} + \alpha + 2) \Gamma(n(1 - \bar{x}) + \beta)}{\Gamma(n + \alpha + \beta + 2)} \\
    &= \frac{(n\bar{x} + \alpha + 1)(n\bar{x} + \alpha)}{(n + \alpha + \beta + 1)(n + \alpha + \beta)}.
\end{align*}
The posterior variance is then given by
\begin{align*}
    \var(\theta \mid x_1, \ldots, x_n) &= \expc(\theta^2 \mid x_1, \ldots, x_n) - (\expc(\theta \mid x_1, \ldots, x_n))^2 \\
    &= \frac{(n\bar{x} + \alpha + 1)(n\bar{x} + \alpha)}{(n + \alpha + \beta + 1)(n + \alpha + \beta)} - \left(\frac{n\bar{x} + \alpha}{n + \alpha + \beta}\right)^2 \\
    &= \frac{(n\bar{x} + \alpha)(n(1 - \bar{x}) + \beta)}{(n + \alpha + \beta + 1)(n + \alpha + \beta)^2}.
\end{align*}
Now $0 \leqslant \bar{x} \leqslant 1$, so
\[
    \var(\theta \mid x_1, \ldots, x_n) = \frac{(n\bar{x} + \alpha)(n(1 - \bar{x}) + \beta)}{(n + \alpha + \beta + 1)(n + \alpha + \beta)^2} \leqslant \frac{(1 + \alpha/n)(1 + \beta/n)}{n(1 + \alpha/n + \beta/n + 1/n)(1 + \alpha/n + \beta/n)^2} \to 0
\]
as $n \to \infty$.
\end{solution}

\begin{exercise}
\label{exer:7.2.7}
Establish that the variance of $\theta_1$ in Example~\ref{ex:7.2.4} is as given in Example~\ref{ex:7.2.6}. Prove that this goes to 0 as $n \to \infty$.
\end{exercise}

\begin{solution}
Recall that the posterior distribution of $\theta_1$ in Example \ref{ex:7.2.2} is $\text{Beta}(f_1 + \alpha_1, f_2 + \cdots + f_k + \alpha_2 + \cdots + \alpha_k)$. To find the posterior variance we need only find the second moment as follows.
\begin{align*}
    \expc(\theta_1^2 \mid x_1, \ldots, x_n) &= \int_0^1 \theta_1^2 \frac{\Gamma(n + \sum_{i=1}^{k} \alpha_i)}{\Gamma(f_1 + \alpha_1) \Gamma(\sum_{i=2}^{k} (f_i + \alpha_i))} (\theta_1)^{f_1 + \alpha_1 - 1} (1 - \theta_1)^{\sum_{i=2}^{k} (f_i + \alpha_i) - 1} \, \mathrm{d}\theta_1 \\
    &= \frac{\Gamma(n + \sum_{i=1}^{k} \alpha_i)}{\Gamma(f_1 + \alpha_1) \Gamma(\sum_{i=2}^{k} (f_i + \alpha_i))} \int_0^1 (\theta_1)^{f_1 + \alpha_1 + 1} (1 - \theta_1)^{\sum_{i=2}^{k} (f_i + \alpha_i) - 1} \, \mathrm{d}\theta_1 \\
    &= \frac{\Gamma(n + \sum_{i=1}^{k} \alpha_i)}{\Gamma(f_1 + \alpha_1) \Gamma(\sum_{i=2}^{k} (f_i + \alpha_i))} \cdot \frac{\Gamma(f_1 + \alpha_1 + 2) \Gamma(\sum_{i=2}^{k} (f_i + \alpha_i))}{\Gamma(n + \sum_{i=1}^{k} \alpha_i + 2)} \\
    &= \frac{(f_1 + \alpha_1 + 1)(f_1 + \alpha_1)}{(n + \sum_{i=1}^{k} \alpha_i + 1)(n + \sum_{i=1}^{k} \alpha_i)}.
\end{align*}
The posterior variance is then given by
\begin{align*}
    \var(\theta_1 \mid x_1, \ldots, x_n) &= \expc(\theta_1^2 \mid x_1, \ldots, x_n) - (\expc(\theta_1 \mid x_1, \ldots, x_n))^2 \\
    &= \frac{(f_1 + \alpha_1 + 1)(f_1 + \alpha_1)}{(n + \sum_{i=1}^{k} \alpha_i + 1)(n + \sum_{i=1}^{k} \alpha_i)} - \left(\frac{f_1 + \alpha_1}{n + \sum_{i=1}^{k} \alpha_i}\right)^2 \\
    &= \frac{(f_1 + \alpha_1)(\sum_{i=2}^{k} (f_i + \alpha_i))}{(n + \sum_{i=1}^{k} \alpha_i + 1)(n + \sum_{i=1}^{k} \alpha_i)^2}.
\end{align*}
Now $0 \leqslant f_1/n \leqslant 1$, so
\[
    \var(\theta_1 \mid x_1, \ldots, x_n) = \frac{(f_1/n + \alpha_1)(\sum_{i=2}^{k} (f_i/n + \alpha_i))}{n(1 + \sum_{i=1}^{k} \alpha_i/n + 1/n)(1 + \sum_{i=1}^{k} \alpha_i/n)^2} \to 0
\]
as $n \to \infty$.
\end{solution}

\begin{exercise}
\label{exer:7.2.8}
In Example~\ref{ex:7.2.14}, which of the two predictors derived there do you find more sensible? Why?
\end{exercise}

\begin{solution}
The posterior mode always takes a value in the set $\{0, 1\}$, and the value we are predicting also is in this set. On the other hand, the posterior expectation can take a value anywhere in the interval $(0, 1)$. Accordingly, the mode seems like a more sensible predictor.
\end{solution}

\begin{exercise}
\label{exer:7.2.9}
In Example~\ref{ex:7.2.15}, prove that the posterior predictive distribution for $X_{n+1}$ is as stated. (Hint: Write the posterior predictive distribution density as an expectation.)
\end{exercise}

\begin{solution}
We have $x_{n+1} \mid \mu, x_1, \ldots, x_n \sim N(\bar{x}, (1/\tau_0^2 + n/\sigma_0^2)^{-1} \sigma_0^2)$ and this is independent of $\mu$. Therefore, since the posterior predictive density of $x_{n+1}$ is obtained by averaging the $N(\bar{x}, (1/\tau_0^2 + n/\sigma_0^2)^{-1} \sigma_0^2)$ density with respect to the posterior density of $\mu$, we must have that this is also the posterior predictive distribution.
\end{solution}

\begin{exercise}
\label{exer:7.2.10}
Suppose that $x_1, \ldots, x_n$ is a sample from the Exponential$(\lambda)$ distribution, where $\lambda > 0$ is unknown and $\lambda \sim \text{Gamma}(\alpha_0, \beta_0)$. Determine the mode of posterior distribution of $\lambda$. Also determine the posterior expectation and posterior variance of $\lambda$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\lambda \mid x_1, \ldots, x_n) = \lambda^n e^{-n\bar{x}\lambda}$. The prior distribution has density given by $\beta_0^{\alpha_0} \lambda^{\alpha_0 - 1} e^{-\beta_0 \lambda} / \Gamma(\alpha_0)$. The posterior density of $\lambda$ is then given by $\pi(\lambda \mid x_1, \ldots, x_n) \propto \lambda^{n + \alpha_0 - 1} e^{-\lambda(n\bar{x} + \beta_0)}$, and we recognize this as being the density of a $\text{Gamma}(n + \alpha_0, n\bar{x} + \beta_0)$ distribution. The posterior mean and variance of $\lambda$ are then given by $\expc(\lambda \mid x_1, \ldots, x_n) = (n + \alpha_0)/(n\bar{x} + \beta_0)$, $\var(\lambda \mid x_1, \ldots, x_n) = (n + \alpha_0)/(n\bar{x} + \beta_0)^2$.

To find the posterior mode we need to maximize $\ln(\lambda^{n + \alpha_0 - 1} e^{-\lambda(n\bar{x} + \beta_0)}) = (\alpha_0 + n - 1) \ln \lambda - \lambda(n\bar{x} + \beta_0)$. This has first derivative given by $(\alpha_0 + n - 1)/\lambda - (n\bar{x} + \beta_0)$ and second derivative $-(\alpha_0 + n - 1)/\lambda^2$. Setting the first derivative equal to 0 and solving gives the solution $\hat{\lambda} = (\alpha_0 + n - 1)/(n\bar{x} + \beta_0)$. The second derivative at this value is $-(n\bar{x} + \beta_0)^2/(\alpha_0 + n - 1)$, which is clearly negative, so $\hat{\lambda}$ is the unique posterior mode.
\end{solution}

\begin{exercise}
\label{exer:7.2.11}
Suppose that $x_1, \ldots, x_n$ is a sample from the Exponential$(\lambda)$ distribution where $\lambda > 0$ is unknown and $\lambda \sim \text{Gamma}(\alpha_0, \beta_0)$. Determine the mode of posterior distribution of a future independent observation $X_{n+1}$. Also determine the posterior expectation of $X_{n+1}$ and posterior variance of $X_{n+1}$. (Hint: Problems~\ref{exer:3.2.16} and~\ref{exer:3.3.20}.)
\end{exercise}

\begin{solution}
First we find the posterior predictive density of $t = x_{n+1}$ as follows.
\begin{align*}
    q(t \mid x_1, \ldots, x_n) &= \int_0^\infty \lambda e^{-\lambda t} \frac{(\beta_0 + n\bar{x})^{n + \alpha_0}}{\Gamma(\alpha_0 + n)} \lambda^{n + \alpha_0 - 1} e^{-\lambda(n\bar{x} + \beta_0)} \, \mathrm{d}\lambda \\
    &= \frac{(\beta_0 + n\bar{x})^{n + \alpha_0}}{\Gamma(\alpha_0 + n)} \int_0^\infty \lambda^{n + \alpha_0} e^{-\lambda(n\bar{x} + \beta_0 + t)} \, \mathrm{d}\lambda \\
    &= \frac{(\beta_0 + n\bar{x})^{n + \alpha_0}}{\Gamma(\alpha_0 + n)} \cdot \frac{\Gamma(n + \alpha_0 + 1)}{(n\bar{x} + \beta_0 + t)^{n + \alpha_0 + 1}} \\
    &= \frac{(n + \alpha_0)(\beta_0 + n\bar{x})^{n + \alpha_0}}{(n\bar{x} + \beta_0 + t)^{n + \alpha_0 + 1}} = \frac{(n + \alpha_0)(\beta_0 + n\bar{x})^{-1}}{(1 + t/(n\bar{x} + \beta_0))^{n + \alpha_0 + 1}}
\end{align*}
which is a rescaled $\text{Pareto}(n + \alpha_0)$ distribution where the rescaling equals $(n\bar{x} + \beta_0)$.

To find the posterior mode we need to maximize $\ln((n\bar{x} + \beta_0 + t)^{-(n + \alpha_0 + 1)}) = -(\alpha_0 + n + 1) \ln(n\bar{x} + \beta_0 + t)$. This has first derivative (with respect to $t$) given by $-(\alpha_0 + n + 1)/(n\bar{x} + \beta_0 + t)$. Since the first derivative is negative for all $t$ and $t \geqslant 0$, the posterior mode is $\hat{t} = 0$.

Now the posterior distribution of $t/(n\bar{x} + \beta_0)$ is $\text{Pareto}(n + \alpha_0)$. By Problem \ref{exer:3.2.19} the posterior expectation of $t$ is therefore $(n\bar{x} + \beta_0)/(n + \alpha_0 - 1)$ and, by Problem \ref{exer:3.3.22}, the posterior variance of $t$ is
\[
    (n\bar{x} + \beta_0)^2 (n + \alpha_0) / (n + \alpha_0 - 1)^2 (n + \alpha_0 - 2).
\]
\end{solution}

\begin{exercise}
\label{exer:7.2.12}
Suppose that in a population of students in a course with a large enrollment, the mark, out of 100, on a final exam is approximately distributed $N(\mu, 9)$. The instructor places the prior $N(65, 1)$ on the unknown parameter $\mu$. A sample of 10 marks is obtained as given below.
\[
46 \quad 68 \quad 34 \quad 86 \quad 75 \quad 56 \quad 77 \quad 73 \quad 53 \quad 64
\]
\begin{enumerate}[(a)]
\item Determine the posterior mode and a 0.95-credible interval for $\mu$. What does this interval tell you about the accuracy of the estimate?
\item Use the 0.95-credible interval for $\mu$ to test the hypothesis $H_0 : \mu = 65$.
\item Suppose we assign prior probability $0.5$ to $\mu = 65$. Using the mixture prior $0.5\pi_1 + 0.5\pi_2$, where $\pi_1$ is degenerate at 65 and $\pi_2$ is the $N(65, 1)$ distribution, compute the posterior probability of the null hypothesis.
\item Compute the Bayes factor in favor of $H_0 : \mu = 65$ when using the mixture prior.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item As in Example \ref{ex:7.2.1}, we have that the posterior distribution of $\mu$ is given by the
    \[
        N\left(\left(1 + \frac{10}{9}\right)^{-1} \left(65 + \frac{10}{9} \cdot 63.20\right), \left(1 + \frac{10}{9}\right)^{-1}\right) = N\left(64.053, \frac{9}{19}\right).
    \]
    The posterior mode is then $\hat{\mu} = 64.053$. A 0.95-credible interval for $\mu$ is given by $64.053 \pm \sqrt{9/19} \, z_{0.975} = (62.704, 65.402)$. Since this interval has length equal to 2.698 and the margin of error is less than 1.5 marks (which is quite small) we conclude that the estimate is quite accurate.
    \item Based on the 0.95-credible interval, we cannot reject $H_0 : \mu = 65$, at the 5\% level since 65 falls inside the interval.
    \item The posterior probability of the null hypothesis above is given by
    \[
        \Pi(\mu = 65 \mid x_1, \ldots, x_n) = \frac{0.5 m_1(s)}{0.5 m_1(s) + 0.5 m_2(s)} \Pi_1(\mu = 65 \mid x_1, \ldots, x_n) + \frac{0.5 m_1(s)}{0.5 m_1(s) + 0.5 m_2(s)} \Pi_2(\mu = 65 \mid x_1, \ldots, x_n)
    \]
    where $\Pi_2(\cdot \mid x_1, \ldots, x_n)$ is as given in part (a) and $\Pi_1(\cdot \mid x_1, \ldots, x_n)$ is degenerate at $\mu = 65$.
    
    The prior predictive under $\Pi_1$ is given by
    \[
        m_1(x_1, \ldots, x_n) = (18\pi)^{-5} \exp\left(-\frac{(10 - 1) \cdot 252.622}{2 \cdot 9}\right) \exp\left(-\frac{10}{18}(63.20 - 65)^2\right) = 3.981 \times 10^{-65}
    \]
    while the prior predictive under $\Pi_2$ is given by
    \[
        m_2(x_1, \ldots, x_n) = (18\pi)^{-5} \exp\left(-\frac{(10 - 1) \cdot 252.622}{2 \cdot 9}\right) \times \exp\left(\frac{1}{2} \cdot \frac{9}{19} (135.22)^2\right) \exp\left(-\frac{1}{2} \cdot 8663.0\right) (0.68825) = 6.2662 \times 10^{-65}.
    \]
    The posterior probability of the null is then equal to
    \[
        \frac{0.5 m_1(s)}{0.5 m_1(s) + 0.5 m_2(s)} = \frac{3.981 \times 10^{-65}}{3.981 \times 10^{-65} + 6.2662 \times 10^{-65}} = 0.3885.
    \]
    \item The Bayes factor in favor of $H_0 : \mu = 65$ is given by
    \[
        \text{BF}_{H_0} = \frac{\exp\left(-\frac{10}{18}(63.20 - 65)^2\right)}{\exp\left(\frac{1}{2} \cdot \frac{9}{19} (135.22)^2\right) \exp\left(-\frac{1}{2} \cdot 8663.0\right) \times 0.68825} = 0.6353.
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.2.13}
A manufacturer believes that a machine produces rods with lengths in centimeters distributed $N(\mu_0, \sigma^2)$, where $\mu_0$ is known and $\sigma^2 > 0$ is unknown, and that the prior distribution $1/\sigma^2 \sim \text{Gamma}(\alpha_0, \beta_0)$ is appropriate.
\begin{enumerate}[(a)]
\item Determine the posterior distribution of $\sigma^2$ based on a sample $x_1, \ldots, x_n$.
\item Determine the posterior mean of $\sigma^2$.
\item Indicate how you would assess the hypothesis $H_0 : \sigma^2 = \sigma_0^2$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The likelihood function is given by $L(\sigma^2 \mid x_1, \ldots, x_n) = (\sigma^2)^{-n/2} \exp\left(-\frac{n-1}{2\sigma^2} s^2\right) \exp\left(-\frac{n}{2\sigma^2}(\bar{x} - \mu_0)^2\right)$. The prior distribution has density given by $\beta_0^{\alpha_0} (\sigma^2)^{-(\alpha_0 - 1)} e^{-\beta_0/\sigma^2} / \Gamma(\alpha_0)$. The posterior density of $1/\sigma^2$ is then proportional to
    \begin{align*}
        &(\sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2}\left((n - 1)s^2 + n(\bar{x} - \mu_0)^2\right)\right) (\sigma^2)^{-(\alpha_0 - 1)} \exp\left(-\frac{\beta_0}{\sigma^2}\right) \\
        &= (\sigma^2)^{-(n/2 + \alpha_0 - 1)} \exp\left(-\frac{1}{2\sigma^2}\left((n - 1)s^2 + n(\bar{x} - \mu_0)^2 + 2\beta_0\right)\right)
    \end{align*}
    which we recognize as being proportional to the $\text{Gamma}(n/2 + \alpha_0, \beta_x)$ density, where $\beta_x = (n - 1)s^2/2 + n(\bar{x} - \mu_0)^2/2 + \beta_0$. Therefore, the posterior distribution of $\sigma^2$ is inverse $\text{Gamma}(n/2 + \alpha_0, \beta_x)$.
    \item The posterior mean of $\sigma^2$ is given by $\expc(\sigma^2 \mid x_1, \ldots, x_n) = \beta_x / (n/2 + \alpha_0 - 1)$.
    \item To assess the hypothesis $H_0 : \sigma^2 \leqslant \sigma_0^2$, which is equivalent to assessing $H_0 : 1/\sigma^2 \geqslant 1/\sigma_0^2$, we compute
    \[
        \Pi(1/\sigma^2 \geqslant 1/\sigma_0^2 \mid x_1, \ldots, x_n) = \Pi(2\beta_x/\sigma^2 \geqslant 2\beta_x/\sigma_0^2 \mid x_1, \ldots, x_n) = 1 - G(2\beta_x/\sigma_0^2; 2\alpha_0 + n)
    \]
    where $G(\cdot; 2\alpha_0 + n)$ is the $\chi^2(2\alpha_0 + n)$ distribution function.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.2.14}
Consider the sampling model and prior in Exercise~\ref{exer:7.1.1}.
\begin{enumerate}[(a)]
\item Suppose we want to estimate $\theta$ based upon having observed $s = 1$. Determine the posterior mode and posterior mean. Which would you prefer in this situation? Explain why.
\item Determine a 0.8 HPD region for $\theta$ based on having observed $s = 1$.
\item Suppose instead interest was in $\psi = \indc_{\{1,2\}}(\theta)$. Identify the prior distribution of $\psi$. Identify the posterior distribution of $\psi$ based on having observed $s = 1$. Determine a 0.5 HPD region for $\psi$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item In Exercise \ref{exer:7.1.1}, the posterior distribution is given by
    \begin{center}
    \begin{tabular}{c|ccc}
    $\theta$ & 1 & 2 & 3 \\
    \hline
    $\pi(\theta \mid s = 1)$ & $3/16$ & $1/4$ & $9/16$
    \end{tabular}
    \end{center}
    Hence, the posterior mode is $\theta = 3$ and the posterior mean is $1 \cdot 3/16 + 2 \cdot 1/4 + 3 \cdot 9/16 = 2.375$. The mode is an actual parameter value while the mean is not so we would prefer to use the mode.
    \item First of all, $\Pi(\theta = 3 \mid s = 1) = 9/16 = 0.5625 < 0.8$. The second highest posterior probability is obtained at $\theta = 2$. $\Pi(\{2, 3\} \mid s = 1) = 13/16 = 0.8125 > 0.8$. Thus, 0.8-HPD region is $\{2, 3\}$.
    \item Since $\psi(1) = \psi(2) = 1$ and $\psi(3) = 0$, the prior probability of $\psi$ is $\Pi(\psi = 0) = \Pi(\theta = 3) = 2/5$ and $\Pi(\psi = 1) = \Pi(\{1, 2\}) = 3/5$. The posterior probability is $\Pi(\psi = 0 \mid s = 1) = \Pi(\theta = 3 \mid s = 1) = 9/16$ and $\Pi(\psi = 1 \mid s = 1) = \Pi(\{1, 2\} \mid s = 1) = 7/16$.
    \begin{center}
    \begin{tabular}{c|cc}
    prior & $\psi = 0$ & $\psi = 1$ \\
    \hline
    $\pi(\psi)$ & $2/5$ & $3/5$
    \end{tabular}
    \quad
    \begin{tabular}{c|cc}
    posterior & $\psi = 0$ & $\psi = 1$ \\
    \hline
    $\pi(\psi \mid s = 1)$ & $9/16$ & $7/16$
    \end{tabular}
    \end{center}
    Thus, the posterior mode is $\psi = 0$. Besides, $\Pi(\psi = 0 \mid s = 1) = 9/16 = 0.5625 > 0.5$ implies 0.5-HPD region is $\{0\}$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.2.15}
For an event $A$, we have that $\prb(A^c) = 1 - \prb(A)$.
\begin{enumerate}[(a)]
\item What is the relationship between the odds in favor of $A$ and the odds in favor of $A^c$?
\item When $A$ is a subset of the parameter space, what is the relationship between the Bayes factor in favor of $A$ and the Bayes factor in favor of $A^c$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The odds in favor of $A$ is defined by $\prb(A)/\prb(A^c)$. Hence,
    \[
        \frac{\prb(A)}{\prb(A^c)} = \frac{\prb(A)}{1 - \prb(A)} = \frac{1 - \prb(A^c)}{\prb(A^c)} = 1 \bigg/ \frac{\prb(A^c)}{1 - \prb(A^c)} = 1 / \text{odds in favor of } A^c.
    \]
    \item The Bayes factor in favor of $A$ is given by $\text{BF}(A) = \text{posterior odds of } A / \text{prior odds of } A$.
    \[
        \text{BF}(A) = \frac{\Pi(A \mid s)}{\Pi(A^c \mid s)} \bigg/ \frac{\Pi(A)}{\Pi(A^c)} = 1 \bigg/ \left[\frac{\Pi(A^c \mid s)}{1 - \Pi(A^c \mid s)} \bigg/ \frac{\Pi(A)}{1 - \Pi(A^c)}\right] = 1 / \text{BF}(A^c).
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.2.16}
Suppose you are told that the odds in favor of a subset $A$ are 3 to 1. What is the probability of $A$? If the Bayes factor in favor of $A$ is 10 and the prior probability of $A$ is 1/2, then determine the posterior probability of $A$.
\end{exercise}

\begin{solution}
The fact that the odds of $A$ is 3 implies $\prb(A)/(1 - \prb(A)) = 3$. This implies that $\prb(A) = 3/4$. If $\Pi(A) = 1/2$, then the prior odds of $A$ is $\Pi(A)/\Pi(A^c) = (1/2)/(1/2) = 1$. The Bayes factor in favor of $A$ is $\text{BF}(A) = \text{posterior odds of } A / \text{prior odds of } A = (\Pi(A \mid s)/(1 - \Pi(A \mid s)))/1 = 10$. This implies that $\Pi(A \mid s) = 10/11$.
\end{solution}

\begin{exercise}
\label{exer:7.2.17}
Suppose data $s$ is obtained. Two statisticians analyze these data using the same sampling model but different priors, and they are asked to assess a hypothesis $H_0$. Both statisticians report a Bayes factor in favor of $H_0$ equal to 100. Statistician I assigned prior probability 1/2 to $H_0$, whereas statistician II assigned prior probability 1/4 to $H_0$. Which statistician has the greatest posterior degree of belief in $H_0$ being true?
\end{exercise}

\begin{solution}
From the equation $\text{BF}(A) = [\Pi(A \mid s)/(1 - \Pi(A \mid s))]/[\Pi(A)/(1 - \Pi(A))]$, we get $\Pi(A \mid s) = 1/[1 + \text{BF}(A)/[\Pi(A)/(1 - \Pi(A))]]$. Both statisticians' Bayes factor equals $\text{BF}(A) = 100$. The prior odds of Statistician I is $\Pi(H_0)/(1 - \Pi(H_0)) = (1/2)/(1/2) = 1$. Thus Statistician I's posterior probability is $\Pi(H_0 \mid s) = 1/[1 + (1)100] = 1/101 = 0.0099$. The prior odds of Statistician II is $\Pi(H_0)/(1 - \Pi(H_0)) = (1/4)/(3/4) = 1/3$ and the posterior probability is $\Pi(H_0 \mid s) = 1/[1 + (1/3)100] = 3/103 = 0.0292$. Hence, Statistician II has the bigger posterior belief in $H_0$.
\end{solution}

\begin{exercise}
\label{exer:7.2.18}
You are told that a 0.95-credible interval, determined using the HPD criterion, for a quantity $\psi = \sigma/\mu$ is given by $(3.3, 2.6)$. If you are asked to assess the hypothesis $H_0 : \psi(\theta) = 0$, then what can you say about the Bayesian P-value? Explain your answer.
\end{exercise}

\begin{solution}
Note that a credible set is an acceptance region and the complement of $\gamma$-credible set is a $(1 - \gamma)$ rejection region. Since $\psi(\theta) = 0 \in (-3.3, 2.6)$, the $P$-value must be greater than $1 - 0.95 = 0.05$.
\end{solution}

\begin{exercise}
\label{exer:7.2.19}
What is the range of possible values for a Bayes factor in favor of $A \subset \Omega$? Under what conditions will a Bayes factor in favor of $A$ take its smallest value?
\end{exercise}

\begin{solution}
Since the posterior probability $\Pi(A \mid s)$ is in $[0, 1]$, the posterior odds ranges in $[0, \infty)$ as does the prior odds. Hence, the range of a Bayes factor in favor of $A$ also ranges in $[0, \infty)$. The smallest Bayes factor is obtained when the posterior probability $\Pi(A \mid s)$ is the smallest. If $A$ has posterior probability equal to 0, then the Bayes factor will be 0.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:7.2.20}
Suppose that $x_1, \ldots, x_n$ is a sample from the Uniform$[0, \theta]$ distribution, where $\theta > 0$ is unknown, and we have $\theta \sim \text{Gamma}(\alpha_0, \beta_0)$. Determine the mode of the posterior distribution of $\theta$. (Hint: The posterior is not differentiable at $x_{(n)}$.)
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{-n} \indc_{[x_{(n)}, \infty)}(\theta)$. The posterior distribution of $\theta$ is then given by $\pi(\theta \mid x_1, \ldots, x_n) \propto \theta^{\alpha - n - 1} e^{-\beta\theta} \indc_{[x_{(n)}, \infty)}(\theta)$. Note that this is not differentiable at $x_{(n)}$. The maximum of $\theta^{\alpha - n - 1} e^{-\beta\theta}$ occurs at the same point as the maximum of $\ln(\theta^{\alpha - n - 1} e^{-\beta\theta}) = (\alpha - n - 1) \ln \theta - \beta\theta$, which has first derivative $(\alpha - n - 1)/\theta - \beta$ and second derivative $-(\alpha - n - 1)/\theta^2$. Setting the first derivative equal to 0 and solving we have that the maximum occurs at $\hat{\theta} = (\alpha - n - 1)/\beta$ whenever $\alpha - n - 1 > 0$. Therefore, the posterior mode is given by $\max\{(\alpha - n - 1)/\beta, x_{(n)}\}$.
\end{solution}

\begin{exercise}
\label{exer:7.2.21}
Suppose that $x_1, \ldots, x_n$ is a sample from the Uniform$[0, \theta]$ distribution, where $0 < \theta \leqslant 1$ is unknown, and we have $\theta \sim \text{Uniform}[0, 1]$. Determine the form of the $\gamma$-credible interval for $\theta$ based on the HPD concept.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{-n} \indc_{(x_{(n)}, \infty)}(\theta)$ and the prior is $\indc_{(0,1)}(\theta)$, so the posterior is
\[
    \frac{\theta^{-n} \indc_{(x_{(n)}, 1)}(\theta)}{\int_{x_{(n)}}^{1} \theta^{-n} \, \mathrm{d}\theta} = \frac{\theta^{-n} \indc_{(x_{(n)}, 1)}(\theta)}{(n - 1)(x_{(n)}^{1-n} - 1)}.
\]
Since this density strictly increases in $(x_{(n)}, 1)$ and HPD interval is of the form $(c, 1)$, $c$ is determined by
\[
    \gamma = \int_c^1 \frac{\theta^{-n} \indc_{(x_{(n)}, 1)}(\theta)}{(n - 1)(x_{(n)}^{1-n} - 1)} \, \mathrm{d}\theta = \frac{c^{1-n} - 1}{x_{(n)}^{1-n} - 1},
\]
so $c = \left\{1 + \gamma(x_{(n)}^{1-n} - 1)\right\}^{1/(1-n)}$.
\end{solution}

\begin{exercise}
\label{exer:7.2.22}
In Example~\ref{ex:7.2.1}, write out the integral given in \eqref{eq:7.2.2}.
\end{exercise}

\begin{solution}
The posterior distribution of $\mu$ given $\sigma^2$ is the $N(\mu_x, (n + 1/\tau_0^2)^{-1} \sigma^2)$ distribution where $\mu_x$ is given by (7.1.7). The posterior distribution of $\sigma^2$ is the $\text{Gamma}(\alpha_0 + n/2, \beta_x)$ distribution, where $\beta_x$ is given by (7.1.8). Therefore, the integral (7.2.2) is given by
\[
    \psi_0^{-2} \int_0^\infty \frac{1}{\sqrt{2\pi}} \left(n + \frac{1}{\tau_0^2}\right)^{1/2} \exp\left(-\frac{\lambda}{2}\left(n + \frac{1}{\tau_0^2}\right)(\psi_0^{-1} \lambda^{-1/2} - \mu_x)^2\right) \times \frac{(\beta_x)^{\alpha_0 + n/2}}{\Gamma(\alpha_0 + n/2)} \lambda^{\alpha_0 + n/2 - 1} \exp(-\beta_x \lambda) \, \mathrm{d}\lambda.
\]
\end{solution}

\begin{exercise}
\label{exer:7.2.23}
(MV) In Example~\ref{ex:7.2.1}, write out the integral that you would need to evaluate if you wanted to compute the posterior density of the third quartile of the population distribution, i.e., $\mu + \sigma z_{0.75}$.
\end{exercise}

\begin{solution}
Let $\psi(\mu, \sigma^2) = \mu + \sigma z_{0.75} = \mu + (1/\sigma^2)^{-1/2} z_{0.75}$ and $\lambda = \lambda(\mu, \sigma^2) = 1/\sigma^2$, so
\[
    J(\theta(\psi, \lambda)) = \left|\det\begin{pmatrix} \dfrac{\partial \psi}{\partial \mu} & \dfrac{\partial \psi}{\partial(1/\sigma^2)} \\[10pt] \dfrac{\partial \lambda}{\partial \mu} & \dfrac{\partial \lambda}{\partial(1/\sigma^2)} \end{pmatrix}\right| = \left|\det\begin{pmatrix} 1 & -\frac{1}{2} z_{0.75} (1/\sigma^2)^{-3/2} \\ 0 & 1 \end{pmatrix}\right| = 1.
\]
Therefore, the posterior density of $\psi$ is given by
\[
    \int_0^\infty \frac{1}{\sqrt{2\pi}} \left(n + \frac{1}{\tau_0^2}\right)^{1/2} \lambda^{1/2} \exp\left(-\frac{\lambda}{2}\left(n + \frac{1}{\tau_0^2}\right)((\psi_0 - \lambda^{-1/2} z_{0.75}) - \mu_x)^2\right) \times \frac{(\beta_x)^{\alpha_0 + n/2}}{\Gamma(\alpha_0 + n/2)} \lambda^{\alpha_0 + n/2 - 1} \exp(-\beta_x \lambda) \, \mathrm{d}\lambda.
\]
which is a difficult integral to evaluate.
\end{solution}

\begin{exercise}
\label{exer:7.2.24}
Consider the location normal model discussed in Example~\ref{ex:7.1.2} and the population coefficient of variation $\psi = \sigma_0/\mu$.
\begin{enumerate}[(a)]
\item Show that the posterior expectation of $\psi$ does not exist. (Hint: Show that we can write the posterior expectation as
\[
\int_{-\infty}^{\infty} \frac{a + bz}{1} \cdot \frac{1}{\sqrt{2\pi}} e^{-z^2/2} \, \mathrm{d}z,
\]
where $b \neq 0$, and show that this integral does not exist by considering the behavior of the integrand at $z = -a/b$.)
\item Determine the posterior density of $\psi$.
\item Show that you can determine the posterior mode of $\psi$ by evaluating the posterior density at two specific points. (Hint: Proceed by maximizing the logarithm of the posterior density using the methods of calculus.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item We can write
    \[
        \psi = \sigma_0/\mu = \sigma_0 \left(\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right) + \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2} Z\right)^{-1} = \sigma_0 (a + bZ)^{-1}
    \]
    where $Z \sim N(0, 1)$,
    \[
        a = \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right), \quad b = \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2}.
    \]
    The posterior mean of $\psi$ is $\expc(\psi \mid x_1, \ldots, x_n) = \int_{-\infty}^{\infty} \frac{\sigma_0}{a + bz} \frac{1}{\sqrt{2\pi}} e^{-z^2/2} \, \mathrm{d}z$, and this integral does not exist because, noting that the integrand becomes infinite at $z = -a/b$, for $c > 0$
    \begin{align*}
        \int_{-a/b}^{\infty} \frac{1}{a + bz} e^{-z^2/2} \, \mathrm{d}z &\geqslant \int_{-a/b}^{-a/b + c} \frac{1}{a + bz} e^{-z^2/2} \, \mathrm{d}z \\
        &\geqslant \min\{e^{-z^2/2} : -a/b \leqslant z \leqslant -a/b + c\} \int_{-a/b}^{-a/b + c} \frac{1}{a + bz} \, \mathrm{d}z \\
        &= \min\{e^{-z^2/2} : -a/b \leqslant z \leqslant -a/b + c\} \left.\frac{\ln(a + bz)}{b}\right|_{-a/b}^{-a/b + c} = \infty
    \end{align*}
    while
    \begin{align*}
        \int_{-\infty}^{-a/b} \frac{1}{a + bz} e^{-z^2/2} \, \mathrm{d}z &\leqslant \int_{-a/b - c}^{-a/b} \frac{1}{a + bz} e^{-z^2/2} \, \mathrm{d}z \\
        &\leqslant \min\{e^{-z^2/2} : -a/b \leqslant z \leqslant -a/b + c\} \int_{-a/b - c}^{-a/b} \frac{1}{a + bz} \, \mathrm{d}z \\
        &= \min\{e^{-z^2/2} : -a/b \leqslant z \leqslant -a/b + c\} \left.\frac{-\ln(a + bz)}{b}\right|_{-a/b + c}^{-a/b} = -\infty.
    \end{align*}
    Therefore, $\expc(\psi \mid x_1, \ldots, x_n) = \infty - \infty$, which is not defined.
    \item The posterior density of $\mu$ is given by
    \[
        \pi(\mu \mid x_1, \ldots, x_n) = \frac{1}{\sqrt{2\pi} b^{1/2}} \exp\left(-\frac{1}{2b}(\mu - a)^2\right).
    \]
    Using Theorem \ref{thm:2.6.2} we can find the posterior density of $\psi = \sigma_0/\mu$ (since this is a differentiable and strictly decreasing function of $\mu$ and excluding the 0 line from the parameter space) as
    \[
        \pi(\psi^{-1}(\phi) \mid x_1, \ldots, x_n) / |\psi'(\psi^{-1}(\phi))| = \frac{1}{\sqrt{2\pi b}} \exp\left(-\frac{1}{2b^2}\left(\frac{\sigma_0}{\phi} - a\right)^2\right) \frac{\sigma_0}{\phi^2}.
    \]
    \item To find the posterior mode we need to maximize
    \[
        \ln\left(\exp\left(-\frac{1}{2b^2}\left(\frac{\sigma_0}{\phi} - a\right)^2\right) \frac{1}{\phi^2}\right) = -\frac{1}{2b^2}\left(\frac{\sigma_0}{\phi} - a\right)^2 - 2\ln\phi.
    \]
    This has first derivative given by
    \[
        \frac{1}{b^2}\left(\frac{\sigma_0}{\phi} - a\right) \frac{\sigma_0}{\phi^2} - \frac{2}{\phi} = \frac{\sigma_0}{b^2 \phi^3} - \frac{a\sigma_0}{b^2 \phi^2} - \frac{2}{\phi}.
    \]
    Setting the first derivative equal to 0 gives the quadratic equation
    \[
        \phi^2 + \frac{a\sigma_0}{2b^2} \phi - \frac{\sigma_0}{2b^2} = 0.
    \]
    Solving this gives the solutions
    \[
        \hat{\phi} = -\frac{a\sigma_0}{4b^2} \pm \frac{1}{2}\sqrt{\frac{a^2\sigma_0^2}{4b^4} + \frac{4a\sigma_0}{2b^2}} = -\frac{a\sigma_0}{4b^2} \pm \frac{1}{2b}\sqrt{\frac{a^2\sigma_0^2}{4b^2} + 2a\sigma_0}
    \]
    and these are real numbers since $b > 0$. Since the posterior density is finite everywhere, goes to 0 at $\pm\infty$, and is 0 at $\phi = 0$, we know that these must both correspond to peaks. Therefore, we can determine the mode by evaluating the posterior density at these values, and the mode is the one that gives the largest value.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.2.25}
(MV) Suppose that $(\theta_1, \ldots, \theta_{k-1}) \sim \text{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_k)$.
\begin{enumerate}[(a)]
\item Prove that $(\theta_1, \ldots, \theta_{k-2}) \sim \text{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_{k-1} + \alpha_k)$. (Hint: In the integral to integrate out $\theta_{k-1}$, make the transformation $\theta_{k-1} = (1 - \theta_1 - \cdots - \theta_{k-2})\eta$.)
\item Prove that $\theta_1 \sim \text{Beta}(\alpha_1, \alpha_2 + \cdots + \alpha_k)$. (Hint: Use part (a).)
\item Suppose $(i_1, \ldots, i_k)$ is a permutation of $(1, \ldots, k)$. Prove that $(\theta_{i_1}, \ldots, \theta_{i_{k-1}}) \sim \text{Dirichlet}(\alpha_{i_1}, \alpha_{i_2}, \ldots, \alpha_{i_k})$. (Hint: What is the Jacobian of this transformation?)
\item Prove that $\theta_i \sim \text{Beta}(\alpha_i, \alpha_{-i})$. (Hint: Use parts (b) and (c).)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The marginal density of $(\theta_1, \ldots, \theta_{k-2})$ is given by
    \begin{align*}
        &f_{(\theta_1, \ldots, \theta_{k-2})}(z_1, \ldots, z_{k-2}) \\
        &= \int_0^{1 - z_1 - \cdots - z_{k-2}} \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} z_1^{\alpha_1 - 1} z_2^{\alpha_2 - 1} \cdots z_{k-2}^{\alpha_{k-2} - 1} z_{k-1}^{\alpha_{k-1} - 1} (1 - z_1 - \cdots - z_{k-1})^{\alpha_k - 1} \, \mathrm{d}z_{k-1} \\
        &= \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} z_1^{\alpha_1 - 1} z_2^{\alpha_2 - 1} \cdots z_{k-2}^{\alpha_{k-2} - 1} \int_0^{1 - z_1 - \cdots - z_{k-2}} z_{k-1}^{\alpha_{k-1} - 1} (1 - z_1 - \cdots - z_{k-1})^{\alpha_k - 1} \, \mathrm{d}z_{k-1} \\
        &= \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} z_1^{\alpha_1 - 1} z_2^{\alpha_2 - 1} \cdots z_{k-2}^{\alpha_{k-2} - 1} (1 - z_1 - \cdots - z_{k-2})^{\alpha_{k-1} + \alpha_k - 2} \\
        &\quad \times \int_0^{1 - z_1 - \cdots - z_{k-2}} \left(\frac{z_{k-1}}{1 - z_1 - \cdots - z_{k-2}}\right)^{\alpha_{k-1} - 1} \left(1 - \frac{z_{k-1}}{1 - z_1 - \cdots - z_{k-2}}\right)^{\alpha_k - 1} \, \mathrm{d}z_{k-1} \\
        &= \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} z_1^{\alpha_1 - 1} z_2^{\alpha_2 - 1} \cdots z_{k-2}^{\alpha_{k-2} - 1} (1 - z_1 - \cdots - z_{k-2})^{\alpha_{k-1} + \alpha_k - 1} \int_0^1 u^{\alpha_{k-1} - 1} (1 - u)^{\alpha_k - 1} \, \mathrm{d}z_{k-1} \\
        &= \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)} \cdot \frac{\Gamma(\alpha_{k-1}) \Gamma(\alpha_k)}{\Gamma(\alpha_{k-1} + \alpha_k)} z_1^{\alpha_1 - 1} z_2^{\alpha_2 - 1} \cdots z_{k-2}^{\alpha_{k-2} - 1} (1 - z_1 - \cdots - z_{k-2})^{\alpha_{k-1} + \alpha_k - 1} \\
        &= \frac{\Gamma(\alpha_1 + \cdots + \alpha_k)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_{k-2}) \Gamma(\alpha_{k-1} + \alpha_k)} z_1^{\alpha_1 - 1} z_2^{\alpha_2 - 1} \cdots z_{k-2}^{\alpha_{k-2} - 1} (1 - z_1 - \cdots - z_{k-2})^{\alpha_{k-1} + \alpha_k - 1}
    \end{align*}
    and this establishes the result.
    \item Iterating the part (a) gives the result.
    \item The Jacobian matrix of this transformation has a 1 in the $i_1$-th position of the first row, a 1 in the $i_2$-th position of the second row, etc. The absolute value of the determinant of this transformation is therefore equal to 1. By the change of variable theorem this implies that $(\theta_{i_1}, \ldots, \theta_{i_{k-1}}) \sim \text{Dirichlet}(\alpha_{i_1}, \alpha_{i_2}, \ldots, \alpha_{i_k})$.
    \item This is immediate from parts (c) and (b) as we just choose a permutation that puts $\theta_i$ as the first coordinate.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.2.26}
(MV) In Example~\ref{ex:7.2.4}, show that the plug-in MLE of $\theta_1$ is given by $x_1/n$, i.e., find the MLE of $(\theta_1, \ldots, \theta_k)$ and determine the first coordinate. (Hint: Show there is a unique solution to the score equations and then use the facts that the log-likelihood is bounded above and goes to $-\infty$ whenever $\theta_i \to 0$.)
\end{exercise}

\begin{solution}
The likelihood function is given by $\theta_1^{f_1} \theta_2^{f_2} \cdots (1 - \theta_1 - \cdots - \theta_{k-1})^{f_k}$, so the log-likelihood is given by $f_1 \ln \theta_1 + f_2 \ln \theta_2 + \cdots + f_k \ln(1 - \theta_1 - \cdots - \theta_{k-1})$. Then vector of partial derivatives has $i$th element equal to $f_i/\theta_i - f_k/(1 - \theta_1 - \cdots - \theta_{k-1})$. Setting these equal to 0 we get the system of equations
\begin{align*}
    f_k \theta_1 &= f_1(1 - \theta_1 - \cdots - \theta_{k-1}) \\
    &\vdots \\
    f_k \theta_{k-1} &= f_{k-1}(1 - \theta_1 - \cdots - \theta_{k-1})
\end{align*}
and summing both sides we obtain $f_k(\theta_1 + \cdots + \theta_{k-1}) = (n - f_k)(1 - \theta_1 - \cdots - \theta_{k-1})$ or $n(\theta_1 + \cdots + \theta_{k-1}) = (n - f_k)$, which implies that $(1 - \theta_1 - \cdots - \theta_{k-1}) = f_k/n$. From this we deduce that the unique solution is $(\hat{\theta}_1, \ldots, \hat{\theta}_{k-1}) = (f_1/n, \ldots, f_{k-1}/n)$. Now since the log-likelihood is bounded above, continuously differentiable, and goes to $-\infty$ whenever $\theta_i \to 0$, this establishes that $(f_1/n, \ldots, f_{k-1}/n)$ is the MLE, so $f_1/n$ is the plug-in MLE.
\end{solution}

\begin{exercise}
\label{exer:7.2.27}
Compare the results obtained in Exercises~\ref{exer:7.2.3} and~\ref{exer:7.2.4}. What do you conclude about the invariance properties of these estimation procedures? (Hint: Consider Theorem~\ref{thm:6.2.1}.)
\end{exercise}

\begin{solution}
In Exercise \ref{exer:7.2.3} we showed that $\expc(1/\sigma^2 \mid x_1, \ldots, x_n) = (\alpha_0 + n/2)/\beta_x$, while in Exercise \ref{exer:7.2.4} we showed that $\expc(\sigma^2 \mid x_1, \ldots, x_n) = \beta_x/(\alpha_0 + n/2 - 1)$. So the estimate of $\sigma^2$ is not equal to one over the estimate of $1/\sigma^2$.

In Exercise \ref{exer:7.2.3} we showed that the posterior mode of $1/\sigma^2$ is $1/\hat{\sigma}^2 = (\alpha_0 + n/2 - 1)/\beta_x$, while in Exercise \ref{exer:7.2.4} we showed that the posterior mode of $\sigma^2$ is $\hat{\sigma}^2 = \beta_x/(\alpha_0 + n/2 + 1)$. So the estimate of $\sigma^2$ is not equal to one over the estimate of $1/\sigma^2$.

These differences indicate that these estimation procedures do not have the invariance property possessed by the MLE.
\end{solution}

\begin{exercise}
\label{exer:7.2.28}
In Example~\ref{ex:7.2.5}, establish that the posterior variance of $\mu$ is as stated in Example~\ref{ex:7.2.6}. (Hint: Problem~\ref{exer:4.6.16}.)
\end{exercise}

\begin{solution}
Since the variance of a $t(\lambda)$ distribution is $\lambda/(\lambda - 2)$, the posterior variance of $\mu$ is given by
\[
    \var\left(\mu_x + \sqrt{\frac{1}{n + 2\alpha_0}} \sqrt{\frac{2\beta_x}{n + 1/\tau_0^2}} t(n + 2\alpha_0)\right) = \left(\sqrt{\frac{1}{n + 2\alpha_0}} \sqrt{\frac{2\beta_x}{n + 1/\tau_0^2}}\right)^2 \frac{n + 2\alpha_0}{n + 2\alpha_0 - 2} = \left(\frac{2\beta_x}{n + 1/\tau_0^2}\right)\left(\frac{1}{n + 2\alpha_0 - 2}\right).
\]
\end{solution}

\begin{exercise}
\label{exer:7.2.29}
In a prediction problem, as described in Section~\ref{ssec:7.2.4}, derive the form of the prior predictive density for $t$ when the joint density of $(s, t, \theta)$ is $q_{\theta,s}(t) f_\theta(s) \pi(\theta)$ (assume $s$ and $\theta$ are real-valued).
\end{exercise}

\begin{solution}
The joint density of $(\theta, s, t)$ is given by $q_\theta(t \mid s) f_\theta(s) \pi(\theta)$. The prior predictive density for $t$ is then the marginal density of $t$ and is given by $q(t) = \int_\Omega \int_{-\infty}^{\infty} q_\theta(t \mid s) f_\theta(s) \pi(\theta) \, \mathrm{d}s \, \mathrm{d}\theta$.
\end{solution}

\begin{exercise}
\label{exer:7.2.30}
In Example~\ref{ex:7.2.16}, derive the posterior predictive probability function of $X_{n+1} + X_{n+2}$ having observed $x_1, \ldots, x_n$ when $X_1, \ldots, X_n, X_{n+1}, X_{n+2}$ are independently and identically distributed (i.i.d.) Bernoulli$(\theta)$.
\end{exercise}

\begin{solution}
The posterior predictive distribution for $t = (x_{n+1}, x_{n+2})$ is given by
\begin{align*}
    q(t \mid x_1, \ldots, x_n) &= \int_0^1 \theta^{t_1 + t_2} (1 - \theta)^{2 - t_1 - t_2} \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \int_0^1 \theta^{t_1 + t_2 + n\bar{x} + \alpha - 1} (1 - \theta)^{2 - t_1 - t_2 + n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta \\
    &= \frac{\Gamma(n + \alpha + \beta)}{\Gamma(n\bar{x} + \alpha) \Gamma(n(1 - \bar{x}) + \beta)} \times \frac{\Gamma(t_1 + t_2 + n\bar{x} + \alpha) \Gamma(2 - t_1 - t_2 + n(1 - \bar{x}) + \beta)}{\Gamma(n + \alpha + \beta + 2)} \\
    &= \begin{cases}
        \dfrac{(n(1 - \bar{x}) + \beta + 1)(n(1 - \bar{x}) + \beta)}{(n + \alpha + \beta + 1)(n + \alpha + \beta)} & t_1 = t_2 = 0 \\[10pt]
        \dfrac{(n\bar{x} + \alpha)(n(1 - \bar{x}) + \beta)}{(n + \alpha + \beta + 1)(n + \alpha + \beta)} & t_1 = 0, t_2 = 1 \\[10pt]
        \dfrac{(n\bar{x} + \alpha)(n(1 - \bar{x}) + \beta)}{(n + \alpha + \beta + 1)(n + \alpha + \beta)} & t_1 = 1, t_2 = 0 \\[10pt]
        \dfrac{(n\bar{x} + \alpha + 1)(n\bar{x} + \alpha)}{(n + \alpha + \beta + 1)(n + \alpha + \beta)} & t_1 = t_2 = 1.
    \end{cases}
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:7.2.31}
In Example~\ref{ex:7.2.15}, derive the posterior predictive distribution for $X_{n+1}$ having observed $x_1, \ldots, x_n$ when $X_1, \ldots, X_n, X_{n+1}$ are i.i.d.\ $N(\mu, \sigma_0^2)$. (Hint: We can write $X_{n+1} = \mu + \sigma_0 Z$, where $Z \sim N(0, 1)$ is independent of the posterior distribution of $\mu$.)
\end{exercise}

\begin{solution}
Put
\[
    a = \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right), \quad b = \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2}.
\]
We can write $X_{n+1} = \mu + \sigma_0 Z$, where $\mu \sim N(a, b^2)$ is independent of $Z \sim N(0, 1)$. Therefore, the posterior predictive of $X_{n+1}$ is given by $X_{n+1} \sim N(a, b^2 + \sigma_0^2)$.
\end{solution}

\begin{exercise}
\label{exer:7.2.32}
For the context of Example~\ref{ex:7.2.1}, prove that the posterior predictive distribution of an additional future observation $X_{n+1}$ from the population distribution has the same distribution as
\[
\mu(x) + \sqrt{\frac{2\beta(x)}{n + \tau_0^{-2}}} \left(1 + \frac{1}{2\alpha_0 + n}\right)^{1/2} T,
\]
where $T \sim t(2\alpha_0 + n)$. (Hint: Note that we can write $X_{n+1} = \mu + \sigma U$, where $U \sim N(0, 1)$ independent of $X_1, \ldots, X_n$, and then reason as in Example~\ref{ex:7.2.1}.)
\end{exercise}

\begin{solution}
We can write $X_{n+1} = \mu + \sigma U$, where $U \sim N(0, 1)$ independent of $X_1, \ldots, X_n, \mu, \sigma$. We also have that $\mu = \mu_x + (n + 1/\tau_0^2)^{-1/2} \sigma Z$, where $Z \sim N(0, 1)$ is independent of $X_1, \ldots, X_n, \sigma$. Therefore, we can write
\begin{align*}
    X_{n+1} &= \mu_x + (n + 1/\tau_0^2)^{-1/2} \sigma Z + \sigma U \\
    &= \mu_x + \sigma \left\{(n + 1/\tau_0^2)^{-1/2} Z + U\right\} \\
    &= \mu_x + \left\{(n + 1/\tau_0^2)^{-1} + 1\right\}^{1/2} \sigma W
\end{align*}
where
\[
    W = \left\{(n + 1/\tau_0^2)^{-1} + 1\right\}^{-1/2} \left\{(n + 1/\tau_0^2)^{-1/2} Z + U\right\} = \frac{X_{n+1} - \mu_x}{\{(n + 1/\tau_0^2)^{-1} + 1\}^{1/2} \sigma} \sim N(0, 1)
\]
is independent of $X_1, \ldots, X_n, \sigma$. Therefore, just as in Example \ref{ex:7.2.1},
\[
    T = \frac{W}{\sqrt{(2\beta_x/\sigma^2)/(2\alpha_0 + n)}} = \frac{X_{n+1} - \mu_x}{\{(n + 1/\tau_0^2)^{-1} + 1\}^{1/2} ((2\beta_x)/(2\alpha_0 + n))^{1/2}} \sim t(2\alpha_0 + n).
\]
\end{solution}

\begin{exercise}
\label{exer:7.2.33}
In Example~\ref{ex:7.2.1}, determine the form of an exact $\gamma$-prediction interval for an additional future observation $X_{n+1}$ from the population distribution, based on the HPD concept. (Hint: Use Problem~\ref{exer:7.2.32}.)
\end{exercise}

\begin{solution}
Using the result in Problem \ref{exer:7.2.32} and the fact that the $t(2\alpha_0 + n)$ distribution is unimodal with mode at 0 and is symmetric about this mode, we have that a $\gamma$-prediction interval for $X_{n+1}$ is given by (following Example \ref{ex:7.2.8})
\[
    \mu_x \pm \sqrt{\frac{2\beta_x \{(n + 1/\tau_0^2)^{-1} + 1\}}{(2\alpha_0 + n)}} \, t_{\frac{1+\gamma}{2}}(2\alpha_0 + n).
\]
\end{solution}

\begin{exercise}
\label{exer:7.2.34}
Suppose that $\pi_1$ and $\pi_2$ are discrete probability distributions on the parameter space $\Omega$. Prove that when the prior $\pi$ is a mixture $p\pi_1 + (1-p)\pi_2$, then the prior predictive for the data $s$ is given by $m(s) = pm_1(s) + (1-p)m_2(s)$ and the posterior probability measure is given by \eqref{eq:7.2.6}.
\end{exercise}

\begin{solution}
The prior predictive probability measure for the data $s$ with a mixture of $\Pi_1$ and $\Pi_2$ prior distributions is given by
\begin{align*}
    m(s) &= E_\Pi(f_\theta(s)) = \sum_\theta f_\theta(s) \Pi(\{\theta\}) \\
    &= \sum_\theta f_\theta(s) (p\Pi_1(\{\theta\}) + (1 - p)\Pi_2(\{\theta\})) \\
    &= p \sum_\theta f_\theta(s) \Pi_1(\{\theta\}) + (1 - p) \sum_\theta f_\theta(s) \Pi_2(\{\theta\}) \\
    &= p f_{\theta_0}(s) + (1 - p) \sum_\theta f_\theta(s) \Pi_2(\{\theta\}) = pm_1(s) + (1 - p)m_2(s).
\end{align*}
The posterior probability measure is given by
\begin{align*}
    \Pi(A \mid s) &= \sum_{\theta \in A} \frac{f_\theta(s) \Pi(\{\theta\})}{m(s)} = \sum_{\theta \in A} \frac{f_\theta(s) (p\Pi_1(\{\theta\}) + (1 - p)\Pi_2(\{\theta\}))}{pm_1(s) + (1 - p)m_2(s)} \\
    &= \frac{pm_1(s)}{pm_1(s) + (1 - p)m_2(s)} \sum_{\theta \in A} \frac{f_\theta(s) \Pi_1(\{\theta\})}{m_1(s)} + \frac{(1 - p)m_2(s)}{pm_1(s) + (1 - p)m_2(s)} \sum_{\theta \in A} \frac{f_\theta(s) \Pi_2(\{\theta\})}{m_2(s)} \\
    &= \frac{pm_1(s)}{pm_1(s) + (1 - p)m_2(s)} \Pi_1(A \mid s) + \frac{(1 - p)m_2(s)}{pm_1(s) + (1 - p)m_2(s)} \Pi_2(A \mid s).
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:7.2.35}
(MV) Suppose that $\theta = (\theta_1, \theta_2) \in R^2$ and $h(\theta) = (\Psi(\theta), H(\theta)) \in R^2$. Assume that $h$ satisfies the necessary conditions and establish \eqref{eq:7.2.1}. (Hint: Theorem~\ref{thm:2.9.2}.)
\end{exercise}

\begin{solution}
The posterior density of $\theta$ is $\pi(\theta \mid s)$. Now make the transformation $\theta \to h(\theta) = (\psi(\theta), \lambda(\theta))$. Then following Section \ref{ssec:2.9.2}, we have that putting
\[
    J(\theta_1, \theta_2) = \begin{pmatrix} \dfrac{\partial \psi(\theta_1, \theta_2)}{\partial \theta_1} & \dfrac{\partial \psi(\theta_1, \theta_2)}{\partial \theta_2} \\[10pt] \dfrac{\partial \lambda(\theta_1, \theta_2)}{\partial \theta_1} & \dfrac{\partial \lambda(\theta_1, \theta_2)}{\partial \theta_2} \end{pmatrix}
\]
and an application of Theorem \ref{thm:2.9.2} establishes that the joint density of $(\psi, \lambda)$ is given by $\pi(h^{-1}(\psi, \lambda) \mid s) |J(h^{-1}(\psi, \lambda))|^{-1}$. Then the marginal density of $\psi$ is given by $\omega(\psi \mid s) = \int_{-\infty}^{\infty} \pi(h^{-1}(\psi, \lambda) \mid s) |J(\psi, \lambda)|^{-1} \, \mathrm{d}\lambda$.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:7.2.36}
Another way to assess the null hypothesis $H_0 : \psi(\theta) = \psi_0$ is to compute the P-value
\begin{equation}
\label{eq:7.2.12}
\pi\left(\psi : \frac{\pi(\psi \mid s)}{\pi(\psi)} \leqslant \frac{\pi(\psi_0 \mid s)}{\pi(\psi_0)} \,\bigg|\, s\right),
\end{equation}
where $\pi(\psi)$ is the marginal prior density or probability function of $\psi$. We call \eqref{eq:7.2.12} the observed relative surprise of $H_0$.

The quantity $\pi(\psi_0 \mid s)/\pi(\psi_0)$ is a measure of how the data $s$ have changed our a priori belief that $\psi_0$ is the true value of $\psi$. When \eqref{eq:7.2.12} is small, $\psi_0$ is a surprising value for $\psi$, as this indicates that the data have increased our belief more for other values of $\psi$.
\begin{enumerate}[(a)]
\item Prove that \eqref{eq:7.2.12} is invariant under 1--1 continuously differentiable transformations of $\psi$.
\item Show that a value $\hat{\psi}_0$ that makes \eqref{eq:7.2.12} smallest, maximizes $\pi(\psi_0 \mid s)/\pi(\psi_0)$. We call such a value a least relative surprise estimate of $\psi$.
\item Indicate how to use \eqref{eq:7.2.12} to form a $\gamma$-credible region, known as a $\gamma$-relative surprise region, for $\psi$.
\item Suppose that $\psi$ is real-valued with prior density $\pi(\psi)$ and posterior density $\pi(\psi \mid s)$ both continuous and positive at $\psi_0$. Let $A_\epsilon = (\psi_0 - \epsilon, \psi_0)$. Show that $BF_{A_\epsilon} \to \pi(\psi_0 \mid s)/\pi(\psi_0)$ as $\epsilon \to 0$. Generalize this to the case where $\psi$ takes its values in an open subset of $R^k$. This shows that we can think of the observed relative surprise as a way of calibrating Bayes factors.
\end{enumerate}
\end{exercise}

\begin{solution}
First, let $t = h(\psi)$ be a 1-1 continuously differentiable transformation of $\psi$. The null hypothesis that we want to test is $H_0 : h(\psi) = h(\psi_0) = t_0$. By Theorem \ref{thm:2.6.2} the prior density of $t$ is given by $q(t) = w(h^{-1}(t)) / |h'(h^{-1}(t))|$. Similarly, the posterior density of $t$ is given by $q(t \mid x) = w(h^{-1}(t) \mid x) / |h'(h^{-1}(t))|$. Hence, since $h^{-1}(t) = \psi$, the ratio of the two is given by $w(t \mid x)/w(t) = w(h^{-1}(t) \mid x)/w(h^{-1}(t)) = w(\psi \mid x)/w(\psi)$, which is the ratio given in (7.2.9). The observed ratio is given by $q(t_0 \mid x)/q(t_0) = w(h^{-1}(t_0) \mid x)/w(h^{-1}(t_0)) = w(\psi_0 \mid x)/w(\psi_0)$. Therefore, the $P$-value computed by (7.2.9) would give the same result, and therefore it is invariant.
\end{solution}

\section{Bayesian Computations}
\label{sec:7.3}

In virtually all the examples in this chapter so far, we have been able to work out the exact form of the posterior distributions and carry out a number of important computations using these. It often occurs, however, that we cannot derive any convenient form for the posterior distribution. Furthermore, even when we can derive the posterior distribution, there computations might arise that cannot be carried out exactly --- e.g., recall the discussion in Example~\ref{ex:7.2.1} that led to the integral \eqref{eq:7.2.2}. These calculations involve evaluating complicated sums or integrals. Therefore, when we apply Bayesian inference in a practical example, we need to have available methods for approximating these quantities.

The subject of approximating integrals is an extensive topic that we cannot deal with fully here.\footnote{See, for example, \emph{Approximating Integrals via Monte Carlo and Deterministic Methods}, by M.\ Evans and T.\ Swartz (Oxford University Press, Oxford, 2000).} We will, however, introduce several approximation methods that arise very naturally in Bayesian inference problems.

\subsection{Asymptotic Normality of the Posterior}
\label{ssec:7.3.1}

In many circumstances, it turns out that the posterior distribution of $\theta \in R^1$ is approximately normally distributed. We can then use this to compute approximate credible regions for the true value of $\theta$, carry out hypothesis assessment, etc. One such result says that, under conditions that we will not describe here, when $x_1, \ldots, x_n$ is a sample from $f_\theta$, then
\[
\pi\left(\frac{\theta - \hat{\theta}(x_1, \ldots, x_n)}{\hat{\sigma}(x_1, \ldots, x_n)} \leqslant z \,\bigg|\, x_1, \ldots, x_n\right) \to \Phi(z)
\]
as $n \to \infty$, where $\hat{\theta}(x_1, \ldots, x_n)$ is the posterior mode, and
\[
\hat{\sigma}^2(x_1, \ldots, x_n) = \left(-\frac{\partial^2 \ln L(\theta \mid x_1, \ldots, x_n)}{\partial \theta^2}\bigg|_{\theta = \hat{\theta}}\right)^{-1}.
\]
Note that this result is similar to Theorem~\ref{thm:6.5.3} for the MLE. Actually, we can replace $\hat{\theta}(x_1, \ldots, x_n)$ by the MLE and replace $\hat{\sigma}^2(x_1, \ldots, x_n)$ by the observed information (see Section~\ref{sec:6.5}), and the result still holds. When $\theta$ is $k$-dimensional, there is a similar but more complicated result.

\subsection{Sampling from the Posterior}
\label{ssec:7.3.2}

Typically, there are many things we want to compute as part of implementing a Bayesian analysis. Many of these can be written as expectations with respect to the posterior distribution of $\theta$. For example, we might want to compute the posterior probability content of a subset $A \subset \Omega$, namely,
\[
\pi(A \mid s) = \expc(\indc_A(\theta) \mid s).
\]
More generally, we want to be able to compute the posterior expectation of some arbitrary function $g(\theta)$, namely
\begin{equation}
\label{eq:7.3.1}
\expc(g(\theta) \mid s).
\end{equation}

It would certainly be convenient if we could compute all these quantities exactly, but quite often we cannot. In fact, it is not really necessary that we evaluate \eqref{eq:7.3.1} exactly. This is because we naturally expect any inference we make about the true value of the parameter to be subject (different data sets of the same size lead to different inferences) to sampling error. It is not necessary to carry out our computations to a much higher degree of precision than what sampling error contributes. For example, if the sampling error only allows us to know the value of a parameter to within only $0.1$ units, then there is no point in computing an estimate to many more digits of accuracy.

In light of this, many of the computational problems associated with implementing Bayesian inference are effectively solved if we can sample from the posterior $\pi(\cdot \mid s)$ for $\theta$. For when this is possible, we simply generate an i.i.d.\ sequence $\theta_1, \theta_2, \ldots, \theta_N$ from the posterior distribution of $\theta$ and estimate \eqref{eq:7.3.1} by
\[
\bar{g} = \frac{1}{N} \sum_{i=1}^{N} g(\theta_i).
\]
We know then, from the strong law of large numbers (see Theorem~\ref{thm:4.3.2}), that $\bar{g} \xrightarrow{\text{a.s.}} \expc(g(\theta) \mid x)$ as $N \to \infty$.

Of course, for any given $N$, the value of $\bar{g}$ only approximates \eqref{eq:7.3.1}; we would like to know that we have chosen $N$ large enough so that the approximation is appropriately accurate. When $\expc(g^2(\theta) \mid s) < \infty$, then the central limit theorem (see Theorem~\ref{thm:4.4.3}) tells us that
\[
\frac{\bar{g} - \expc(g(\theta) \mid s)}{\sigma/\sqrt{N}} \xrightarrow{D} N(0, 1)
\]
as $N \to \infty$, where $\sigma^2 = \var(g(\theta) \mid s)$. In general, we do not know the value of $\sigma^2$, but we can estimate it by
\[
s_g^2 = \frac{1}{N-1} \sum_{i=1}^{N} (g(\theta_i) - \bar{g})^2
\]
when $g(\theta)$ is a quantitative variable, and by $s_g^2 = \bar{g}(1 - \bar{g})$ when $g = \indc_A$ for $A \subset \Omega$. As shown in Section~\ref{ssec:4.4.2}, in either case, $s_g^2$ is a consistent estimate of $\sigma^2$. Then, by Corollary~\ref{cor:4.4.4}, we have that
\[
\frac{\bar{g} - \expc(g(\theta) \mid s)}{s_g/\sqrt{N}} \xrightarrow{D} N(0, 1)
\]
as $N \to \infty$.

From this result we know that
\[
\bar{g} \pm 3 \frac{s_g}{\sqrt{N}}
\]
is an approximate 100\% confidence interval for $\expc(g(\theta) \mid s)$, so we can look at $3s_g/\sqrt{N}$ to determine whether or not $N$ is large enough for the accuracy required.

One caution concerning this approach to assessing error is that $3s_g/\sqrt{N}$ is itself subject to error, as $s_g$ is an estimate of $\sigma$, so this could be misleading. A common recommendation then is to monitor the value of $3s_g/\sqrt{N}$ for successively larger values of $N$ and stop the sampling only when it is clear that the value of $3s_g/\sqrt{N}$ is small enough for the accuracy desired and appears to be declining appropriately. Even this approach, however, will not give a guaranteed bound on the accuracy of the computations, so it is necessary to be cautious.

It is also important to remember that application of these results requires that $\sigma^2 < \infty$. For a bounded $g$, this is always true, as any bounded random variable always has a finite variance. For an unbounded $g$, however, this must be checked --- sometimes this is very difficult to do.

We consider an example where it is possible to exactly sample from the posterior.

\begin{example}[Location-Scale Normal]
\label{ex:7.3.1}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution where $\mu \in R^1$ and $\sigma > 0$ are unknown, and we use the prior given in Example~\ref{ex:7.1.4}. The posterior distribution for $(\mu, \sigma^2)$ developed there is
\begin{equation}
\label{eq:7.3.2}
\mu \mid \sigma^2, x_1, \ldots, x_n \sim N\left(\mu(x), (n + \tau_0^{-2})^{-1} \sigma^2\right)
\end{equation}
and
\begin{equation}
\label{eq:7.3.3}
1/\sigma^2 \mid x_1, \ldots, x_n \sim \text{Gamma}(\alpha_0 + n/2, \beta(x)),
\end{equation}
where $\mu(x)$ is given by \eqref{eq:7.1.7} and $\beta(x)$ is given by \eqref{eq:7.1.8}.

Most statistical packages have built-in generators for gamma distributions and for the normal distribution. Accordingly, it is very easy to generate a sample $(\mu_1, \sigma_1^2), \ldots, (\mu_N, \sigma_N^2)$ from this posterior. We simply generate a value for $1/\sigma_i^2$ from the specified gamma distribution; then, given this value, we generate the value of $\mu_i$ from the specified normal distribution.

Suppose, then, that we want to derive the posterior distribution of the coefficient of variation $\psi = \sigma/\mu$. To do this we generate $N$ values from the joint posterior of $(\mu, \sigma^2)$, using \eqref{eq:7.3.2} and \eqref{eq:7.3.3}, and compute $\psi$ for each of these. We then know immediately that $\psi_1, \ldots, \psi_N$ is a sample from the posterior distribution of $\psi$.

As a specific numerical example, suppose that we observed the following sample $x_1, \ldots, x_{15}$:
\begin{align*}
&11.6714 \quad 1.8957 \quad 2.1228 \quad 2.1286 \quad 1.0751 \\
&8.1631 \quad 1.8236 \quad 4.0362 \quad 6.8513 \quad 7.6461 \\
&1.9020 \quad 7.4899 \quad 4.9233 \quad 8.3223 \quad 7.9486
\end{align*}
Here, $\bar{x} = 5.2$ and $s = 3.3$. Suppose further that the prior is specified by $\mu_0 = 4$, $\tau_0^2 = 2$, $\alpha_0 = 2$, and $\beta_0 = 1$.

From \eqref{eq:7.1.7}, we have
\[
\mu(x) = (15 + 1/2)^{-1}(4/2 + 15 \cdot 5.2) = 5.161,
\]
and from \eqref{eq:7.1.8},
\[
\beta(x) = 1 + \frac{15}{2}(5.2)^2 \cdot \frac{4/2}{2^2} + \frac{14}{2}(3.3)^2 + \frac{1}{2} \cdot \frac{15(5.2)^2}{1/2 + 15} = 77.578.
\]
Therefore, we generate
\[
1/\sigma^2 \mid x_1, \ldots, x_n \sim \text{Gamma}(9.5, 77.578)
\]
followed by
\[
\mu \mid \sigma^2, x_1, \ldots, x_n \sim N(5.161, (15.5)^{-1}\sigma^2).
\]
See Appendix B for some code that can be used to generate from this joint distribution.

In Figure~\ref{fig:7.3.1}, we have plotted a sample of $N = 200$ values of $(\mu, \sigma^2)$ from this joint posterior. In Figure~\ref{fig:7.3.2}, we have plotted a density histogram of the 200 values of $\psi$ that arise from this sample.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_3_1.pdf}
  \caption{A sample of 200 values of $(\mu, \sigma^2)$ from the joint posterior in Example~\ref{ex:7.3.1} when $n = 15$, $\bar{x} = 5.2$, $s = 3.3$, $\mu_0 = 4$, $\tau_0^2 = 2$, $\alpha_0 = 2$, and $\beta_0 = 1$.}
  \label{fig:7.3.1}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_3_2.pdf}
  \caption{A density histogram of 200 values from the posterior distribution of $\psi$ in Example~\ref{ex:7.3.1}.}
  \label{fig:7.3.2}
\end{figure}

A sample of 200 is not very large, so we next generated a sample of $N = 10^3$ values from the posterior distribution of $\psi$. A density histogram of these values is provided in Figure~\ref{fig:7.3.3}. In Figure~\ref{fig:7.3.4}, we have provided a density histogram based on a sample of $N = 10^4$ values. We can see from this that at $N = 10^3$, the basic shape of the distribution has been obtained, although the right tail is not being very accurately estimated. Things look better in the right tail for $N = 10^4$, but note there are still some extreme values quite disconnected from the main mass of values. As is characteristic of most distributions, we will need very large values of $N$ to accurately estimate the tails. In any case, we have learned that this distribution is skewed to the right with a long right tail.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_3_3.pdf}
  \caption{A density histogram of 1000 values from the posterior distribution of $\psi$ in Example~\ref{ex:7.3.1}.}
  \label{fig:7.3.3}
\end{figure}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_3_4.pdf}
  \caption{A density histogram of $N = 10^4$ values from the posterior distribution of $\psi$ in Example~\ref{ex:7.3.1}.}
  \label{fig:7.3.4}
\end{figure}

Suppose we want to estimate
\[
\pi(\psi \leqslant 0.5 \mid x_1, \ldots, x_n) = \expc(\indc_{(\psi \leqslant 0.5)}(\psi) \mid x_1, \ldots, x_n).
\]
Now $\indc_{(\psi \leqslant 0.5)}$ is bounded so its posterior variance exists. In the following table, we have recorded the estimates for each $N$ together with the standard error based on each of the generated samples. We have included some code for computing these estimates and their standard errors in Appendix B. Based on the results from $N = 10^4$, it would appear that this posterior probability is in the interval $0.289 \pm 3(0.0045) = [0.2755, 0.3025]$.

\begin{center}
\begin{tabular}{ccc}
$N$ & Estimate of $\pi(\psi \leqslant 0.5 \mid x_1, \ldots, x_n)$ & Standard Error \\
\hline
200 & 0.265 & 0.0312 \\
$10^3$ & 0.271 & 0.0141 \\
$10^4$ & 0.289 & 0.0045
\end{tabular}
\end{center}

This example also demonstrates an important point. It would be very easy for us to calculate the sample mean of the values of $\psi$ generated from its posterior distribution and then consider this as an estimate of the posterior mean of $\psi$. But Problem~\ref{exer:7.2.24} suggests (see Problem~\ref{exer:7.3.15}) that this mean will not exist. Accordingly, a Monte Carlo estimate of this quantity does not make any sense! So we must always check first that any expectation we want to estimate exists, before we proceed with some estimation procedure.

When we cannot sample directly from the posterior, then the methods of the following section are needed.
\end{example}

\subsection{Sampling from the Posterior Via Gibbs Sampling (Advanced)}
\label{ssec:7.3.3}

Sampling from the posterior, as described in Section~\ref{ssec:7.3.2}, is very effective, when it can be implemented. Unfortunately, it is often difficult or even impossible to do this directly, as we did in Example~\ref{ex:7.3.1}. There are, however, a number of algorithms that allow us to approximately sample from the posterior. One of these, known as Gibbs sampling, is applicable in many statistical contexts.

To describe this algorithm, suppose we want to generate samples from the joint distribution of $(Y_1, \ldots, Y_k) \in R^k$. Further suppose that we can generate from each of the full conditional distributions $Y_i \mid Y_{-i} = y_{-i}$, where
\[
Y_{-i} = (Y_1, \ldots, Y_{i-1}, Y_{i+1}, \ldots, Y_k),
\]
namely, we can generate from the conditional distribution of $Y_i$ given the values of all the other coordinates. The Gibbs sampler then proceeds iteratively as follows.

\begin{enumerate}
\item Specify an initial value $(y_1^{(0)}, \ldots, y_k^{(0)})$ for $(Y_1, \ldots, Y_k)$.

\item For $N = 0, 1, 2, \ldots$, generate $Y_i^{(N)}$ from its conditional distribution given $(y_1^{(N)}, \ldots, y_{i-1}^{(N)}, y_{i+1}^{(N-1)}, \ldots, y_k^{(N-1)})$ for each $i = 1, \ldots, k$.
\end{enumerate}

For example, if $k = 3$, we first specify $(y_1^{(0)}, y_2^{(0)}, y_3^{(0)})$. Then we generate
\begin{align*}
Y_1^{(1)} &\mid Y_2^{(0)} = y_2^{(0)}, Y_3^{(0)} = y_3^{(0)}, \\
Y_2^{(1)} &\mid Y_1^{(1)} = y_1^{(1)}, Y_3^{(0)} = y_3^{(0)}, \\
Y_3^{(1)} &\mid Y_1^{(1)} = y_1^{(1)}, Y_2^{(1)} = y_2^{(1)},
\end{align*}
to obtain $(Y_1^{(1)}, Y_2^{(1)}, Y_3^{(1)})$. Next we generate
\begin{align*}
Y_1^{(2)} &\mid Y_2^{(1)} = y_2^{(1)}, Y_3^{(1)} = y_3^{(1)}, \\
Y_2^{(2)} &\mid Y_1^{(2)} = y_1^{(2)}, Y_3^{(1)} = y_3^{(1)}, \\
Y_3^{(2)} &\mid Y_1^{(2)} = y_1^{(2)}, Y_2^{(2)} = y_2^{(2)},
\end{align*}
to obtain $(Y_1^{(2)}, Y_2^{(2)}, Y_3^{(2)})$, etc. Note that we actually did not need to specify $Y_1^{(0)}$ as it is never used.

It can then be shown (see Section~\ref{sec:11.3}) that, in fairly general circumstances, $(Y_1^{(N)}, \ldots, Y_k^{(N)})$ converges in distribution to the joint distribution of $(Y_1, \ldots, Y_k)$ as $N \to \infty$. So for large $N$, we have that the distribution of $(Y_1^{(N)}, \ldots, Y_k^{(N)})$ is approximately the same as the joint distribution of $(Y_1, \ldots, Y_k)$ from which we want to sample. So Gibbs sampling provides an approximate method for sampling from a distribution of interest.

Furthermore, and this is the result that is most relevant for simulations, it can be shown that, under conditions,
\[
\frac{1}{N} \sum_{i=1}^{N} g(Y_1^{(i)}, \ldots, Y_k^{(i)}) \xrightarrow{\text{a.s.}} \expc(g(Y_1, \ldots, Y_k)).
\]

Estimation of the variance of $\bar{g}$ is different than in the i.i.d.\ case, where we used the sample variance, because now the $(Y_1^{(i)}, \ldots, Y_k^{(i)})$ terms are not independent.

There are several approaches to estimating the variance of $\bar{g}$, but perhaps the most commonly used is the technique of batching. For this we divide the sequence
\[
(Y_1^{(0)}, \ldots, Y_k^{(0)}), \ldots, (Y_1^{(N)}, \ldots, Y_k^{(N)})
\]
into $N/m$ nonoverlapping sequential batches of size $m$ (assuming here that $N$ is divisible by $m$), calculate the mean $\bar{g}$ in each batch obtaining $\bar{g}_1, \ldots, \bar{g}_{N/m}$, and then estimate the variance of $\bar{g}$ by
\begin{equation}
\label{eq:7.3.4}
\frac{s_b^2}{N/m},
\end{equation}
where $s_b^2$ is the sample variance obtained from the batch means, i.e.,
\[
s_b^2 = \frac{1}{N/m - 1} \sum_{i=1}^{N/m} (\bar{g}_i - \bar{g})^2.
\]

It can be shown that $(Y_1^{(i)}, \ldots, Y_k^{(i)})$ and $(Y_1^{(i+m)}, \ldots, Y_k^{(i+m)})$ are approximately independent for $m$ large enough. Accordingly, we choose the batch size $m$ large enough so that the batch means are approximately independent, but not so large as to leave very few degrees of freedom for the estimation of the variance. Under ideal conditions, $\bar{g}_1, \ldots, \bar{g}_{N/m}$ is an i.i.d.\ sequence with sample mean
\[
\bar{g} = \frac{1}{N/m} \sum_{i=1}^{N/m} \bar{g}_i,
\]
and, as usual, we estimate the variance of $\bar{g}$ by \eqref{eq:7.3.4}.

Sometimes even Gibbs sampling cannot be directly implemented because we cannot obtain algorithms to generate from all the full conditionals. There are a variety of techniques for dealing with this, but in many statistical applications the technique of latent variables often works. For this, we search for some random variables, say $V_1, \ldots, V_l$, where each $Y_i$ is a function of $(V_1, \ldots, V_l)$ and such that we can apply Gibbs sampling to the joint distribution of $(V_1, \ldots, V_l)$. We illustrate Gibbs sampling via latent variables in the following example.

\begin{example}[Location-Scale Student]
\label{ex:7.3.2}
Suppose now that $x_1, \ldots, x_n$ is a sample from a distribution that is of the form $X = \mu + \sigma Z$, where $Z \sim t(\nu)$ (see Section~\ref{ssec:4.6.2} and Problem~\ref{exer:4.6.14}). If $\nu > 2$, then $\mu$ is the mean and $\sigma\sqrt{\nu/(\nu - 2)}$ is the standard deviation of the distribution (see Problem~\ref{exer:4.6.16}). Note that $\nu = \infty$ corresponds to normal variation, while $\nu = 1$ corresponds to Cauchy variation.

We will fix $\nu$ at some specified value to reflect the fact that we are interested in modeling situations in which the variable under consideration has a distribution with longer tails than the normal distribution. Typically, this manifests itself in a histogram of the data with a roughly symmetric shape but exhibiting a few extreme values out in the tails, so a $t$ distribution might be appropriate.

Suppose we place the prior on $(\mu, \sigma^2)$, given by $\mu \mid \sigma^2 \sim N(\mu_0, \tau_0^2 \sigma^2)$ and $1/\sigma^2 \sim \text{Gamma}(\alpha_0, \beta_0)$. The likelihood function is given by
\begin{equation}
\label{eq:7.3.5}
\left(\frac{1}{\sigma^2}\right)^{n/2} \prod_{i=1}^{n} \left(1 + \frac{(x_i - \mu)^2}{\nu\sigma^2}\right)^{-(\nu+1)/2},
\end{equation}
hence the posterior density of $(\mu, 1/\sigma^2)$ is proportional to
\begin{align*}
&\left(\frac{1}{\sigma^2}\right)^{n/2} \prod_{i=1}^{n} \left(1 + \frac{(x_i - \mu)^2}{\nu\sigma^2}\right)^{-(\nu+1)/2} \left(\frac{1}{\sigma^2}\right)^{1/2} \exp\left\{-\frac{1}{2\tau_0^2 \sigma^2}(\mu - \mu_0)^2\right\} \\
&\qquad \times \left(\frac{1}{\sigma^2}\right)^{\alpha_0 - 1} \exp\left\{-\frac{\beta_0}{\sigma^2}\right\}.
\end{align*}
This distribution is not immediately recognizable, and it is not at all clear how to generate from it.

It is natural, then, to see if we can implement Gibbs sampling. To do this directly, we need an algorithm to generate from the posterior of $\mu$ given the value of $\sigma^2$ and an algorithm to generate from the posterior of $\sigma^2$ given $\mu$. Unfortunately, neither of these conditional distributions is amenable to the techniques discussed in Section~\ref{sec:2.10}, so we cannot implement Gibbs sampling directly.

Recall, however, that when $V = \chi^2(\nu) \sim \text{Gamma}(\nu/2, 1/2)$ (see Problem~\ref{exer:4.6.13}) independent of $Y \sim N(0, \sigma^2)$, then (Problem~\ref{exer:4.6.14})
\[
Z = \frac{Y}{\sqrt{V/\nu}} \sim t(\nu).
\]
Therefore, writing
\[
X = \mu + \sigma Z = \mu + \sigma \frac{Y}{\sqrt{V/\nu}} = \mu + \frac{Y}{\sqrt{V/\nu}},
\]
we have that $X \mid V = v \sim N(\mu, \sigma^2 \nu/v)$.

We now introduce the $n$ latent or hidden variables $V_1, \ldots, V_n$ which are i.i.d.\ $\chi^2(\nu)$ and suppose $X_i \mid V_i = v_i \sim N(\mu, \sigma^2 \nu/v_i)$. The $V_i$ are considered latent because they are not really part of the problem formulation but have been added here for convenience (as we shall see). Then, noting that there is a factor $(v_i/\sigma^2)^{1/2}$ associated with the density of $X_i \mid V_i = v_i$, the joint density of the values $(X_1, V_1), \ldots, (X_n, V_n)$ is proportional to
\[
\left(\frac{1}{\sigma^2}\right)^{n/2} \prod_{i=1}^{n} \exp\left\{-\frac{v_i}{2\sigma^2}(x_i - \mu)^2\right\} v_i^{(\nu+1)/2 - 1} \exp\left\{-\frac{v_i}{2}\right\}.
\]
From the above argument, the marginal joint density of $(X_1, \ldots, X_n)$ (after integrating out the $v_i$'s) is proportional to \eqref{eq:7.3.5}, namely, a sample of $n$ from the distribution specified by $X = \mu + \sigma Z$, where $Z \sim t(\nu)$. With the same prior structure as before, we have that the joint density of
\[
((X_1, V_1), \ldots, (X_n, V_n), \mu, 1/\sigma^2)
\]
is proportional to
\begin{align}
&\left(\frac{1}{\sigma^2}\right)^{n/2} \prod_{i=1}^{n} \exp\left\{-\frac{v_i}{2\sigma^2}(x_i - \mu)^2\right\} v_i^{(\nu+1)/2 - 1} \exp\left\{-\frac{v_i}{2}\right\} \notag \\
&\qquad \times \left(\frac{1}{\sigma^2}\right)^{1/2} \exp\left\{-\frac{1}{2\tau_0^2 \sigma^2}(\mu - \mu_0)^2\right\} \left(\frac{1}{\sigma^2}\right)^{\alpha_0 - 1} \exp\left\{-\frac{\beta_0}{\sigma^2}\right\}. \label{eq:7.3.6}
\end{align}

In \eqref{eq:7.3.6}, treat $x_1, \ldots, x_n$ as constants (we observed these values) and consider the conditional distributions of each of the variables $V_1, \ldots, V_n, \mu, 1/\sigma^2$ given all the other variables. From \eqref{eq:7.3.6}, we have that the full conditional density of $\mu$ is proportional to
\[
\exp\left\{-\frac{1}{2\sigma^2}\left(\sum_{i=1}^{n} v_i (x_i - \mu)^2 + \frac{1}{\tau_0^2}(\mu - \mu_0)^2\right)\right\},
\]
which is proportional to
\[
\exp\left\{-\frac{1}{2\sigma^2}\left(\sum_{i=1}^{n} v_i + \frac{1}{\tau_0^2}\right)\left(\mu - \left(\sum_{i=1}^{n} v_i + \frac{1}{\tau_0^2}\right)^{-1}\left(\sum_{i=1}^{n} v_i x_i + \frac{\mu_0}{\tau_0^2}\right)\right)^2\right\}.
\]
From this, we immediately deduce that
\[
\mu \mid x_1, \ldots, x_n, v_1, \ldots, v_n, \sigma^2 \sim N\left(r(v_1, \ldots, v_n)^{-1}\left(\sum_{i=1}^{n} v_i x_i + \frac{\mu_0}{\tau_0^2}\right), r(v_1, \ldots, v_n)^{-1} \sigma^2\right),
\]
where
\[
r(v_1, \ldots, v_n) = \left(\sum_{i=1}^{n} v_i + \frac{1}{\tau_0^2}\right)^{-1}.
\]

From \eqref{eq:7.3.6}, we have that the conditional density of $1/\sigma^2$ is proportional to
\[
\left(\frac{1}{\sigma^2}\right)^{n/2 + \alpha_0 + 1/2 - 1} \exp\left\{-\left(\sum_{i=1}^{n} v_i(x_i - \mu)^2 + \frac{1}{\tau_0^2}(\mu - \mu_0)^2 + 2\beta_0\right) \frac{1}{2\sigma^2}\right\},
\]
and we immediately deduce that
\[
\frac{1}{\sigma^2} \mid x_1, \ldots, x_n, v_1, \ldots, v_n, \mu \sim \text{Gamma}\left(\frac{n}{2} + \alpha_0 + \frac{1}{2}, \frac{1}{2}\left(\sum_{i=1}^{n} v_i(x_i - \mu)^2 + \frac{1}{\tau_0^2}(\mu - \mu_0)^2 + 2\beta_0\right)\right).
\]

Finally, the conditional density of $V_i$ is proportional to
\[
v_i^{(\nu+1)/2 - 1} \exp\left\{-\left(\frac{(x_i - \mu)^2}{2\sigma^2} + \frac{1}{2}\right) v_i\right\},
\]
and it is immediate that
\[
V_i \mid x_1, \ldots, x_n, v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n, \mu, \sigma^2 \sim \text{Gamma}\left(\frac{\nu+1}{2}, \frac{1}{2}\left(\frac{(x_i - \mu)^2}{\sigma^2} + 1\right)\right).
\]

We can now easily generate from all these distributions and implement a Gibbs sampling algorithm. As we are not interested in the values of $V_1, \ldots, V_n$, we simply discard these as we iterate.

Let us now consider a specific computation using the same data and prior as in Example~\ref{ex:7.3.1}. The analysis of Example~\ref{ex:7.3.1} assumed that the data were coming from a normal distribution, but now we are going to assume that the data are a sample from a $t(3)$ distribution, i.e., $\nu = 3$. We again consider approximating the posterior distribution of the coefficient of variation $\psi = \sigma/\mu$.

We carry out the Gibbs sampling iteration in the order $v_1, \ldots, v_n, \mu, 1/\sigma^2$. This implies that we need starting values only for $\mu$ and $\sigma^2$ (the full conditionals of the $v_i$ do not depend on the other $v_j$). We take the starting value of $\mu$ to be $\bar{x} = 5.2$ and the starting value of $\sigma$ to be $s = 3.3$. For each generated value of $(\mu, \sigma^2)$, we calculate $\psi$ to obtain the sequence $\psi_1, \psi_2, \ldots, \psi_N$.

The values $\psi_1, \psi_2, \ldots, \psi_N$ are not i.i.d.\ from the posterior of $\psi$. The best we can say is that
\[
\psi_m \xrightarrow{D} \pi(\psi \mid x_1, \ldots, x_n)
\]
as $m \to \infty$, where $\pi(\psi \mid x_1, \ldots, x_n)$ is the posterior density of $\psi$. Also, values sufficiently far apart in the sequence, will be like i.i.d.\ values from $\pi(\psi \mid x_1, \ldots, x_n)$. Thus, one approach is to determine an appropriate value $m$ and then extract $\psi_m, \psi_{2m}, \psi_{3m}, \ldots$ as an approximate i.i.d.\ sequence from the posterior. Often it is difficult to determine an appropriate value for $m$, however.

In any case, it is known that, under fairly weak conditions,
\[
\frac{1}{N} \sum_{i=1}^{N} g(\psi_i) \xrightarrow{\text{a.s.}} \expc(g(\psi) \mid x_1, \ldots, x_n)
\]
as $N \to \infty$. So we can use the whole sequence $\psi_1, \psi_2, \ldots, \psi_N$ and record a density histogram for $\psi$, just as we did in Example~\ref{ex:7.3.1}. The value of the density histogram between two cut points will converge almost surely to the correct value as $N \to \infty$. However, we will have to take $N$ larger when using the Gibbs sampling algorithm than with i.i.d.\ sampling, to achieve the same accuracy. For many examples, the effect of the deviation of the sequence from being i.i.d.\ is very small, so $N$ will not have to be much larger. We always need to be cautious, however, and the general recommendation is to compute estimates for successively higher values of $N$, only stopping when the results seem to have stabilized.

In Figure~\ref{fig:7.3.5}, we have plotted the density histogram of the $\psi$ values that resulted from $10^4$ iterations of the Gibbs sampler. In this case, plotting the density histogram of $\psi$ based upon $N = 5 \times 10^4$ and $N = 8 \times 10^4$ resulted in only minor deviations from this plot. Note that this density looks very similar to that plotted in Example~\ref{ex:7.3.1}, but it is not quite so peaked and it has a shorter right tail.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_3_5.pdf}
  \caption{A density histogram of $N = 10^4$ values of $\psi$ generated sequentially via Gibbs sampling in Example~\ref{ex:7.3.2}.}
  \label{fig:7.3.5}
\end{figure}

We can also estimate $\pi(\psi \leqslant 0.5 \mid x_1, \ldots, x_n)$ just as we did in Example~\ref{ex:7.3.1}, by recording the proportion of $\psi$ values in the sequence that are smaller than 0.5, i.e., $g(\psi) = \indc_A(\psi)$, where $A = \{\psi : \psi \leqslant 0.5\}$. In this case, we obtained the estimate $0.5441$, which is quite different from the value obtained in Example~\ref{ex:7.3.1}. So using a $t(3)$ distribution to describe the variation in the response has made a big difference in the results.

Of course, we must also quantify how accurate we believe our estimate is. Using a batch size of $m = 10$, we obtained the standard error of the estimate $0.5441$ to be $0.00639$. When we took the batch size to be $m = 20$, the standard error of the mean is $0.00659$; with a batch size of $m = 40$, the standard error of the mean is $0.00668$. So we feel quite confident that we are assessing the error in the estimate appropriately. Again, under conditions, we have that $\bar{g}$ is asymptotically normal so that in this case we can assert that the interval $0.5441 \pm 3(0.0066) = [0.5243, 0.5639]$ contains the true value of $\pi(\psi \leqslant 0.5 \mid x_1, \ldots, x_n)$ with virtual certainty.

See Appendix B for some code that was used to implement the Gibbs sampling algorithm described here.

It is fair to say that the introduction of Gibbs sampling has resulted in a revolution in statistical applications due to the wide variety of previously intractable problems that it successfully handles. There are a number of modifications and closely related algorithms. We refer the interested reader to Chapter~\ref{ch:11}, where the general theory of what is called Markov chain Monte Carlo (MCMC) is discussed.
\end{example}

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:7.3}}

\begin{itemize}
\item Implementation of Bayesian inference often requires the evaluation of complicated integrals or sums.

\item If, however, we can sample from the posterior of the parameter, this will often lead to sufficiently accurate approximations to these integrals or sums via Monte Carlo.

\item It is often difficult to sample exactly from a posterior distribution of interest. In such circumstances, Gibbs sampling can prove to be an effective method for generating an approximate sample from this distribution.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:7.3.1}
Suppose we have the following sample from an $N(\theta, 2)$ distribution, where $\theta$ is unknown.
\begin{align*}
&2.6 \quad 4.2 \quad 3.1 \quad 5.2 \quad 3.7 \quad 3.8 \quad 5.6 \quad 1.8 \quad 5.3 \quad 4.0 \\
&3.0 \quad 4.0 \quad 4.1 \quad 3.2 \quad 2.2 \quad 3.4 \quad 4.5 \quad 2.9 \quad 4.7 \quad 5.2
\end{align*}
If the prior on $\theta$ is Uniform$[2, 6]$, determine an approximate 0.95-credible interval for $\theta$ based on the large sample results described in Section~\ref{ssec:7.3.1}.
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\mu \mid x_1, \ldots, x_n) = (4\pi)^{-10} \exp\left(-5(\bar{x} - \mu)^2\right) \exp\left(-\frac{19}{4} s^2\right).
\]
The prior distribution has density given by $\pi(\mu) = \frac{1}{4} \indc_{[2,6]}(\mu)$. The posterior density is then proportional to $(4\pi)^{-10} \exp(-5(\bar{x} - \mu)^2) \exp(-\frac{19}{4} s^2) \frac{1}{4} \indc_{[2,6]}(\mu)$. To find the posterior mode we need only maximize $\exp(-5(\bar{x} - \mu)^2) \indc_{[2,6]}(\mu)$, which is clearly maximized at $\hat{\mu} = \bar{x}$ when $\bar{x} \in [2, 6]$, at $\hat{\mu} = 2$ when $\bar{x} < 2$, and at $\hat{\mu} = 6$ when $\bar{x} > 6$. In this case the posterior mode is then $\hat{\mu} = \bar{x} = 3.825$. It has variance, estimated by
\[
    \hat{\sigma}^2(x_1, \ldots, x_n) = \left[-\frac{\partial^2 \ln\left((4\pi)^{-10} \exp(-5(\bar{x} - \mu)^2) \times \exp\left(-\frac{19}{4} s^2\right) \frac{1}{4} \indc_{[2,6]}(\mu)\right)}{\partial \mu^2} \bigg|_{\mu = 3.825}\right]^{-1} = \frac{1}{10}.
\]
A 0.95 credible interval for $\mu$ based on the large sample result is then given by
\[
    \bar{x} \pm \hat{\sigma}(x_1, \ldots, x_n) z_{0.975} = 3.825 \pm \frac{1}{\sqrt{10}} \cdot 1.96 = (3.2052, 4.4448).
\]
\end{solution}

\begin{exercise}
\label{exer:7.3.2}
Determine the form of the approximate 0.95-credible interval of Section~\ref{ssec:7.3.1}, for the Bernoulli model with a Beta$(\alpha, \beta)$ prior, discussed in Example~\ref{ex:7.2.2}.
\end{exercise}

\begin{solution}
Let $X_1, \ldots, X_n$ be a random sample from $\text{Bernoulli}(\theta)$. Then, $T = X_1 + \cdots + X_n$ is a minimal sufficient statistic having a distribution $\text{Binomial}(n, \theta)$. The likelihood function is $L(\theta \mid x_1, \ldots, x_n) = L(\theta \mid t) = \theta^t (1 - \theta)^{n-t}$. Note $L(\theta \mid t) \pi(\theta) \propto \theta^{t + \alpha - 1} (1 - \theta)^{n - t + \beta - 1}$. Hence, the posterior mode is $\hat{\theta} = (t + \alpha - 1)/(n + \alpha - 2)$. Then, we get $\frac{\partial}{\partial \theta} \ln L(\theta \mid t) \pi(\theta) = (t + \alpha - 1)/\theta - (n - t + \beta - 1)/(1 - \theta)$, $\frac{\partial^2}{\partial \theta^2} \ln L(\theta \mid t) \pi(\theta) = -(t + \alpha - 1)/\theta^2 - (n - t + \beta - 1)/(1 - \theta)^2$. The asymptotic variance of the posterior mode is
\[
    \hat{\sigma}^2(x_1, \ldots, x_n) = \left(-\frac{\partial^2 \ln L(\theta \mid t) \pi(\theta)}{\partial \theta^2} \bigg|_{\theta = \hat{\theta}}\right)^{-1} = \left(\frac{t + \alpha - 1}{\hat{\theta}^2} + \frac{n - t + \beta - 1}{(1 - \hat{\theta})^2}\right)^{-1}.
\]
Hence, the asymptotic $\gamma$-credible interval is
\[
    (\hat{\theta} - z_{(1+\gamma)/2} \hat{\sigma}, \, \hat{\theta} + z_{(1+\gamma)/2} \hat{\sigma}).
\]
\end{solution}

\begin{exercise}
\label{exer:7.3.3}
Determine the form of the approximate 0.95-credible intervals of Section~\ref{ssec:7.3.1}, for the location-normal model with an $N(\mu_0, \tau_0^2)$ prior, discussed in Example~\ref{ex:7.2.3}.
\end{exercise}

\begin{solution}
Let $X_1, \ldots, X_n$ be a random sample from $N(\mu, v_0^2)$. Then, $T = \bar{X} = (X_1 + \cdots + X_n)/n$ is a minimal sufficient statistic having a distribution $N(\mu, v_0^2/n)$. The likelihood function is $L(\mu \mid x_1, \ldots, x_n) = L(\mu \mid t) = \exp(-(\mu - t)^2/(2v_0^2/n))$. Note $L(\mu \mid t) \pi(\mu) \propto \exp(-(\mu - \mu_1)^2/(2\sigma_1^2))$ where $\mu_1 = (nt/v_0^2 + \mu_0/\sigma_0^2)/(n/v_0^2 + 1/\sigma_0^2)$ and $\sigma_1^2 = (n/v_0^2 + 1/\sigma_0^2)^{-1}$. Hence, the posterior mode estimator is $\hat{\mu} = \mu_1 = (nt/v_0^2 + \mu_0/\sigma_0^2)/(n/v_0^2 + 1/\sigma_0^2)$. We get $\frac{\partial}{\partial \mu} \ln L(\mu \mid t) \pi(\mu) = -(\mu - \mu_1)/\sigma_1^2$ and $\frac{\partial^2}{\partial \mu^2} \ln L(\mu \mid t) \pi(\mu) = -1/\sigma_1^2$. The variance estimate is
\[
    \hat{\sigma}^2(x_1, \ldots, x_n) = \left(-\frac{\partial \ln L(\mu \mid t) \pi(\mu)}{\partial \mu^2} \bigg|_{\mu = \hat{\mu}}\right)^{-1} = \sigma_1^2.
\]
Hence, the asymptotic $\gamma$-credible interval is
\[
    (\hat{\mu} - z_{(1+\gamma)/2} \hat{\sigma}, \, \hat{\mu} + z_{(1+\gamma)/2} \hat{\sigma}).
\]
\end{solution}

\begin{exercise}
\label{exer:7.3.4}
Suppose that $X \mid \theta \sim \text{Uniform}[0, 1/\theta]$ and $\theta \sim \text{Exponential}(1)$. Derive a crude Monte Carlo algorithm, based on generating from a gamma distribution, to generate a value from the conditional distribution $\theta \mid X = x$. Generalize this to a sample of $n$ from the Uniform$[0, 1/\theta]$ distribution. When will this algorithm be inefficient in the sense that we need a lot of computation to generate a single value?
\end{exercise}

\begin{solution}
The posterior density is proportional to $f_\theta(x) \cdot \pi(\theta) = \theta \indc_{[0, 1/\theta]}(x) \cdot e^{-\theta} = \indc_{(0, 1/x]}(\theta) \theta e^{-\theta}$. Hence, the posterior distribution is a $\text{Gamma}(2, 1)$ distribution restricted to $(0, 1/x]$. A simple Monte Carlo algorithm is:
\begin{enumerate}[1:]
    \item Generate $\eta$ from $\text{Gamma}(2, 1)$
    \item Accept $\eta$ if it is in $(0, 1/x]$. Return to step 1 otherwise.
\end{enumerate}

In general, the posterior density is proportional to $f_\theta(x_1, \ldots, x_n) \cdot \pi(\theta) = \indc_{(0, 1/x_{(n)}]}(\theta) \theta^n e^{-\theta}$ that is proportional to $\text{Gamma}(n + 1, 1)$ restricted on $(0, 1/x_{(n)}]$. Also we have a simple Monte Carlo algorithm:
\begin{enumerate}[1:]
    \item Generate $\eta$ from $\text{Gamma}(n + 1, 1)$
    \item Accept $\eta$ if it is in $(0, 1/x_{(n)}]$. Return to step 1 otherwise.
\end{enumerate}

Note that the mean of a $\text{Gamma}(n + 1, 1)$ distribution is $n + 1$. That means the $\text{Gamma}(n + 1, 1)$ distribution shifts to the right as $n \to \infty$. So the rejection rate will increase to 1 as $n \to \infty$. Hence, this algorithm cannot be used for large $n$.
\end{solution}

\begin{exercise}
\label{exer:7.3.5}
Suppose that $X \mid \theta \sim N(\theta, 1)$ and $\theta \sim \text{Uniform}[0, 1]$. Derive a crude Monte Carlo algorithm, based on generating from a normal distribution, to generate from the conditional distribution $\theta \mid X = x$. Generalize this to a sample of $n$ from the $N(\theta, 1)$ distribution. When will this algorithm be inefficient in the sense that we need a lot of computation to generate a single value?
\end{exercise}

\begin{solution}
The posterior density when $X = x$ is observed is proportional to $\exp(-(x - \theta)^2/2) \indc_{[0,1]}(\theta)$. Hence, the posterior distribution is $N(x, 1)$ restricted to $[0, 1]$. Hence, a very simple Monte Carlo algorithm is given by:
\begin{enumerate}[1:]
    \item Generate $\eta$ from $N(x, 1)$
    \item Accept $\eta$ if it is in $[0, 1]$. Return to step 1 otherwise.
\end{enumerate}

In general, when a sample $(x_1, \ldots, x_n)$ is observed, the posterior density is proportional to $\exp(-\sum_{i=1}^{n} (x_i - \theta)^2/2) \cdot \indc_{[0,1]}(\theta) \propto \exp(-n(\theta - \bar{x})^2/2) \indc_{[0,1]}(\theta)$. Thus, the posterior distribution is $N(\bar{x}, 1/n)$ restricted to $[0, 1]$. A simple Monte Carlo algorithm for the posterior distribution is:
\begin{enumerate}[1:]
    \item Generate $\eta$ from $N(\bar{x}, 1/n)$
    \item Accept $\eta$ if it is in $[0, 1]$. Return to step 1 otherwise.
\end{enumerate}

If the true parameter $\theta^*$ is not in $[0, 1]$, then the acceptance rate is extremely small. For example, suppose $\theta^* > 1$ and $n$ is sufficiently large enough to $\bar{x} > 1$. Then the acceptance rate given by
\[
    \prb(\eta \in [0, 1]) = \Phi(-\sqrt{n}(\bar{x} - 1)) - \Phi(-\sqrt{n}\bar{x}) \leqslant \frac{\exp(-n(\bar{x} - 1)^2/2)}{(\bar{x} - 1)\sqrt{2\pi n}} \to 0
\]
converges to 0 exponentially. Hence, the Monte Carlo algorithm is not appropriate when $n$ is big.
\end{solution}

\begin{exercise}
\label{exer:7.3.6}
Suppose that $X \mid \theta \sim 0.5 N(\theta, 1) + 0.5 N(\theta, 2)$ and $\theta \sim \text{Uniform}[0, 1]$. Derive a crude Monte Carlo algorithm, based on generating from a mixture of normal distributions, to generate from the conditional distribution $\theta \mid X = x$. Generalize this to a sample of $n \geqslant 2$ from the $0.5 N(\theta, 1) + 0.5 N(\theta, 2)$ distribution.
\end{exercise}

\begin{solution}
The posterior density is proportional to $(\exp(-({\theta - x})^2/2) + \exp(-(\theta - x)^2/4)/\sqrt{2}) \indc_{[0,1]}(\theta)$. Hence, the posterior distribution given $X = x$ is the mixture of normals $0.5 N(x, 1) + 0.5 N(x, 2)$ restricted to $[0, 1]$. A crude Monte Carlo algorithm is obtained as follows.
\begin{enumerate}[1:]
    \item Generate $\eta$ from $0.5 N(x, 1) + 0.5 N(x, 2)$
    \item Accept $\eta$ if it is in $[0, 1]$. Return to step 1 otherwise.
\end{enumerate}

Suppose $n = 2$. The likelihood function is proportional to
\begin{align*}
    &(\exp(-(x_1 - \theta)^2/2) + \exp(-(x_1 - \theta)^2/4)/\sqrt{2}) \times (\exp(-(x_2 - \theta)^2/2) + \exp(-(x_2 - \theta)^2/4)/\sqrt{2}) \\
    &= \exp(-(x_1 - x_2)^2/4) \exp(-(\theta - (x_1 + x_2)/2)^2) \\
    &\quad + \exp(-(x_1 - x_2)^2/6) \exp(-(\theta - (2x_1 + x_2)/3)^2/(4/3))/\sqrt{2} \\
    &\quad + \exp(-(x_1 - x_2)^2/6) \exp(-(\theta - (x_1 + 2x_2)/3)^2/(4/3))/\sqrt{2} \\
    &\quad + \exp(-(x_1 - x_2)^2/8) \exp(-(\theta - (x_1 + x_2)/2)^2/2)/2.
\end{align*}
Hence, the posterior distribution given $X_1 = x_1$, $X_2 = x_2$ is a mixture normal restricted on $[0, 1]$. A crude Monte Carlo algorithm can be devised easily.
\begin{enumerate}[1:]
    \item Generate $\eta$ from $p_1 N((x_1 + x_2)/2, 1/2) + p_2 N((2x_1 + x_2)/3, 2/3) + p_3 N((x_1 + 2x_2)/3, 2/3) + p_4 N((x_1 + x_2)/2, 1)$ where $p_i = q_i/(q_1 + \cdots + q_4)$, $q_1 = \exp(-(x_1 - x_2)^2/4)/\sqrt{2}$, $q_2 = q_3 = \exp(-(x_1 - x_2)^2/6)/\sqrt{3}$ and $q_4 = \exp(-(x_1 - x_2)^2/8)/2$.
    \item Accept $\eta$ if it is in $[0, 1]$. Return to step 1 otherwise.
\end{enumerate}
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:7.3.7}
In the context of Example~\ref{ex:7.3.1}, construct a density histogram of the posterior distribution of $\mu + \sigma z_{0.25}$, i.e., the population first quartile, using $N = 5 \times 10^3$ and $N = 10^4$ and compare the results. Estimate the posterior mean of this distribution and assess the error in your approximation. (Hint: Modify the program in Appendix B.)
\end{exercise}

\begin{solution}
Below is the R program (modifying the one in Appendix B for Example \ref{ex:7.3.1}) used to generate the sample of size $N = 10^4$ from the posterior distribution of $\psi = \mu + \sigma z_{0.25}$.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(34256734)

# Parameters of the posterior
k1 <- 9.5       # first parameter of the gamma distribution = (alpha_0 + n/2)
k2 <- 1/77.578  # 1/beta
k3 <- 5.161     # posterior mean
k4 <- 1/15.5    # (n + 1/(tau_0 squared))^(-1)

# z_0.25 = -0.6745
z_025 <- qnorm(0.25)

N <- 10000
c3 <- numeric(N)  # generated value of sigma^2
c4 <- numeric(N)  # generated value of mu
c5 <- numeric(N)  # generated value of first quartile

for (i in 1:N) {
    c1 <- rgamma(1, shape = k1, rate = 1/k2)
    c3[i] <- 1/c1
    k6 <- sqrt(k4/c1)
    c4[i] <- rnorm(1, mean = k3, sd = k6)
    c5[i] <- c4[i] + z_025 * sqrt(c3[i])
}

# Results for N = 10000
k1_est <- mean(c5)
k2_se <- sd(c5)/sqrt(N)
k3_lower <- k1_est - 3*k2_se
k4_upper <- k1_est + 3*k2_se
cat("K1:", k1_est, "\n")
cat("K3:", k3_lower, "\n")
cat("K4:", k4_upper, "\n")
\end{minted}
\caption{Simulation for posterior mean of first quartile (prob7307.R)}
\label{lst:prob7307}
\end{listing}

Below are the density histograms based on samples of $N = 5 \times 10^3$ and $N = 10^4$, respectively.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig7307.pdf}
    \caption{Density histograms for samples of $N = 5 \times 10^3$ (left) and $N = 10^4$ (right) for Exercise 7.3.7}
    %\label{fig:density-histograms-7307}
\end{figure}

For $N = 5 \times 10^3$ we obtained the following estimates. So the estimate of the posterior mean of the first quartile is 3.17641, and the exact value lies in the interval $(3.14068, 3.21214)$ with virtual certainty.

For $N = 10^4$ we obtained the following estimates. So the estimate of the posterior mean of the first quartile is 3.15800 and the exact value lies in the interval $(3.13253, 3.18346)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:7.3.8}
Suppose that a manufacturer takes a random sample of manufactured items and tests each item as to whether it is defective or not. The responses are felt to be i.i.d.\ Bernoulli$(\theta)$, where $\theta$ is the probability that the item is defective. The manufacturer places a Beta$(0.5, 10)$ distribution on $\theta$. If a sample of $n = 100$ items is taken and 5 defectives are observed, then, using a Monte Carlo sample with $N = 1000$, estimate the posterior probability that $\theta \leqslant 0.1$ and assess the error in your estimate.
\end{exercise}

\begin{solution}
Recall that from Example \ref{ex:7.1.1} we have that the posterior distribution of $\theta$ is $\text{Beta}(5.5, 105)$. Below is the R code for doing this problem.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
c1 <- rbeta(1000, shape1 = 5.5, shape2 = 105)
c2 <- as.numeric(c1 < 0.1)
k1 <- mean(c2)
k2 <- sqrt(k1 * (1 - k1)) / sqrt(1000)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2
cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Simulation for posterior probability $\theta < 0.1$ (prob7308.R)}
\label{lst:prob7308}
\end{listing}

The estimate of the posterior probability that $\theta < 0.1$ based on a sample of 1000 from the posterior is 0.980000, and the exact value lies in the interval $(0.966718, 0.993282)$ with virtual certainty.
\end{solution}

\begin{exercise}
\label{exer:7.3.9}
Suppose that lifelengths (in years) of a manufactured item are known to follow an Exponential$(\lambda)$ distribution, where $\lambda > 0$ is unknown and for the prior we take $\lambda \sim \text{Gamma}(10, 2)$. Suppose that the lifelengths 4.3, 6.2, 8.4, 3.1, 6.0, 5.5, and 7.8 were observed.
\begin{enumerate}[(a)]
\item Using a Monte Carlo sample of size $N = 10^3$, approximate the posterior probability that $\lambda \in [3, 6]$ and assess the error of your estimate.
\item Using a Monte Carlo sample of size $N = 10^3$, approximate the posterior probability function of $\lfloor 1/\lambda \rfloor$ ($\lfloor x \rfloor$ equals the greatest integer less than or equal to $x$).
\item Using a Monte Carlo sample of size $N = 10^3$, approximate the posterior expectation of $1/\lambda$ and assess the error in your approximation.
\end{enumerate}
\end{exercise}

\begin{solution}
Recall that from Exercise \ref{exer:7.2.10} we have that the posterior distribution of $\lambda$, with $n = 7$, $\bar{x} = 5.9$, is $\text{Gamma}(17, 43.3)$.

\begin{enumerate}[(a)]
    \item The estimate of the posterior probability that $1/\lambda \in [3, 6]$ based on a sample of $N = 1000$ from the posterior of $1/\lambda$ is obtained via the following R program.
    
\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
k1 <- 17
k2 <- 43.3
c1 <- rgamma(1000, shape = k1, rate = k2)
c2 <- 1/c1
c3 <- as.numeric(c2 >= 3 & c2 <= 6)
k3 <- mean(c3)
k4 <- sqrt(k3 * (1 - k3) / 1000)
k5 <- k3 - 3 * k4
k6 <- k3 + 3 * k4
cat("K3:", k3, "\n")
cat("K5:", k5, "\n")
cat("K6:", k6, "\n")
\end{minted}
\caption{Simulation for posterior probability $1/\lambda \in [3, 6]$ (prob7309a.R)}
\label{lst:prob7309a}
\end{listing}

    The estimate of the posterior probability that $1/\lambda \in [3, 6]$ is 0.296000, and the exact value of the posterior probability lies in the interval $(0.252693, 0.339307)$ with virtual certainty.
    
    \item The probability function of $\lfloor 1/\lambda \rfloor$ is estimated as follows.
    
\begin{listing}[!htbp]
\begin{minted}{R}
c4 <- floor(c2)
table(c4)
prop.table(table(c4)) * 100
\end{minted}
\caption{Estimation of probability function of $\lfloor 1/\lambda \rfloor$ (prob7309b.R)}
\label{lst:prob7309b}
\end{listing}

    So, for example, we estimate the posterior probability that $\lfloor 1/\lambda \rfloor$ equals 0 by 0 and the posterior probability that $\lfloor 1/\lambda \rfloor$ equals 1 by 0.13, etc.
    
    \item The estimate of the posterior expectation of $\lfloor 1/\lambda \rfloor$ based on a Monte Carlo sample of size $N = 10^3$ is given below.
    
\begin{listing}[!htbp]
\begin{minted}{R}
k1 <- mean(c2)
k2 <- sd(c2) / sqrt(1000)
k3 <- k1 - 3 * k2
k4 <- k1 + 3 * k2
cat("K1:", k1, "\n")
cat("K3:", k3, "\n")
cat("K4:", k4, "\n")
\end{minted}
\caption{Posterior expectation of $\lfloor 1/\lambda \rfloor$ (prob7309c.R)}
\label{lst:prob7309c}
\end{listing}

    The estimate of the posterior mean of $\lfloor 1/\lambda \rfloor$ is 2.725230 and the true value of the posterior expectation lies in the interval $(2.65789, 2.79257)$ with virtual certainty.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.3.10}
Generate a sample of $n = 10$ from a Pareto$(2)$ distribution. Now pretend you only know that you have a sample from a Pareto$(\alpha)$ distribution, where $\alpha > 0$ is unknown, and place a Gamma$(2, 1)$ prior on $\alpha$. Using a Monte Carlo sample of size $N = 10^4$, approximate the posterior expectation of $\alpha/(\alpha - 1)$ based on the observed sample, and assess the accuracy of your approximation by quoting an interval that contains the exact value with virtual certainty. (Hint: Problem~\ref{exer:2.10.15}.)
\end{exercise}

\begin{solution}
The inverse cdf of a $\text{Pareto}(\alpha)$ distribution is given by $x = F^{-1}(u) = (1 - u)^{-1/\alpha} - 1$. Therefore, the following R code generates a sample of 100 from the $\text{Pareto}(2)$ distribution.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(123)
# Generate sample from Pareto(2)
c1 <- runif(100)
c2 <- (1 - c1)^(-1/2) - 1
\end{minted}
\caption{Generating Pareto(2) sample (prob7310a.R)}
\label{lst:prob7310a}
\end{listing}

The likelihood function is given by $L(\alpha \mid x_1, \ldots, x_n) = \alpha^n \prod(1 + x_i)^{-\alpha - 1}$. The prior distribution has density given by $\pi(\alpha) = \alpha e^{-\alpha}$. The posterior density is then proportional to $\alpha^{n+1} e^{-\alpha} \prod(1 + x_i)^{-\alpha} = \alpha^{n+1} e^{-\alpha} \exp(-\alpha \ln(\prod(1 + x_i))) = \alpha^{n+1} \exp[-\alpha(\ln(\prod(1 + x_i)) + 1)]$, and we recognize this as being proportional to the $\text{Gamma}(n + 1, \ln(\prod(1 + x_i)) + 1)$ density. The following R code estimates the posterior expectation of $1/(\alpha + 1)$.

\begin{listing}[!htbp]
\begin{minted}{R}
c3 <- log(1 + c2)
k1 <- 101
k2 <- 1/(sum(c3) + 1)
cat("K1:", k1, "\n")
cat("K2:", k2, "\n")

set.seed(456)
c4 <- rgamma(10000, shape = k1, rate = 1/k2)
c4 <- 1/(c4 + 1)
k5 <- mean(c4)
k6 <- sd(c4)/sqrt(10000)
k7 <- k5 - 3*k6
k8 <- k5 + 3*k6
cat("K5:", k5, "\n")
cat("K7:", k7, "\n")
cat("K8:", k8, "\n")
\end{minted}
\caption{Posterior expectation of $1/(\alpha + 1)$ (prob7310b.R)}
\label{lst:prob7310b}
\end{listing}

The estimate of the posterior mean of $1/(\alpha + 1)$ is 0.343067, and the exact value of the posterior expectation lies in the interval $(0.342392, 0.343741)$ with virtual certainty.

The true value of $1/(\alpha + 1)$, however, is 0.33333, so note that it is not contained in the above interval. Note that the above interval is in essence a confidence interval for the exact value of the posterior expectation and not the true value of $1/(\alpha + 1)$.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:7.3.11}
Suppose $X_1, \ldots, X_n$ is a sample from the model $\{f_\theta : \theta \in \Omega \subset R^1\}$ and all the regularity conditions of Section~\ref{sec:6.5} apply. Assume that the prior $\pi$ is a continuous function of $\theta$ and that the posterior mode $\hat{\theta}(X_1, \ldots, X_n) \xrightarrow{\text{a.s.}} \theta$ when $X_1, \ldots, X_n$ is a sample from $f_\theta$ (the latter assumption holds under very general conditions).
\begin{enumerate}[(a)]
\item Using the fact that, if $Y_n \xrightarrow{\text{a.s.}} Y$ and $g$ is a continuous function, then $g(Y_n) \xrightarrow{\text{a.s.}} g(Y)$, prove that
\[
-\frac{1}{n} \frac{\partial^2 \ln L(\theta \mid x_1, \ldots, x_n)}{\partial \theta^2}\bigg|_{\theta = \hat{\theta}} \xrightarrow{\text{a.s.}} I(\theta)
\]
when $X_1, \ldots, X_n$ is a sample from $f_\theta$.
\item Explain to what extent the large sample approximate methods of Section~\ref{ssec:7.3.1} depend on the prior if the assumptions just described apply.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item We have that $\frac{1}{n} \ln(L(\hat{\theta} \mid x_1, \ldots, x_n) \pi(\hat{\theta})) = \frac{1}{n} \sum_{i=1}^{n} \ln L(\hat{\theta} \mid x_i) + \frac{1}{n} \ln \pi(\hat{\theta}) \xrightarrow{a.s.} E_\theta(\ln L(\theta \mid X)) = I(\theta)$ by the strong law of large numbers.
    \item Then from the results of part (a) we have that, denoting the true value of $\theta$ by $\theta_0$,
    \[
        \frac{\theta - \hat{\theta}(X_1, \ldots, X_n)}{\hat{\sigma}(X_1, \ldots, X_n)/\sqrt{n}} \xrightarrow{a.s.} \sqrt{n} I(\theta_0)(\theta - \theta_0)
    \]
    when $\theta \sim \Pi(\cdot \mid X_1, \ldots, X_n)$. This implies that when the sample size is large then inferences will be independent of the prior.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.3.12}
In Exercise~\ref{exer:7.3.10}, explain why the interval you constructed to contain the posterior mean of $\alpha/(\alpha - 1)$ with virtual certainty may or may not contain the true value of $\alpha/(\alpha - 1)$.
\end{exercise}

\begin{solution}
As we increase the Monte Carlo sample size $N$, the interval that contains the exact value of the posterior expectation with virtual certainty becomes shorter and shorter. But for a given sample size $n$ for the data, the posterior expectation will not be equal to the true value of $1/(\alpha + 1)$, so this interval will inevitably exclude the true value.
\end{solution}

\begin{exercise}
\label{exer:7.3.13}
Suppose that $(X, Y)$ is distributed Bivariate Normal$(\mu_1, \mu_2, \sigma_1, \sigma_2, \rho)$. Determine a Gibbs sampling algorithm to generate from this distribution. Assume that you have an algorithm for generating from univariate normal distributions. Is this the best way to sample from this distribution? (Hint: Problem~\ref{exer:2.8.27}.)
\end{exercise}

\begin{solution}
From Problem \ref{exer:2.8.27} we have that $Y$ given $X = x$ is distributed $N(\mu_2 + \rho\sigma_2(x - \mu_1)/\sigma_1, (1 - \rho^2)\sigma_2^2)$ and similarly $X$ given $Y = y$ is distributed $N(\mu_1 + \rho\sigma_1(y - \mu_2)/\sigma_2, (1 - \rho^2)\sigma_1^2)$. Therefore, a Gibbs sampling algorithm for this problem is given by the following. Select $x_0$, then generate $Y_1 \sim N(\mu_2 + \rho\sigma_2(x_0 - \mu_1)/\sigma_1, (1 - \rho^2)\sigma_2^2)$ obtaining $y_1$, then generate $X_1 \sim N(\mu_1 + \rho\sigma_1(y - \mu_2)/\sigma_2, (1 - \rho^2)\sigma_1^2)$ obtaining $x_1$, then generate $Y_2 \sim N(\mu_2 + \rho\sigma_2(x_0 - \mu_1)/\sigma_1, (1 - \rho^2)\sigma_2^2)$ obtaining $y_2$, etc. The sample is $(x_1, y_1), (x_2, y_2), \ldots$. Since this method is not exact, a better method for generating from the bivariate normal is to use (2.7.1), which is exact.
\end{solution}

\begin{exercise}
\label{exer:7.3.14}
Suppose that the joint density of $(X, Y)$ is given by $f_{X,Y}(x, y) = 8xy$ for $0 < x < y < 1$. Fully describe a Gibbs sampling algorithm for this distribution. In particular, indicate how you would generate all random variables. Can you design an algorithm to generate exactly from this distribution?
\end{exercise}

\begin{solution}
The marginal density of $X$ is given by $\int_x^1 8xy \, \mathrm{d}y = 4x(1 - x^2)$ and the marginal density of $Y$ is given by $\int_0^y 8xy \, \mathrm{d}x = 4y^3$. Therefore, $f_{X|Y}(x \mid y) = 8xy/4y^3 = 2x/y^2$ for $0 < x < y$ and $f_{Y|X}(y \mid x) = 8xy/(4x(1 - x^2)) = 2y/(1 - x^2)$ for $x < y < 1$.

The distribution function associated with $f_{Y|X}$ is given by $F_{Y|X}(y) = y^2/(1 - x^2)$ for $x < y < 1$. Therefore, the inverse cdf is given by $F_{Y|X}^{-1}(u) = ((1 - x^2)u)^{1/2}$ for $0 < u < 1$. Therefore, we can generate $Y$ given $X = x$ by generating $U \sim \text{Uniform}[0, 1]$ and putting $Y = ((1 - x^2)U)^{1/2}$.

The distribution function associated with $f_{X|Y}$ is given by $F_{X|Y}(x) = x^2/y^2$ for $0 < x < y$. Therefore the inverse cdf is given by $F_{X|Y}^{-1}(u) = (y^2 u)^{1/2} = yu^{1/2}$ for $0 < u < 1$. Therefore we can generate $X$ given $Y = y$ by generating $U \sim \text{Uniform}[0, 1]$ and putting $X = yU^{1/2}$.

So we select $x_0$. Then we generate $Y \sim f_{Y|X}(\cdot \mid x_0)$, using the above algorithm, obtaining $y_1$. Next we generate $X \sim f_{X|Y}(\cdot \mid y_1)$, using the above algorithm, obtaining $x_1$. Then we generate $Y \sim f_{Y|X}(\cdot \mid x_1)$, using the above algorithm, obtaining $y_2$, etc.

We can generate exactly from this distribution as follows. The marginal cdf of $Y$ is $F_Y(y) = y^4$ for $0 < y < 1$. Then the inverse cdf is given by $F_Y^{-1}(u) = u^{1/4}$ for $0 < u < 1$. So we can generate $Y \sim F_Y$ by generating $U \sim \text{Uniform}[0, 1]$ and putting $y = U^{1/4}$. Then we use the above algorithm to generate $X \sim f_{X|Y}(\cdot \mid y)$. Then we have that $(X, Y) \sim F_{X,Y}$ by the theorem of total probability.
\end{solution}

\begin{exercise}
\label{exer:7.3.15}
In Example~\ref{ex:7.3.1}, prove that the posterior mean of $\psi = \sigma/\mu$ does not exist. (Hint: Use Problem~\ref{exer:7.2.24} and the theorem of total expectation to split the integral into two parts, where one part has value $+\infty$ and the other part has value $-\infty$.)
\end{exercise}

\begin{solution}
Suppose that the posterior expectation of $\psi$ exists. Then by the theorem of total expectation we have that
\begin{align*}
    \expc(\psi \mid x_1, \ldots, x_n) &= \expc\left(\frac{\sigma}{\mu} \mid x_1, \ldots, x_n\right) \\
    &= \expc\left(\frac{\sigma}{\mu}(\indc_{(-\infty, 0)}(\mu) + \indc_{(0, \infty)}(\mu)) \mid x_1, \ldots, x_n\right) \\
    &= \expc\left(\frac{\sigma}{\mu} \indc_{(-\infty, 0)}(\mu) \mid x_1, \ldots, x_n\right) + \expc\left(\frac{\sigma}{\mu} \indc_{(0, \infty)}(\mu) \mid x_1, \ldots, x_n\right) \\
    &= \expc\left(\expc\left(\frac{\sigma}{\mu} \indc_{(-\infty, 0)}(\mu) \mid \sigma, x_1, \ldots, x_n\right) \mid x_1, \ldots, x_n\right) \\
    &\quad + \expc\left(\expc\left(\frac{\sigma}{\mu} \indc_{(0, \infty)}(\mu) \mid \sigma, x_1, \ldots, x_n\right) \mid x_1, \ldots, x_n\right)
\end{align*}
and reasoning as in Problem \ref{exer:7.2.24}, we have that $\expc(\frac{\sigma}{\mu} \indc_{(-\infty, 0)}(\mu) \mid \sigma, x_1, \ldots, x_n) = -\infty$ and $\expc(\frac{\sigma}{\mu} \indc_{(0, \infty)}(\mu) \mid \sigma, x_1, \ldots, x_n) = \infty$, so $\expc(\psi \mid x_1, \ldots, x_n) = \infty - \infty$ which is undefined.
\end{solution}

\begin{exercise}
\label{exer:7.3.16}
(Importance sampling based on the prior) Suppose we have an algorithm to generate from the prior.
\begin{enumerate}[(a)]
\item Indicate how you could use this to approximate a posterior expectation using importance sampling (see Problem~\ref{exer:4.5.21}).
\item What do you suppose is the major weakness is of this approach?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Suppose the posterior expectation of $g(\theta)$ is
    \[
        E_{\Pi(\cdot|s)}[g(\theta)] = \frac{\int g(\theta) f_\theta(s) \pi(\theta) \, \mathrm{d}\theta}{\int f_\theta(s) \pi(\theta) \, \mathrm{d}\theta} = \frac{E_\Pi[g(\theta) f_\theta(s)]}{E_\Pi[f_\theta(s)]}.
    \]
    Hence, generate $\theta_1, \ldots, \theta_m$ from $\Pi$ and estimate the posterior expectation of $g(\theta)$ by
    \[
        \frac{\frac{1}{m} \sum_{i=1}^{m} g(\theta_i) f_{\theta_i}(s)}{\frac{1}{m} \sum_{i=1}^{m} f_{\theta_i}(s)}.
    \]
    \item Whenever the posterior density is quite different from the prior density then we can expect that this estimator will perform very badly, even though the estimator in (a) will converge with probability 1 to the correct answer.
\end{enumerate}
\end{solution}

\subsection*{Computer Problems}

\begin{exercise}
\label{exer:7.3.17}
In the context of Example~\ref{ex:7.3.2}, construct a density histogram of the posterior distribution of $\mu + \sigma z_{0.25}$, i.e., the population first quartile, using $N = 10^4$. Estimate the posterior mean of this distribution and assess the error in your approximation.
\end{exercise}

\begin{solution}
We use the program in Appendix B for Example \ref{ex:7.3.2} to generate a sample of $10^4$ from the joint posterior distribution of $(\mu, \sigma^2)$. The values of $\mu$ are stored in C21 and the values of $\sigma^2$ are stored in C20. The values of $\mu + \sigma z_{0.25}$ are stored in C22.

\begin{listing}[!htbp]
\begin{minted}{R}
# First, find z_0.25
z_025 <- qnorm(0.25)
cat("z_0.25:", z_025, "\n")

# Assuming c20 contains sigma^2 and c21 contains mu from Gibbs sampler
# c22 = mu + sqrt(sigma^2) * z_0.25
c22 <- c21 + sqrt(c20) * z_025

k1 <- mean(c22)
cat("Estimate of posterior mean:", k1, "\n")
\end{minted}
\caption{Computing posterior mean of $\mu + \sigma z_{0.25}$ (prob7317.R)}
\label{lst:prob7317}
\end{listing}

The estimate of the posterior mean of $\mu + \sigma z_{0.25}$ is then 3.30113.

To estimate the error in this approximation we use the batching method and for this we used the R code given in Appendix B. For a batch size of $m = 10$, we obtained a standard error of 0.0147612. For a batch size of $m = 20$ we obtained a standard error of 0.0150728. For a batch size of $m = 40$ we obtained a standard error of 0.0151834. This leads to the interval $3.30113 \pm 3(0.0151834) = (3.2556, 3.3467)$ that contains the true value of the posterior mean with virtual certainty.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Choosing Priors}
\label{sec:7.4}

The issue of selecting a prior for a problem is an important one. Of course, the idea is that we choose a prior to reflect our a priori beliefs about the true value of $\theta$. Because this will typically vary from statistician to statistician, this is often criticized as being too subjective for scientific studies. It should be remembered, however, that the sampling model $\{f_\theta : \theta \in \Omega\}$ is also a subjective choice by the statistician. These choices are guided by the statistician's judgment. What then justifies one choice of a statistical model or prior over another?

In effect, when statisticians choose a prior and a model, they are prescribing a joint distribution for $(s, \theta)$. The only way to assess whether or not an appropriate choice was made is to check whether the observed $s$ is reasonable given this choice. If $s$ is surprising, when compared to the distribution prescribed by the model and prior, then we have evidence against the statistician's choices. Methods designed to assess this are called model-checking procedures, and are discussed in Chapter~\ref{ch:9}. At this point, however, we should recognize the subjectivity that enters into statistical analyses, but take some comfort that we have a methodology for checking whether or not the choices made by the statistician make sense.

Often a statistician will consider a particular family $\{\pi_\lambda : \lambda \in \Lambda\}$ of priors for a problem and try to select a suitable prior $\pi_{\lambda_0} : \lambda_0 \in \Lambda$. In such a context the parameter $\lambda$ is called a hyperparameter. Note that this family could be the set of all possible priors, so there is no restriction in this formulation. We now discuss some commonly used families $\{\pi_\lambda : \lambda \in \Lambda\}$ and methods for selecting $\lambda_0$.

\subsection{Conjugate Priors}
\label{ssec:7.4.1}

Depending on the sampling model, the family $\{\pi_\lambda : \lambda \in \Lambda\}$ may be conjugate.

\begin{definition}
\label{def:7.4.1}
The family of priors $\{\pi_\lambda : \lambda \in \Lambda\}$ for the parameter $\theta$ of the model $\{f_\theta : \theta \in \Omega\}$ is conjugate, if for all data $s \in S$ and all $\lambda \in \Lambda$, the posterior $\pi_\lambda(\cdot \mid s) \in \{\pi_\lambda : \lambda \in \Lambda\}$.
\end{definition}

Conjugacy is usually a great convenience as we start with some choice $\lambda_0 \in \Lambda$ for the prior, and then we find the relevant $\lambda_s \in \Lambda$ for the posterior, often without much computation. While conjugacy can be criticized as a mere mathematical convenience, it has to be acknowledged that many conjugate families offer sufficient variety to allow for the expression of a wide spectrum of prior beliefs.

\begin{example}[Conjugate Families]
\label{ex:7.4.1}
In Example~\ref{ex:7.1.1}, we have effectively shown that the family of all Beta distributions is conjugate for sampling from the Bernoulli model. In Example~\ref{ex:7.1.2}, it is shown that the family of normal priors is conjugate for sampling from the location normal model. In Example~\ref{ex:7.1.3}, it is shown that the family of Dirichlet distributions is conjugate for Multinomial models. In Example~\ref{ex:7.1.4}, it is shown that the family of priors specified there is conjugate for sampling from the location-scale normal model.
\end{example}

Of course, using a conjugate family does not tell us how to select $\lambda_0$. Perhaps the most justifiable approach is to use prior elicitation.

\subsection{Elicitation}
\label{ssec:7.4.2}

Elicitation involves explicitly using the statistician's beliefs about the true value of $\theta$ to select a prior in $\{\pi_\lambda : \lambda \in \Lambda\}$ that reflects these beliefs. Typically, these involve the statistician asking questions of himself, or of experts in the application area, in such a way that the answers specify a prior from the family.

\begin{example}[Location Normal]
\label{ex:7.4.2}
Suppose we are sampling from an $N(\mu, \sigma_0^2)$ distribution with $\mu$ unknown and $\sigma_0^2$ known, and we restrict attention to the family $\{N(\mu_0, \tau_0^2) : \mu_0 \in R^1, \tau_0^2 > 0\}$ of priors for $\mu$. So here, $\lambda = (\mu_0, \tau_0^2)$ and there are two degrees of freedom in this family. Thus, specifying two independent characteristics specifies a prior.

Accordingly, we could ask an expert to specify two quantiles of his or her prior distribution for $\mu$ (see Exercise~\ref{exer:7.4.10}), as this specifies a prior in the family. For example, we might ask an expert to specify a number $\mu_0$ such that the true value of $\mu$ was as likely to be greater than as less than $\mu_0$, so that $\mu_0$ is the median of the prior. We might also ask the expert to specify a value $\xi_0$ such that there is 99\% certainty that the true value of $\mu$ is less than $\xi_0$. This of course is the 0.99-quantile of their prior.

Alternatively, we could ask the expert to specify the center $\mu_0$ of their prior distribution and for a constant $\tau_0$ such that $\mu_0 \pm 3\tau_0$ contains the true value of $\mu$ with virtual certainty. Clearly, in this case, $\mu_0$ is the prior mean and $\tau_0$ is the prior standard deviation.
\end{example}

Elicitation is an important part of any Bayesian statistical analysis. If the experts used are truly knowledgeable about the application, then it seems intuitively clear that we will improve a statistical analysis by including such prior information.

The process of elicitation can be somewhat involved, however, for complicated problems. Furthermore, there are various considerations that need to be taken into account involving, prejudices and flaws in the way we reason about probability outside of a mathematical formulation. See Garthwaite, Kadane and O'Hagan (2005), ``Statistical methods for eliciting probability distributions'', \emph{Journal of the American Statistical Association} (Vol.\ 100, No.\ 470, pp.\ 680--700), for a deeper discussion of these issues.

\subsection{Empirical Bayes}
\label{ssec:7.4.3}

When the choice of $\lambda_0$ is based on the data $s$, these methods are referred to as empirical Bayesian methods. Logically, such methods would seem to violate a basic principle of inference, namely, the principle of conditional probability. For when we compute the posterior distribution of $\theta$ using a prior based on $s$, in general this is no longer the conditional distribution of $\theta$ given the data. While this is certainly an important concern, in many problems the application of empirical Bayes leads to inferences with satisfying properties.

For example, one empirical Bayesian method is to compute the prior predictive $m_\lambda(s)$ for the data $s$ and then base the choice of $\lambda$ on these values. Note that the prior predictive is like a likelihood function for $\lambda$ (as it is the density or probability function for the observed $s$), and so the methods of Chapter~\ref{ch:6} apply for inference about $\lambda$. For example, we could select the value of $\lambda_s$ that maximizes $m_\lambda(s)$. The required computations can be extensive, as $\lambda$ is typically multidimensional. We illustrate with a simple example.

\begin{example}[Bernoulli]
\label{ex:7.4.3}
Suppose we have a sample $x_1, \ldots, x_n$ from a Bernoulli$(\theta)$ distribution and we contemplate putting a Beta$(\lambda, \lambda)$ prior on $\theta$ for some $\lambda > 0$. So the prior is symmetric about 1/2 and the spread in this distribution is controlled by $\lambda$. Since the prior mean is $1/2$ and the prior variance is $2\lambda[(2\lambda + 1)(2\lambda)^2]^{-1} = 1/(4(2\lambda + 1)) \to 0$ as $\lambda \to \infty$, we see that choosing $\lambda$ large leads to a very precise prior. Then we have that
\begin{align*}
m_\lambda(x_1, \ldots, x_n) &= \frac{\Gamma(2\lambda)}{\Gamma(\lambda)^2} \int_0^1 \theta^{n\bar{x} + \lambda - 1} (1 - \theta)^{n(1-\bar{x}) + \lambda - 1} \, \mathrm{d}\theta \\
&= \frac{\Gamma(2\lambda)}{\Gamma(\lambda)^2} \cdot \frac{\Gamma(n\bar{x} + \lambda) \Gamma(n(1-\bar{x}) + \lambda)}{\Gamma(n + 2\lambda)}.
\end{align*}
It is difficult to find the value of $\lambda$ that maximizes this, but for real data we can tabulate and plot $m_\lambda(x_1, \ldots, x_n)$ to obtain this value. More advanced computational methods can also be used.

For example, suppose that $n = 20$ and we obtained $n\bar{x} = 5$ as the number of 1's observed. In Figure~\ref{fig:7.4.1} we have plotted the graph of $m_\lambda(x_1, \ldots, x_n)$ as a function of $\lambda$. We can see from this that the maximum occurs near $\lambda = 2$. More precisely, from a tabulation we determine that $\lambda = 2/3$ is close to the maximum. Accordingly, we use the Beta$(5 + 2/3, 15 + 2/3) = \text{Beta}(17/3, 47/3)$ distribution for inferences about $\theta$.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig7_4_1.pdf}
  \caption{Plot of $m_\lambda(x_1, \ldots, x_n)$ in Example~\ref{ex:7.4.3}.}
  \label{fig:7.4.1}
\end{figure}
\end{example}

There are many issues concerning empirical Bayes methods. This represents an active area of statistical research.

\subsection{Hierarchical Bayes}
\label{ssec:7.4.4}

An alternative to choosing a prior for $\theta$ in $\{\pi_\lambda : \lambda \in \Lambda\}$ consists of putting yet another prior distribution $\tau$, called a hyperprior, on $\lambda$. This approach is commonly called hierarchical Bayes. The prior for $\theta$ basically becomes $\int_\Lambda \pi_\lambda \, \mathrm{d}\tau(\lambda)$, so we have in effect integrated out the hyperparameter. The problem then is how to choose the prior $\tau$. In essence, we have simply replaced the problem of choosing the prior on $\theta$ with choosing the hyperprior on $\lambda$. It is common, in applications using hierarchical Bayes, that default choices are made for $\tau$, although we could also make use of elicitation techniques. We will discuss this further in Section~\ref{ssec:7.4.5}.

So in this situation, the posterior density of $\theta$ is equal to
\[
\pi(\theta \mid s) = \frac{\int_\Lambda f_\theta(s) \pi_\lambda(\theta) \, \mathrm{d}\tau(\lambda)}{m(s)} = \frac{f_\theta(s)}{m(s)} \int_\Lambda \frac{\pi_\lambda(\theta) m_\lambda(s)}{m(s)} \, \mathrm{d}\tau(\lambda),
\]
where $m(s) = \int_\Omega \int_\Lambda f_\theta(s) \pi_\lambda(\theta) \, \mathrm{d}\tau(\lambda) \, \mathrm{d}\theta = \int_\Lambda m_\lambda(s) \, \mathrm{d}\tau(\lambda)$ and, for fixed $\lambda$, $m_\lambda(s) = \int_\Omega f_\theta(s) \pi_\lambda(\theta) \, \mathrm{d}\theta$ (assuming $\lambda$ is continuous with prior density given by $\tau$).

Note that the posterior density of $\lambda$ is $m_\lambda(s) \tau(\lambda)/m(s)$ while $f_\theta(s) \pi_\lambda(\theta)/m_\lambda(s)$ is the posterior density of $\theta$ given $\lambda$.

Therefore, we can use $\pi(\theta \mid s)$ for inferences about the model parameter $\theta$ (e.g., estimation, credible regions, and hypothesis assessment) and $m_\lambda(s) \tau(\lambda)/m(s)$ for inferences about $\lambda$. Typically, however, we are not interested in $\lambda$, and in fact it doesn't really make sense to talk about the ``true'' value of $\lambda$. The true value of $\theta$ corresponds to the distribution that actually produced the observed data $s$, at least when the model is correct, while we are not thinking of $\lambda$ as being generated from $\tau$. This also implies another distinction between $\theta$ and $\lambda$. For $\theta$ is part of the likelihood function based on how the data was generated, while $\lambda$ is not.

\begin{example}[Location-Scale Normal]
\label{ex:7.4.4}
Suppose the situation is as is discussed in Example~\ref{ex:7.1.4}. In that case, both $\mu$ and $\sigma^2$ are part of the likelihood function and so are model parameters, while $\mu_0$, $\tau_0^2$, $\alpha_0$, and $\beta_0$ are not, and so they are hyperparameters. To complete this specification as a hierarchical model, we need to specify a prior $\tau(\mu_0, \tau_0^2, \alpha_0, \beta_0)$, a task we leave to a higher-level course.
\end{example}

\subsection{Improper Priors and Noninformativity}
\label{ssec:7.4.5}

One approach to choosing a prior, and to stop the chain of priors in a hierarchical Bayes approach, is to prescribe a noninformative prior based on ignorance. Such a prior is also referred to as a default prior or reference prior. The motivation is to specify a prior that puts as little information into the analysis as possible and in some sense characterizes ignorance. Surprisingly, in many contexts, statisticians have been led to choose noninformative priors that are improper, i.e., $\int_\Omega \pi(\theta) \, \mathrm{d}\theta = \infty$, so they do not correspond to probability distributions.

The idea here is to give a rule such that, if a statistician has no prior beliefs about the value of a parameter or hyperparameter, then a prior is prescribed that reflects this. In the hierarchical Bayes approach, one continues up the chain until the statistician declares ignorance, and a default prior completes the specification.

Unfortunately, just how ignorance is to be expressed turns out to be a rather subtle issue. In many cases, the default priors turn out to be improper, i.e., the integral or sum of the prior over the whole parameter space equals $\infty$, e.g., $\int_\Omega \pi(\theta) \, \mathrm{d}\theta = \infty$, so the prior is not a probability distribution. The interpretation of an improper prior is not at all clear, and their use is somewhat controversial. Of course, $\pi(\theta \mid s)$ no longer has a joint probability distribution when we are using improper priors, and we cannot use the principle of conditional probability to justify basing our inferences on the posterior.

There have been numerous difficulties associated with the use of improper priors, which is perhaps not surprising. In particular, it is important to note that there is no reason in general for the posterior of $\theta$ to exist as a proper probability distribution when $\pi$ is improper. If an improper prior is being used, then we should always check to make sure the posterior is proper, as inferences will not make sense if we are using an improper posterior.

When using an improper prior $\pi$, it is completely equivalent to instead use the prior $c\pi$ for any $c > 0$: for the posterior under $\pi$ is proper if and only if the posterior under $c\pi$ is proper; then the posteriors are identical (see Exercise~\ref{exer:7.4.6}).

The following example illustrates the use of an improper prior.

\begin{example}[Location Normal Model with an Improper Prior]
\label{ex:7.4.5}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma_0^2)$ distribution, where $\mu \in R^1$ is unknown and $\sigma_0^2$ is known. Many arguments for default priors in this context lead to the choice $\pi(\mu) = 1$, which is clearly improper.

Proceeding as in Example~\ref{ex:7.1.2}, namely, pretending that this is a proper probability density, we get that the posterior density of $\mu$ is proportional to
\[
\exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right\}.
\]
This immediately implies that the posterior distribution of $\mu$ is $N(\bar{x}, \sigma_0^2/n)$. Note that this is the same as the limiting posterior obtained in Example~\ref{ex:7.1.2} as $\tau_0^2 \to \infty$, although the point of view is quite different.
\end{example}

One commonly used method of selecting a default prior is to use, when it is available, the prior given by $\pi(\theta) = I(\theta)^{1/2}$ when $\theta \in R^1$ (and by $\pi(\theta) = (\det I(\theta))^{1/2}$ in the multidimensional case), where $I(\theta)$ is the Fisher information for the statistical model as defined in Section~\ref{sec:6.5}. This is referred to as Jeffreys' prior. Note that Jeffreys' prior is dependent on the model.

Jeffreys' prior has an important invariance property. From Challenge~\ref{exer:6.5.19}, we have that, under some regularity conditions, if we make a 1--1 transformation of the real-valued parameter $\theta$ via $\psi = \Psi(\theta)$, then the Fisher information of $\psi$ is given by
\[
\indc_\psi(\psi) = \indc_\theta(\Psi^{-1}(\psi)) \left(\frac{\mathrm{d}\Psi^{-1}(\psi)}{\mathrm{d}\psi}\right)^2.
\]
Therefore, the default Jeffreys' prior for $\psi$ is
\begin{equation}
\label{eq:7.4.1}
\pi(\psi) = \indc_\psi(\psi)^{1/2} = \indc_\theta(\Psi^{-1}(\psi))^{1/2} \left|\frac{\mathrm{d}\Psi^{-1}(\psi)}{\mathrm{d}\psi}\right|.
\end{equation}
Now we see that, if we had started with the default prior $\pi(\theta) = \indc_\theta(\theta)^{1/2}$ for $\theta$ and made the change of variable to $\psi$, then this prior transforms to \eqref{eq:7.4.1} by Theorems~\ref{thm:2.6.2} and~\ref{thm:2.6.3}. A similar result can be obtained when $\theta$ is multidimensional.

Jeffreys' prior often turns out to be improper, as the next example illustrates.

\begin{example}[Location Normal (Example~\ref{ex:7.4.5} continued)]
\label{ex:7.4.6}
In this case, Jeffreys' prior is given by $\pi(\mu) = n/\sigma_0$, which gives the same posterior as in Example~\ref{ex:7.4.5}. Note that Jeffreys' prior is effectively a constant and hence the prior of Example~\ref{ex:7.4.5} is equivalent to Jeffreys' prior.
\end{example}

Research into rules for determining noninformative priors and the consequences of using such priors is an active area in statistics. While the impropriety seems counterintuitive, their usage often produces inferences with good properties.

\bigskip
\noindent\textbf{Summary of Section~\ref{sec:7.4}}

\begin{itemize}
\item To implement Bayesian inference, the statistician must choose a prior as well as the sampling model for the data.

\item These choices must be checked if the inferences obtained are supposed to have practical validity. This topic is discussed in Chapter~\ref{ch:9}.

\item Various techniques have been devised to allow for automatic selection of a prior. These include empirical Bayes methods, hierarchical Bayes, and the use of noninformative priors to express ignorance.

\item Noninformative priors are often improper. We must always check that an improper prior leads to a proper posterior.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:7.4.1}
Prove that the family $\{\text{Gamma}(\alpha, \beta) : \alpha > 0, \beta > 0\}$ is a conjugate family of priors with respect to sampling from the model given by Pareto$(\alpha)$ distributions with $\alpha > 0$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\lambda \mid x_1, \ldots, x_n) = \lambda^n \prod(1 + x_i)^{-\lambda - 1}$. The prior distribution has density given by $\pi(\lambda) = \beta^\alpha \lambda^{\alpha - 1} e^{-\beta\lambda} / \Gamma(\alpha)$. The posterior density is then proportional to $\lambda^{n + \alpha - 1} \prod(1 + x_i)^{-\lambda} e^{-\beta\lambda} = \lambda^{n + \alpha - 1} \exp(-\lambda \ln(\prod(1 + x_i))) e^{-\beta\lambda} = \lambda^{n + \alpha - 1} \exp[-\lambda(\ln(\prod(1 + x_i)) + \beta)]$, and so the posterior is a $\text{Gamma}(n + \alpha, \ln(\prod(1 + x_i)) + \beta)$ distribution. Hence, this is a conjugate family.
\end{solution}

\begin{exercise}
\label{exer:7.4.2}
Prove that the family $\{\pi_{\alpha_1, \beta_1} : \alpha_1 > 0, \beta_1 > 0\}$ of priors given by
\[
\pi_{\alpha_1, \beta_1}(\theta) \propto \theta^{-\alpha_1 - 1} \indc_{[\beta_1^{-1}, \infty)}(\theta)
\]
is a conjugate family of priors with respect to sampling from the model given by the Uniform$[0, \theta]$ distributions with $\theta > 0$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{-n} \indc_{[x_{(n)}, \infty)}(\theta)$. The prior distribution has density given by $\pi(\theta) = \theta^{-\alpha} \indc_{[\beta, \infty)}(\theta) / (\alpha - 1)\beta^{\alpha - 1}$, where $\alpha \geqslant 1$ and $\beta > 0$. The posterior density is then proportional to $\theta^{-n - \alpha} \indc_{[x_{(n)}, \infty)}(\theta) \indc_{[\beta, \infty)}(\theta) = \theta^{-n - \alpha} \indc_{[\max\{x_{(n)}, \beta\}, \infty)}$, which is of the same form as the family of priors and so this is a conjugate family for this problem.
\end{solution}

\begin{exercise}
\label{exer:7.4.3}
Suppose that the statistical model is given by
\begin{center}
\begin{tabular}{c|cccc}
 & $p_\theta(1)$ & $p_\theta(2)$ & $p_\theta(3)$ & $p_\theta(4)$ \\
\hline
$\theta = a$ & $1/3$ & $1/6$ & $1/3$ & $1/6$ \\
$\theta = b$ & $1/2$ & $1/4$ & $1/8$ & $1/8$
\end{tabular}
\end{center}
and that we consider the family of priors given by
\begin{center}
\begin{tabular}{c|cc}
$\lambda$ & $\pi_\lambda(a)$ & $\pi_\lambda(b)$ \\
\hline
$\lambda_1$ & $1/2$ & $1/2$ \\
$\lambda_2$ & $1/3$ & $2/3$
\end{tabular}
\end{center}
and we observe the sample $x_1 = 1$, $x_2 = 1$, $x_2 = 3$.
\begin{enumerate}[(a)]
\item If we use the maximum value of the prior predictive for the data to determine the value of $\lambda$ and hence the prior, which prior is selected here?
\item Determine the posterior of $\theta$ based on the selected prior.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First, we compute the prior predictive for the data as follows.
    \[
        m_\tau(1, 1, 3) = \sum_{\theta = 1}^{2} \pi(\theta) f_\theta(1, 1, 3) = \begin{cases}
            \dfrac{1}{2} \left(\dfrac{1}{3}\right)^3 + \dfrac{1}{2} \left(\dfrac{1}{2}\right)^2 \dfrac{1}{8} = \dfrac{59}{1728} & \tau = 1 \\[10pt]
            \dfrac{1}{3} \left(\dfrac{1}{3}\right)^3 + \dfrac{2}{3} \left(\dfrac{1}{2}\right)^2 \dfrac{1}{8} = \dfrac{43}{1296} & \tau = 2
        \end{cases}
    \]
    The maximum value of the prior predictive is obtained when $\tau = 1$, therefore we choose the first prior.
    \item The posterior of $\theta$ given $\tau = 1$ is
    \[
        \pi_1(\theta \mid 1, 1, 3) = \begin{cases}
            \dfrac{\frac{1}{2} \left(\frac{1}{3}\right)^3}{\frac{59}{1728}} = \dfrac{32}{59} & \theta = a \\[10pt]
            \dfrac{\frac{1}{2} \left(\frac{1}{2}\right)^2 \frac{1}{8}}{\frac{59}{1728}} = \dfrac{27}{59} & \theta = b.
        \end{cases}
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.4.4}
For the situation described in Exercise~\ref{exer:7.4.3}, put a uniform prior on the hyperparameter $\lambda$ and determine the posterior of $\theta$. (Hint: Theorem of total probability.)
\end{exercise}

\begin{solution}
The posterior of $\theta$ given $\tau = 2$ is
\[
    \pi_2(\theta \mid 1, 1, 3) = \begin{cases}
        \dfrac{\frac{1}{3} \left(\frac{1}{3}\right)^3}{\frac{43}{1296}} = \dfrac{16}{43} & \theta = a \\[10pt]
        \dfrac{\frac{2}{3} \left(\frac{1}{2}\right)^2 \frac{1}{8}}{\frac{43}{1296}} = \dfrac{27}{43} & \theta = b.
    \end{cases}
\]
Therefore, by the theorem of total probability the unconditional posterior of $\theta$ is given by
\begin{align*}
    \pi(\theta \mid 1, 1, 3) &= \frac{1}{2} \pi_1(\theta \mid 1, 1, 3) + \frac{1}{2} \pi_2(\theta \mid 1, 1, 3) \\
    &= \begin{cases}
        \dfrac{1}{2} \cdot \dfrac{32}{59} + \dfrac{1}{2} \cdot \dfrac{16}{43} = \dfrac{1160}{2537} & \theta = a \\[10pt]
        \dfrac{1}{2} \cdot \dfrac{27}{59} + \dfrac{1}{2} \cdot \dfrac{27}{43} = \dfrac{1377}{2537} & \theta = b.
    \end{cases}
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:7.4.5}
For the model for proportions described in Example~\ref{ex:7.1.1}, determine the prior predictive density. If $n = 10$ and $n\bar{x} = 7$, which of the priors given by $\alpha = \beta = 1$ or $\alpha = \beta = 5$ would the prior predictive criterion select for further inferences about $\theta$?
\end{exercise}

\begin{solution}
The prior predictive for the model described in Example \ref{ex:7.1.1} is given by
\[
    m_{\alpha, \beta}(x_1, \ldots, x_n) = \int_0^1 \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{n\bar{x} + \alpha - 1} (1 - \theta)^{n(1 - \bar{x}) + \beta - 1} \, \mathrm{d}\theta = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \cdot \frac{\Gamma(\alpha + n\bar{x}) \Gamma(\beta + n(1 - \bar{x}))}{\Gamma(\alpha + \beta + n)}.
\]
When $n = 10$, $n\bar{x} = 7$, $\alpha = 1$, $\beta = 1$ then
\[
    m_{1,1}(x_1, \ldots, x_n) = \frac{\Gamma(2)}{\Gamma(1) \Gamma(1)} \cdot \frac{\Gamma(8) \Gamma(4)}{\Gamma(12)} = \frac{7! \cdot 3!}{11!} = \frac{1}{1320}.
\]
When $\alpha = 5$, $\beta = 5$ then
\[
    m_{5,5}(x_1, \ldots, x_n) = \frac{\Gamma(10)}{\Gamma(5) \Gamma(5)} \cdot \frac{\Gamma(12) \Gamma(8)}{\Gamma(22)} = \frac{9!}{4! \cdot 4!} \cdot \frac{11! \cdot 7!}{21!} = \frac{1}{403104}.
\]
Therefore, using the prior predictive we would select the prior given by $\alpha = 1$, $\beta = 1$ for further inferences about $\theta$.
\end{solution}

\begin{exercise}
\label{exer:7.4.6}
Prove that when using an improper prior $\pi$, the posterior under $\pi$ is proper if and only if the posterior under $c\pi$ is proper for $c > 0$, and then the posteriors are identical.
\end{exercise}

\begin{solution}
First, for $c > 0$, $\int f_\theta(s) \pi(\theta) \, \mathrm{d}\theta < \infty$ if and only if $\int f_\theta(s) c\pi(\theta) \, \mathrm{d}\theta < \infty$. Then assuming this, the posterior density under $\pi$ is given by $\pi(\theta \mid s) = f_\theta(s) \pi(\theta) / \int f_\theta(s) \pi(\theta) \, \mathrm{d}\theta = f_\theta(s) c\pi(\theta) / \int f_\theta(s) c\pi(\theta) \, \mathrm{d}\theta$, and the result is established.
\end{solution}

\begin{exercise}
\label{exer:7.4.7}
Determine Jeffreys' prior for the Bernoulli$(\theta)$ model and determine the posterior distribution of $\theta$ based on this prior.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{n\bar{x}} (1 - \theta)^{n(1 - \bar{x})}$. By Example \ref{ex:6.5.4}, the Fisher information function for this model is given by $n/\{\theta(1 - \theta)\}$. Therefore, Jeffreys' prior for this model is $\sqrt{n} \theta^{-1/2} (1 - \theta)^{-1/2}$. The posterior density of $\theta$ is then proportional to $\theta^{n\bar{x} - 1/2} (1 - \theta)^{n(1 - \bar{x}) - 1/2}$ so the posterior is a $\text{Beta}(n\bar{x} + 1/2, n(1 - \bar{x}) + 1/2)$ distribution.
\end{solution}

\begin{exercise}
\label{exer:7.4.8}
Suppose we are sampling from a Uniform$[0, \theta]$, $\theta > 0$, model and we want to use the improper prior $\pi(\theta) = 1/\theta$.
\begin{enumerate}[(a)]
\item Does the posterior exist in this context?
\item Does Jeffreys' prior exist in this context?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The likelihood function is given by $\theta^{-n} \indc_{[x_{(n)}, \infty)}(\theta)$. The posterior exists whenever $\int_{-\infty}^{\infty} \theta^{-n} \indc_{[x_{(n)}, \infty)}(\theta) \, \mathrm{d}\theta = \int_{x_{(n)}}^{\infty} \theta^{-n} \, \mathrm{d}\theta = \left.-\frac{\theta^{-n+1}}{n-1}\right|_{x_{(n)}}^{\infty} = \frac{x_{(n)}^{-n+1}}{n-1} < \infty$, and this is the case whenever $n > 1$. Therefore, the posterior exists except when $n = 1$.
      \item From Example \ref{ex:6.5.1} we have that the Fisher information does not exist and so Jeffreys' prior cannot exist.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.4.9}
Suppose a student wants to put a prior on the mean grade out of 100 that their class will obtain on the next statistics exam. The student feels that a normal prior centered at 66 is appropriate and that the interval $[40, 92]$ should contain 99\% of the marks. Fully identify the prior.
\end{exercise}

\begin{solution}
Suppose the prior distribution is $\theta \sim N(66, \sigma^2)$. We choose a $\sigma^2$ for the prior to satisfy $\prb(\theta \in (40, 92)) > 0.99$. Since $\prb(\theta \in (40, 92)) = \Phi(26/\sigma) - \Phi(-26/\sigma) = 2\Phi(26/\sigma) - 1 > 0.99$, the standard deviation $\sigma$ must satisfy $26/\sigma > z_{0.995} = 2.576$. Hence we get $\sigma < 26/2.576 = 10.09$. Equivalently, $\sigma^2 < 101.86$.
\end{solution}

\begin{exercise}
\label{exer:7.4.10}
A lab has conducted many measurements in the past on water samples from a particular source to determine the existence of a certain contaminant. From their records, it was determined that 50\% of the samples had contamination less than 5.3 parts per million, while 95\% had contamination less than 7.3 parts per million. If a normal prior is going to be used for a future analysis, what prior do these data determine?
\end{exercise}

\begin{solution}
According to the description, we will find a prior $\theta \sim N(\mu, \sigma^2)$ satisfying $\prb(\theta < 5.3) = 0.5$ and $\prb(\theta < 7.3) = 0.95$. From $\prb(\theta < x) = \Phi((x - \mu)/\sigma)$, $(5.3 - \mu)/\sigma = z_{0.5} = 0$ and $(7.3 - \mu)/\sigma = z_{0.95} = 1.645$. Hence, $\mu = 5.3$ and $\sigma = z_{0.95}/2 = 1.216$ is good if it also satisfies $\prb(\theta > 0) > 0.999$. Note that when $\mu = 5.3$ and $\sigma = 1.216$, $\prb(\theta > 0) = 1 - \prb(\theta \leqslant 0) = 1 - \Phi(-\mu/\sigma) = 0.9999935$ so this prior places little mass on negative values which we know are impossible.
\end{solution}

\begin{exercise}
\label{exer:7.4.11}
Suppose that a manufacturer wants to construct a 0.95-credible interval for the mean lifetime $\mu$ of an item sold by the company. A consulting engineer is 99\% certain that the mean lifetime is less than 50 months. If the prior on $\mu$ is an Exponential$(\lambda)$, then determine $\lambda$ based on this information.
\end{exercise}

\begin{solution}
Let the prior be $\theta \sim \text{Exponential}(\lambda)$. The prior also satisfies $\prb(\theta > 50) = 0.01$. The probability $\prb(\theta > 50) = \int_{50}^{\infty} \lambda e^{-\lambda\theta} \, \mathrm{d}\theta = -e^{-\lambda\theta}|_{\theta = 50}^{\theta = \infty} = e^{-50\lambda}$. From $e^{-50\lambda} \approx 0.01$, we have $\lambda = 0.092103$.
\end{solution}

\begin{exercise}
\label{exer:7.4.12}
Suppose the prior on a model parameter $\mu$ is taken to be $N(\mu_0, \tau_0^2)$, where $\mu_0$ and $\tau_0^2$ are hyperparameters. The statistician is able to elicit a value for $\mu_0$ but feels unable to do this for $\tau_0^2$. Accordingly, the statistician puts a hyperprior on $\tau_0^2$ given by $1/\tau_0^2 \sim \text{Gamma}(\alpha_0, 1)$ for some value of $\alpha_0$. Determine the prior on $\mu$. (Hint: Write $\mu = \mu_0 + \tau_0 z$, where $z \sim N(0, 1)$.)
\end{exercise}

\begin{solution}
The values $\mu_0$ and $\alpha_0$ are fixed after an elicitation. Then, the prior can be specified as follows.
\begin{align*}
    \mu \mid \sigma_0^2 &\sim N(\mu_0, \sigma_0^2) \\
    1/\sigma_0^2 &\sim \text{Gamma}(\alpha_0, 1).
\end{align*}
Hence, the prior density of $\mu$ can be obtained by marginalizing the joint density.
\begin{align*}
    \pi(\mu) &= \int_0^{\infty} \frac{1}{(2\pi\sigma_0^2)^{1/2}} \exp\left(-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}\right) \cdot \frac{1}{\Gamma(\alpha_0)} \left(\frac{1}{\sigma_0^2}\right)^{\alpha_0 - 1} \exp\left(-\frac{1}{\sigma_0^2}\right) \mathrm{d}\left(\frac{1}{\sigma_0^2}\right) \\
    &= \frac{1}{(2\pi)^{1/2} \Gamma(\alpha_0)} \int_0^{\infty} \left(\frac{1}{\sigma_0^2}\right)^{\alpha_0 - 1/2} \exp\left(-\frac{1}{\sigma_0^2}\left(1 + \frac{(\mu - \mu_0)^2}{2}\right)\right) \mathrm{d}\left(\frac{1}{\sigma_0^2}\right) \\
    &= \frac{\Gamma(\alpha_0 + 1/2)}{(2\pi)^{1/2} \Gamma(\alpha_0)} \left(1 + \frac{(\mu - \mu_0)^2}{2}\right)^{-\alpha_0 - 1/2}.
\end{align*}
Hence, $(\mu - \mu_0)\sqrt{\alpha_0}$ has a general $t$ distribution with parameter $2\alpha_0$ which is discussed in Problem \ref{exer:4.6.17}.
\end{solution}

\subsection*{COMPUTER EXERCISES}

\begin{exercise}
\label{exer:7.4.13}
Consider the situation discussed in Exercise~\ref{exer:7.4.5}.
\begin{enumerate}[(a)]
\item If we observe $n = 10$, $n\bar{x} = 7$, and we are using a symmetric prior, i.e., $\alpha = \beta$, plot the prior predictive as a function of $\alpha$ in the range $(0, 20]$ (you will need a statistical package that provides evaluations of the gamma function for this). Does this graph clearly select a value for $\alpha$?
\item If we observe $n = 10$, $n\bar{x} = 9$, plot the prior predictive as a function of $\alpha$ in the range $(0, 20]$. Compare this plot with that in part (a).
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The prior predictive, as a function of $\alpha$ is given by $m_\alpha(x_1, \ldots, x_n) = \frac{\Gamma(2\alpha)}{(\Gamma(\alpha))^2} \cdot \frac{\Gamma(\alpha + 7) \Gamma(\alpha + 3)}{\Gamma(2\alpha + 10)}$. Then $\ln m_\alpha(x_1, \ldots, x_n) = \ln \Gamma(2\alpha) - 2\ln \Gamma(\alpha) + \ln \Gamma(\alpha + 7) + \ln \Gamma(\alpha + 3) - \ln \Gamma(2\alpha + 10)$. The plot of this function is given below.
    
    \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig7413a.pdf}
        \caption{Log prior predictive as function of $\alpha$ for Exercise 7.4.13(a)}
        %\label{fig:log-prior-predictive-7413a}
    \end{figure}
    
    Note that this graph does not discriminate amongst large values of $\alpha$. Actually from the numbers used to compute the plot the maximum occurs around 7.4, but it is difficult to detect this on the graph.
    \item When $n\bar{x} = 9$ the log prior predictive is plotted below. This is much more discriminating.
    
    \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig7413b.pdf}
        \caption{Log prior predictive as function of $\alpha$ when $n\bar{x} = 9$ for Exercise 7.4.13(b)}
        %\label{fig:log-prior-predictive-7413b}
    \end{figure}
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.4.14}
Reproduce the plot given in Example~\ref{ex:7.4.3} and verify that the maximum occurs near $\lambda = 2/3$.
\end{exercise}

\begin{solution}
Using the equation in Example \ref{ex:7.4.3}, a graph of the prior predictive distribution is drawn below. The maximum is 0.0000035 at $\lambda = 2.32$.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig7414.pdf}
    \caption{Prior predictive distribution for Exercise 7.4.14}
    %\label{fig:prior-predictive-7414}
\end{figure}

The R code for the computation is given below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Set the constants
N <- 20
NXBAR <- 5

# Computation
c1 <- seq(0, 20, length.out = 2001)
c1[1] <- c1[2]  # to prevent the computation of gamma(0)

c2 <- exp(lgamma(2*c1) - 2*lgamma(c1) + lgamma(NXBAR + c1) + 
          lgamma(N - NXBAR + c1) - lgamma(N + 2*c1))
c2[1] <- 0
c1[1] <- 0

# Find maximum
max_idx <- which.max(c2)
cat("Maximum:", c2[max_idx], "at lambda =", c1[max_idx], "\n")

# Plot
plot(c1, c2, type = "l", xlab = "lambda", ylab = "prior predictive")
\end{minted}
\caption{Computing and plotting prior predictive (prob7414.R)}
\label{lst:prob7414}
\end{listing}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:7.4.15}
Show that a distribution in the family $\{N(\mu_0, \tau_0^2) : \mu_0 \in R^1, \tau_0^2 > 0\}$ is completely determined once we specify two quantiles of the distribution.
\end{exercise}

\begin{solution}
First, if $X \sim N(\mu_0, \tau_0^2)$, then the $p$ quantile of this distribution satisfies $x_p = \mu_0 + \tau_0 z_p$, where $z_p$ is the $p$th quantile of the $N(0, 1)$ distribution. Therefore, once we specify two quantiles of the distribution, say $x_{p_1}$ and $x_{p_2}$, we can solve $x_{p_1} = \mu_0 + \tau_0 z_{p_1}$, $x_{p_2} = \mu_0 + \tau_0 z_{p_2}$ to obtain $\tau_0 = (x_{p_1} - x_{p_2})/(z_{p_1} - z_{p_2})$ and $\mu_0 = x_{p_1} - ((x_{p_1} - x_{p_2})/(z_{p_1} - z_{p_2})) z_{p_1}$.
\end{solution}

\begin{exercise}
\label{exer:7.4.16}
(Scale normal model) Consider the family of $N(\mu_0, \sigma^2)$ distributions, where $\mu_0$ is known and $\sigma^2 > 0$ is unknown. Determine Jeffreys' prior for this model.
\end{exercise}

\begin{solution}
From Exercise \ref{exer:6.5.1} the Fisher information is $n/2\sigma^4$. Therefore, Jeffreys' prior is given by $1/\sigma^2$.
\end{solution}

\begin{exercise}
\label{exer:7.4.17}
Suppose that for the location-scale normal model described in Example~\ref{ex:7.1.4}, we use the prior formed by the Jeffreys' prior for the location model (just a constant) times the Jeffreys' prior for the scale normal model. Determine the posterior distribution of $(\mu, \sigma^2)$.
\end{exercise}

\begin{solution}
We use the prior $1/\sigma^2$. The posterior distribution is proportional to
\[
    \left(\frac{1}{\sigma^2}\right)^{n/2} \exp\left(-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2\right) \exp\left(-\frac{(n - 1)s^2}{2\sigma^2}\right) \frac{1}{\sigma^2} = \left(\frac{1}{\sigma^2}\right)^{1/2} \exp\left(-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2\right) \exp\left(-\frac{(n - 1)s^2}{2\sigma^2}\right) \left(\frac{1}{\sigma^2}\right)^{(n+1)/2}.
\]
So the posterior distribution of $(\mu, \sigma^2)$ is given by $\mu \mid \sigma^2, x_1, \ldots, x_n \sim N(\bar{x}, \sigma^2/n)$ and $1/\sigma^2 \mid x_1, \ldots, x_n \sim \text{Gamma}(\frac{n+3}{2}, \frac{n-1}{2} s^2)$.
\end{solution}

\begin{exercise}
\label{exer:7.4.18}
Consider the location normal model described in Example~\ref{ex:7.1.2}.
\begin{enumerate}[(a)]
\item Determine the prior predictive density $m$. (Hint: Write down the joint density of the sample and $\mu$. Use \eqref{eq:7.1.2} to integrate out $\mu$ and do not worry about getting $m$ into a recognizable form.)
\item How would you generate a value $(X_1, \ldots, X_n)$ from this distribution?
\item Are $X_1, \ldots, X_n$ mutually independent? Justify your answer. (Hint: Write $X_i = \sigma_0 Z_i + \tau_0 \sigma_0 Z$, where $Z, Z_1, \ldots, Z_n$ are i.i.d.\ $N(0, 1)$.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The joint density of $(\mu, X_1, \ldots, X_n)$ is given by
    \[
        (2\pi\tau_0^2)^{-1/2} \exp\left(-\frac{1}{2\tau_0^2}(\mu - \mu_0)^2\right) \times (2\pi\sigma_0^2)^{-n/2} \exp\left(-\frac{n - 1}{2\sigma_0^2} s^2\right) \exp\left(-\frac{n}{2\sigma_0^2}(\bar{x} - \mu)^2\right).
    \]
    To calculate $m(x_1, \ldots, x_n)$ we need to integrate out $\mu$. Using (7.1.2) we see that this integral equals
    \begin{align*}
        &(2\pi\tau_0^2)^{-1/2} (2\pi\sigma_0^2)^{-n/2} \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1/2} \exp\left(-\frac{n - 1}{2\sigma_0^2} s^2\right) \\
        &\times \exp\left(\frac{1}{2}\left(\frac{1}{\tau_0^2} + \frac{n}{\sigma_0^2}\right)^{-1} \left(\frac{\mu_0}{\tau_0^2} + \frac{n}{\sigma_0^2} \bar{x}\right)^2\right) \exp\left(-\frac{1}{2}\left(\frac{\mu_0^2}{\tau_0^2} + \frac{n\bar{x}^2}{\sigma_0^2}\right)\right).
    \end{align*}
    \item We have that $(X_1, \ldots, X_n)$ given $\mu$ is a sample from the $N(\mu, \sigma_0^2)$ and $\mu \sim N(\mu, \tau_0^2)$. So we can generate a value $(X_1, \ldots, X_n)$ from $m$ by generating $\mu \sim N(\mu_0, \tau_0^2)$ and then generating $X_1, \ldots, X_n$ i.i.d.\ $N(\mu, \sigma_0^2)$.
    \item No, they are not mutually independent, for we can write $X_i = \mu + \sigma_0 Z_i$, where $Z_1, \ldots, Z_n$ are i.i.d.\ $N(0, 1)$ and $\mu = \mu_0 + \tau_0 Z$ where $Z \sim N(0, 1)$ independent of $Z_1, \ldots, Z_n$. Therefore, $\expc(X_i) = \expc(\mu_0 + \tau_0 Z + \sigma_0 Z_i) = \mu_0 + \tau_0 \expc(Z) + \sigma_0 \expc(Z_i) = \mu_0$, and when $i \neq j$,
    \begin{align*}
        \cov(X_i, X_j) &= \expc((\mu_0 + \tau_0 Z + \sigma_0 Z_i - \mu_0)(\mu_0 + \tau_0 Z + \sigma_0 Z_j - \mu_0)) \\
        &= \expc((\tau_0 Z + \sigma_0 Z_i)(\tau_0 Z + \sigma_0 Z_j)) = \expc(\tau_0^2 Z^2 + \sigma_0 \tau_0 Z Z_j + \sigma_0 \tau_0 Z Z_i + \sigma_0^2 Z_i Z_j) \\
        &= \tau_0^2 \expc(Z^2) + \sigma_0 \tau_0 \expc(Z) \expc(Z_j) + \sigma_0 \tau_0 \expc(Z) \expc(Z_i) + \sigma_0^2 \expc(Z_i) \expc(Z_j) = \tau_0^2 \neq 0
    \end{align*}
    and so they are not independent.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:7.4.19}
Consider Example~\ref{ex:7.3.2}, but this time use the prior $(\mu, \sigma^2) \sim \pi(\mu) \cdot \pi(1/\sigma^2)$. Develop the Gibbs sampling algorithm for this situation. (Hint: Simply adjust each full conditional in Example~\ref{ex:7.3.2} appropriately.)
\end{exercise}

\begin{solution}
The joint posterior distribution of $(X_1, \ldots, X_n, \mu, 1/\sigma^2)$ is proportional to $(1/\sigma^2)^{n/2} \prod_{i=1}^{n} [1 + \frac{1}{\lambda}(\frac{x_i - \mu}{\sigma})^2]^{-(\lambda + 1)/2} (1/\sigma^2)$. Following Example \ref{ex:7.3.2}, we introduce the $n$ latent or hidden variables $(V_1, \ldots, V_n)$, which are i.i.d.\ $\chi^2(\lambda)$ and suppose $X_i \mid v_i \sim N(\mu, \sigma^2 \lambda/v_i)$. With the same prior structure as before, we have that the joint density of $(X_1, V_1), \ldots, (X_n, V_n), \mu, 1/\sigma^2$ is proportional to $(1/\sigma^2)^{n/2} \prod_{i=1}^{n} \exp(-\frac{v_i}{2\sigma^2 \lambda}(x_i - \mu)^2) v_i^{\lambda/2 - 1/2} \exp(-v_i/2) (1/\sigma^2)$. From this we have that the conditional density of $\mu$ is proportional to $\exp\{-\frac{1}{2\sigma^2} \sum_{i=1}^{n} \frac{v_i}{\lambda}(x_i - \mu)^2\}$, which is proportional to $\exp\{-\frac{1}{2\sigma^2}(\sum_{i=1}^{n} \frac{v_i}{\lambda})\mu^2 + \frac{2}{2\sigma^2}(\sum_{i=1}^{n} \frac{v_i}{\lambda} x_i)\mu\}$. From this we immediately deduce that
\[
    \mu \mid x_1, \ldots, x_n, v_1, \ldots, v_n, \sigma^2 \sim N\left(\left(\sum_{i=1}^{n} \frac{v_i}{\lambda}\right)^{-1} \left(\sum_{i=1}^{n} \frac{v_i}{\lambda} x_i\right), \left(\sum_{i=1}^{n} \frac{v_i}{\lambda}\right)^{-1} \sigma^2\right).
\]
The conditional density of $1/\sigma^2$ is proportional to $(1/\sigma^2)^{n/2 + 1} \exp\{-\frac{1}{2}(\sum_{i=1}^{n} \frac{v_i}{\lambda}(x_i - \mu)^2)(1/\sigma^2)\}$ and we immediately deduce that $1/\sigma^2 \mid x_1, \ldots, x_n, v_1, \ldots, v_n, \mu \sim \text{Gamma}(\frac{n}{2} + 2, \frac{1}{2} \sum_{i=1}^{n} \frac{v_i}{\lambda}(x_i - \mu)^2)$. The conditional density of $V_i$ is proportional to $v_i^{\lambda/2 - 1/2} \exp\{-[(x_i - \mu)^2/2\sigma^2\lambda + 1/2]v_i\}$, so $V_i \mid x_1, \ldots, x_n, v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n, \mu, \sigma^2 \sim \text{Gamma}(\frac{\lambda}{2} + \frac{1}{2}, \frac{1}{2}((x_i - \mu)^2/\sigma^2\lambda + 1))$.
\end{solution}

\subsection*{Computer Problems}

\begin{exercise}
\label{exer:7.4.20}
Use the formulation described in Problem~\ref{exer:7.4.17} and the data in the following table
\begin{align*}
&2.6 \quad 4.2 \quad 3.1 \quad 5.2 \quad 3.7 \quad 3.8 \quad 5.6 \quad 1.8 \quad 5.3 \quad 4.0 \\
&3.0 \quad 4.0 \quad 4.1 \quad 3.2 \quad 2.2 \quad 3.4 \quad 4.5 \quad 2.9 \quad 4.7 \quad 5.2
\end{align*}
generate a sample of size $N = 10^4$ from the posterior. Plot a density histogram estimate of the posterior density of $\mu$ based on this sample.
\end{exercise}

\begin{solution}
First, note that the posterior distribution of $(\mu, \sigma^2)$ is $\mu \mid \sigma^2, x_1, \ldots, x_n \sim N(3.825, \frac{\sigma^2}{20})$ and $1/\sigma^2 \mid x_1, \ldots, x_n \sim \text{Gamma}(11.5, 10.75)$. We modified the R program in Appendix B for Example \ref{ex:7.3.1} as follows.

\begin{listing}[!htbp]
\begin{minted}{R}
set.seed(34256734)

# Parameters of the posterior
k1 <- 11.5      # first parameter of gamma = (n+3)/2
k2 <- 1/10.75   # 2/(n-1)s^2
k3 <- 3.825     # posterior mean of mu
k4 <- 1/20      # 1/n

N <- 10000
c3 <- numeric(N)  # generated value of sigma^2
c4 <- numeric(N)  # generated value of mu
c5 <- numeric(N)  # generated value of coefficient of variation

for (i in 1:N) {
    c1 <- rgamma(1, shape = k1, rate = 1/k2)
    c3[i] <- 1/c1
    k6 <- sqrt(k4/c1)
    c4[i] <- rnorm(1, mean = k3, sd = k6)
    c5[i] <- sqrt(c3[i])/c4[i]
}

# Density histogram
hist(c5, breaks = 50, freq = FALSE, 
     xlab = "coefficient of variation", main = "")
\end{minted}
\caption{Posterior distribution of $\psi = \sigma/\mu$ (prob7420.R)}
\label{lst:prob7420}
\end{listing}

A density histogram of the sample of $10^4$ from the posterior distribution of $\psi = \sigma/\mu$ is given below.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig7420.pdf}
    \caption{Density histogram of posterior distribution of coefficient of variation for Problem 7.4.20}
    %\label{fig:coefficient-variation-7420}
\end{figure}
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:7.4.21}
When $\theta = (\theta_1, \theta_2)$, the Fisher information matrix $I(\theta_1, \theta_2)$ is defined in Problem~\ref{exer:6.5.15}. The Jeffreys' prior is then defined as $\pi(\theta_1, \theta_2) = (\det I(\theta_1, \theta_2))^{1/2}$. Determine Jeffreys' prior for the location-scale normal model and compare this with the prior used in Problem~\ref{exer:7.4.17}.
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\mu, \sigma^2 \mid x_1, \ldots, x_n) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2\right) \exp\left(-\frac{n - 1}{2\sigma^2} s^2\right).
\]
The log-likelihood function is then given by
\[
    \ell(\mu, \sigma^2 \mid x_1, \ldots, x_n) = -\frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln \sigma^2 - \frac{n(\bar{x} - \mu)^2}{2\sigma^2} - \frac{n - 1}{2\sigma^2} s^2.
\]
Using the methods discussed in Section \ref{ssec:6.2.1} we obtain the score function as follows
\[
    S(\mu, \sigma^2 \mid x) = \begin{pmatrix} \dfrac{n}{\sigma^2}(\bar{x} - \mu) \\[10pt] -\dfrac{n}{2\sigma^2} + \dfrac{n}{2\sigma^4}(\bar{x} - \mu)^2 + \dfrac{n - 1}{2\sigma^4} s^2 \end{pmatrix}.
\]
The Fisher information matrix is then given by
\begin{align*}
    I(\mu, \sigma^2) &= -E_{(\mu, \sigma^2)} \begin{pmatrix} -\dfrac{n}{\sigma^2} & -\dfrac{n}{\sigma^4}(\bar{x} - \mu) \\[10pt] -\dfrac{n}{\sigma^4}(\bar{x} - \mu) & \dfrac{n}{2\sigma^4} - \dfrac{n}{\sigma^6}(\bar{x} - \mu)^2 - \dfrac{(n - 1)}{\sigma^6} s^2 \end{pmatrix} \\
    &= \begin{pmatrix} \dfrac{n}{\sigma^2} & 0 \\[10pt] 0 & -\dfrac{n}{2\sigma^4} + \dfrac{1}{\sigma^4} + \dfrac{n - 1}{\sigma^4} \end{pmatrix} = \begin{pmatrix} \dfrac{n}{\sigma^2} & 0 \\[10pt] 0 & \dfrac{n}{2\sigma^4} \end{pmatrix}.
\end{align*}
Jeffreys' prior is then given by
\[
    (\det I(\mu, \sigma^2))^{1/2} = \left(\frac{n}{\sigma^2} \cdot \frac{n}{2\sigma^4}\right)^{1/2} = \frac{n}{\sqrt{2}\sigma^3}.
\]
Note that this is different from the prior used in 7.4.12.
\end{solution}

\subsection*{DISCUSSION TOPICS}

\begin{exercise}
\label{exer:7.4.22}
Using empirical Bayes methods to determine a prior violates the Bayesian principle that all unknowns should be assigned probability distributions. Comment on this. Is the hierarchical Bayesian approach a solution to this problem?
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Further Proofs (Advanced)}
\label{sec:7.5}

\subsection*{Derivation of the Posterior Distribution for the Location-Scale Normal Model}

In Example~\ref{ex:7.1.4}, the likelihood function is given by
\[
L(\mu, \sigma^2 \mid x_1, \ldots, x_n) = (2\pi\sigma^2)^{-n/2} \exp\left\{-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2\right\} \exp\left\{-\frac{n-1}{2\sigma^2}s^2\right\}.
\]
The prior on $(\mu, \sigma^2)$ is given by $\mu \mid \sigma^2 \sim N(\mu_0, \tau_0^2 \sigma^2)$ and $1/\sigma^2 \sim \text{Gamma}(\alpha_0, \beta_0)$, where $\mu_0$, $\tau_0^2$, $\alpha_0$, and $\beta_0$ are fixed and known.

The posterior density of $(\mu, \sigma^2)$ is then proportional to the likelihood times the joint prior. Therefore, retaining only those parts of the likelihood and the prior that depend on $\mu$ and $\sigma^2$, the joint posterior density is proportional to
\begin{align*}
&\left(\frac{1}{\sigma^2}\right)^{n/2} \exp\left\{-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2\right\} \exp\left\{-\frac{n-1}{2\sigma^2}s^2\right\} \\
&\qquad \times \left(\frac{1}{\sigma^2}\right)^{1/2} \exp\left\{-\frac{1}{2\tau_0^2 \sigma^2}(\mu - \mu_0)^2\right\} \left(\frac{1}{\sigma^2}\right)^{\alpha_0 - 1} \exp\left\{-\frac{\beta_0}{\sigma^2}\right\} \\
&= \exp\left\{-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2 - \frac{1}{2\tau_0^2 \sigma^2}(\mu - \mu_0)^2\right\} \left(\frac{1}{\sigma^2}\right)^{\alpha_0 + n/2 + 1/2 - 1} \\
&\qquad \times \exp\left\{-\beta_0 \frac{1}{\sigma^2} - \frac{n}{2}\bar{x}^2 \frac{1}{\sigma^2} - \frac{1}{2\tau_0^2}\mu_0^2 \frac{1}{\sigma^2} - \frac{n-1}{2}s^2 \frac{1}{\sigma^2}\right\} \\
&= \left(\frac{1}{\sigma^2}\right)^{1/2} \exp\left\{-\frac{1}{2\sigma^2}\left(n + \frac{1}{\tau_0^2}\right)\left(\mu - \left(n + \frac{1}{\tau_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + n\bar{x}\right)\right)^2\right\} \\
&\qquad \times \left(\frac{1}{\sigma^2}\right)^{\alpha_0 + n/2 - 1} \exp\left\{-\left(\beta_0 + \frac{n}{2}\bar{x}^2 + \frac{1}{2\tau_0^2}\mu_0^2 + \frac{n-1}{2}s^2 - \frac{1}{2}\left(n + \frac{1}{\tau_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + n\bar{x}\right)^2\right) \frac{1}{\sigma^2}\right\}.
\end{align*}

From this, we deduce that the posterior distribution of $(\mu, \sigma^2)$ is given by
\[
\mu \mid \sigma^2, x \sim N\left(\mu(x), (n + \tau_0^{-2})^{-1} \sigma^2\right)
\]
and
\[
1/\sigma^2 \mid x \sim \text{Gamma}(\alpha_0 + n/2, \beta(x)),
\]
where
\[
\mu(x) = (n + \tau_0^{-2})^{-1}\left(\frac{\mu_0}{\tau_0^2} + n\bar{x}\right)
\]
and
\begin{align*}
\beta(x) &= \beta_0 + \frac{n}{2}\bar{x}^2 + \frac{1}{2\tau_0^2}\mu_0^2 + \frac{n-1}{2}s^2 - \frac{1}{2}\left(n + \frac{1}{\tau_0^2}\right)^{-1}\left(\frac{\mu_0}{\tau_0^2} + n\bar{x}\right)^2 \\
&= \beta_0 + \frac{n-1}{2}s^2 + \frac{1}{2} \cdot \frac{n(\bar{x} - \mu_0)^2}{1 + n\tau_0^2}.
\end{align*}

\subsection*{Derivation of $|J| = \psi^{-1}\sigma^{-2}$ for the Location-Scale Normal}

Here we have that
\[
\psi = \Psi(\mu, \sigma^2) = (\sigma^2)^{1/2}/\mu
\]
and
\[
\eta = H(\mu, \sigma^2) = \sigma^2.
\]
We have that
\begin{align*}
\left|\det \begin{pmatrix} \dfrac{\partial \psi}{\partial \mu} & \dfrac{\partial \psi}{\partial \sigma^2} \\[10pt] \dfrac{\partial \eta}{\partial \mu} & \dfrac{\partial \eta}{\partial \sigma^2} \end{pmatrix}\right| &= \left|\det \begin{pmatrix} -(\sigma^2)^{1/2}/\mu^2 & \dfrac{1}{2}(\sigma^2)^{-1/2} \mu^{-1} \\[10pt] 0 & 1 \end{pmatrix}\right| \\
&= \left|\det \begin{pmatrix} -\sigma/\mu^2 & (2\sigma\mu)^{-1} \\ 0 & 1 \end{pmatrix}\right| \\
&= |-\sigma/\mu^2|
\end{align*}
and so
\[
|J| = \frac{|\mu|^2}{\sigma} = \psi^{-1} \sigma^{-2}.
\]

