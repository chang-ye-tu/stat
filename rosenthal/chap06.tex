\chapter{Likelihood Inference}
\label{ch:6}

\section*{Chapter Outline}
\begin{description}
\item[Section 1] The Likelihood Function
\item[Section 2] Maximum Likelihood Estimation
\item[Section 3] Inferences Based on the MLE
\item[Section 4] Distribution-Free Methods
\item[Section 5] Large Sample Behavior of the MLE (Advanced)
\end{description}

In this chapter, we discuss some of the most basic approaches to inference. In essence, we want our inferences to depend only on the model $\{\prb_\theta : \theta \in \Omega\}$ and the data $s$.

These methods are very minimal in the sense that they require few assumptions. While successful for certain problems, it seems that the additional structure of Chapter~\ref{ch:7} or Chapter~\ref{ch:8} is necessary in more involved situations.

The likelihood function is one of the most basic concepts in statistical inference. Entire theories of inference have been constructed based on it. We discuss likelihood methods in Sections~\ref{sec:6.1}, \ref{sec:6.2}, \ref{sec:6.3}, and \ref{sec:6.5}. In Section~\ref{sec:6.4}, we introduce some distribution-free methods of inference. These are not really examples of likelihood methods, but they follow the same basic idea of having the inferences depend on as few assumptions as possible.

\section{The Likelihood Function}
\label{sec:6.1}

Likelihood inferences are based only on the data $s$ and the model $\{\prb_\theta : \theta \in \Omega\}$ --- the set of possible probability measures for the system under investigation. From these ingredients we obtain the basic entity of likelihood inference, namely, the likelihood function.

To motivate the definition of the likelihood function, suppose we have a statistical model in which each $\prb_\theta$ is discrete, given by probability function $f_\theta$. Having observed $s$, consider the function $L(\cdot \mid s)$ defined on the parameter space $\Omega$ and taking values in $R^1$, given by
\[
L(\theta \mid s) = f_\theta(s).
\]
We refer to $L(\cdot \mid s)$ as the \emph{likelihood function} determined by the model and the data. The value $L(\theta \mid s)$ is called the \emph{likelihood} of $\theta$. Note that for the likelihood function, we are fixing the data and varying the value of the parameter.

We see that $f_\theta(s)$ is just the probability of obtaining the data $s$ when the true value of the parameter is $\theta$. This imposes a belief ordering on $\Omega$, namely, we believe in $\theta_1$ as the true value of $\theta$ over $\theta_2$ whenever $f_{\theta_1}(s) > f_{\theta_2}(s)$. This is because the inequality says that the data are more likely under $\theta_1$ than $\theta_2$. We are indifferent between $\theta_1$ and $\theta_2$ whenever $f_{\theta_1}(s) = f_{\theta_2}(s)$. Likelihood inference about $\theta$ is based on this ordering.

It is important to remember the correct interpretation of $L(\theta \mid s)$. The value $L(\theta \mid s)$ is the probability of $s$ given that $\theta$ is the true value --- it is \emph{not} the probability of $\theta$ given that we have observed $s$. Also, it is possible that the value of $L(\theta \mid s)$ is very small for every value of $\theta$. So it is not the actual value of the likelihood that is telling us how much support to give to a particular $\theta$, but rather its value relative to the likelihoods of other possible parameter values.

\begin{example}
\label{ex:6.1.1}
Suppose $S = \{1, 2, \ldots\}$ and that the statistical model is $\{\prb_\theta : \theta \in \{1, 2\}\}$, where $\prb_1$ is the uniform distribution on the integers $\{1, \ldots, 10^3\}$ and $\prb_2$ is the uniform distribution on $\{1, \ldots, 10^6\}$. Further suppose that we observe $s = 10$. Then $L(\theta_1 \mid 10) = 1/10^3$ and $L(\theta_2 \mid 10) = 1/10^6$. Both values are quite small, but note that the likelihood supports $\theta_1$ a thousand times more than it supports $\theta_2$.
\end{example}

Accordingly, we are only interested in likelihood ratios
\[
\frac{L(\theta_1 \mid s)}{L(\theta_2 \mid s)}
\]
for $\theta_1, \theta_2 \in \Omega$ when it comes to determining inferences for $\theta$ based on the likelihood function. This implies that any function that is a positive multiple of $L(\cdot \mid s)$, i.e., $L^*(\theta \mid s) = cL(\theta \mid s)$ for some fixed $c > 0$, can serve equally well as a likelihood function. We call two likelihoods \emph{equivalent} if they are proportional in this way. In general, we refer to any positive multiple of $L(\cdot \mid s)$ as a likelihood function.

\begin{example}
\label{ex:6.1.2}
Suppose that a coin is tossed $n = 10$ times and that $s = 4$ heads are observed. With no knowledge whatsoever concerning the probability $\theta$ of getting a head on a single toss, the appropriate statistical model for the data is the $\text{Binomial}(10, \theta)$ model with $\theta \in [0, 1]$. The likelihood function is given by
\begin{equation}
\label{eq:6.1.1}
L(\theta \mid 4) = \binom{10}{4} \theta^4 (1 - \theta)^6,
\end{equation}
which is plotted in Figure~\ref{fig:6.1.1}.

This likelihood peaks at $\theta = 0.4$ and takes the value $0.2508$ there. We will examine uses of the likelihood to estimate the unknown $\theta$ and assess the accuracy of the estimate. Roughly speaking, however, this is based on where the likelihood takes its maximum and how much spread there is in the likelihood about its peak.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_1_1.pdf}
  \caption{Likelihood function from the $\text{Binomial}(10, \theta)$ model when $s = 4$ is observed.}
  \label{fig:6.1.1}
\end{figure}

There is a range of approaches to obtaining inferences via the likelihood function. At one extreme is the likelihood principle.

\medskip
\noindent\textbf{Likelihood Principle:} If two model and data combinations yield equivalent likelihood functions, then inferences about the unknown parameter must be the same.
\medskip

This principle dictates that anything we want to say about the unknown value of $\theta$ must be based only on $L(\cdot \mid s)$. For many statisticians, this is viewed as a very severe proscription. Consider the following example.

\begin{example}
\label{ex:6.1.3}
Suppose a coin is tossed in independent tosses until four heads are obtained and the number of tails observed until the fourth head is $s = 6$. Then $s$ is distributed $\text{Negative-Binomial}(4, \theta)$, and the likelihood specified by the observed data is
\[
L(\theta \mid 6) = \binom{9}{6} \theta^4 (1 - \theta)^6.
\]
Note that this likelihood function is a positive multiple of \eqref{eq:6.1.1}.

So the likelihood principle asserts that these two model and data combinations must yield the same inferences about the unknown $\theta$. In effect, the likelihood principle says we must ignore the fact that the data were obtained in entirely different ways. If, however, we take into account additional model features beyond the likelihood function, then it turns out that we can derive different inferences for the two situations. In particular, assessing a hypothesized value $\theta_0$ can be carried out in different ways when the sampling method is taken into account. Many statisticians believe this additional information should be used when deriving inferences.
\end{example}

As an example of an inference derived from a likelihood function, consider a set of the form
\[
C(s) = \{\theta : L(\theta \mid s) \geqslant c\}
\]
for some $c > 0$. The set $C(s)$ is referred to as a \emph{likelihood region}. It contains all those values of $\theta$ for which their likelihood is at least $c$. A likelihood region, for some $c$, seems like a sensible set to quote as possibly containing the true value of $\theta$. For, if $\theta' \notin C(s)$, then $L(\theta' \mid s) < L(\theta \mid s)$ for every $\theta \in C(s)$, and so $\theta'$ is not as well-supported by the observed data as any value in $C(s)$. The size of $C(s)$ can then be taken as a measure of how uncertain we are about the true value of $\theta$.

We are left with the problem, however, of choosing a suitable value for $c$ and, as Example~\ref{ex:6.1.1} seems to indicate, the likelihood itself does not suggest a natural way to do this. In Section~\ref{ssec:6.3.2}, we will discuss a method for choosing $c$ that is based upon additional model properties beyond the likelihood function.

So far in this section, we have assumed that our statistical models are comprised of discrete distributions. The definition of the likelihood is quite natural, as $L(\theta \mid s)$ is simply the probability of $s$ occurring when $\theta$ is the true value. This interpretation is clearly not directly available, however, when we have a continuous model because every data point has probability 0 of occurring. Imagine, however, that $f_{\theta_1}(s) > f_{\theta_2}(s)$ and that $s \in R^1$. Then, assuming the continuity of every $f_\theta$ at $s$, we have
\[
\prb_{\theta_1}(V) = \int_a^b f_{\theta_1}(s) \, \mathrm{d}x > \prb_{\theta_2}(V) = \int_a^b f_{\theta_2}(s) \, \mathrm{d}x
\]
for every interval $V = (a, b)$ containing $s$ that is small enough. We interpret this to mean that the probability of $s$ occurring when $\theta_1$ is true is greater than the probability of $s$ occurring when $\theta_2$ is true. So the data $s$ support $\theta_1$ more than $\theta_2$. A similar interpretation applies when $s \in R^n$ for $n > 1$ and $V$ is a region containing $s$.

Therefore, in the continuous case, we again define the likelihood function by $L(\theta \mid s) = f_\theta(s)$ and interpret the ordering this imposes on the values of $\theta$ exactly as we do in the discrete case.\footnote{Note, however, that whenever we have a situation in which $f_{\theta_1}(s) = f_{\theta_2}(s)$, we could still have $\prb_{\theta_1}(V) > \prb_{\theta_2}(V)$ for every $V$ containing $s$ and small enough. This implies that $\theta_1$ is supported more than $\theta_2$, rather than these two values having equal support, as implied by the likelihood. This phenomenon does not occur in the examples we discuss, so we will ignore it here.} Again, two likelihoods will be considered equivalent if one is a positive multiple of the other.

Now consider a very important example.

\begin{example}[Location Normal Model]
\label{ex:6.1.4}
Suppose that $x_1, \ldots, x_n$ is an observed independently and identically distributed (i.i.d.) sample from an $N(\theta, \sigma_0^2)$ distribution where $\theta \in R^1$ is unknown and $\sigma_0^2 > 0$ is known. The likelihood function is given by
\begin{align*}
L(\theta \mid x_1, \ldots, x_n) &= \prod_{i=1}^{n} f_\theta(x_i) = \prod_{i=1}^{n} (2\pi\sigma_0^2)^{-1/2} \exp\left\{-\frac{1}{2\sigma_0^2}(x_i - \theta)^2\right\}
\end{align*}
and clearly this simplifies to
\begin{align*}
L(\theta \mid x_1, \ldots, x_n) &= (2\pi\sigma_0^2)^{-n/2} \exp\left\{-\frac{1}{2\sigma_0^2} \sum_{i=1}^{n} (x_i - \theta)^2\right\} \\
&= (2\pi\sigma_0^2)^{-n/2} \exp\left\{-\frac{n}{2\sigma_0^2} (\bar{x} - \theta)^2\right\} \exp\left\{-\frac{n-1}{2\sigma_0^2} s^2\right\}.
\end{align*}
An equivalent, simpler version of the likelihood function is then given by
\[
L(\theta \mid x_1, \ldots, x_n) = \exp\left\{-\frac{n}{2\sigma_0^2} (\bar{x} - \theta)^2\right\}
\]
and we will use this version.

For example, suppose $n = 25$, $\sigma_0^2 = 1$, and we observe $\bar{x} = 3.3$. This function is plotted in Figure~\ref{fig:6.1.2}.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_1_2.pdf}
  \caption{Likelihood from a location normal model based on a sample of 25 with $\bar{x} = 3.3$.}
  \label{fig:6.1.2}
\end{figure}

The likelihood peaks at $\theta = \bar{x} = 3.3$ and the plotted function takes the value 1 there. The likelihood interval
\[
C(\bar{x}) = \{\theta : L(\theta \mid x_1, \ldots, x_n) \geqslant 0.5\} = (3.0645, 3.53548)
\]
contains all those values of $\theta$ whose likelihood is at least 0.5 of the value of the likelihood at its peak.

The location normal model is impractical for many applications, as it assumes that the variance is known, while the mean is unknown. For example, if we are interested in the distribution of heights in a population, it seems unlikely that we will know the population variance but not know the population mean. Still, it is an important statistical model, as it is a context where inference methods can be developed fairly easily. The methodology developed for this situation is often used as a paradigm for inference methods in much more complicated models.

The parameter $\theta$ need not be one-dimensional. The interpretation of the likelihood is still the same, but it is not possible to plot it --- at least not when the dimension of $\theta$ is greater than 2.

\begin{example}[Multinomial Models]
\label{ex:6.1.5}
In Example~\ref{ex:2.8.5}, we introduced multinomial distributions. These arise in applications when we have a categorical response variable $s$ that can take a finite number $k$ of values, say, $\{1, \ldots, k\}$, and $\prb(s = i) = \theta_i$.

Suppose, then, that $k = 3$ and we do not know the value of $(\theta_1, \theta_2, \theta_3)$. In this case, the parameter space is given by
\[
\Omega = \{(\theta_1, \theta_2, \theta_3) : \theta_i \geqslant 0 \text{ for } i = 1, 2, 3 \text{ and } \theta_1 + \theta_2 + \theta_3 = 1\}.
\]
Notice that it is really only two-dimensional, because as soon as we know the value of any two of the $\theta_i$'s, say, $\theta_1$ and $\theta_2$, we immediately know the value of the remaining parameter, as $\theta_3 = 1 - \theta_1 - \theta_2$. This fact should always be remembered when we are dealing with multinomial models.

Now suppose we observe a sample of $n$ from this distribution, say, $s_1, \ldots, s_n$. The likelihood function for this sample is given by
\begin{equation}
\label{eq:6.1.2}
L((\theta_1, \theta_2, \theta_3) \mid s_1, \ldots, s_n) = \theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3},
\end{equation}
where $x_i$ is the number of $i$'s in the sample.

Using the fact that we can treat positive multiples of the likelihood as being equivalent, we see that the likelihood based on the observed counts $(x_1, x_2, x_3)$ (since they arise from a $\text{Multinomial}(n, (\theta_1, \theta_2, \theta_3))$ distribution) is given by
\[
L((\theta_1, \theta_2, \theta_3) \mid x_1, x_2, x_3) = \theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3}.
\]
This is identical to the likelihood (as functions of $\theta_1$, $\theta_2$, and $\theta_3$) for the original sample. It is certainly simpler to deal with the counts rather than the original sample. This is a very important phenomenon in statistics and is characterized by the concept of \emph{sufficiency}, discussed in the next section.
\end{example}

\subsection{Sufficient Statistics}
\label{ssec:6.1.1}

The equivalence for inference of positive multiples of the likelihood function leads to a useful equivalence amongst possible data values coming from the same model. For example, suppose data values $s_1$ and $s_2$ are such that $L(\theta \mid s_1) = cL(\theta \mid s_2)$ for some $c > 0$. From the point of view of likelihood, we are indifferent as to whether we obtained the data $s_1$ or the data $s_2$, as they lead to the same likelihood ratios.

This leads to the definition of a sufficient statistic.

\begin{definition}
\label{def:6.1.1}
A function $T$ defined on the sample space $S$ is called a \emph{sufficient statistic} for the model if, whenever $T(s_1) = T(s_2)$, then
\[
L(\theta \mid s_1) = c(s_1, s_2) L(\theta \mid s_2)
\]
for some constant $c(s_1, s_2) > 0$.
\end{definition}

The terminology is motivated by the fact that we need only observe the value $t$ for the function $T$, as we can pick any value
\[
s \in T^{-1}(\{t\}) = \{s : T(s) = t\}
\]
and use the likelihood based on $s$. All of these choices give the same likelihood ratios. Typically, $T(s)$ will be of lower dimension than $s$, so we can consider replacing $s$ by $T(s)$ as a data reduction which simplifies the analysis somewhat.

We illustrate the computation of a sufficient statistic in a simple context.

\begin{example}
\label{ex:6.1.6}
Suppose that $S = \{1, 2, 3, 4\}$, $\Omega = \{a, b\}$, and the two probability distributions are given by the following table.
\begin{center}
\begin{tabular}{c|cccc}
& $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$a$ & $1/2$ & $1/6$ & $1/6$ & $1/6$ \\
$b$ & $1/4$ & $1/4$ & $1/4$ & $1/4$
\end{tabular}
\end{center}
Then $L(\cdot \mid 2) = L(\cdot \mid 3) = L(\cdot \mid 4)$ (e.g., $L(a \mid 2) = 1/6$ and $L(b \mid 2) = 1/4$), so the data values in $\{2, 3, 4\}$ all give the same likelihood ratios. Therefore, $T : S \to \{0, 1\}$ given by $T(1) = 0$ and $T(2) = T(3) = T(4) = 1$ is a sufficient statistic. The model has simplified a bit, as now the sample space for $T$ has only two elements instead of four for the original model.
\end{example}

The following result helps identify sufficient statistics.

\begin{theorem}[Factorization theorem]
\label{thm:6.1.1}
If the density (or probability function) for a model factors as $f_\theta(s) = h(s) g_\theta(T(s))$, where $g_\theta$ and $h$ are nonnegative, then $T$ is a sufficient statistic.
\end{theorem}

\begin{proof}
By hypothesis, it is clear that, when $T(s_1) = T(s_2)$, we have
\[
L(\theta \mid s_1) = h(s_1) g_\theta(T(s_1)) = \frac{h(s_1) g_\theta(T(s_1))}{h(s_2) g_\theta(T(s_2))} h(s_2) g_\theta(T(s_2)) = \frac{h(s_1)}{h(s_2)} h(s_2) g_\theta(T(s_2)) = c(s_1, s_2) L(\theta \mid s_2)
\]
because $g_\theta(T(s_1)) = g_\theta(T(s_2))$.
\end{proof}

Note that the name of this result is motivated by the fact that we have factored $f_\theta$ as a product of two functions. The important point about a sufficient statistic $T$ is that we are indifferent, at least when considering inferences about $\theta$, between observing the full data $s$ or the value of $T(s)$. We will see in Chapter~\ref{ch:9} that there is information in the data, beyond the value of $T(s)$, that is useful when we want to check assumptions.

\subsubsection*{Minimal Sufficient Statistics}

Given that a sufficient statistic makes a reduction in the data, without losing relevant information in the data for inferences about $\theta$, we look for a sufficient statistic that makes the greatest reduction. Such a statistic is called a \emph{minimal sufficient statistic}.

\begin{definition}
\label{def:6.1.2}
A sufficient statistic $T$ for a model is a \emph{minimal sufficient statistic}, whenever the value of $T(s)$ can be calculated once we know the likelihood function $L(\cdot \mid s)$.
\end{definition}

So a relevant likelihood function can always be obtained from the value of any sufficient statistic $T$, but if $T$ is minimal sufficient as well, then we can also obtain the value of $T$ from any likelihood function. It can be shown that a minimal sufficient statistic gives the greatest reduction of the data in the sense that, if $T$ is minimal sufficient and $U$ is sufficient, then there is a function $h$ such that $T = h(U)$. Note that the definitions of sufficient statistic and minimal sufficient statistic depend on the model, i.e., different models can give rise to different sufficient and minimal sufficient statistics.

While the idea of a minimal sufficient statistic is a bit subtle, it is usually quite simple to find one, as the following examples illustrate.

\begin{example}[Location Normal Model]
\label{ex:6.1.7}
By the factorization theorem we see immediately, from the discussion in Example~\ref{ex:6.1.4}, that $\bar{x}$ is a sufficient statistic. Now any likelihood function for this model is a positive multiple of
\[
\exp\left\{-\frac{n}{2\sigma_0^2} (\bar{x} - \theta)^2\right\}.
\]
Notice that any such function of $\theta$ is completely specified by the point where it takes its maximum, namely, at $\bar{x}$. So we have that $\bar{x}$ can be obtained from any likelihood function for this model, and it is therefore a minimal sufficient statistic.
\end{example}

\begin{example}[Location-Scale Normal Model]
\label{ex:6.1.8}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution in which $\mu \in R^1$ and $\sigma > 0$ are unknown. Recall the discussion and application of this model in Examples~\ref{ex:5.3.4} and~\ref{ex:5.5.6}.

The parameter in this model is two-dimensional and is given by $(\mu, \sigma^2) \in R^1 \times (0, \infty)$. Therefore, the likelihood function is given by
\begin{align*}
L((\mu, \sigma^2) \mid x_1, \ldots, x_n) &= (2\pi\sigma^2)^{-n/2} \exp\left\{-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2\right\} \\
&= (2\pi\sigma^2)^{-n/2} \exp\left\{-\frac{n}{2\sigma^2} (\bar{x} - \mu)^2\right\} \exp\left\{-\frac{n-1}{2\sigma^2} s^2\right\}.
\end{align*}
We see immediately, from the factorization theorem, that $(\bar{x}, s^2)$ is a sufficient statistic.

Now, fixing $\sigma^2$, any positive multiple of $L((\mu, \sigma^2) \mid x_1, \ldots, x_n)$ is maximized, as a function of $\mu$, at $\bar{x}$. This is independent of $\sigma^2$. Fixing $\mu$ at $\bar{x}$, we have that
\[
L((\bar{x}, \sigma^2) \mid x_1, \ldots, x_n) = (2\pi\sigma^2)^{-n/2} \exp\left\{-\frac{n-1}{2\sigma^2} s^2\right\}
\]
is maximized, as a function of $\sigma^2$, at the same point as $\ln L((\bar{x}, \sigma^2) \mid x_1, \ldots, x_n)$ because $\ln$ is a strictly increasing function. Now
\[
\ln L((\bar{x}, \sigma^2) \mid x) = -\frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln \sigma^2 - \frac{n-1}{2\sigma^2} s^2
\]
\[
\frac{\partial}{\partial \sigma^2} \ln L = -\frac{n}{2\sigma^2} + \frac{n-1}{2\sigma^4} s^2.
\]
Setting this equal to 0 yields the solution
\[
\hat{\sigma}^2 = \frac{n-1}{n} s^2,
\]
which is a 1--1 function of $s^2$. So, given any likelihood function for this model, we can compute $(\bar{x}, s^2)$, which establishes that $(\bar{x}, s^2)$ is a minimal sufficient statistic for the model. In fact, the likelihood is maximized at $(\bar{x}, \hat{\sigma}^2)$ (Problem~\ref{exer:6.1.22}).
\end{example}

\begin{example}[Multinomial Models]
\label{ex:6.1.9}
We saw in Example~\ref{ex:6.1.5} that the likelihood function for a sample is given by \eqref{eq:6.1.2}. This makes clear that if two different samples have the same counts, then they have the same likelihood, so the counts $(x_1, x_2, x_3)$ comprise a sufficient statistic.

Now it turns out that this likelihood function is maximized by taking
\[
(\theta_1, \theta_2, \theta_3) = \left(\frac{x_1}{n}, \frac{x_2}{n}, \frac{x_3}{n}\right).
\]
So, given the likelihood, we can compute the counts (the sample size $n$ is assumed known). Therefore, $(x_1, x_2, x_3)$ is a minimal sufficient statistic.
\end{example}

\subsection*{Summary of Section~\ref{sec:6.1}}

\begin{itemize}
\item The likelihood function for a model and data shows how the data support the various possible values of the parameter. It is not the actual value of the likelihood that is important but the ratios of the likelihood at different values of the parameter.
\item A sufficient statistic $T$ for a model is any function of the data $s$ such that once we know the value of $T(s)$, then we can determine the likelihood function $L(\cdot \mid s)$ (up to a positive constant multiple).
\item A minimal sufficient statistic $T$ for a model is any sufficient statistic such that once we know a likelihood function $L(\cdot \mid s)$ for the model and data, then we can determine $T(s)$.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:6.1.1}
Suppose a sample of $n$ individuals is being tested for the presence of an antibody in their blood and that the number with the antibody present is recorded. Record an appropriate statistical model for this situation when we assume that the responses from individuals are independent. If we have a sample of 10 and record 3 positives, graph a representative likelihood function.
\end{exercise}

\begin{solution}
The appropriate statistical model is the $\text{Binomial}(n, \theta)$, where $\theta \in \Omega = [0, 1]$ is the probability of having this antibody in the blood. (We can also think of $\theta$ as the unknown proportion of the population who have this antibody in their blood.) The likelihood function is given by $L(\theta \mid s) = \binom{n}{s} \theta^s (1 - \theta)^{n-s}$, where $s$ is the number of people whose result was positive. The likelihood function for $n = 10$ people and $s = 3$ is given by $L(\theta \mid 3) = \binom{10}{3} \theta^3 (1 - \theta)^7$, and the graph of this function is given below.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig_6_1_1.pdf}
    \caption{Likelihood function for Exercise 6.1.1}
    %\label{fig:likelihood-6.1.1}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:6.1.2}
Suppose that suicides occur in a population at a rate $p$ per person year and that $p$ is assumed completely unknown. If we model the number of suicides observed in a population with a total of $N$ person years as $\text{Poisson}(Np)$, then record a representative likelihood function for $p$ when we observe 22 suicides with $N = 30{,}345$.
\end{exercise}

\begin{solution}
The likelihood function for $p$ when we observe 22 suicides with $N = 30{,}345$ is given by $L(p \mid 22) = (30345p)^{22} \exp(-30345p)$.
\end{solution}

\begin{exercise}
\label{exer:6.1.3}
Suppose that the lifelengths (in thousands of hours) of light bulbs are distributed $\text{Exponential}(\lambda)$, where $\lambda > 0$ is unknown. If we observe $\bar{x} = 5.2$ for a sample of 20 light bulbs, record a representative likelihood function. Why is it that we only need to observe the sample average to obtain a representative likelihood?
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_{20}) = \theta^{20} \exp(-(20\bar{x}) \theta)$. By the factorization theorem (Theorem~\ref{thm:6.1.1}) $\bar{x}$ is a sufficient statistic, so we only need to observe its value to obtain a representative likelihood. The likelihood function when $\bar{x} = 5.2$ is given by $L(\theta \mid x_1, \ldots, x_{20}) = \theta^{20} \exp(-20 (5.2) \theta)$.
\end{solution}

\begin{exercise}
\label{exer:6.1.4}
Suppose we take a sample of $n = 100$ students from a university with over 50,000 students enrolled. We classify these students as either living on campus, living off campus with their parents, or living off campus independently. Suppose we observe the counts $(x_1, x_2, x_3) = (34, 44, 22)$. Determine the form of the likelihood function for the unknown proportions of students in the population that are in these categories.
\end{exercise}

\begin{solution}
Since the sample size of 100 is small relative to the total population size, we can think of the counts as a sample from the $\text{Multinomial}(1, \theta_1, \theta_2, \theta_3)$ distribution. The likelihood function is then given by $L(\theta_1, \theta_2, \theta_3 \mid 34, 44, 22) = \theta_1^{34} \theta_2^{44} \theta_3^{22}$.
\end{solution}

\begin{exercise}
\label{exer:6.1.5}
Determine the constant that makes the likelihood functions in Examples~\ref{ex:6.1.2} and~\ref{ex:6.1.3} equal.
\end{exercise}

\begin{solution}
If we denote the likelihood in Example~\ref{ex:6.1.2} by $L_1(\theta \mid 4)$ and the likelihood in Example~\ref{ex:6.1.3} by $L_2(\theta \mid 4)$, then $L_1(\theta \mid 4) = cL_2(\theta \mid 4)$, where $c = \binom{10}{4} / \binom{9}{3}$.
\end{solution}

\begin{exercise}
\label{exer:6.1.6}
Suppose that $x_1, \ldots, x_n$ is a sample from the Bernoulli$(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown. Determine the likelihood function and a minimal sufficient statistic for this model. (Hint: Use the factorization theorem and maximize the logarithm of the likelihood function.)
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\theta \mid x_1, \ldots, x_n) = \prod_{i=1}^{n} \theta^{x_i} (1 - \theta)^{1-x_i} = \theta^{\sum x_i} (1 - \theta)^{n - \sum x_i} = \theta^{n\bar{x}} (1 - \theta)^{n(1-\bar{x})}.
\]
By the factorization theorem $\bar{x}$ is a sufficient statistic. If we differentiate $\ln L(\theta \mid x_1, \ldots, x_n) = n\bar{x} \ln \theta + n(1 - \bar{x}) \ln(1 - \theta)$, we get
\[
    (\ln L(\theta \mid x_1, \ldots, x_n))' = \frac{n\bar{x}}{\theta} - \frac{n(1 - \bar{x})}{1 - \theta}
\]
and setting this equal to 0 gives the solution $\theta = \bar{x}$. Therefore, we can obtain $\bar{x}$ from the likelihood and we conclude that it is a minimal sufficient statistic.
\end{solution}

\begin{exercise}
\label{exer:6.1.7}
Suppose $x_1, \ldots, x_n$ is a sample from the Poisson$(\lambda)$ distribution where $\lambda > 0$ is unknown. Determine the likelihood function and a minimal sufficient statistic for this model. (Hint: the Factorization Theorem and maximization of the logarithm of the likelihood function.)
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\theta \mid x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{\theta^{x_i} e^{-\theta}}{x_i!} = \frac{\theta^{n\bar{x}} e^{-n\theta}}{\prod x_i!}.
\]
By the factorization theorem $\bar{x}$ is a sufficient statistic. If we differentiate $\ln L(\theta \mid x_1, \ldots, x_n) = -\ln \prod x_i! + n\bar{x} \ln \theta - n\theta$, we get
\[
    (\ln L(\theta \mid x_1, \ldots, x_n))' = \frac{n\bar{x}}{\theta} - n
\]
and setting this equal to 0 gives the solution $\theta = \bar{x}$. Therefore, we can obtain $\bar{x}$ from the likelihood and we conclude that it is a minimal sufficient statistic.
\end{solution}

\begin{exercise}
\label{exer:6.1.8}
Suppose that a statistical model is comprised of two distributions given by the following table:
\begin{center}
\begin{tabular}{c|ccc}
& $s = 1$ & $s = 2$ & $s = 3$ \\
\hline
$f_1(s)$ & 0.3 & 0.1 & 0.6 \\
$f_2(s)$ & 0.1 & 0.7 & 0.2
\end{tabular}
\end{center}
\begin{enumerate}[(a)]
\item Plot the likelihood function for each possible data value $s$.
\item Find a sufficient statistic that makes a reduction in the data.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The three likelihood functions are as follows.
    \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig_6_1_8.pdf}
        \caption{Three likelihood functions for Exercise 6.1.8}
        %\label{fig:likelihood-6.1.8}
    \end{figure}
    
    \item Since $L(1|1)/L(2|1) = 0.3/0.1 = 3 = L(1|3)/L(2|3)$ and $L(1|2)/L(2|2) = 0.1/0.7 = 1/7 \neq 3$, a statistic $T : S \to \{1, 2\}$ given by $T(1) = T(3) = 1$ and $T(2) = 2$ is a sufficient statistic.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:6.1.9}
Suppose a statistical model is given by $\{f_1, f_2\}$, where $f_i$ is an $N(i, 1)$ distribution. Compute the likelihood ratio $L(\theta_1 \mid 0)/L(\theta_2 \mid 0)$ and explain how you interpret this number.
\end{exercise}

\begin{solution}
Since the density function $f_i(s) = (2\pi)^{-1/2} \exp(-(s - i)^2/2)$ for $i = 1, 2$, the likelihood ratio is
\[
    \frac{L(1|0)}{L(2|0)} = \frac{\exp(-(0 - 1)^2/2)}{\exp(-(0 - 2)^2/2)} = e^{3/2} = 4.4817.
\]
When $s = 0$ is observed, the distribution $f_1$ is 4.4817 times more likely than $f_2$.
\end{solution}

\begin{exercise}
\label{exer:6.1.10}
Explain why a likelihood function can never take negative values. Can a likelihood function be equal to 0 at a parameter value?
\end{exercise}

\begin{solution}
A likelihood function $L$ is defined by $L(\theta|s) = f_\theta(s)$. A probability density function or a probability function cannot take negative values. Thus any likelihood function cannot take negative values. However, a likelihood function may be equal to 0 in some cases. Consider $X \sim \text{Uniform}[0, \theta]$ and $\theta \in \mathbb{R}$. Suppose that $X = 1$ is observed. The density function is $f_\theta(x) = 1/\theta$ if $x \in [0, \theta]$ and 0 otherwise. This implies $L(\theta|1) = 1/\theta$ if $\theta \geqslant 1$ and 0 if $\theta < 1$. Hence $L$ can be 0 at some parameter values.
\end{solution}

\begin{exercise}
\label{exer:6.1.11}
Suppose we have a statistical model $\{f_\theta : \theta \in [0, 1]\}$ and we observe $x_0$. Is it true that $\int_0^1 L(\theta \mid x_0) \, \mathrm{d}\theta = 1$? Explain why or why not.
\end{exercise}

\begin{solution}
The integral $\int_0^1 L(\theta|x_0) \, \mathrm{d}\theta$ cannot be 1 in general because a likelihood function is not a density function with respect to $\theta$. Consider $X \sim \text{Uniform}[0, \theta]$ and $\theta \in [0, 1]$. The likelihood function at $X = x_0$ is $L(\theta|x_0) = f_\theta(x_0) = (1/\theta)\indc_{[0,\theta]}(x_0) = (1/\theta)\indc_{[x_0,1]}(\theta)$. The integral of the likelihood function is
\[
    \int_0^1 L(\theta|x_0) \, \mathrm{d}\theta = \int_{x_0}^1 \frac{1}{\theta} \, \mathrm{d}\theta = -\ln(x_0).
\]
This is not 1 unless $x_0 = 1/e$.
\end{solution}

\begin{exercise}
\label{exer:6.1.12}
Suppose that $x_1, \ldots, x_n$ is a sample from a Geometric$(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown. Determine the likelihood function and a minimal sufficient statistic for this model. (Hint: Use the factorization theorem and maximize the logarithm of the likelihood.)
\end{exercise}

\begin{solution}
The joint density function is given by $f_\theta(s) = f_\theta(x_1) \cdots f_\theta(x_n) = \theta^n (1 - \theta)^{x_1 + \cdots + x_n}$. Hence the likelihood function is $L(\theta|s) = f_\theta(s) = \theta^n (1 - \theta)^{x_1 + \cdots + x_n}$. Let $h(s) = 1$, $g_\theta(t) = \theta^n (1 - \theta)^t$ and $T(s) = x_1 + \cdots + x_n$. Then, the joint density function can be factorized as $f_\theta(s) = h(s) \cdot g_\theta(T(s))$. Hence, $T = X_1 + \cdots + X_n$ is a sufficient statistic. Then, find a maximizer of the logarithm of the likelihood function. The derivative of the log-likelihood function is given by
\[
    \frac{\partial L(\theta|s)}{\partial \theta} = \frac{\partial}{\partial \theta} \left( n \ln(\theta) + T(s) \ln(1 - \theta) \right) = \frac{n}{\theta} - \frac{T(s)}{1 - \theta}.
\]
Setting this equal to 0 yields the solution $\hat{\theta} = n/(n + T(s))$ which is a 1--1 function of $T(s)$. Hence, the sufficient statistic $T$ is a minimal sufficient statistic.
\end{solution}

\begin{exercise}
\label{exer:6.1.13}
Suppose you are told that the likelihood of a particular parameter value $\theta$ is $10^{-9}$. Is it possible to interpret this number in any meaningful way? Explain why or why not.
\end{exercise}

\begin{solution}
The likelihood at a parameter value does not have any particular meaning. Suppose $L(\theta_1|s) = 10^9$ and $L(\theta_k|s) = 10^{9k}$ for $k \geqslant 1$. Even though $10^9$ is a very big number, the ratio of $10^9$ to $10^{9 \cdot 2} = 10^{81}$ is almost zero ($10^{-72}$). In other words, a big likelihood value does not have any meaning. However, a very big value of the likelihood ratio of two parameter points, say $\theta_2$ to $\theta_1$, indicates $\theta_2$ is more likely than $\theta_1$.
\end{solution}

\begin{exercise}
\label{exer:6.1.14}
Suppose one statistician records a likelihood function as $\theta^2$ for $\theta \in [0, 1]$ while another statistician records a likelihood function as $100\theta^2$ for $\theta \in [0, 1]$. Explain why these likelihood functions are effectively the same.
\end{exercise}

\begin{solution}
As we have seen in Exercise~\ref{exer:6.1.13}, a ratio of likelihood values has to be considered to have a meaningful interpretation. Let $L_1(\theta) = \theta^2$, $L_2(\theta) = 100\theta^2$, $\theta_1$ and $\theta_2$ be two parameter values. The likelihood ratios at two points $\theta_1$ and $\theta_2$ are
\[
    \frac{L_1(\theta_1)}{L_1(\theta_2)} = \left( \frac{\theta_1}{\theta_2} \right)^2 = \frac{100\theta_1^2}{100\theta_2^2} = \frac{L_2(\theta_1)}{L_2(\theta_2)}.
\]
Thus, the ratios of likelihood functions at any two points are the same. Therefore, any inferences based on two likelihood functions $L_1$, $L_2$ are effectively the same.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:6.1.15}
Show that $T$ defined in Example~\ref{ex:6.1.6} is a minimal sufficient statistic. (Hint: Show that once you know the likelihood function, you can determine which of the two possible values for $T$ has occurred.)
\end{exercise}

\begin{solution}
Example~\ref{ex:6.1.6} showed that $T : S \to \{0, 1\}$ given by $T(1) = 0$ and $T(2) = T(3) = T(4) = 1$ is a sufficient statistic. Now given the likelihood $L(\cdot \mid s)$, we know whether or not the likelihood ratio of $a$ to $b$ is 2 or $4/6$, and so can identify whether or not $T(s)$ takes the value 0 or 1. Therefore, $T$ is minimal sufficient.
\end{solution}

\begin{exercise}
\label{exer:6.1.16}
Suppose that $S = \{1, 2, 3, 4\}$, $\Omega = \{a, b, c\}$, where the three probability distributions are given by the following table.
\begin{center}
\begin{tabular}{c|cccc}
& $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$a$ & $1/2$ & $1/6$ & $1/6$ & $1/6$ \\
$b$ & $1/4$ & $1/4$ & $1/4$ & $1/4$ \\
$c$ & $1/2$ & $1/4$ & $1/4$ & $0$
\end{tabular}
\end{center}
Determine a minimal sufficient statistic for this model. Is the minimal sufficient statistic in Example~\ref{ex:6.1.6} sufficient for this model?
\end{exercise}

\begin{solution}
We see that $L(\cdot \mid 2) = L(\cdot \mid 3)$, so the data values in $\{2, 3\}$ all give the same likelihood ratios. Therefore, $T : S \to \{0, 1\}$ given by $T(1) = 0$, $T(2) = T(3) = 1$, and $T(4) = 3$ is a sufficient statistic. We also see that once we know the likelihood function $L(\cdot \mid s)$, we can determine all the likelihood ratios and so determine if $s = 1$, $s \in \{2, 3\}$ or $s = 4$ has occurred.

The minimal sufficient statistic in Example~\ref{ex:6.1.6} is not sufficient for this model since the data value $s = 4$ does not give the same likelihood ratios when $s = 2$ or $s = 3$.
\end{solution}

\begin{exercise}
\label{exer:6.1.17}
Suppose that $x_1, \ldots, x_n$ is a sample from the $N(\mu, \sigma_0^2)$ distribution where $\mu \in R^1$ is unknown. Determine the form of likelihood intervals for this model.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\mu \mid x_1, \ldots, x_n) = \exp \left( -n(\bar{x} - \mu)^2 / 2\sigma_0^2 \right)$. A likelihood interval has the form
\begin{align*}
    \left\{ \mu : \exp \left( -n(\bar{x} - \mu)^2 / 2\sigma_0^2 \right) > c \right\} &= \left\{ \mu : -n(\bar{x} - \mu)^2 / 2\sigma_0^2 > \ln c \right\} \\
    &= \left\{ \mu : \bar{x} - \frac{\sigma_0}{\sqrt{2n}} \ln c < \mu < \bar{x} + \frac{\sigma_0}{\sqrt{2n}} \ln c \right\} \\
    &= \left( \bar{x} - \frac{\sigma_0}{\sqrt{2n}} \ln c, \bar{x} + \frac{\sigma_0}{\sqrt{2n}} \ln c \right).
\end{align*}
So for any constant $a$, the interval $(\bar{x} - a, \bar{x} + a)$ is a likelihood interval for this model.
\end{solution}

\begin{exercise}
\label{exer:6.1.18}
Suppose that $x_1, \ldots, x_n \in R^n$ is a sample from $f_\theta$, where $\theta$ is unknown. Show that the order statistics $x_{(1)} \leqslant \cdots \leqslant x_{(n)}$ comprise a sufficient statistic for the model.
\end{exercise}

\begin{solution}
We have that the likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \prod_{i=1}^{n} f_\theta(x_i) = \prod_{i=1}^{n} f_\theta(x_{(i)})$, so once we know the order statistics, we know the likelihood function and so they are sufficient.
\end{solution}

\begin{exercise}
\label{exer:6.1.19}
Determine a minimal sufficient statistic for a sample of $n$ from the rate gamma model, i.e.,
\[
f_\lambda(x) = \frac{\lambda^{\alpha_0}}{\Gamma(\alpha_0)} x^{\alpha_0 - 1} \exp\{-\lambda x\}
\]
for $x > 0$, $\lambda > 0$, and where $\alpha_0 > 0$ is fixed.
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\theta \mid x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{1}{\Gamma(\alpha_0)} (\theta x_i)^{\alpha_0 - 1} \exp\{-\theta x_i\} \theta = \Gamma^{-n}(\alpha_0) \left( \prod x_i \right)^{\alpha_0 - 1} \theta^{n\alpha_0} \exp(-\theta n \bar{x}).
\]
By the factorization theorem $\bar{x}$ is a sufficient statistic. The logarithm of the likelihood is given by $\ln L(\theta \mid x_1, \ldots, x_n) = \ln\{\Gamma^{-n}(\alpha_0) (\prod x_i)^{\alpha_0 - 1}\} + n\alpha_0 \ln \theta - \theta n \bar{x}$. Differentiating this and setting it equal to 0, we obtain $\theta = \alpha_0/\bar{x}$. So given a likelihood function, we can determine $\bar{x}$ and this proves that $\bar{x}$ is minimal sufficient.
\end{solution}

\begin{exercise}
\label{exer:6.1.20}
Determine the form of a minimal sufficient statistic for a sample of size $n$ from the $\text{Uniform}[0, \theta]$ model where $\theta > 0$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{-n} \indc_{[x_{(n)}, \infty)}(\theta)$ when $\theta > 0$. By the factorization theorem $x_{(n)}$ is a sufficient statistic. Now notice that the likelihood function is 0 to the left of $x_{(n)}$ and positive to the right. So given the likelihood, we can determine $x_{(n)}$ and it is minimal sufficient.
\end{solution}

\begin{exercise}
\label{exer:6.1.21}
Determine the form of a minimal sufficient statistic for a sample of size $n$ from the $\text{Uniform}[\theta_1, \theta_2]$ model where $\theta_1 < \theta_2$.
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\theta_1, \theta_2 \mid x_1, \ldots, x_n) = \left( \frac{1}{\theta_2 - \theta_1} \right)^n \indc_{(-\infty, x_{(1)})}(\theta_1) \indc_{[x_{(n)}, \infty)}(\theta_2).
\]
By the factorization theorem $(x_{(1)}, x_{(n)})$ is a sufficient statistic. Now given the likelihood function, we see that the likelihood becomes 0 at $x_{(1)}$ on the left and at $x_{(n)}$ on the right. So given the likelihood, we can determine these points. This implies that $(x_{(1)}, x_{(n)})$ is a minimal sufficient statistic.
\end{solution}

\begin{exercise}
\label{exer:6.1.22}
For the location-scale normal model, establish that the point where the likelihood is maximized is given by $(\bar{x}, \hat{\sigma}^2)$ as defined in Example~\ref{ex:6.1.8}. (Hint: Show that the second derivative of $\ln L((\bar{x}, \sigma^2) \mid x)$, with respect to $\sigma^2$, is negative at $\hat{\sigma}^2$ and then argue that $(\bar{x}, \hat{\sigma}^2)$ is the maximum.)
\end{exercise}

\begin{solution}
From the argument in Example~\ref{ex:6.1.8} we have that $L(\bar{x}, \sigma^2 \mid x_1, \ldots, x_n) \geqslant L(\mu, \sigma^2 \mid x_1, \ldots, x_n)$ for every $\mu$. Further, the argument there shows that, as a function of $\sigma^2$, the function $\ln L(\bar{x}, \sigma^2 \mid x_1, \ldots, x_n)$ has a critical point at $\hat{\sigma}^2$. The second derivative of this function at $\hat{\sigma}^2$ is given by
\begin{align*}
    \frac{\partial^2 \ln L(\bar{x}, \sigma^2 \mid x)}{\partial (\sigma^2)^2} \bigg|_{\sigma^2 = \hat{\sigma}^2} &= \frac{\partial}{\partial \sigma^2} \left( -\frac{n}{2\sigma^2} + \frac{n-1}{2\sigma^4} s^2 \right) \bigg|_{\sigma^2 = \hat{\sigma}^2} \\
    &= \frac{1}{\sigma^4} \left( \frac{n}{2} - \frac{n-1}{\sigma^2} s^2 \right) \bigg|_{\sigma^2 = \hat{\sigma}^2} \\
    &= \frac{1}{\hat{\sigma}^4} \left( \frac{n}{2} - n \right) < 0
\end{align*}
so $L(\bar{x}, \hat{\sigma}^2 \mid x_1, \ldots, x_n) \geqslant L(\mu, \sigma^2 \mid x_1, \ldots, x_n)$ for every $\mu$ and $\sigma^2$.
\end{solution}

\begin{exercise}
\label{exer:6.1.23}
Suppose we have a sample of $n$ from a Bernoulli$(\theta)$ distribution where $\theta \in [0, 0.5]$. Determine a minimal sufficient statistic for this model. (Hint: It is easy to establish the sufficiency of $\bar{x}$, but this point will not maximize the likelihood when $\bar{x} > 0.5$, so $\bar{x}$ cannot be obtained from the likelihood by maximization, as in Exercise~\ref{exer:6.1.6}. In general, consider the second derivative of the log of the likelihood at any point $\theta_0 \in (0, 0.5)$ and note that knowing the likelihood means that we can compute any of its derivatives at any values where these exist.)
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{n\bar{x}} (1 - \theta)^{n(1-\bar{x})}$ for $\theta \in [0, 0.5]$. The factorization theorem establishes that $\bar{x}$ is sufficient. Just as with the full $\text{Bernoulli}(\theta)$ model, we can determine $\bar{x}$ as the point where this function is maximized --- provided that $\bar{x} \in [0, 0.5]$ --- otherwise we do not know the form of this function outside of $[0, 0.5]$. In general, the maximum value of this likelihood function is attained at $\min\{0.5, \bar{x}\}$. When the maximum occurs at $0.5$, we only know that $0.5 \leqslant \bar{x} \leqslant 1$. But the second derivative of the log of the likelihood is given by
\[
    -\frac{n\bar{x}}{\theta^2} - \frac{n - n\bar{x}}{(1 - \theta)^2} = \bar{x} \left( \frac{n}{(1 - \theta)^2} + \frac{n}{\theta^2} \right) - \frac{n}{(1 - \theta)^2}
\]
so we can determine $\bar{x}$ from this value at any specified $\theta \in (0, 0.5)$ (since specifying $\theta$ allows us to compute $n/(1 - \theta)^2$, $n/\theta^2$ and then knowing the value of the right-hand side allows us to compute $\bar{x}$). Therefore, $\bar{x}$ is minimal sufficient.
\end{solution}

\begin{exercise}
\label{exer:6.1.24}
Suppose we have a sample of $n$ from the $\text{Multinomial}(1, (\theta, 2\theta, 1 - 3\theta))$ distribution, where $\theta \in [0, 1/3]$ is unknown. Determine the form of the likelihood function and show that $(x_1, x_2)$ is a minimal sufficient statistic where $x_i$ is the number of sample values corresponding to an observation in the $i$th category. (Hint: Problem~\ref{exer:6.1.23}.)
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\theta | x_1, x_2, x_3) = \theta^{x_1} (2\theta)^{x_2} (1 - 3\theta)^{x_3} = 2^{x_2} \theta^{x_1 + x_2} (1 - 3\theta)^{n - (x_1 + x_2)}
\]
for $\theta \in [0, 1/3]$. By the factorization theorem $x_1 + x_2$ is sufficient. To show $x_1 + x_2$ is minimal, suppose
\[
    \frac{L(\theta | x_1, x_2, x_3)}{L(\theta | y_1, y_2, y_3)} = \frac{2^{x_2} \theta^{x_1 + x_2} (1 - \theta)^{n - (x_1 + x_2)}}{2^{y_2} \theta^{y_1 + y_2} (1 - \theta)^{n - (y_1 + y_2)}} = 2^{x_2 - y_2} \left( \frac{\theta}{1 - \theta} \right)^{x_1 + x_2 - (y_1 + y_2)}
\]
is a constant when $x_1, x_2, x_3, y_1, y_2$ and $y_3$ are fixed. The likelihood ratio is constant if and only if $x_1 + x_2 = y_1 + y_2$. Hence, $x_1 + x_2$ is a minimal sufficient statistic.
\end{solution}

\begin{exercise}
\label{exer:6.1.25}
Suppose we observe $s$ from a statistical model with two densities, $f_1$ and $f_2$. Show that the likelihood ratio $T(s) = f_1(s)/f_2(s)$ is a minimal sufficient statistic. (Hint: Use the definition of sufficiency directly.)
\end{exercise}

\begin{solution}
The likelihood is given by $L(i | s) = f_i(s)$ for $i \in \{1, 2\}$. Now note that when $T(s_1) = f_1(s_1)/f_2(s_1) = f_1(s_2)/f_2(s_2) = T(s_2)$,
\begin{align*}
    L(1 \mid s_1) &= f_1(s_1) = \frac{f_2(s_1)}{f_2(s_2)} f_1(s_2) = \frac{f_2(s_1)}{f_2(s_2)} L(1 \mid s_2) \\
    L(2 \mid s_1) &= f_2(s_1) = \frac{f_1(s_1)}{f_1(s_2)} f_2(s_2) = \frac{f_2(s_1)}{f_2(s_2)} L(2 \mid s_2)
\end{align*}
and so $T$ is sufficient. Once we know $L(i | s)$, we can certainly compute $T(s)$ and so $T$ is minimal sufficient.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:6.1.26}
Consider the location-scale gamma model, i.e.,
\[
f_{(\theta,\lambda)}(x) = \frac{\lambda^{\alpha_0}}{\Gamma(\alpha_0)} (x - \theta)^{\alpha_0 - 1} \exp\{-\lambda(x - \theta)\}
\]
for $x \in R^1$, $\lambda > 0$, and where $\alpha_0 > 0$ is fixed.
\begin{enumerate}[(a)]
\item Determine the minimal sufficient statistic for a sample of $n$ when $\alpha_0 \geqslant 1$. (Hint: Determine where the likelihood is positive and calculate the partial derivative of the log of the likelihood with respect to $\lambda$.)
\item Determine the minimal sufficient statistic for a sample of $n$ when $\alpha_0 < 1$. (Hint: Use Problem~\ref{exer:6.1.18}, the partial derivative of the log of the likelihood with respect to $\lambda$, and determine where it is infinite.)
\end{enumerate}
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\mu, \sigma \mid x_1, \ldots, x_n) = \left( \prod_{i=1}^{n} (x_i - \mu) \right)^{\alpha_0 - 1} \exp\left\{ -n \frac{\bar{x} - \mu}{\sigma} \right\} \left( \frac{1}{\sigma} \right)^{n\alpha_0}
\]
for $\mu > x_{(1)}$, $\sigma > 0$ and is 0 otherwise.

Now observe that the logarithm of the likelihood function is given by
\[
    \ln L(\mu, \sigma \mid x_1, \ldots, x_n) = (\alpha_0 - 1) \sum_{i=1}^{n} \ln(x_{(i)} - \mu) - n \frac{\bar{x} - \mu}{\sigma} - n\alpha_0 \ln \sigma
\]
and
\[
    \frac{\partial}{\partial \mu} \ln L(\mu, \sigma \mid x_1, \ldots, x_n) = -(\alpha_0 - 1) \sum_{i=1}^{n} \frac{1}{x_{(i)} - \mu} + \frac{n}{\sigma}.
\]

\begin{enumerate}[(a)]
    \item When $\alpha_0 = 1$ the likelihood function is determined by $(x_{(1)}, \bar{x})$, so $(x_{(1)}, \bar{x})$ is sufficient. Given the likelihood, we can determine $x_{(1)}$ (this is the point where the likelihood becomes 0), and $\bar{x}$ is the point where the derivative of the log of the likelihood becomes 0. Therefore, we can determine $(x_{(1)}, \bar{x})$ from the likelihood and it is minimal sufficient.
    
    \item When $\alpha_0 \neq 1$ this derivative is infinite at each order statistic and nowhere else. So when $\alpha_0 \neq 1$ we can calculate the order statistic from the likelihood by determining every point where the log of the likelihood has an infinite derivative. Also, by Problem~\ref{exer:6.1.18} the order statistic is sufficient. Therefore, the order statistic is minimal sufficient in this case.
\end{enumerate}
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:6.1.27}
How important do you think it is for a statistician to try to quantify how much error there is in an inference drawn? For example, if an estimate is being quoted for some unknown quantity, is it important that the statistician give some indication about how accurate (or inaccurate) this inference is?
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Maximum Likelihood Estimation}
\label{sec:6.2}

In Section~\ref{sec:6.1}, we introduced the likelihood function $L(\cdot \mid s)$ as a basis for making inferences about the unknown true value $\theta$. We now begin to consider the specific types of inferences discussed in Section~\ref{ssec:5.5.3} and start with estimation.

When we are interested in a point estimate of $\theta$, then a value $\hat{\theta}(s)$ that maximizes $L(\cdot \mid s)$ is a sensible choice, as this value is the best supported by the data, i.e.,
\begin{equation}
\label{eq:6.2.1}
L(\hat{\theta}(s) \mid s) \geqslant L(\theta \mid s)
\end{equation}
for every $\theta \in \Omega$.

\begin{definition}
\label{def:6.2.1}
We call $\hat{\theta} : S \to \Omega$ satisfying \eqref{eq:6.2.1} for every $\theta$ a \emph{maximum likelihood estimator}, and the value $\hat{\theta}(s)$ is called a \emph{maximum likelihood estimate}, or \emph{MLE} for short.
\end{definition}

Notice that, if we use $cL(\cdot \mid s)$ as the likelihood function, for fixed $c > 0$, then $\hat{\theta}(s)$ is also an MLE using this version of the likelihood. So we can use any version of the likelihood to calculate an MLE.

\begin{example}
\label{ex:6.2.1}
Suppose the sample space is $S = \{1, 2, 3\}$, the parameter space is $\Omega = \{1, 2\}$, and the model is given by the following table.
\begin{center}
\begin{tabular}{c|ccc}
& $s = 1$ & $s = 2$ & $s = 3$ \\
\hline
$f_1(s)$ & 0.3 & 0.4 & 0.3 \\
$f_2(s)$ & 0.1 & 0.7 & 0.2
\end{tabular}
\end{center}
Further suppose we observe $s = 1$. So, for example, we could be presented with one of two bowls of chips containing these proportions of chips labeled 1, 2, and 3. We draw a chip, observe that it is labelled 1, and now want to make inferences about which bowl we have been presented with.

In this case, the MLE is given by $\hat{\theta}(1) = 1$ since $0.3 = L(1 \mid 1) > 0.1 = L(2 \mid 1)$. If we had instead observed $s = 2$, then $\hat{\theta}(2) = 2$; if we had observed $s = 3$, then $\hat{\theta}(3) = 1$.
\end{example}

Note that an MLE need not be unique. For example, in Example~\ref{ex:6.2.1}, if $f_2$ was defined by $f_2(1) = 0$, $f_2(2) = 0.7$, and $f_2(3) = 0.3$, then an MLE is as given there, but putting $\hat{\theta}(3) = 2$ also gives an MLE.

The MLE has a very important \emph{invariance property}. Suppose we reparameterize a model via a 1--1 function $\psi$ defined on $\Omega$. By this we mean that, instead of labelling the individual distributions in the model using $\theta \in \Omega$, we use $\phi = \psi(\theta) \in \Psi$:
\[
\Psi = \{\psi(\theta) : \theta \in \Omega\}.
\]
For example, in Example~\ref{ex:6.2.1}, we could take $\psi(1) = a$ and $\psi(2) = b$ so that $\Psi = \{a, b\}$. So the model is now given by $\{g_\phi : \phi \in \Psi\}$ where $g_\phi = f_\theta$ for the unique value $\theta$ such that $\phi = \psi(\theta)$. We have a new parameter $\phi$ and a new parameter space $\Psi$. Nothing has changed about the probability distributions in the statistical model, only the way they are labelled. We then have the following result.

\begin{theorem}
\label{thm:6.2.1}
If $\hat{\theta}(s)$ is an MLE for the original parameterization and, if $\psi$ is a 1--1 function defined on $\Omega$, then $\hat{\phi}(s) = \psi(\hat{\theta}(s))$ is an MLE in the new parameterization.
\end{theorem}

\begin{proof}
If we select the likelihood function for the new parameterization to be $L^*(\phi \mid s) = g_\phi(s)$ and the likelihood for the original parameterization to be $L(\theta \mid s) = f_\theta(s)$, then we have
\[
L^*(\hat{\phi}(s) \mid s) = g_{\hat{\phi}(s)}(s) = f_{\hat{\theta}(s)}(s) = L(\hat{\theta}(s) \mid s) \geqslant L(\theta \mid s) = L^*(\phi \mid s)
\]
for every $\phi \in \Psi$. This implies that $L^*(\hat{\phi}(s) \mid s) \geqslant L^*(\phi \mid s)$ for every $\phi \in \Psi$ and establishes the result.
\end{proof}

Theorem~\ref{thm:6.2.1} shows that no matter how we parameterize the model, the MLE behaves in a consistent way under the reparameterization. This is an important property, and not all estimation procedures satisfy this.

\subsection{Computation of the MLE}
\label{ssec:6.2.1}

An important issue is the computation of MLEs. In Example~\ref{ex:6.2.1}, we were able to do this by simply examining the table giving the distributions. With more complicated models, this approach is not possible. In many situations, however, we can use the methods of calculus to compute $\hat{\theta}(s)$. For this we require that $f_\theta(s)$ be a continuously differentiable function of $\theta$, so that we can use optimization methods from calculus.

Rather than using the likelihood function, it is often convenient to use the log-likelihood function.

\begin{definition}
\label{def:6.2.2}
For likelihood function $L(\cdot \mid s)$, the \emph{log-likelihood function} $l(\cdot \mid s)$ defined on $\Omega$, is given by $l(\theta \mid s) = \ln L(\theta \mid s)$.
\end{definition}

Note that $\ln x$ is a 1--1 increasing function of $x > 0$ and this implies that $L(\hat{\theta}(s) \mid s) \geqslant L(\theta \mid s)$ for every $\theta \in \Omega$ if and only if $l(\hat{\theta}(s) \mid s) \geqslant l(\theta \mid s)$ for every $\theta \in \Omega$. So we can maximize $l(\cdot \mid s)$ instead when computing an MLE. The convenience of the log-likelihood arises from the fact that, for a sample $s_1, \ldots, s_n$ from $\{f_\theta : \theta \in \Omega\}$, the likelihood function is given by
\[
L(\theta \mid s_1, \ldots, s_n) = \prod_{i=1}^{n} f_\theta(s_i)
\]
whereas the log-likelihood is given by
\[
l(\theta \mid s_1, \ldots, s_n) = \sum_{i=1}^{n} \ln f_\theta(s_i).
\]
It is typically much easier to differentiate a sum than a product.

Because we are going to be differentiating the log-likelihood, it is convenient to give a name to this derivative. We define the \emph{score function} $S(\theta \mid s)$ of a model to be the derivative of its log-likelihood function whenever this exists. So when $\theta$ is a one-dimensional real-valued parameter, then
\[
S(\theta \mid s) = \frac{\partial l(\theta \mid s)}{\partial \theta}
\]
provided this partial derivative exists (see Appendix A.5 for a definition of partial derivative). We restrict our attention now to the situation in which $\theta$ is one-dimensional.

To obtain the MLE, we must then solve the \emph{score equation}
\begin{equation}
\label{eq:6.2.2}
S(\theta \mid s) = 0
\end{equation}
for $\theta$. Of course, a solution to \eqref{eq:6.2.2} is not necessarily an MLE, because such a point may be a local minimum or only a local maximum rather than a global maximum. To guarantee that a solution $\hat{\theta}(s)$ is at least a local maximum, we must also check that
\begin{equation}
\label{eq:6.2.3}
\left.\frac{\partial S(\theta \mid s)}{\partial \theta}\right|_{\hat{\theta}(s)} = \left.\frac{\partial^2 l(\theta \mid s)}{\partial \theta^2}\right|_{\hat{\theta}(s)} < 0.
\end{equation}
Then we must evaluate $l(\theta \mid s)$ at each local maximum in order to determine the global maximum.

Let us compute some MLEs using calculus.

\begin{example}[Location Normal Model]
\label{ex:6.2.2}
Consider the likelihood function
\[
L(\theta \mid x_1, \ldots, x_n) = \exp\left\{-\frac{n}{2\sigma_0^2} (\bar{x} - \theta)^2\right\}
\]
obtained in Example~\ref{ex:6.1.4} for a sample $x_1, \ldots, x_n$ from the $N(\theta, \sigma_0^2)$ model where $\theta \in R^1$ is unknown and $\sigma_0^2$ is known. The log-likelihood function is then
\[
l(\theta \mid x_1, \ldots, x_n) = -\frac{n}{2\sigma_0^2} (\bar{x} - \theta)^2
\]
and the score function is
\[
S(\theta \mid x_1, \ldots, x_n) = \frac{n}{\sigma_0^2} (\bar{x} - \theta).
\]
The score equation is given by
\[
\frac{n}{\sigma_0^2} (\bar{x} - \theta) = 0.
\]
Solving this for $\theta$ gives the unique solution $\hat{\theta}(x_1, \ldots, x_n) = \bar{x}$. To check that this is a local maximum, we calculate
\[
\frac{\partial S(\theta \mid x_1, \ldots, x_n)}{\partial \theta} = -\frac{n}{\sigma_0^2}
\]
which is negative, and thus indicates that $\bar{x}$ is a local maximum. Because we have only one local maximum, it is also the global maximum and we have indeed obtained the MLE.
\end{example}

\begin{example}[Exponential Model]
\label{ex:6.2.3}
Suppose that a lifetime is known to be distributed $\text{Exponential}(1/\theta)$ where $\theta > 0$ is unknown. Then based on a sample $x_1, \ldots, x_n$, the likelihood is given by
\[
L(\theta \mid x_1, \ldots, x_n) = \left(\frac{1}{\theta}\right)^n \exp\left\{-\frac{n\bar{x}}{\theta}\right\},
\]
the log-likelihood is given by
\[
l(\theta \mid x_1, \ldots, x_n) = -n \ln \theta - \frac{n\bar{x}}{\theta},
\]
and the score function is given by
\[
S(\theta \mid x_1, \ldots, x_n) = -\frac{n}{\theta} + \frac{n\bar{x}}{\theta^2}.
\]
Solving the score equation gives $\hat{\theta}(x_1, \ldots, x_n) = \bar{x}$ and because $\bar{x} > 0$,
\[
\left.\frac{\partial S(\theta \mid x_1, \ldots, x_n)}{\partial \theta}\right|_{\bar{x}} = \frac{n}{\theta^2} - \frac{2n\bar{x}}{\theta^3}\bigg|_{\bar{x}} = \frac{n}{\bar{x}^2} - \frac{2n}{\bar{x}^2} = -\frac{n}{\bar{x}^2} < 0
\]
so $\bar{x}$ is indeed the MLE.
\end{example}

In both examples just considered, we were able to derive simple formulas for the MLE. This is not always possible. Consider the following example.

\begin{example}
\label{ex:6.2.4}
Consider a population in which individuals are classified according to one of three types labelled 1, 2, and 3, respectively. Further suppose that the proportions of individuals falling in these categories are known to follow the law $(p_1, p_2, p_3) = (\theta, \theta^2, 1 - \theta - \theta^2)$ where
\[
\theta \in [0.5(1 - \sqrt{2}), 0] \cup [0, 0.618\ldots03]
\]
is unknown. Here, $p_i$ denotes the proportion of individuals in the $i$th class. Note that the requirement that $0 \leqslant \theta^2 \leqslant 1$ imposes the upper bound on $\theta$, and the precise bound is obtained by solving $\theta + \theta^2 = 1 = 0$ for $\theta$ using the formula for the roots of a quadratic. Relationships like this, amongst the proportions of the distribution of a categorical variable, often arise in genetics. For example, the categorical variable might serve to classify individuals into different genotypes.

For a sample of $n$ (where $n$ is small relative to the size of the population so that we can assume observations are i.i.d.), the likelihood function is given by
\[
L(\theta \mid x_1, \ldots, x_n) = \theta^{x_1 + 2x_2} (1 - \theta - \theta^2)^{x_3}
\]
where $x_i$ denotes the sample count in the $i$th class. The log-likelihood function is then
\[
l(\theta \mid s_1, \ldots, s_n) = (x_1 + 2x_2) \ln \theta + x_3 \ln(1 - \theta - \theta^2),
\]
and the score function is
\[
S(\theta \mid s_1, \ldots, s_n) = \frac{x_1 + 2x_2}{\theta} - \frac{x_3(1 + 2\theta)}{1 - \theta - \theta^2}.
\]
The score equation then leads to a solution being a root of the quadratic
\begin{align*}
&(x_1 + 2x_2)(1 - \theta - \theta^2) - x_3(1 + 2\theta)\theta \\
&= -(x_1 + 2x_2 + 2x_3)\theta^2 - (x_1 + 2x_2 + x_3)\theta + (x_1 + 2x_2) = 0.
\end{align*}
Using the formula for the roots of a quadratic, we obtain
\[
\theta = \frac{1}{2(x_1 + 2x_2 + 2x_3)}\left\{-(x_1 + 2x_2 + x_3) \pm \sqrt{5x_1^2 + 20x_1x_2 + 10x_1x_3 + 20x_2^2 + 20x_2x_3 + x_3^2}\right\}.
\]
Notice that the formula for the roots does not determine the MLE in a clear way. In fact, we cannot even tell if either of the roots lies in $[0, 1]$! So there are four possible values for the MLE at this point --- either of the roots or the boundary points 0 and $0.61803\ldots$.

We can resolve this easily in an application by simply numerically evaluating the likelihood at the four points. For example, if $x_1 = 70$, $x_2 = 5$, and $x_3 = 25$, then the roots are $1.28616$ and $0.47847$, so it is immediate that the MLE is $\hat{\theta}(x_1, \ldots, x_n) = 0.47847$. We can see this graphically in the plot of the log-likelihood provided in Figure~\ref{fig:6.2.1}.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_2_1.pdf}
  \caption{The log-likelihood function in Example~\ref{ex:6.2.4} when $x_1 = 70$, $x_2 = 5$, and $x_3 = 25$.}
  \label{fig:6.2.1}
\end{figure}

In general, the score equation \eqref{eq:6.2.2} must be solved numerically, using an iterative routine like Newton--Raphson. Example~\ref{ex:6.2.4} demonstrates that we must be very careful not to just accept a solution from such a procedure as the MLE, but to check that the fundamental defining property \eqref{eq:6.2.1} is satisfied. We also have to be careful that the necessary smoothness conditions are satisfied so that calculus can be used. Consider the following example.

\begin{example}[{$\text{Uniform}[0, \theta]$ Model}]
\label{ex:6.2.5}
Suppose $x_1, \ldots, x_n$ is a sample from the $\text{Uniform}[0, \theta]$ model where $\theta > 0$ is unknown. Then the likelihood function is given by
\[
L(\theta \mid x_1, \ldots, x_n) = \begin{cases}
\theta^{-n} & x_i \leqslant \theta \text{ for } i = 1, \ldots, n \\
0 & x_i > \theta \text{ for some } i
\end{cases} = \theta^{-n} \indc_{[x_{(n)}, \infty)}(\theta)
\]
where $x_{(n)}$ is the largest order statistic from the sample. In Figure~\ref{fig:6.2.2}, we have graphed this function when $n = 10$ and $x_{(n)} = 1.916$. Notice that the maximum clearly occurs at $x_{(n)}$; we cannot obtain this value via differentiation, as $L(\theta \mid x_1, \ldots, x_n)$ is not differentiable there.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_2_2.pdf}
  \caption{Plot of the likelihood function in Example~\ref{ex:6.2.5} when $n = 10$ and $x_{(10)} = 1.916$.}
  \label{fig:6.2.2}
\end{figure}

The lesson of Examples~\ref{ex:6.2.4} and~\ref{ex:6.2.5} is that we have to be careful when computing MLEs. We now look at an example of a two-dimensional problem in which the MLE can be obtained using one-dimensional methods.

\begin{example}[Location-Scale Normal Model]
\label{ex:6.2.6}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution, where $\mu \in R^1$ and $\sigma > 0$ are unknown. The parameter in this model is two-dimensional, given by $(\mu, \sigma^2) \in R^1 \times (0, \infty)$. The likelihood function is then given by
\[
L((\mu, \sigma^2) \mid x_1, \ldots, x_n) = (2\pi\sigma^2)^{-n/2} \exp\left\{-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2\right\} \exp\left\{-\frac{n-1}{2\sigma^2} s^2\right\}
\]
as shown in Example~\ref{ex:6.1.8}. The log-likelihood function is given by
\begin{equation}
\label{eq:6.2.4}
l((\mu, \sigma^2) \mid x_1, \ldots, x_n) = -\frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln \sigma^2 - \frac{n}{2\sigma^2}(\bar{x} - \mu)^2 - \frac{n-1}{2\sigma^2} s^2.
\end{equation}
As discussed in Example~\ref{ex:6.1.8}, it is clear that, for fixed $\sigma^2$, \eqref{eq:6.2.4} is maximized, as a function of $\mu$, by $\mu = \bar{x}$. Note that this does not involve $\sigma^2$, so this must be the first coordinate of the MLE.

Substituting $\mu = \bar{x}$ into \eqref{eq:6.2.4}, we obtain
\begin{equation}
\label{eq:6.2.5}
-\frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln \sigma^2 - \frac{n-1}{2\sigma^2} s^2,
\end{equation}
and the second coordinate of the MLE must be the value of $\sigma^2$ that maximizes \eqref{eq:6.2.5}. Differentiating \eqref{eq:6.2.5} with respect to $\sigma^2$ and setting this equal to 0 gives
\begin{equation}
\label{eq:6.2.6}
-\frac{n}{2\sigma^2} + \frac{n-1}{2(\sigma^2)^2} s^2 = 0.
\end{equation}
Solving \eqref{eq:6.2.6} for $\sigma^2$ leads to the solution
\[
\hat{\sigma}^2 = \frac{n-1}{n} s^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2.
\]
Differentiating \eqref{eq:6.2.6} with respect to $\sigma^2$ and substituting in $\hat{\sigma}^2$, we see that the second derivative is negative, hence $\hat{\sigma}^2$ is a point where the maximum is attained.

Therefore, we have shown that the MLE of $(\mu, \sigma^2)$ is given by
\[
\left(\bar{x}, \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2\right).
\]
In the following section we will show that this result can also be obtained using multidimensional calculus.
\end{example}

So far we have talked about estimating only the full parameter $\theta$ for a model. What about estimating a general characteristic of interest $\psi(\theta)$ for some function $\psi$ defined on the parameter space $\Omega$? Perhaps the obvious answer here is to use the estimate $\hat{\psi}(s) = \psi(\hat{\theta}(s))$ where $\hat{\theta}(s)$ is an MLE of $\theta$. This is sometimes referred to as the \emph{plug-in MLE} of $\psi(\theta)$. Notice, however, that the plug-in MLE is not necessarily a true MLE, in the sense that we have a likelihood function for a model indexed by $\psi$ and that takes its maximum value at $\hat{\psi}(s)$. If $\psi$ is a 1--1 function defined on $\Omega$, then Theorem~\ref{thm:6.2.1} establishes that $\hat{\psi}(s)$ is a true MLE but not otherwise.

If $\psi$ is not 1--1, then we can often find a complementing function $\xi$ defined on $\Omega$ so that $(\psi, \xi)$ is a 1--1 function of $\theta$. Then, by Theorem~\ref{thm:6.2.1},
\[
(\hat{\psi}(s), \hat{\xi}(s)) = (\psi(\hat{\theta}(s)), \xi(\hat{\theta}(s)))
\]
is the joint MLE, but $\hat{\psi}(s)$ is still not formally an MLE. Sometimes a plug-in MLE can perform badly, as it ignores the information in $\hat{\xi}(s)$ about the true value of $\psi(\theta)$. An example illustrates this phenomenon.

\begin{example}[Sum of Squared Means]
\label{ex:6.2.7}
Suppose that $X_i \sim N(\mu_i, 1)$ for $i = 1, \ldots, n$ and that these are independent with the $\mu_i$ completely unknown. So here, $\theta = (\mu_1, \ldots, \mu_n)$ and $\Omega = R^n$. Suppose we want to estimate $\psi(\theta) = \mu_1^2 + \cdots + \mu_n^2$.

The log-likelihood function is given by
\[
l(\theta \mid x_1, \ldots, x_n) = -\frac{1}{2} \sum_{i=1}^{n} (x_i - \mu_i)^2.
\]
Clearly this is maximized by $\hat{\theta}(x_1, \ldots, x_n) = (x_1, \ldots, x_n)$. So the plug-in MLE of $\psi(\theta)$ is given by $\sum_{i=1}^{n} x_i^2$.

Now observe that
\[
\expc_\theta\left[\sum_{i=1}^{n} X_i^2\right] = \sum_{i=1}^{n} \expc_\theta[X_i^2] = \sum_{i=1}^{n} (\var_\theta(X_i) + \mu_i^2) = n + \psi(\theta),
\]
where $\expc_\theta(g)$ refers to the expectation of $g(s)$ when $s \sim f_\theta$. So when $n$ is large, it is likely that $\hat{\psi}$ is far from the true value. An immediate improvement in this estimator is to use $\sum_{i=1}^{n} x_i^2 - n$ instead.
\end{example}

There have been various attempts to correct problems such as the one illustrated in Example~\ref{ex:6.2.7}. Typically, these involve modifying the likelihood in some way. We do not pursue this issue further in this text but we do advise caution when using plug-in MLEs. Sometimes, as in Example~\ref{ex:6.2.6}, where we estimate $\mu$ by $\bar{x}$ and $\sigma^2$ by $s^2$, they seem appropriate; other times, as in Example~\ref{ex:6.2.7}, they do not.

\subsection{The Multidimensional Case (Advanced)}
\label{ssec:6.2.2}

We now consider the situation in which $\theta = (\theta_1, \ldots, \theta_k) \in R^k$ is multidimensional, i.e., $k > 1$. The likelihood and log-likelihood are then defined just as before, but the score function is now given by
\[
S(\theta \mid s) = \begin{pmatrix}
\dfrac{\partial l(\theta \mid s)}{\partial \theta_1} \\[2ex]
\dfrac{\partial l(\theta \mid s)}{\partial \theta_2} \\[1ex]
\vdots \\[1ex]
\dfrac{\partial l(\theta \mid s)}{\partial \theta_k}
\end{pmatrix}
\]
provided all these partial derivatives exist. For the score equation, we get
\[
\begin{pmatrix}
\dfrac{\partial l(\theta \mid s)}{\partial \theta_1} \\[2ex]
\dfrac{\partial l(\theta \mid s)}{\partial \theta_2} \\[1ex]
\vdots \\[1ex]
\dfrac{\partial l(\theta \mid s)}{\partial \theta_k}
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
\vdots \\
0
\end{pmatrix}
\]
and we must solve this $k$-dimensional equation for $(\theta_1, \ldots, \theta_k)$. This is often much more difficult than in the one-dimensional case, and we typically have to resort to numerical methods.

A necessary and sufficient condition for $(\theta_1, \ldots, \theta_k)$ to be a local maximum, when the log-likelihood has continuous second partial derivatives, is that the matrix of second partial derivatives of the log-likelihood, evaluated at $(\theta_1, \ldots, \theta_k)$, must be negative definite (equivalently, all of its eigenvalues must be negative). We then must evaluate the likelihood at each of the local maxima obtained to determine the global maximum or MLE.

We will not pursue the numerical computation of MLEs in the multidimensional case any further here, but we restrict our attention to a situation in which we carry out the calculations in closed form.

\begin{example}[Location-Scale Normal Model]
\label{ex:6.2.8}
We determined the log-likelihood function for this model in \eqref{eq:6.2.4}. The score function is then
\[
S((\mu, \sigma^2) \mid x_1, \ldots, x_n) = \begin{pmatrix}
S_\mu(\theta \mid x_1, \ldots, x_n) \\[1ex]
S_{\sigma^2}(\theta \mid x_1, \ldots, x_n)
\end{pmatrix} = \begin{pmatrix}
\dfrac{n}{\sigma^2}(\bar{x} - \mu) \\[2ex]
-\dfrac{n}{2\sigma^2} + \dfrac{n}{2\sigma^4}(\bar{x} - \mu)^2 + \dfrac{n-1}{2\sigma^4} s^2
\end{pmatrix}.
\]
The score equation is
\[
\begin{pmatrix}
\dfrac{n}{\sigma^2}(\bar{x} - \mu) \\[2ex]
-\dfrac{n}{2\sigma^2} + \dfrac{n}{2\sigma^4}(\bar{x} - \mu)^2 + \dfrac{n-1}{2\sigma^4} s^2
\end{pmatrix} = \begin{pmatrix}
0 \\
0
\end{pmatrix},
\]
and the first of these equations immediately implies that $\mu = \bar{x}$. Substituting this value for $\mu$ into the second equation and solving for $\sigma^2$ leads to the solution
\[
\hat{\sigma}^2 = \frac{n-1}{n} s^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2.
\]
From Example~\ref{ex:6.2.6}, we know that this solution does indeed give the MLE.
\end{example}

\subsection*{Summary of Section~\ref{sec:6.2}}

\begin{itemize}
\item An MLE (maximum likelihood estimator) is a value of the parameter that maximizes the likelihood function. It is the value of $\theta$ that is best supported by the model and data.
\item We can often compute an MLE by using the methods of calculus. When applicable, this leads to solving the score equation for $\theta$ either explicitly or using numerical algorithms. Always be careful to check that these methods are applicable to the specific problem at hand. Furthermore, always check that any solution to the score equation is a maximum and indeed an absolute maximum.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:6.2.1}
Suppose that $S = \{1, 2, 3, 4\}$, $\Omega = \{a, b\}$, where the two probability distributions are given by the following table.
\begin{center}
\begin{tabular}{c|cccc}
& $s = 1$ & $s = 2$ & $s = 3$ & $s = 4$ \\
\hline
$a$ & $1/2$ & $1/6$ & $1/6$ & $1/6$ \\
$b$ & $1/3$ & $1/3$ & $1/3$ & $0$
\end{tabular}
\end{center}
Determine the MLE of $\theta$ for each possible data value.
\end{exercise}

\begin{solution}
The MLEs are $\hat{\theta}(1) = a$, $\hat{\theta}(2) = b$, $\hat{\theta}(3) = b$, $\hat{\theta}(4) = a$.
\end{solution}

\begin{exercise}
\label{exer:6.2.2}
If $x_1, \ldots, x_n$ is a sample from a Bernoulli$(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown, then determine the MLE of $\theta$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{n\bar{x}} (1 - \theta)^{n(1-\bar{x})}$. The log-likelihood function is given by $l(\theta \mid x_1, \ldots, x_n) = n\bar{x} \ln \theta + n(1 - \bar{x}) \ln(1 - \theta)$. The score function is given by
\[
    S(\theta \mid x_1, \ldots, x_n) = \frac{n\bar{x}}{\theta} - \frac{n(1 - \bar{x})}{1 - \theta}.
\]
Solving the score equation gives $\hat{\theta}(x_1, \ldots, x_n) = \bar{x}$. Note that since $0 \leqslant \bar{x} \leqslant 1$ we have that
\[
    \frac{\partial S(\theta \mid x_1, \ldots, x_n)}{\partial \theta} \bigg|_{\theta = \bar{x}} = \left. -\frac{n\bar{x}}{\theta^2} - \frac{n(1 - \bar{x})}{(1 - \theta)^2} \right|_{\theta = \bar{x}} = -\frac{n}{\bar{x}} - \frac{n}{1 - \bar{x}} < 0.
\]
So $\bar{x}$ is indeed the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.3}
If $x_1, \ldots, x_n$ is a sample from a Bernoulli$(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown, then determine the MLE of $\theta^2$.
\end{exercise}

\begin{solution}
Since $\psi(\theta) = \theta^2$ is a 1--1 transformation of $\theta$ when $\theta$ is restricted to $[0, 1]$, we can apply Theorem~\ref{thm:6.2.1}, so the MLE is $\psi(\hat{\theta}(x_1, \ldots, x_n)) = \bar{x}^2$.
\end{solution}

\begin{exercise}
\label{exer:6.2.4}
If $x_1, \ldots, x_n$ is a sample from a Poisson$(\lambda)$ distribution, where $\lambda > 0$ is unknown, then determine the MLE of $\lambda$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = e^{-n\theta} \theta^{n\bar{x}}$, the log-likelihood function is given by $l(\theta \mid x_1, \ldots, x_n) = -n\theta + n\bar{x} \ln \theta$, and the score function is given by
\[
    S(\theta \mid x_1, \ldots, x_n) = -n + \frac{n\bar{x}}{\theta}.
\]
Solving the score equation gives $\hat{\theta}(x_1, \ldots, x_n) = \bar{x}$. Note that since $\bar{x} \geqslant 0$, we have
\[
    \frac{\partial S(\theta \mid x_1, \ldots, x_n)}{\partial \theta} \bigg|_{\theta = \bar{x}} = \left. -\frac{n\bar{x}}{\theta^2} \right|_{\theta = \bar{x}} = -\frac{n}{\bar{x}} < 0
\]
so $\bar{x}$ is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.5}
If $x_1, \ldots, x_n$ is a sample from a $\text{Gamma}(\alpha_0, \lambda)$ distribution, where $\alpha_0 > 0$ and $\lambda > 0$ is unknown, then determine the MLE of $\lambda$.
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta \mid x_1, \ldots, x_n) = \theta^{n\alpha_0} \exp(-n\bar{x}\theta)$, the log-likelihood function is given by $l(\theta \mid x_1, \ldots, x_n) = n\alpha_0 \ln \theta - n\bar{x}\theta$, and the score function is given by $S(\theta \mid x_1, \ldots, x_n) = n\alpha_0/\theta - n\bar{x}$. Solving the score equation gives $\hat{\theta}(x_1, \ldots, x_n) = \alpha_0/\bar{x}$. Note that since $\bar{x} > 0$ we have that
\[
    \frac{\partial S(\theta \mid x_1, \ldots, x_n)}{\partial \theta} \bigg|_{\theta = \frac{\alpha_0}{\bar{x}}} = \left. -\frac{n\alpha_0}{\theta^2} \right|_{\theta = \frac{\alpha_0}{\bar{x}}} = -\frac{n\bar{x}^2}{\alpha_0} < 0,
\]
so $\hat{\theta} = \alpha_0/\bar{x}$ is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.6}
Suppose that $x_1, \ldots, x_n$ is the result of independent tosses of a coin where we toss until the first head occurs and where the probability of a head on a single toss is $\theta \in (0, 1]$. Determine the MLE of $\theta$.
\end{exercise}

\begin{solution}
First, note that each $x_i$ comes from a $\text{Geometric}(\theta)$ distribution. The likelihood function is then given by $L(\theta \mid x_1, \ldots, x_n) = \theta^n (1 - \theta)^{n\bar{x}}$, the log-likelihood function is given by $l(\theta \mid x_1, \ldots, x_n) = n \ln \theta + n\bar{x} \ln(1 - \theta)$, and the score function is given by
\[
    S(\theta \mid x_1, \ldots, x_n) = \frac{n}{\theta} - \frac{n\bar{x}}{1 - \theta}.
\]
Solving the score equation gives $\hat{\theta}(x_1, \ldots, x_n) = 1/(1 + \bar{x})$. Note that since $0 \leqslant \bar{x} \leqslant 1$, we have that
\[
    \frac{\partial S(\theta \mid x_1, \ldots, x_n)}{\partial \theta} \bigg|_{\hat{\theta} = \frac{1}{1+\bar{x}}} = \left. -\frac{n}{\theta^2} - \frac{n\bar{x}}{(1 - \theta)^2} \right|_{\hat{\theta} = \frac{1}{1+\bar{x}}} = -n \left( (1 + \bar{x})^2 + \frac{(1 + \bar{x})^2}{\bar{x}} \right) < 0.
\]
So $\hat{\theta} = 1/(1 + \bar{x})$ is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.7}
If $x_1, \ldots, x_n$ is a sample from a $\text{Beta}(\alpha, 1)$ distribution (see Problem~\ref{exer:2.4.24}) where $\alpha > 0$ is unknown, then determine the MLE of $\alpha$. (Hint: Assume $\Gamma$ is a differentiable function of $\alpha$.)
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\alpha \mid x_1, \ldots, x_n) = \left( \frac{\Gamma(\alpha + 1)}{\Gamma(\alpha)} \right)^n \prod_{i=1}^{n} x_i^{\alpha - 1} = \left( \prod_{i=1}^{n} x_i \right)^{\alpha - 1}.
\]
The log-likelihood function is given by
\[
    l(\alpha \mid x_1, \ldots, x_n) = n \ln(\Gamma(\alpha + 1)) - n \ln(\Gamma(\alpha)) + (\alpha - 1) \sum_{i=1}^{n} \ln x_i.
\]
The score function is given by
\begin{align*}
    S(\alpha \mid x_1, \ldots, x_n) &= n \frac{(\Gamma(\alpha + 1))'}{\Gamma(\alpha + 1)} - n \frac{\Gamma'(\alpha)}{\Gamma(\alpha)} + \sum_{i=1}^{n} \ln x_i \\
    &= n \frac{\Gamma(\alpha) + \alpha \Gamma'(\alpha)}{\alpha \Gamma(\alpha)} - n \frac{\Gamma'(\alpha)}{\Gamma(\alpha)} + \sum_{i=1}^{n} \ln x_i = \frac{n}{\alpha} + \sum_{i=1}^{n} \ln x_i.
\end{align*}
Then the solution to the score equation is given by
\[
    \hat{\alpha} = -\frac{n}{\sum_{i=1}^{n} \ln x_i}.
\]
The second derivative of the score at $\hat{\alpha}$ is given by
\[
    \left. -\frac{n}{\alpha^2} \right|_{\alpha = \hat{\alpha}} = -\frac{n}{\hat{\alpha}^2} < 0
\]
so $\hat{\alpha}$ is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.8}
If $x_1, \ldots, x_n$ is a sample from a Weibull distribution (see Problem~\ref{exer:2.4.19}), where $\lambda > 0$ is unknown, then determine the score equation for the MLE of $\lambda$.
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\beta \mid x_1, \ldots, x_n) = \beta^n \left( \prod_{i=1}^{n} x_i \right)^{\beta - 1} \exp\left( -\sum_{i=1}^{n} x_i^\beta \right),
\]
the log-likelihood function is given by
\[
    l(\beta \mid x_1, \ldots, x_n) = n \ln \beta + (\beta - 1) \left( \sum_{i=1}^{n} \ln x_i \right) - \sum_{i=1}^{n} x_i^\beta,
\]
and the score equation is given by
\[
    S(\beta \mid x_1, \ldots, x_n) = \frac{n}{\beta} + \sum_{i=1}^{n} \ln x_i - \sum_{i=1}^{n} x_i^\beta \ln x_i = 0.
\]
\end{solution}

\begin{exercise}
\label{exer:6.2.9}
If $x_1, \ldots, x_n$ is a sample from a Pareto distribution (see Problem~\ref{exer:2.4.20}), where $\beta > 0$ is unknown, then determine the MLE of $\beta$.
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\alpha \mid x_1, \ldots, x_n) = \prod_{i=1}^{n} \alpha (1 + x_i)^{-(\alpha + 1)} = \alpha^n \left( \prod_{i=1}^{n} (1 + x_i) \right)^{-(\alpha + 1)},
\]
the log-likelihood function is given by
\[
    l(\alpha \mid x_1, \ldots, x_n) = n \ln \alpha - (\alpha + 1) \sum_{i=1}^{n} \ln(1 + x_i),
\]
and the score function is given by
\[
    S(\alpha \mid x_1, \ldots, x_n) = \frac{n}{\alpha} - \sum_{i=1}^{n} \ln(1 + x_i).
\]
Solving the score equation gives
\[
    \hat{\alpha}(x_1, \ldots, x_n) = \frac{n}{\sum_{i=1}^{n} \ln(1 + x_i)}.
\]
Note also that $\frac{\partial}{\partial \alpha} S(\alpha \mid x_1, \ldots, x_n) = -n/\alpha^2 < 0$ for every $\alpha$, so $\hat{\alpha}$ is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.10}
If $x_1, \ldots, x_n$ is a sample from a Log-normal distribution (see Problem~\ref{exer:2.6.17}), where $\mu \in R^1$ is unknown, then determine the MLE of $\mu$.
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\tau \mid x_1, \ldots, x_n) = \left( \frac{1}{\sqrt{2\pi}\tau} \right)^n \exp\left( -\frac{\sum_{i=1}^{n} (\ln x_i)^2}{2\tau^2} \right) \prod_{i=1}^{n} \frac{1}{x_i},
\]
the log-likelihood function is given by
\[
    l(\tau \mid x_1, \ldots, x_n) = -\frac{n}{2} \ln(2\pi) - n \ln \tau - \frac{1}{2\tau^2} \sum_{i=1}^{n} (\ln x_i)^2 + \sum_{i=1}^{n} \ln \frac{1}{x_i},
\]
and the score function is given by
\[
    S(\tau \mid x_1, \ldots, x_n) = -\frac{n}{\tau} + \frac{1}{\tau^3} \sum_{i=1}^{n} (\ln x_i)^2.
\]
Solving the score equation gives
\[
    \hat{\tau}(x_1, \ldots, x_n) = \pm \sqrt{\frac{\sum_{i=1}^{n} (\ln x_i)^2}{n}}
\]
and since $\tau > 0$, we take the positive root. Now
\[
    \frac{\partial S(\tau \mid x_1, \ldots, x_n)}{\partial \tau} \bigg|_{\tau = \hat{\tau}} = \left. \frac{n}{\tau^2} - \frac{3}{\tau^4} \sum_{i=1}^{n} (\ln x_i)^2 \right|_{\tau = \hat{\tau}} = -\frac{2n^2}{\sum_{i=1}^{n} (\ln x_i)^2} < 0.
\]
So $\hat{\tau}$ is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.11}
Suppose you are measuring the volume of a cubic box in centimeters by taking repeated independent measurements of one of the sides. Suppose it is reasonable to assume that a single measurement follows an $N(\theta, \sigma_0^2)$ distribution, where $\theta$ is unknown and $\sigma_0^2$ is known. Based on a sample of measurements, you obtain the MLE of $\theta$ as 3.2 cm. What is your estimate of the volume of the box? How do you justify this in terms of the likelihood function?
\end{exercise}

\begin{solution}
The parameter of the interest is changed to the volume $\eta = \mu^3$ from the length of a side $\mu$. Then the likelihood function is also changed to
\[
    L_v(\eta | s) = L_v(\mu^3 | s) = L_l(\mu | s)
\]
where $L_v$ is the likelihood function when the volume parameter $\eta = \mu^3$ is of the interest and $L_l$ is the likelihood function of the length of a side parameter $\mu$. The maximizer $\eta$ of $L_v(\eta | s)$ is also a maximizer of $L_l(\eta^{1/3} | s)$. In other words, the MLE is invariant under 1--1 smooth parameter transformations. Hence, the MLE of $\eta$ is equal to $\hat{\mu}^3 = (3.2 \text{cm})^3 = 32.768 \text{cm}^3$.
\end{solution}

\begin{exercise}
\label{exer:6.2.12}
If $x_1, \ldots, x_n$ is a sample from an $N(\mu_0, \sigma^2)$ distribution, where $\sigma^2 > 0$ is unknown and $\mu_0$ is known, then determine the MLE of $\sigma^2$. How does this MLE differ from the plug-in MLE of $\sigma^2$ computed using the location-scale normal model?
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\sigma^2 | x_1, \ldots, x_n) = (\sigma^2)^{-n/2} \exp\left( -\sum_{i=1}^{n} (x_i - \mu_0)^2 / (2\sigma^2) \right).
\]
The derivative of the log-likelihood function with respect to $\sigma^2$ is
\[
    -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^{n} (x_i - \mu_0)^2.
\]
Hence, the maximum likelihood estimator is $\hat{\sigma}^2 = n^{-1} \sum_{i=1}^{n} (x_i - \mu_0)^2$. If the location parameter $\mu_0$ is also unknown, then the estimator for $\sigma^2$ is $\tilde{\sigma}^2 = (n-1)^{-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$ as in Example~\ref{ex:6.2.6}. The difference of two estimators is
\begin{align*}
    \hat{\sigma}^2 - \tilde{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu_0)^2 - \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 \\
    &= -\frac{1}{n(n-1)} \sum_{i=1}^{n} (x_i - \bar{x})^2 + (\bar{x} - \mu_0)^2 \\
    &= -s^2/n + (\bar{x} - \mu_0)^2.
\end{align*}
In the second equality, the expansion $(x_i - \mu_0)^2 = (x_i - \bar{x})^2 + (\bar{x} - \mu_0)^2 + 2(\bar{x} - \mu_0)(x_i - \bar{x})$ is used. Thus, the summation becomes $\sum_{i=1}^{n} (x_i - \mu_0)^2 = \sum_{i=1}^{n} (x_i - \bar{x})^2 + n(\bar{x} - \mu_0)^2 + 2(\bar{x} - \mu_0) \sum_{i=1}^{n} (x_i - \bar{x})$. The last term is zero because the summation in the last term is zero. By the law of large numbers, $\bar{x} \xrightarrow{P} \mu_0$ and $s^2 \xrightarrow{P} \sigma^2$. Hence, the difference $\hat{\sigma}^2 - \tilde{\sigma}^2 \xrightarrow{P} 0$ as $n \to \infty$.
\end{solution}

\begin{exercise}
\label{exer:6.2.13}
Explain why it is not possible that the function $3 \exp\{-5(\theta - 3)^2\}$ for $\theta \in R^1$ is a likelihood function.
\end{exercise}

\begin{solution}
A likelihood function must have non-negative values but $\theta^3 \exp(-(\theta - 5.3)^2) < 0$ for all $\theta < 0$. Hence, $\theta^3 \exp(-(\theta - 5.3)^2)$ for $\theta \in \mathbb{R}^1$ cannot be a likelihood function.
\end{solution}

\begin{exercise}
\label{exer:6.2.14}
Suppose you are told that a likelihood function has local maxima at the points $-2.2$, 4.6, and 9.2, as determined using calculus. Explain how you would determine the MLE.
\end{exercise}

\begin{solution}
Suppose the likelihood function has only three local maxima. The MLE is the point having the maximum likelihood. Hence, the point among $-2.2$, $4.6$ and $9.2$ having the biggest likelihood is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.15}
If two functions of $\theta$ are equivalent versions of the likelihood when one is a positive multiple of the other, then when are two log-likelihood functions equivalent?
\end{exercise}

\begin{solution}
We have that $L_1(\theta | s) = c L_2(\theta | s)$ for some $c > 0$, if and only if $\ln L_1(\theta | s) = \ln c + \ln L_2(\theta | s)$. So, two equivalent log-likelihood functions differ by an additive constant.
\end{solution}

\begin{exercise}
\label{exer:6.2.16}
Suppose you are told that the likelihood of $\theta$ at $\theta = 2$ is given by $1/4$. Is this the probability that $\theta = 2$? Explain why or why not.
\end{exercise}

\begin{solution}
A function that is proportional to the density as a function of parameter is a likelihood function. So any likelihood function $L$ can be written as $L(\theta | s) = c(s) f_\theta(s)$ for some function $c$. Hence, $L(\theta | s) = 1/4$ does not imply $f_\theta(s) = 1/4$.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:6.2.17}
A likelihood function is given by $\exp\{-\frac{1}{2}(\theta - 2)^3\} \exp\{-2(\theta - 2)^2\}$ for $\theta \in R^1$. Numerically approximate the MLE by evaluating this function at 1000 equispaced points in $[-10, 10]$. Also plot the likelihood function.
\end{exercise}

\begin{solution}
The approximate MLE is $\hat{\theta} = 1.80000$ (obtained from the values in C1 and C2) and the maximum likelihood is 3.66675. The following code was used.

\begin{listing}[!htbp]
\begin{minted}{R}
# Set up theta values
theta <- seq(-10, 10, length.out = 1000)

# Calculate likelihood function
likelihood <- exp(-(theta - 1)^2/2) + 3*exp(-(theta - 2)^2/2)

# Find the MLE
mle_index <- which.max(likelihood)
mle_theta <- theta[mle_index]
max_likelihood <- likelihood[mle_index]

cat("Approximate MLE:", mle_theta, "\n")
cat("Maximum likelihood:", max_likelihood, "\n")

# Plot
plot(theta, likelihood, type = "l", xlab = "theta", ylab = "likelihood")
\end{minted}
\caption{R code for Exercise 6.2.17}
\label{lst:ex6.2.17}
\end{listing}
\end{solution}

\begin{exercise}
\label{exer:6.2.18}
A likelihood function is given by $\exp\{-\frac{1}{2}(\theta - 2)^3\} + \exp\{-5(\theta - 2)^2\}$ for $\theta \in R^1$. Numerically approximate the MLE by evaluating this function at 1000 equispaced points in $[-10, 10]$. Also plot the likelihood function. Comment on the form of likelihood intervals.
\end{exercise}

\begin{solution}
The approximate MLE is $\hat{\theta} = 5.00000$ and the maximum likelihood is 3.00034. The following code was used.

\begin{listing}[!htbp]
\begin{minted}{R}
# Set up theta values
theta <- seq(-10, 10, length.out = 1000)

# Calculate likelihood function
likelihood <- exp(-(theta - 1)^2/2) + 3*exp(-(theta - 5)^2/2)

# Find the MLE
mle_index <- which.max(likelihood)
mle_theta <- theta[mle_index]
max_likelihood <- likelihood[mle_index]

cat("Approximate MLE:", mle_theta, "\n")
cat("Maximum likelihood:", max_likelihood, "\n")

# Plot
plot(theta, likelihood, type = "l", xlab = "theta", ylab = "likelihood")
\end{minted}
\caption{R code for Exercise 6.2.18}
\label{lst:ex6.2.18}
\end{listing}

Note that the likelihood graph is bimodal. If $\gamma$ is big enough, then the likelihood region will be just one interval. However, if $\gamma$ is small, then the likelihood region will be the union of two disjoint intervals.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig_6_2_18.pdf}
    \caption{Bimodal likelihood function for Exercise 6.2.18}
    %\label{fig:bimodal-likelihood}
\end{figure}
\end{solution}

\subsection*{Problems}

\begin{exercise}[Hardy--Weinberg law]
\label{exer:6.2.19}
The Hardy--Weinberg law in genetics says that the proportions of genotypes AA, Aa, and aa are $\theta^2$, $2\theta(1 - \theta)$, and $(1 - \theta)^2$, respectively, where $\theta \in [0, 1]$. Suppose that in a sample of $n$ from the population (small relative to the size of the population), we observe $x_1$ individuals of type AA, $x_2$ individuals of type Aa and $x_3$ individuals of type aa.
\begin{enumerate}[(a)]
\item What distribution do the counts $(X_1, X_2, X_3)$ follow?
\item Record the likelihood function, the log-likelihood function, and the score function for $\theta$.
\item Record the form of the MLE for $\theta$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item The counts are distributed $\text{Multinomial}\left( \theta^2, 2\theta(1 - \theta), (1 - \theta)^2 \right)$.
    
    \item The likelihood function is given by
    \[
        L(\theta \mid s_1, \ldots, s_n) = \theta^{2x_1} (2\theta(1 - \theta))^{x_2} (1 - \theta)^{2x_3} = 2^{x_2} \theta^{2x_1 + x_2} (1 - \theta)^{x_2 + 2x_3},
    \]
    the log-likelihood function is given by
    \[
        l(\theta \mid s_1, \ldots, s_n) = x_2 \ln 2 + (2x_1 + x_2) \ln \theta + (x_2 + 2x_3) \ln(1 - \theta),
    \]
    and the score function is given by
    \[
        S(\theta \mid s_1, \ldots, s_n) = \frac{2x_1 + x_2}{\theta} - \frac{x_2 + 2x_3}{1 - \theta}.
    \]
    
    \item Solving the score equation gives
    \[
        \hat{\theta}(s_1, \ldots, s_n) = \frac{2x_1 + x_2}{2(x_1 + x_2 + x_3)}.
    \]
    Since
    \[
        \frac{\partial S(\theta \mid s_1, \ldots, s_n)}{\partial \theta} = -\frac{2x_1 + x_2}{\theta^2} - \frac{x_2 + 2x_3}{(1 - \theta)^2} < 0
    \]
    for every $\theta \in [0, 1]$ this is the MLE for $\theta$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:6.2.20}
If $x_1, \ldots, x_n$ is a sample from an $N(\mu, 1)$ distribution where $\mu \in R^1$ is unknown, determine the MLE of the probability content of the interval $(\mu - 1, \mu + 1)$. Justify your answer.
\end{exercise}

\begin{solution}
First, recall that the MLE for $\mu$ is $\bar{x}$ (Example~\ref{ex:6.2.2}). The parameter of interest now is $\psi(\mu) = \prb_\mu(X < 1) = \Phi(1 - \mu)$, where $\Phi$ is the cdf of a $N(0, 1)$. Since $\Phi(1 - \mu)$ is a strictly decreasing function of $\mu$, then $\psi$ is a 1--1 function of $\mu$. Hence, we can apply Theorem~\ref{thm:6.2.1} and conclude that $\hat{\psi} = \Phi(1 - \bar{x})$ is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.21}
If $x_1, \ldots, x_n$ is a sample from an $N(0, \sigma^2)$ distribution where $\sigma > 0$ is unknown, determine the MLE of $\sigma$.
\end{exercise}

\begin{solution}
The log-likelihood function is $l(\mu \mid x_1, \ldots, x_n) = -n(\bar{x} - \mu)^2/2$, and, as a function of $\mu$, its graph is a concave parabola and its maximum value occurs at $\bar{x}$. So if $\bar{x} \geqslant 0$, this is the MLE. If $\bar{x} < 0$, however, the maximum occurs at 0 and this is the MLE.
\end{solution}

\begin{exercise}
\label{exer:6.2.22}
Prove that, if $\hat{\theta}(s)$ is the MLE for a model for response $s$ and if $T$ is a sufficient statistic for the model, then $\hat{\theta}(s)$ is also the MLE for the model for $T(s)$.
\end{exercise}

\begin{solution}
By the factorization theorem $L(\theta \mid s) = f_\theta(s) = h(s) g_\theta(T(s))$. The probability function for $T$ is given by
\[
    f_\theta^T(t) = \sum_{\{s : T(s) = t\}} f_\theta(s) = \sum_{\{s : T(s) = t\}} h(s) g_\theta(T(s)) = g_\theta(t) \sum_{\{s : T(s) = t\}} h(s).
\]
So the likelihood based on the observed value $T(s) = t$ is given by $L(\theta \mid t) = g_\theta(t)$, and this is a positive multiple times the likelihood based on the observed $s$. Therefore, the MLE based on $s$ is the same as the MLE based on $T$.
\end{solution}

\begin{exercise}
\label{exer:6.2.23}
Suppose that $(X_1, X_2, X_3) \sim \text{Multinomial}(n, (\theta_1, \theta_2, \theta_3))$ (see Example~\ref{ex:6.1.5}), where
\[
\Omega = \{(\theta_1, \theta_2, \theta_3) : 0 < \theta_i < 1, \theta_1 + \theta_2 + \theta_3 = 1\}
\]
and we observe $(X_1, X_2, X_3) = (x_1, x_2, x_3)$.
\begin{enumerate}[(a)]
\item Determine the MLE of $(\theta_1, \theta_2, \theta_3)$.
\item What is the plug-in MLE of $\theta_1^2 + \theta_2^2 + \theta_3^2$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
  \item First, note that $\theta_3 = 1 - \theta_1 - \theta_2$, so the likelihood function is only a function of $\theta_1$ and $\theta_2$ and is given by $L(\theta_1, \theta_2 \mid x_1, x_2, x_3) = \theta_1^{x_1} \theta_2^{x_2} (1 - \theta_1 - \theta_2)^{x_3}$. The log-likelihood function is then given by $l(\theta_1, \theta_2 \mid x_1, x_2, x_3) = x_1 \ln \theta_1 + x_2 \ln \theta_2 + x_3 \ln(1 - \theta_1 - \theta_2)$. Using the methods discussed in Section \ref{ssec:6.2.1} we obtain the score function as
    \[
        S(\theta_1, \theta_2 \mid x_1, x_2, x_3) = \begin{pmatrix} \frac{x_1}{\theta_1} - \frac{x_3}{1 - \theta_1 - \theta_2} \\ \frac{x_2}{\theta_2} - \frac{x_3}{1 - \theta_1 - \theta_2} \end{pmatrix}.
    \]
    The score equation is given by
    \[
        \frac{x_1}{\theta_1} - \frac{x_3}{1 - \theta_1 - \theta_2} = 0, \quad \frac{x_2}{\theta_2} - \frac{x_3}{1 - \theta_1 - \theta_2} = 0
    \]
    so $x_1 = (x_1 + x_3)\theta_1 + x_1 \theta_2$, and $x_2 = x_2 \theta_1 + (x_2 + x_3)\theta_2$. The solution to this system of linear equations is given by
    \[
        \hat{\theta}_1 = \frac{x_1}{x_1 + x_2 + x_3} = \frac{x_1}{n}, \quad \hat{\theta}_2 = \frac{x_2}{x_1 + x_2 + x_3} = \frac{x_2}{n}.
    \]
    Also note that the matrix of second partial derivatives is given by
    \[
        \frac{\partial S(\theta_1, \theta_2 \mid x_1, x_2, x_3)}{\partial \theta} = \begin{pmatrix} -\frac{x_1}{\theta_1^2} - \frac{x_3}{(1 - \theta_1 - \theta_2)^2} & -\frac{x_3}{(1 - \theta_1 - \theta_2)^2} \\ -\frac{x_3}{(1 - \theta_1 - \theta_2)^2} & -\frac{x_2}{\theta_2^2} - \frac{x_3}{(1 - \theta_1 - \theta_2)^2} \end{pmatrix}
    \]
    and evaluated at $(\hat{\theta}_1, \hat{\theta}_2)$ this equals
    \[
        -n^2 \begin{pmatrix} \frac{1}{x_1} + \frac{1}{x_3} & \frac{1}{x_3} \\ \frac{1}{x_3} & \frac{1}{x_2} + \frac{1}{x_3} \end{pmatrix}.
    \]
    Now the negative of this matrix has $(1, 1)$ entry greater than 0 and its determinant equals
    \[
        \left( \frac{1}{x_1} + \frac{1}{x_3} \right) \left( \frac{1}{x_2} + \frac{1}{x_3} \right) - \left( \frac{1}{x_3} \right)^2 > 0
    \]
    so the matrix is positive definite. This implies that the matrix of second partial derivatives of the log-likelihood evaluated at $(\hat{\theta}_1, \hat{\theta}_2)$ is negative definite. Therefore,
    \[
        (\hat{\theta}_1, \hat{\theta}_2, \hat{\theta}_3) = \left( \frac{x_1}{n}, \frac{x_2}{n}, 1 - \frac{x_1}{n} - \frac{x_2}{n} \right) = \left( \frac{x_1}{n}, \frac{x_2}{n}, \frac{x_3}{n} \right)
    \]
    is the MLE for $(\theta_1, \theta_2, \theta_3)$.
    
    \item The plug-in MLE of $\theta_1 + \theta_2^2 - \theta_3^2$ is $x_1/n + (x_2/n)^2 - (x_3/n)^2$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:6.2.24}
If $x_1, \ldots, x_n$ is a sample from a $\text{Uniform}[\theta_1, \theta_2]$ distribution with
\[
(\theta_1, \theta_2) \in \{(\theta_1, \theta_2) \in R^2 : \theta_1 < \theta_2\}
\]
determine the MLE of $(\theta_1, \theta_2)$. (Hint: You cannot use calculus. Instead, directly determine the maximum over $\theta_1$ when $\theta_2$ is fixed, and then vary $\theta_2$.)
\end{exercise}

\begin{solution}
The likelihood function is given by
\[
    L(\theta_1, \theta_2 \mid x_1, \ldots, x_n) = \left( \frac{1}{\theta_2 - \theta_1} \right)^n \indc_{(-\infty, x_{(1)})}(\theta_1) \indc_{[x_{(n)}, \infty)}(\theta_2).
\]
Fixing $\theta_2$, we see that $L(\cdot, \theta_2 \mid x_1, \ldots, x_n)$ is largest when $\theta_2 - \theta_1$ is smallest, and this occurs when $\theta_1 = x_{(1)}$. Now $L(x_{(1)}, \cdot \mid x_1, \ldots, x_n)$ is largest when $\theta_2 - x_{(1)}$ is smallest, and this occurs when $\theta_2 = x_{(n)}$. Therefore, $L(\theta_1, \theta_2 \mid x_1, \ldots, x_n) \leqslant L(x_{(1)}, \theta_2 \mid x_1, \ldots, x_n) \leqslant L(x_{(1)}, x_{(n)} \mid x_1, \ldots, x_n)$ and $(x_{(1)}, x_{(n)})$ is the MLE.
\end{solution}

\subsection*{Computer Problems}

\begin{exercise}
\label{exer:6.2.25}
Suppose the proportion of left-handed individuals in a population is $\theta$. Based on a simple random sample of 20, you observe four left-handed individuals.
\begin{enumerate}[(a)]
\item Assuming the sample size is small relative to the population size, plot the log-likelihood function and determine the MLE.
\item If instead the population size is only 50, then plot the log-likelihood function and determine the MLE. (Hint: Remember that the number of left-handed individuals follows a hypergeometric distribution. This forces $\theta$ to be of the form $i/50$ for some integer $i$ between 4 and 34. From a tabulation of the log-likelihood, you can obtain the MLE.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Assuming that the individuals are independent (sample size small relative to the population size), the log-likelihood function is given by $4 \ln \theta + 16 \ln(1 - \theta)$. The plot of this function is provided here (note that it goes to $-\infty$ at 0 and 1). We can determine the MLE exactly in this case as $\hat{\theta} = 4/20 = 0.20$.
    
    \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig_6_2_25a.pdf}
        \caption{Log-likelihood function for Exercise 6.2.25(a)}
        %\label{fig:loglik-6.2.25a}
    \end{figure}
    
    \item The sample size is not small relative to the population size, so the number of left-handed individuals in the sample is distributed $\text{Hypergeometric}(50, 50\theta, 20)$. Note that $\theta$ is no longer a continuous variable but must take a value in $0, 1/50, 2/50, \ldots, 49/50, 1$. The log-likelihood is then given by (ignoring the denominator in the hypergeometric)
    \begin{align*}
        \ln \binom{50\theta}{4} + \ln \binom{50(1 - \theta)}{16} &= \ln \Gamma(50\theta + 1) - \ln \Gamma(50\theta - 4 + 1) - \ln \Gamma(4 + 1) \\
        &\quad + \ln \Gamma(50(1 - \theta) + 1) - \ln \Gamma(50(1 - \theta) - 16 + 1) - \ln \Gamma(16 + 1)
    \end{align*}
    for $50\theta = 4, 5, \ldots, 34$. Ignoring the $\ln \Gamma(4 + 1)$ and $\ln \Gamma(16 + 1)$ (as they do not involve $\theta$), we plot the log-likelihood below. From the tabulation required for this plot we obtain the MLE as $\hat{\theta} = 0.22$.
    
    \begin{figure}[!htbp]
        \centering
        %\includegraphics[scale=0.5]{fig_6_2_25b.pdf}
        \caption{Log-likelihood function for Exercise 6.2.25(b)}
        %\label{fig:loglik-6.2.25b}
    \end{figure}
\end{enumerate}
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:6.2.26}
If $x_1, \ldots, x_n$ is a sample from a distribution with density
\[
f_\theta(x) = \frac{1}{2} \exp\{-|x - \theta|\}
\]
for $x \in R^1$ and where $\theta \in R^1$ is unknown, then determine the MLE of $\theta$. (Hint: You cannot use calculus. Instead, maximize the log-likelihood in each of the intervals $(-\infty, x_{(1)}]$, $[x_{(1)}, x_{(2)}]$, etc.)
\end{exercise}

\begin{solution}
First we write the density as
\[
    f_\theta(x) = \begin{cases} \frac{1}{2} \exp(\theta - x) & \theta \leqslant x \\ \frac{1}{2} \exp(x - \theta) & \theta \geqslant x. \end{cases}
\]
The log-likelihood function is then given by
\[
    l(\theta \mid x_1, \ldots, x_n) = \sum_{\theta \leqslant x_{(j)}} (x_{(j)} - \theta) + \sum_{\theta \geqslant x_{(j)}} (x_{(j)} - \theta).
\]
When $\theta < x_{(1)}$, $l(\theta \mid x_1, \ldots, x_n) = n\theta - \sum_{i=1}^{n} x_{(i)}$, and this is maximized by taking $\theta = x_{(1)}$, giving the value $nx_{(1)} - \sum_{i=1}^{n} x_{(i)} \leqslant 0$.

When $\theta \geqslant x_{(n)}$, $l(\theta \mid x_1, \ldots, x_n) = \sum_{i=1}^{n} x_{(i)} - n\theta$, and this is maximized by taking $\theta = x_{(n)}$, giving the value $\sum_{i=1}^{n} x_{(i)} - nx_{(n)} \leqslant 0$.

When $x_{(i)} \leqslant \theta < x_{(i+1)}$,
\begin{align*}
    l(\theta \mid x_1, \ldots, x_n) &= \sum_{j=1}^{i} (x_{(j)} - \theta) + \sum_{j=i+1}^{n} (\theta - x_{(j)}) \\
    &= (n - 2i)\theta + \sum_{j=1}^{i} x_{(j)} - \sum_{j=i+1}^{n} x_{(j)} = (n - 2i)\theta + 2\sum_{j=1}^{i} x_{(j)} - \sum_{j=1}^{n} x_{(j)}
\end{align*}
and this is maximized (provided $n \neq 2i$) by taking $\theta = x_{(i+1)}$ when $i \leqslant n/2$ and by $\theta = x_{(i)}$ when $i > n/2$. When $n = 2i$ all values in $x_{(i)} \leqslant \theta < x_{(i+1)}$ are maximizers.

When $n = 1$, then $\hat{\theta} = x_{(1)}$. Now suppose $n > 1$. We have that $i = 1 \leqslant n/2$ and
\[
    (n - 2)x_{(2)} + 2\sum_{j=1}^{1} x_{(j)} - \sum_{j=1}^{n} x_{(j)} = nx_{(2)} - \sum_{j=1}^{n} x_{(j)} \geqslant nx_{(1)} - \sum_{j=1}^{n} x_{(j)}.
\]
Now suppose $i < i + 1 \leqslant n/2$. Then
\begin{align*}
    &(n - 2i)x_{(i+1)} + 2\sum_{j=1}^{i} x_{(j)} - \sum_{j=1}^{n} x_{(j)} \\
    &= (n - 2(i+1))x_{(i+2)} + 2\sum_{j=1}^{i+1} x_{(j)} - \sum_{j=1}^{n} x_{(j)} + (n - 2i)(x_{(i+1)} - x_{(i+2)}) \\
    &\leqslant (n - 2(i+1))x_{(i+2)} + 2\sum_{j=1}^{i+1} x_{(j)} - \sum_{j=1}^{n} x_{(j)}.
\end{align*}
If $n/2 \leqslant i < i + 1$, then
\begin{align*}
    &(n - 2i)x_{(i)} + 2\sum_{j=1}^{i} x_{(j)} - \sum_{j=1}^{n} x_{(j)} \\
    &= (n - 2(i+1))x_{(i+1)} + 2\sum_{j=1}^{i+1} x_{(j)} - \sum_{j=1}^{n} x_{(j)} + (n - 2i)(x_{(i)} - x_{(i+1)}) \\
    &\geqslant (n - 2(i+1))x_{(i+2)} + 2\sum_{j=1}^{i+1} x_{(j)} - \sum_{j=1}^{n} x_{(j)}
\end{align*}
and finally when $i = n$, then $(n - 2n)x_{(n)} + 2\sum_{j=1}^{n} x_{(j)} - \sum_{j=1}^{n} x_{(j)} = \sum_{j=1}^{n} x_{(j)} - nx_{(n)}$.

When $n$ is odd this argument shows that $l(\theta \mid x_1, \ldots, x_n)$ increases in $(-\infty, x_{(\lfloor n/2 \rfloor)})$ and decreases in $[x_{(\lfloor n/2 \rfloor)}, \infty)$, so $\hat{\theta} = x_{(\lfloor n/2 \rfloor)}$ (the middle value).

When $n$ is even this argument shows that $l(\theta \mid x_1, \ldots, x_n)$ increases in $(-\infty, x_{(n/2)})$, is constant in $[x_{(\lfloor n/2 \rfloor)}, x_{(\lfloor n/2 \rfloor + 1)})$, and decreases in $[x_{(\lfloor n/2 \rfloor)}, \infty)$, so any value $\hat{\theta} \in [x_{(\lfloor n/2 \rfloor)}, x_{(\lfloor n/2 \rfloor + 1)})$ is a maximizer.
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:6.2.27}
One approach to quantifying the uncertainty in an MLE $\hat{\theta}(s)$ is to report the MLE together with a likelihood interval $\{\theta : L(\theta \mid s) \geqslant cL(\hat{\theta}(s) \mid s)\}$ for some constant $c \in (0, 1)$. What problems do you see with this approach? In particular, how would you choose $c$?
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inferences Based on the MLE}
\label{sec:6.3}

In Table~\ref{tab:6.3.1}, we have recorded $n = 66$ measurements of the speed of light (passage time recorded as deviations from 24,800 nanoseconds between two mirrors 7400 meters apart) made by A.\,A.\ Michelson and S.\ Newcomb in 1882.

\begin{table}[!htbp]
\centering
\begin{tabular}{rrrrrrrrrrr}
28 & 26 & 33 & 24 & 34 & $-44$ & 27 & 16 & 40 & $-2$ & 29 \\
22 & 24 & 21 & 25 & 30 & 23 & 29 & 31 & 19 & 24 & 20 \\
36 & 32 & 36 & 28 & 25 & 21 & 28 & 29 & 37 & 25 & 28 \\
26 & 30 & 32 & 36 & 26 & 30 & 22 & 36 & 23 & 27 & 27 \\
28 & 27 & 31 & 27 & 26 & 33 & 26 & 32 & 32 & 24 & 39 \\
28 & 24 & 25 & 32 & 25 & 29 & 27 & 28 & 29 & 16 & 23
\end{tabular}
\caption{Speed of light measurements.}
\label{tab:6.3.1}
\end{table}

Figure~\ref{fig:6.3.1} is a boxplot of these data with the variable labeled as $x$. Notice there are two outliers at $x = -2$ and $x = -44$. We will presume there is something very special about these observations and discard them for the remainder of our discussion.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_3_1.pdf}
  \caption{Boxplot of the data values in Table~\ref{tab:6.3.1}.}
  \label{fig:6.3.1}
\end{figure}
Figure~\ref{fig:6.3.2} presents a histogram of these data minus the two data values identified as outliers. Notice that the histogram looks reasonably symmetrical, so it seems plausible to assume that these data are from an $N(\mu, \sigma^2)$ distribution for some values of $\mu$ and $\sigma^2$. Accordingly, a reasonable statistical model for these data would appear to be the location-scale normal model. In Chapter~\ref{ch:9}, we will discuss further how to assess the validity of the normality assumption.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_3_2.pdf}
  \caption{Density histogram of the data in Table~\ref{tab:6.3.1} with the outliers removed.}
  \label{fig:6.3.2}
\end{figure}

If we accept that the location-scale normal model makes sense, the question arises concerning how to make inferences about the unknown parameters $\mu$ and $\sigma^2$. The purpose of this section is to develop methods for handling problems like this. The methods developed in this section depend on special features of the MLE in a given context. In Section~\ref{sec:6.5}, we develop a more general approach based on the MLE.

\subsection{Standard Errors, Bias, and Consistency}
\label{ssec:6.3.1}

Based on the justification for the likelihood, the MLE $\hat{\theta}(s)$ seems like a natural estimate of the true value of $\theta$. Let us suppose that we will then use the plug-in MLE estimate $\hat{\psi}(s) = \psi(\hat{\theta}(s))$ for a characteristic of interest $\psi(\theta)$ (e.g., $\psi(\theta)$ might be the first quartile or the variance).

In an application, we want to know how reliable the estimate $\hat{\psi}(s)$ is. In other words, can we expect $\hat{\psi}(s)$ to be close to the true value of $\psi(\theta)$, or is there a reasonable chance that $\hat{\psi}(s)$ is far from the true value? This leads us to consider the sampling distribution of $\hat{\psi}(s)$, as this tells us how much variability there will be in $\hat{\psi}(s)$ under repeated sampling from the true distribution $f_\theta$. Because we do not know what the true value of $\theta$ is, we have to look at the sampling distribution of $\hat{\psi}(s)$ for every $\theta \in \Omega$.

To simplify this, we substitute a numerical measure of how concentrated these sampling distributions are about $\psi(\theta)$. Perhaps the most commonly used measure of the accuracy of a general estimator $T(s)$ of $\psi(\theta)$, i.e., we are not restricting ourselves to plug-in MLEs, is the mean-squared error.

\begin{definition}
\label{def:6.3.1}
The \emph{mean-squared error} (MSE) of the estimator $T$ of $\psi(\theta) \in R^1$ is given by $\text{MSE}_\theta(T) = \expc_\theta[(T - \psi(\theta))^2]$ for each $\theta \in \Omega$.
\end{definition}

Clearly, the smaller $\text{MSE}_\theta(T)$ is, the more concentrated the sampling distribution of $T(s)$ is about the value $\psi(\theta)$.

Looking at $\text{MSE}_\theta(T)$ as a function of $\theta$ gives us some idea of how reliable $T(s)$ is as an estimate of the true value of $\psi(\theta)$. Because we do not know the true value of $\theta$, and thus the true value of $\text{MSE}_\theta(T)$, statisticians record an estimate of the mean-squared error at the true value. Often
\[
\widehat{\text{MSE}}(T) = \text{MSE}_{\hat{\theta}(s)}(T)
\]
is used for this. In other words, we evaluate $\text{MSE}_\theta(T)$ at $\hat{\theta}(s)$ as a measure of the accuracy of the estimate $T(s)$.

The following result gives an important identity for the MSE.

\begin{theorem}
\label{thm:6.3.1}
If $\psi(\theta) \in R^1$ and $T$ is a real-valued function defined on $S$ such that $\expc_\theta(T)$ exists, then
\begin{equation}
\label{eq:6.3.1}
\emph{MSE}_\theta(T) = \var_\theta(T) + (\expc_\theta(T) - \psi(\theta))^2.
\end{equation}
\end{theorem}

\begin{proof}
We have
\begin{align*}
\expc_\theta[(T - \psi(\theta))^2] &= \expc_\theta[(T - \expc_\theta(T) + \expc_\theta(T) - \psi(\theta))^2] \\
&= \expc_\theta[(T - \expc_\theta(T))^2] + 2\expc_\theta[(T - \expc_\theta(T))(\expc_\theta(T) - \psi(\theta))] \\
&\quad + (\expc_\theta(T) - \psi(\theta))^2 \\
&= \var_\theta(T) + (\expc_\theta(T) - \psi(\theta))^2
\end{align*}
because
\[
\expc_\theta[(T - \expc_\theta(T))(\expc_\theta(T) - \psi(\theta))] = (\expc_\theta(T) - \psi(\theta))(\expc_\theta(T) - \expc_\theta(T)) = 0.
\]
\end{proof}

The second term in \eqref{eq:6.3.1} is the square of the \emph{bias} in the estimator $T$.

\begin{definition}
\label{def:6.3.2}
The \emph{bias} in the estimator $T$ of $\psi(\theta)$ is given by $\expc_\theta(T) - \psi(\theta)$ whenever $\expc_\theta(T)$ exists. When the bias in an estimator $T$ is 0 for every $\theta$, we call $T$ an \emph{unbiased} estimator of $\psi(\theta)$, i.e., $T$ is unbiased whenever $\expc_\theta(T) = \psi(\theta)$ for every $\theta$.
\end{definition}

Note that when the bias in an estimator is 0, then the MSE is just the variance. Unbiasedness tells us that, in a sense, the sampling distribution of the estimator is centered on the true value. For unbiased estimators,
\[
\widehat{\text{MSE}}(T) = \var_{\hat{\theta}(s)}(T)
\]
and
\[
\widehat{\text{Sd}}(T) = \sqrt{\var_{\hat{\theta}(s)}(T)}
\]
is an estimate of the standard deviation of $T$ and is referred to as the \emph{standard error} of the estimate $T(s)$. As a principle of good statistical practice, whenever we quote an estimate of a quantity, we should also provide its standard error --- at least when we have an unbiased estimator, as this tells us something about the accuracy of the estimate.

We consider some examples.

\begin{example}[Location Normal Model]
\label{ex:6.3.1}
Consider the likelihood function
\[
L(\theta \mid x_1, \ldots, x_n) = \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \theta)^2\right\}
\]
obtained in Example~\ref{ex:6.1.4} for a sample $x_1, \ldots, x_n$ from the $N(\theta, \sigma_0^2)$ model, where $\theta \in R^1$ is unknown and $\sigma_0^2 > 0$ is known. Suppose we want to estimate $\theta$. The MLE of $\theta$ was computed in Example~\ref{ex:6.2.2} to be $\bar{x}$.

In this case, we can determine the sampling distribution of the MLE exactly from the results in Section~\ref{sec:4.6}. We have that $\bar{X} \sim N(\theta, \sigma_0^2/n)$ and so $\bar{X}$ is unbiased, and
\[
\text{MSE}_\theta(\bar{X}) = \var_\theta(\bar{X}) = \frac{\sigma_0^2}{n},
\]
which is independent of $\theta$. So we do not need to estimate the MSE in this case. The standard error of the estimate is given by
\[
\widehat{\text{Sd}}(\bar{X}) = \frac{\sigma_0}{\sqrt{n}}.
\]
Note that the standard error decreases as the population variance $\sigma_0^2$ decreases and as the sample size $n$ increases.
\end{example}

\begin{example}[Bernoulli Model]
\label{ex:6.3.2}
Suppose $x_1, \ldots, x_n$ is a sample from a Bernoulli$(\theta)$ distribution where $\theta \in [0, 1]$ is unknown. Suppose we wish to estimate $\theta$. The likelihood function is given by
\[
L(\theta \mid x_1, \ldots, x_n) = \theta^{n\bar{x}} (1 - \theta)^{n(1 - \bar{x})}
\]
and the MLE of $\theta$ is $\bar{x}$ (Exercise~\ref{exer:6.2.2}), the proportion of successes in the $n$ performances. We have $\expc_\theta(\bar{X}) = \theta$ for every $\theta \in [0, 1]$, so the MLE is an unbiased estimator of $\theta$.

Therefore,
\[
\text{MSE}_\theta(\bar{X}) = \var_\theta(\bar{X}) = \frac{\theta(1 - \theta)}{n}
\]
and the estimated MSE is
\[
\widehat{\text{MSE}}(\bar{X}) = \frac{\bar{x}(1 - \bar{x})}{n}.
\]
The standard error of the estimate $\bar{x}$ is then given by
\[
\widehat{\text{Sd}}(\bar{X}) = \sqrt{\frac{\bar{x}(1 - \bar{x})}{n}}.
\]
Note how this standard error is quite different from the standard error of $\bar{x}$ in Example~\ref{ex:6.3.1}.
\end{example}

\begin{example}[Application of the Bernoulli Model]
\label{ex:6.3.3}
A polling organization is asked to estimate the proportion of households in the population in a specific district who will participate in a proposed recycling program by separating their garbage into various components. The pollsters decided to take a sample of $n = 1000$ from the population of approximately 1.5 million households (we will say more on how to choose this number later).

Each respondent will indicate either yes or no to a question concerning their participation. Given that the sample size is small relative to the population size, we can assume that we are sampling from a Bernoulli model where $\theta \in [0, 1]$ is the proportion of individuals in the population who will respond yes.

After conducting the sample, there were 790 respondents who replied yes and 210 who responded no. Therefore, the MLE of $\theta$ is
\[
\bar{x} = \frac{790}{1000} = 0.79
\]
and the standard error of the estimate is
\[
\sqrt{\frac{\bar{x}(1 - \bar{x})}{1000}} = \sqrt{\frac{0.79(1 - 0.79)}{1000}} = 0.01288.
\]
Notice that it is not entirely clear how we should interpret the value $0.01288$. Does it mean our estimate $0.79$ is highly accurate, modestly accurate, or not accurate at all? We will discuss this further in Section~\ref{ssec:6.3.2}.
\end{example}

\begin{example}[Location-Scale Normal Model]
\label{ex:6.3.4}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution where $\mu \in R^1$ and $\sigma^2 > 0$ are unknown. The parameter in this model is given by $(\mu, \sigma^2) \in R^1 \times (0, \infty)$. Suppose that we want to estimate $\mu$, i.e., just the first coordinate of the full model parameter.

In Example~\ref{ex:6.1.8}, we determined that the likelihood function is given by
\[
L((\mu, \sigma^2) \mid x_1, \ldots, x_n) = (2\pi\sigma^2)^{-n/2} \exp\left\{-\frac{n}{2\sigma^2}(\bar{x} - \mu)^2\right\} \exp\left\{-\frac{n-1}{2\sigma^2} s^2\right\}.
\]
In Example~\ref{ex:6.2.6} we showed that the MLE of $(\mu, \sigma^2)$ is
\[
\left(\bar{x}, \frac{n-1}{n} s^2\right).
\]
Furthermore, from Theorem~\ref{thm:4.6.6}, the sampling distribution of the MLE is given by
\[
\bar{X} \sim N(\mu, \sigma^2/n) \quad \text{independent of} \quad (n-1)S^2/\sigma^2 \sim \chi^2(n-1).
\]
The plug-in MLE of $\mu$ is $\bar{x}$. This estimator is unbiased and has
\[
\text{MSE}_{(\mu,\sigma^2)}(\bar{X}) = \var_{(\mu,\sigma^2)}(\bar{X}) = \frac{\sigma^2}{n}.
\]
Since $\sigma^2$ is unknown we estimate $\text{MSE}_{(\mu,\sigma^2)}(\bar{X})$ by
\[
\widehat{\text{MSE}}(\bar{X}) = \frac{\frac{n-1}{n} s^2}{n} = \frac{n-1}{n^2} s^2 = \frac{s^2}{n}.
\]
The value $s^2/n$ is commonly used instead of $\widehat{\text{MSE}}(\bar{X})$, because (Corollary~\ref{cor:4.6.2})
\[
\expc_{(\mu,\sigma^2)}(S^2) = \sigma^2,
\]
i.e., $S^2$ is an unbiased estimator of $\sigma^2$. The quantity $s/\sqrt{n}$ is referred to as the \emph{standard error} of the estimate $\bar{x}$.
\end{example}

\begin{example}[Application of the Location-Scale Normal Model]
\label{ex:6.3.5}
In Example~\ref{ex:5.5.6}, we have a sample of $n = 30$ heights (in inches) of students. We calculated $\bar{x} = 64.517$ as our estimate of the mean population height $\mu$. In addition, we obtained the estimate $s = 2.379$ of $\sigma$. Therefore, the standard error of the estimate $\bar{x} = 64.517$ is $s/\sqrt{30} = 2.379/\sqrt{30} = 0.43434$. As in Example~\ref{ex:6.3.3}, we are faced with interpreting exactly what this number means in terms of the accuracy of the estimate.
\end{example}

\subsubsection*{Consistency of Estimators}

Perhaps the most important property that any estimator $T$ of a characteristic $\psi(\theta)$ can have is that it be \emph{consistent}. Broadly speaking, this means that as we increase the amount of data we collect, then the sequence of estimates should converge to the true value of $\psi(\theta)$. To see why this is a necessary property of any estimation procedure, consider the finite population sampling context discussed in Section~\ref{ssec:5.4.1}. When the sample size is equal to the population size, then of course we have the full information and can compute exactly every characteristic of the distribution of any measurement defined on the population. So it would be an error to use an estimation procedure for a characteristic of interest that did not converge to the true value of the characteristic as we increase the sample size.

Fortunately, we have already developed the necessary mathematics in Chapter~\ref{ch:4} to define precisely what we mean by consistency.

\begin{definition}
\label{def:6.3.3}
A sequence of of estimates $T_1, T_2, \ldots$ is said to be \emph{consistent (in probability)} for $\psi(\theta)$ if $T_n \xrightarrow{P} \psi(\theta)$ as $n \to \infty$ for every $\theta \in \Omega$. A sequence of estimates $T_1, T_2, \ldots$ is said to be \emph{consistent (almost surely)} for $\psi(\theta)$ if $T_n \xrightarrow{a.s.} \psi(\theta)$ as $n \to \infty$ for every $\theta \in \Omega$.
\end{definition}

Notice that Theorem~\ref{thm:4.3.1} says that if the sequence is consistent almost surely, then it is also consistent in probability.

Consider now a sample $x_1, \ldots, x_n$ from a model $\{f_\theta : \theta \in \Omega\}$ and let $T_n = n^{-1} \sum_{i=1}^{n} x_i$ be the $n$th sample average as an estimator of $\expc_\theta(X)$, which we presume exists. The weak and strong laws of large numbers immediately give us the consistency of the sequence $T_1, T_2, \ldots$ for $\expc_\theta(X)$. We see immediately that this gives the consistency of some of the estimators discussed in this section. In fact, Theorem~\ref{thm:6.5.2} gives the consistency of the MLE in very general circumstances. Furthermore, the plug-in MLE will also be consistent under weak restrictions on $\psi$. Accordingly, we can think of maximum likelihood estimation as doing the right thing in a problem at least from the point of view of consistency.

More generally, we should always restrict our attention to statistical procedures that perform correctly as the amount of data increases. Increasing the amount of data means that we are acquiring more information and thus reducing our uncertainty so that in the limit we know everything. A statistical procedure that was inconsistent would be potentially misleading.

\subsection{Confidence Intervals}
\label{ssec:6.3.2}

While the standard error seems like a reasonable quantity for measuring the accuracy of an estimate of $\psi(\theta)$, its interpretation is not entirely clear at this point. It turns out that this is intrinsically tied up with the idea of a confidence interval.

Consider the construction of an interval
\[
C(s) = (l(s), u(s))
\]
based on the data $s$ that we believe is likely to contain the true value of $\psi(\theta)$. To do this, we have to specify the lower endpoint $l(s)$ and upper endpoint $u(s)$ for each data value $s$. How should we do this?

One approach is to specify a probability $\gamma \in [0, 1]$ and then require that random interval $C$ have the \emph{confidence property}, as specified in the following definition.

\begin{definition}
\label{def:6.3.4}
An interval $C(s) = (l(s), u(s))$ is a \emph{$\gamma$-confidence interval} for $\psi(\theta)$ if $\prb_\theta(\psi(\theta) \in C(s)) = \prb_\theta(l(s) < \psi(\theta) < u(s)) \geqslant \gamma$ for every $\theta \in \Omega$.
\end{definition}

We refer to $\gamma$ as the \emph{confidence level} of the interval.

So $C$ is a $\gamma$-confidence interval for $\psi(\theta)$ if, whenever we are sampling from $\prb_\theta$, the probability that $\psi(\theta)$ is in the interval is at least equal to $\gamma$. For a given data set, such an interval either covers $\psi(\theta)$ or it does not. So note that it is \emph{not} correct to say that a particular instance of a $\gamma$-confidence region has probability $\gamma$ of containing the true value of $\psi(\theta)$.

If we choose $\gamma$ to be a value close to 1, then we are highly confident that the true value of $\psi(\theta)$ is in $C(s)$. Of course, we can always take $C(s) = R^1$ (a very big interval!), and we are then 100\% confident that the interval contains the true value. But this tells us nothing we did not already know. So the idea is to try to make use of the information in the data to construct an interval such that we have a high confidence, say, $\gamma = 0.95$ or $\gamma = 0.99$, that it contains the true value and is not any longer than necessary. We then interpret the length of the interval as a measure of how accurately the data allow us to know the true value of $\psi(\theta)$.

\subsubsection*{$z$-Confidence Intervals}

Consider the following example, which provides one approach to the construction of confidence intervals.

\begin{example}[Location Normal Model and $z$-Confidence Intervals]
\label{ex:6.3.6}
Suppose we have a sample $x_1, \ldots, x_n$ from the $N(\theta, \sigma_0^2)$ model, where $\theta \in R^1$ is unknown and $\sigma_0^2 > 0$ is known. The likelihood function is as specified in Example~\ref{ex:6.3.1}. Suppose we want a confidence interval for $\theta$.

The reasoning that underlies the likelihood function leads naturally to the following restriction for such a region: If $\theta_1 \in C(x_1, \ldots, x_n)$ and
\[
L(\theta_2 \mid x_1, \ldots, x_n) \geqslant L(\theta_1 \mid x_1, \ldots, x_n),
\]
then we should also have $\theta_2 \in C(x_1, \ldots, x_n)$. This restriction is implied by the likelihood because the model and the data support $\theta_2$ at least as well as $\theta_1$. Thus, if we conclude that $\theta_1$ is a plausible value, so is $\theta_2$.

Therefore, $C(x_1, \ldots, x_n)$ is of the form
\[
C(x_1, \ldots, x_n) = \{\theta : L(\theta \mid x_1, \ldots, x_n) \geqslant k(x_1, \ldots, x_n)\}
\]
for some $k(x_1, \ldots, x_n)$, i.e., $C(x_1, \ldots, x_n)$ is a likelihood interval for $\theta$. Then
\begin{align*}
C(x_1, \ldots, x_n) &= \left\{\theta : \exp\left\{-\frac{n}{2\sigma_0^2}(\bar{x} - \theta)^2\right\} \geqslant k(x_1, \ldots, x_n)\right\} \\
&= \left\{\theta : -\frac{n}{2\sigma_0^2}(\bar{x} - \theta)^2 \geqslant \ln k(x_1, \ldots, x_n)\right\} \\
&= \left\{\theta : (\bar{x} - \theta)^2 \leqslant -\frac{2\sigma_0^2}{n} \ln k(x_1, \ldots, x_n)\right\} \\
&= \left(\bar{x} - k^*(x_1, \ldots, x_n) \frac{\sigma_0}{\sqrt{n}}, \bar{x} + k^*(x_1, \ldots, x_n) \frac{\sigma_0}{\sqrt{n}}\right)
\end{align*}
where $k^*(x_1, \ldots, x_n) = \sqrt{-2 \ln k(x_1, \ldots, x_n)}$.

We are now left to choose $k$ (or equivalently $k^*$), so that the interval $C$ is a $\gamma$-confidence interval for $\theta$. Perhaps the simplest choice is to try to choose $k$ so that $k^*(x_1, \ldots, x_n)$ is constant and is such that the interval as short as possible. Because
\begin{equation}
\label{eq:6.3.2}
Z = \frac{\bar{X} - \theta}{\sigma_0/\sqrt{n}} \sim N(0, 1),
\end{equation}
we have
\begin{align}
\prb_\theta(\theta \in C(x_1, \ldots, x_n)) &= \prb_\theta\left(\bar{X} - k^* \frac{\sigma_0}{\sqrt{n}} < \theta < \bar{X} + k^* \frac{\sigma_0}{\sqrt{n}}\right) \notag \\
&= \prb_\theta\left(-k^* < \frac{\bar{X} - \theta}{\sigma_0/\sqrt{n}} < c\right) = \prb_\theta\left(\left|\frac{\bar{X} - \theta}{\sigma_0/\sqrt{n}}\right| < k^*\right) \notag \\
&= 1 - 2(1 - \Phi(k^*)) \label{eq:6.3.3}
\end{align}
for every $\theta \in R^1$, where $\Phi$ is the $N(0, 1)$ cumulative distribution function. We have equality in \eqref{eq:6.3.3} whenever
\[
k^* = \Phi^{-1}\left(\frac{1 + \gamma}{2}\right)
\]
and so $k^* = z_{(1+\gamma)/2}$ where $z_\alpha$ denotes the $\alpha$th quantile of the $N(0, 1)$ distribution. This is the smallest constant $k^*$ satisfying \eqref{eq:6.3.3}.

We have shown that the likelihood interval given by
\begin{equation}
\label{eq:6.3.4}
\left(\bar{x} - z_{(1+\gamma)/2} \frac{\sigma_0}{\sqrt{n}}, \bar{x} + z_{(1+\gamma)/2} \frac{\sigma_0}{\sqrt{n}}\right)
\end{equation}
is an exact $\gamma$-confidence interval for $\theta$. As these intervals are based on the $z$-statistic, given by \eqref{eq:6.3.2}, they are called \emph{$z$-confidence intervals}. For example, if we take $\gamma = 0.95$, then $(1 + \gamma)/2 = 0.975$ and, from a statistical package (or Table D.2 in Appendix D), we obtain $z_{0.975} = 1.96$. Therefore, in repeated sampling, 95\% of the intervals of the form
\[
\left(\bar{x} - 1.96 \frac{\sigma_0}{\sqrt{n}}, \bar{x} + 1.96 \frac{\sigma_0}{\sqrt{n}}\right)
\]
will contain the true value of $\theta$.

This is illustrated in Figure~\ref{fig:6.3.3}. Here we have plotted the upper and lower endpoints of the 0.95-confidence intervals for $\theta$ for each of $N = 25$ samples of size $n = 10$ generated from an $N(0, 1)$ distribution. The theory says that when $N$ is large, approximately 95\% of these intervals will contain the true value $\theta = 0$. In the plot, coverage means that the lower endpoint (denoted by $\circ$) must be below the horizontal line at 0 and that the upper endpoint (denoted by $\bullet$) must be above this horizontal line. We see that only the fourth and twenty-third confidence intervals do not contain 0, so $23/25 = 92\%$ of the intervals contain 0. As $N \to \infty$, this proportion will converge to 0.95.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_3_3.pdf}
  \caption{Plot of 0.95-confidence intervals for $\theta = 0$ (lower endpoint $\circ$, upper endpoint $\bullet$) for $N = 25$ samples of size $n = 10$ from an $N(0, 1)$ distribution.}
  \label{fig:6.3.3}
\end{figure}

Notice that interval \eqref{eq:6.3.4} is symmetrical about $\bar{x}$. Accordingly, the half-length of this interval,
\[
z_{(1+\gamma)/2} \frac{\sigma_0}{\sqrt{n}}
\]
is a measure of the accuracy of the estimate $\bar{x}$. The half-length is often referred to as the \emph{margin of error}.

From the margin of error, we now see how to interpret the standard error; the standard error controls the lengths of the confidence intervals for the unknown $\theta$. For example, we know that with probability approximately equal to 1 (actually 0.9974), the interval $[\bar{x} - 3\sigma_0/\sqrt{n}]$ contains the true value of $\theta$.

Example~\ref{ex:6.3.6} serves as a standard example for how confidence intervals are often constructed in statistics. Basically, the idea is that we take an estimate and then look at the intervals formed by taking symmetrical intervals around the estimate via multiples of its standard error. We illustrate this via some further examples.

\begin{example}[Bernoulli Model]
\label{ex:6.3.7}
Suppose that $x_1, \ldots, x_n$ is a sample from a Bernoulli$(\theta)$ distribution where $\theta \in [0, 1]$ is unknown and we want a $\gamma$-confidence interval for $\theta$. Following Example~\ref{ex:6.3.2}, we have that the MLE is $\bar{x}$ (see Exercise~\ref{exer:6.2.2}) and the standard error of this estimate is
\[
\sqrt{\frac{\bar{x}(1 - \bar{x})}{n}}.
\]
For this model, likelihood intervals take the form
\[
C(x_1, \ldots, x_n) = \{\theta : \theta^{n\bar{x}} (1 - \theta)^{n(1 - \bar{x})} \geqslant k(x_1, \ldots, x_n)\}
\]
for some $k(x_1, \ldots, x_n)$. Again restricting to constant $k$, we see that to determine these intervals, we have to find the roots of equations of the form
\[
\theta^{n\bar{x}} (1 - \theta)^{n(1 - \bar{x})} = k(x_1, \ldots, x_n).
\]
While numerical root-finding methods can handle this quite easily, this approach is not very tractable when we want to find the appropriate value of $k(x_1, \ldots, x_n)$ to give a $\gamma$-confidence interval.

To avoid these computational complexities, it is common to use an approximate likelihood and confidence interval based on the central limit theorem. The central limit theorem (see Example~\ref{ex:4.4.9}) implies that
\[
\frac{\sqrt{n}(\bar{X} - \theta)}{\sqrt{\theta(1 - \theta)}} \xrightarrow{D} N(0, 1)
\]
as $n \to \infty$. Furthermore, a generalization of the central limit theorem (see Section~\ref{ssec:4.4.2}), shows that
\[
Z = \frac{\sqrt{n}(\bar{X} - \theta)}{\sqrt{\bar{X}(1 - \bar{X})}} \xrightarrow{D} N(0, 1).
\]
Therefore, we have
\[
\lim_{n \to \infty} \prb_\theta\left(-z_{(1+\gamma)/2} < \frac{\sqrt{n}(\bar{X} - \theta)}{\sqrt{\bar{X}(1 - \bar{X})}} < z_{(1+\gamma)/2}\right) = \lim_{n \to \infty} \prb_\theta\left(\bar{X} - z_{(1+\gamma)/2} \sqrt{\frac{\bar{X}(1 - \bar{X})}{n}} < \theta < \bar{X} + z_{(1+\gamma)/2} \sqrt{\frac{\bar{X}(1 - \bar{X})}{n}}\right)
\]
and
\begin{equation}
\label{eq:6.3.5}
\left(\bar{x} - z_{(1+\gamma)/2} \sqrt{\frac{\bar{x}(1 - \bar{x})}{n}}, \bar{x} + z_{(1+\gamma)/2} \sqrt{\frac{\bar{x}(1 - \bar{x})}{n}}\right)
\end{equation}
is an \emph{approximate} $\gamma$-confidence interval for $\theta$. Notice that this takes the same form as the interval in Example~\ref{ex:6.3.6}, except that the standard error has changed.

For example, if we want an approximate 0.95-confidence interval for $\theta$ in Example~\ref{ex:6.3.3}, then based on the observed $\bar{x} = 0.79$ we obtain
\[
\left(0.79 - 1.96\sqrt{\frac{0.79(1 - 0.79)}{1000}}\right) = [0.76475, 0.81525].
\]
The margin of error in this case equals $0.025245$, so we can conclude that we know the true proportion with reasonable accuracy based on our sample. Actually, it may be that this accuracy is not good enough or is even too good. We will discuss methods for ensuring that we achieve appropriate accuracy in Section~\ref{ssec:6.3.5}.

The $\gamma$-confidence interval derived here for $\theta$ is one of many that you will see recommended in the literature. Recall that \eqref{eq:6.3.5} is only an \emph{approximate} $\gamma$-confidence interval for $\theta$, and $n$ may need to be large for the approximation to be accurate. In other words, the true confidence level for \eqref{eq:6.3.5} will not equal $\gamma$ and could be far from that value if $n$ is too small. In particular, if the true $\theta$ is near 0 or 1, then $n$ may need to be very large. In an actual application, we usually have some idea of a small range of possible values a population proportion can take. Accordingly, it is advisable to carry out some simulation studies to assess whether or not \eqref{eq:6.3.5} is going to provide an acceptable approximation for $\theta$ in that range (see Computer Exercise~\ref{exer:6.3.21}).
\end{example}

\subsubsection*{$t$-Confidence Intervals}

Now we consider confidence intervals for $\mu$ in an $N(\mu, \sigma^2)$ model when we drop the unrealistic assumption that we know the population variance.

\begin{example}[Location-Scale Normal Model and $t$-Confidence Intervals]
\label{ex:6.3.8}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution, where $\mu \in R^1$ and $\sigma > 0$ are unknown. The parameter in this model is given by $(\mu, \sigma^2) \in R^1 \times (0, \infty)$. Suppose we want to form confidence intervals for $\mu = \psi((\mu, \sigma^2))$.

The likelihood function in this case is a function of two variables, $\mu$ and $\sigma^2$, and so the reasoning we employed in Example~\ref{ex:6.3.6} to determine the form of the confidence interval is not directly applicable. In Example~\ref{ex:6.3.4}, we developed $s/\sqrt{n}$ as the standard error of the estimate $\bar{x}$ of $\mu$. Accordingly, we restrict our attention to confidence intervals of the form
\[
C(x_1, \ldots, x_n) = \left(\bar{x} - k^* \frac{s}{\sqrt{n}}, \bar{x} + k^* \frac{s}{\sqrt{n}}\right)
\]
for some constant $k^*$.

We then have
\begin{align*}
\prb_{(\mu, \sigma^2)}\left(\bar{X} - k^* \frac{S}{\sqrt{n}} < \mu < \bar{X} + k^* \frac{S}{\sqrt{n}}\right) &= \prb_{(\mu, \sigma^2)}\left(-k^* < \frac{\bar{X} - \mu}{S/\sqrt{n}} < k^*\right) \\
&= \prb_{(\mu, \sigma^2)}\left(\left|\frac{\bar{X} - \mu}{S/\sqrt{n}}\right| < k^*\right) = 1 - 2(1 - G(k^* \mid n - 1))
\end{align*}
where $G(\cdot \mid n - 1)$ is the distribution function of
\begin{equation}
\label{eq:6.3.6}
T = \frac{\bar{X} - \mu}{S/\sqrt{n}}.
\end{equation}
Now, by Theorem~\ref{thm:4.6.6},
\[
\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)
\]
independent of $(n - 1)S^2/\sigma^2 \sim \chi^2(n - 1)$. Therefore, by Definition~\ref{def:4.6.2},
\[
T = \frac{(\bar{X} - \mu)/(\sigma/\sqrt{n})}{\sqrt{(n - 1)S^2/\sigma^2/(n - 1)}} = \frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t(n - 1).
\]
So if we take
\[
k^* = t_{(1+\gamma)/2}(n - 1),
\]
where $t_\alpha(\nu)$ is the $\alpha$th quantile of the $t(\nu)$ distribution,
\[
\left(\bar{x} - t_{(1+\gamma)/2}(n - 1) \frac{s}{\sqrt{n}}, \bar{x} + t_{(1+\gamma)/2}(n - 1) \frac{s}{\sqrt{n}}\right)
\]
is an \emph{exact} $\gamma$-confidence interval for $\mu$. The quantiles of the $t$ distributions are available from a statistical package (or Table D.4 in Appendix D). As these intervals are based on the $t$-statistic, given by \eqref{eq:6.3.6}, they are called \emph{$t$-confidence intervals}.

These confidence intervals for $\mu$ tend to be longer than those obtained in Example~\ref{ex:6.3.6}, and this reflects the greater uncertainty due to $\sigma$ being unknown. When $n = 5$, then it can be shown that $\bar{x} \pm 3s/\sqrt{n}$ is a 0.97-confidence interval. When we replace $s$ by the true value of $\sigma$, then $\bar{x} \pm 3\sigma/\sqrt{n}$ is a 0.9974-confidence interval.

As already noted, the intervals $\bar{x} \pm ks/\sqrt{n}$ are not likelihood intervals for $\mu$. So the justification for using these must be a little different from that given in Example~\ref{ex:6.3.6}. In fact, the likelihood is defined for the full parameter $(\mu, \sigma^2)$, and it is not entirely clear how to extract inferences from it when our interest is in a marginal parameter like $\mu$. There are a number of different attempts at resolving this issue. Here, however, we rely on the intuitive reasonableness of these intervals. In Chapter~\ref{ch:7}, we will see that these intervals also arise from another approach to inference, which reinforces our belief that the use of these intervals is appropriate.

In Example~\ref{ex:6.3.5}, we have a sample of $n = 30$ heights (in inches) of students. We calculated $\bar{x} = 64.517$ as our estimate of $\mu$ with standard error $s/\sqrt{30} = 0.43434$. Using software (or Table D.4), we obtain $t_{0.975}(29) = 2.0452$. So a 0.95-confidence interval for $\mu$ is given by
\[
[64.517 - 2.0452 \times 0.43434, \ldots] = [63.629, 65.405].
\]
The margin of error is $0.888$, so we are very confident that the estimate $\bar{x} = 64.517$ is within an inch of the true mean height.
\end{example}

\subsection{Testing Hypotheses and P-Values}
\label{ssec:6.3.3}

As discussed in Section~\ref{ssec:5.5.3}, another class of inference procedures is concerned with what we call hypothesis assessment. Suppose there is a theory, conjecture, or hypothesis that specifies a value for a characteristic of interest $\psi(\theta)$, say $\psi(\theta) = \psi_0$. Often this hypothesis is written $H_0 : \psi(\theta) = \psi_0$ and is referred to as the \emph{null hypothesis}.

The word \emph{null} is used because, as we will see in Chapter~\ref{ch:10}, the value specified in $H_0$ is often associated with a treatment having no effect. For example, if we want to assess whether or not a proposed new drug does a better job of treating a particular condition than a standard treatment does, the null hypothesis will often be equivalent to the new drug providing no improvement. Of course, we have to show how this can be expressed in terms of some characteristic $\psi(\theta)$ of an unknown distribution, and we will do so in Chapter~\ref{ch:10}.

The statistician is then charged with assessing whether or not the observed $s$ is in accord with this hypothesis. So we wish to assess the evidence in $s$ for $\psi(\theta) = \psi_0$ being true. A statistical procedure that does this can be referred to as a \emph{hypothesis assessment}, a \emph{test of significance}, or a \emph{test of hypothesis}. Such a procedure involves measuring how surprising the observed $s$ is when we assume $H_0$ to be true. It is clear that $s$ is surprising whenever $s$ lies in a region of low probability for each of the distributions specified by the null hypothesis, i.e., for each of the distributions in the model for which $\psi(\theta) = \psi_0$ is true. If we decide that the data are surprising under $H_0$, then this is evidence against $H_0$. This assessment is carried out by calculating a probability, called a \emph{P-value}, so that small values of the P-value indicate that $s$ is surprising.

It is important to always remember that while a P-value is a probability, this probability is a measure of surprise. Small values of the P-value indicate to us that a surprising event has occurred if the null hypothesis $H_0$ was true. A large P-value is \emph{not} evidence that the null hypothesis is true. Moreover, a P-value is \emph{not} the probability that the null hypothesis is true. The power of a hypothesis assessment method (see Section~\ref{ssec:6.3.6}) also has a bearing on how we interpret a P-value.

\subsubsection*{$z$-Tests}

We now illustrate the computation and use of P-values via several examples.

\begin{example}[Location Normal Model and the $z$-Test]
\label{ex:6.3.9}
Suppose we have a sample $x_1, \ldots, x_n$ from the $N(\mu, \sigma_0^2)$ model, where $\mu \in R^1$ is unknown and $\sigma_0^2 > 0$ is known, and we have a theory that specifies a value for the unknown mean, say, $H_0 : \mu = \mu_0$. Note that, by Corollary~\ref{cor:4.6.1}, when $H_0$ is true, the sampling distribution of the MLE is given by $\bar{X} \sim N(\mu_0, \sigma_0^2/n)$.

So one method of assessing whether or not the hypothesis $H_0$ makes sense is to compare the observed value $\bar{x}$ with this distribution. If $\bar{x}$ is in a region of low probability for the $N(\mu_0, \sigma_0^2/n)$ distribution, then this is evidence that $H_0$ is false. Because the density of the $N(\mu_0, \sigma_0^2/n)$ distribution is unimodal, the regions of low probability for this distribution occur in its tails. The farther out in the tails $\bar{x}$ lies, the more surprising this will be when $H_0$ is true, and thus the more evidence we will have against $H_0$.

In Figure~\ref{fig:6.3.4}, we have plotted a density of the MLE together with an observed value $\bar{x}$ that lies far in the right tail of the distribution. This would clearly be a surprising value from this distribution.

So we want to measure how far out in the tails of the $N(\mu_0, \sigma_0^2/n)$ distribution the value $\bar{x}$ is. We can do this by computing the probability of observing a value of $\bar{x}$ as far, or farther, away from the center of the distribution under $H_0$ as $\bar{x}$. The center of this distribution is given by $\mu_0$. Because
\begin{equation}
\label{eq:6.3.7}
Z = \frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} \sim N(0, 1)
\end{equation}
under $H_0$, the P-value is then given by
\[
\prb_{\mu_0}\left(|\bar{X} - \mu_0| \geqslant |\bar{x} - \mu_0|\right) = \prb_{\mu_0}\left(\left|\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}}\right| \geqslant \left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right) = 2\left(1 - \Phi\left(\left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right)\right),
\]
where $\Phi$ denotes the $N(0, 1)$ distribution function. If the P-value is small, then we have evidence that $\bar{x}$ is a surprising value because this tells us that $\bar{x}$ is out in a tail of the $N(\mu_0, \sigma_0^2/n)$ distribution. Because this P-value is based on the statistic $Z$ defined in \eqref{eq:6.3.7}, this is referred to as the \emph{$z$-test} procedure.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_3_4.pdf}
  \caption{Plot of the density of the MLE in Example~\ref{ex:6.3.9} when $\mu_0 = 3$, $\sigma_0^2 = 1$, and $n = 10$ together with the observed value $\bar{x} = 4.2$ ($\bullet$).}
  \label{fig:6.3.4}
\end{figure}

\begin{example}[Application of the $z$-Test]
\label{ex:6.3.10}
We generated the following sample of $n = 10$ from an $N(26, 4)$ distribution.
\begin{center}
\begin{tabular}{ccccc}
29.0651 & 27.3980 & 23.4346 & 26.3665 & 23.4994 \\
28.6592 & 25.5546 & 29.4477 & 28.0979 & 25.2850
\end{tabular}
\end{center}
Even though we know the true value of $\mu$, let us suppose we do not and test the hypothesis $H_0 : \mu = 25$. To assess this, we compute (using a statistical package to evaluate $\Phi$) the P-value
\[
2\left(1 - \Phi\left(\left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right)\right) = 2\left(1 - \Phi\left(\left|\frac{26.6808 - 25}{2/\sqrt{10}}\right|\right)\right) = 2(1 - \Phi(2.6576)) = 0.0078,
\]
which is quite small. For example, if the hypothesis $H_0$ is correct, then, in repeated sampling, we would see data giving a value of $\bar{x}$ at least as surprising as what we have observed only $0.78\%$ of the time. So we conclude that we have evidence against $H_0$ being true, which, of course, is appropriate in this case.

If you do not use a statistical package for the evaluation of $\Phi(2.6576)$, then you will have to use Table D.2 of Appendix D to get an approximation. For example, rounding $2.6576$ to $2.66$, Table D.2 gives $\Phi(2.66) = 0.9961$ and the approximate P-value is $2(1 - 0.9961) = 0.0078$. In this case, the approximation is exact to four decimal places.
\end{example}

\begin{example}[Bernoulli Model]
\label{ex:6.3.11}
Suppose that $x_1, \ldots, x_n$ is a sample from a Bernoulli$(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown, and we want to test $H_0 : \theta = \theta_0$. As in Example~\ref{ex:6.3.7}, when $H_0$ is true, we have
\[
Z = \frac{\sqrt{n}(\bar{X} - \theta_0)}{\sqrt{\theta_0(1 - \theta_0)}} \xrightarrow{D} N(0, 1)
\]
as $n \to \infty$. So we can test this hypothesis by computing the approximate P-value
\[
\prb_\theta\left(|Z| \geqslant \left|\frac{\sqrt{n}(\bar{x} - \theta_0)}{\sqrt{\theta_0(1 - \theta_0)}}\right|\right) = 2\left(1 - \Phi\left(\left|\frac{\sqrt{n}(\bar{x} - \theta_0)}{\sqrt{\theta_0(1 - \theta_0)}}\right|\right)\right)
\]
when $n$ is large.

As a specific example, suppose that a psychic claims the ability to predict the value of a randomly tossed fair coin. To test this, a coin was tossed 100 times and the psychic's guesses were recorded as successes or failures. A total of 54 successes were observed.

If the psychic has no predictive ability, then we would expect the successes to occur randomly, just as heads occur when we toss the coin. Therefore, we want to test the null hypothesis that the probability of a success occurring is equal to $\theta_0 = 1/2$. This is equivalent to saying that the psychic has no predictive ability. The MLE is 0.54 and the approximate P-value is given by
\[
2\left(1 - \Phi\left(\left|\frac{\sqrt{100}(0.54 - 0.5)}{\sqrt{0.5(1 - 0.5)}}\right|\right)\right) = 2(1 - \Phi(0.8)) = 2(1 - 0.7881) = 0.4238,
\]
and we would appear to have no evidence that $H_0$ is false, i.e., no reason to doubt that the psychic has no predictive ability.

Often cutoff values like 0.05 or 0.01 are used to determine whether the results of a test are significant or not. For example, if the P-value is less than 0.05, then the results are said to be \emph{statistically significant at the 5\% level}. There is nothing sacrosanct about the 0.05 level, however, and different values can be used depending on the application. For example, if the result of concluding that we have evidence against $H_0$ is that something very expensive or important will take place, then naturally we might demand that the cutoff value be much smaller than 0.05.
\end{example}

\subsubsection*{When Is Statistical Significance Practically Significant?}

It is also important to point out here the difference between \emph{statistical significance} and \emph{practical significance}. Consider the situation in Example~\ref{ex:6.3.9}, when the true value of $\mu$ is $\mu_1 \neq \mu_0$ but $\mu_1$ is so close to $\mu_0$ that, practically speaking, they are indistinguishable. By the strong law of large numbers, we have that $\bar{X} \xrightarrow{a.s.} \mu_1$ as $n \to \infty$ and therefore
\[
\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} \xrightarrow{a.s.} \infty.
\]
This implies that
\[
2\left(1 - \Phi\left(\left|\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right)\right) \xrightarrow{a.s.} 0.
\]
We conclude that, if we take a large enough sample size $n$, we will inevitably conclude that $\mu \neq \mu_0$ because the P-value of the $z$-test goes to 0. Of course, this is correct because the hypothesis is false.

In spite of this, we do not want to conclude that just because we have statistical significance, the difference between the true value and $\mu_0$ is of any practical importance. If we examine the observed absolute difference $|\bar{x} - \mu_0|$ as an estimate of $|\mu - \mu_0|$, however, we will not make this mistake. If this absolute difference is smaller than some threshold $\Delta$ that we consider represents a practically significant difference, then even if the P-value leads us to conclude that $\mu \neq \mu_0$, we might conclude that no difference of any importance exists. Of course, the value of $\Delta$ is application dependent. For example, in coin tossing, where we are testing $\theta = 1/2$, we might not care if the coin is slightly unfair, say, $\Delta_0 = 0.01$. In testing the abilities of a psychic, as in Example~\ref{ex:6.3.11}, however, we might take $\Delta$ much lower, as any evidence of psychic powers would be an astounding finding. The issue of practical significance is something we should always be aware of when conducting a test of significance.

\subsubsection*{Hypothesis Assessment via Confidence Intervals}

Another approach to testing hypotheses is via confidence intervals. For example, if we have a $\gamma$-confidence interval $C(s)$ for $\psi(\theta)$ and $\psi_0 \notin C(s)$, then this seems like clear evidence against $H_0 : \psi(\theta) = \psi_0$, at least when $\gamma$ is close to 1. It turns out that in many problems, the approach to testing via confidence intervals is equivalent to using P-values with a specific cutoff for the P-value to determine statistical significance. We illustrate this equivalence using the $z$-test and $z$-confidence intervals.

\begin{example}[An Equivalence Between $z$-Tests and $z$-Confidence Intervals]
\label{ex:6.3.12}
We develop this equivalence by showing that obtaining a P-value less than $1 - \gamma$ for $H_0 : \mu = \mu_0$ is equivalent to $\mu_0$ not being in a $\gamma$-confidence interval for $\mu$. Observe that
\[
1 - \gamma > 2\left(1 - \Phi\left(\left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right)\right)
\]
if and only if
\[
\Phi\left(\left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right) > \frac{1 + \gamma}{2}.
\]
This is true if and only if
\[
\left|\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right| > z_{(1+\gamma)/2},
\]
which holds if and only if
\[
\mu_0 \notin \left(\bar{x} - z_{(1+\gamma)/2} \frac{\sigma_0}{\sqrt{n}}, \bar{x} + z_{(1+\gamma)/2} \frac{\sigma_0}{\sqrt{n}}\right).
\]
This implies that the $\gamma$-confidence interval for $\mu$ comprises those values $\mu_0$ for which the P-value for the hypothesis $H_0 : \mu = \mu_0$ is greater than $1 - \gamma$.

Therefore, the P-value, based on the $z$-statistic, for the null hypothesis $H_0 : \mu = \mu_0$, will be smaller than $1 - \gamma$ if and only if $\mu_0$ is not in the $\gamma$-confidence interval for $\mu$ derived in Example~\ref{ex:6.3.6}. For example, if we decide that for any P-values less than $1 - \gamma = 0.05$ we will declare the results statistically significant, then we know the results will be significant whenever the 0.95-confidence interval for $\mu$ does not contain $\mu_0$. For the data of Example~\ref{ex:6.3.10}, a 0.95-confidence interval is given by $[25.441, 27.920]$. As this interval does not contain $\mu_0 = 25$, we have evidence against the null hypothesis at the 0.05 level.

We can apply the same reasoning for tests about $\theta$ when we are sampling from a Bernoulli$(\theta)$ model. For the data in Example~\ref{ex:6.3.11}, we obtain the 0.95-confidence interval
\[
\left(\bar{x} - z_{0.975} \sqrt{\frac{\bar{x}(1 - \bar{x})}{n}}\right) = \left(0.54 - 1.96\sqrt{\frac{0.54(1 - 0.54)}{100}}\right) = [0.44231, 0.63769],
\]
which includes the value $\theta_0 = 0.5$. So we have no evidence against the null hypothesis of no predictive ability for the psychic at the 0.05 level.
\end{example}

\subsubsection*{$t$-Tests}

We now consider an example pertaining to the important location-scale normal model.

\begin{example}[Location-Scale Normal Model and $t$-Tests]
\label{ex:6.3.13}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution, where $\mu \in R^1$ and $\sigma > 0$ are unknown, and suppose we want to test the null hypothesis $H_0 : \mu = \mu_0$. In Example~\ref{ex:6.3.8}, we obtained a $\gamma$-confidence interval for $\mu$. This was based on the $t$-statistic given by \eqref{eq:6.3.6}. So we base our test on this statistic also. In fact, it can be shown that the test we derive here is equivalent to using the confidence intervals to assess the hypothesis as described in Example~\ref{ex:6.3.12}.

As in Example~\ref{ex:6.3.8}, we can prove that when the null hypothesis is true, then
\begin{equation}
\label{eq:6.3.8}
T = \frac{\bar{X} - \mu_0}{S/\sqrt{n}}
\end{equation}
is distributed $t(n - 1)$. The $t$ distributions are unimodal, with the mode at 0, and the regions of low probability are given by the tails. So we test, or assess, this hypothesis by computing the probability of observing a value as far or farther away from 0 as \eqref{eq:6.3.8}. Therefore, the P-value is given by
\[
\prb_{\mu_0}\left(2|T| \geqslant \left|\frac{\bar{x} - \mu_0}{s/\sqrt{n}}\right|\right) = 2\left(1 - G\left(\left|\frac{\bar{x} - \mu_0}{s/\sqrt{n}}\right| \mid n - 1\right)\right),
\]
where $G(\cdot \mid n - 1)$ is the distribution function of the $t(n - 1)$ distribution. We then have evidence against $H_0$ whenever this probability is small. This procedure is called the \emph{$t$-test}. Again, it is a good idea to look at the difference $|\bar{x} - \mu_0|$, when we conclude that $H_0$ is false, to determine whether or not the detected difference is of practical importance.

Consider now the data in Example~\ref{ex:6.3.10} and let us pretend that we do not know $\mu$ or $\sigma^2$. Then we have $\bar{x} = 26.6808$ and $s = \sqrt{4.8620} = 2.2050$, so to test $H_0 : \mu = 25$ the value of the $t$-statistic is
\[
t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}} = \frac{26.6808 - 25}{2.2050/\sqrt{10}} = 2.4105.
\]
From a statistics package (or Table D.4) we obtain $t_{0.975}(9) = 2.2622$, so we have a statistically significant result at the 5\% level and conclude that we have evidence against $H_0 : \mu = 25$. Using a statistical package, we can determine the precise value of the P-value to be 0.039 in this case.
\end{example}

\subsubsection*{One-Sided Tests}

All the tests we have discussed so far in this section for a characteristic of interest $\psi(\theta)$ have been \emph{two-sided} tests. This means that the null hypothesis specified the value of $\psi(\theta)$ to be a single value $\psi_0$. Sometimes, however, we want to test a null hypothesis of the form $H_0 : \psi(\theta) \leqslant \psi_0$ or $H_0 : \psi(\theta) \geqslant \psi_0$. To carry out such tests, we use the same test statistics as we have developed in the various examples here but compute the P-value in a way that reflects the one-sided nature of the null. These are known as \emph{one-sided tests}. We illustrate a one-sided test using the location normal model.

\begin{example}[One-Sided Tests]
\label{ex:6.3.14}
Suppose we have a sample $x_1, \ldots, x_n$ from the $N(\mu, \sigma_0^2)$ model, where $\mu \in R^1$ is unknown and $\sigma_0^2 > 0$ is known. Suppose further that it is hypothesized that $H_0 : \mu \leqslant \mu_0$ is true, and we wish to assess this after observing the data.

We will base our test on the $z$-statistic
\[
Z = \frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} = \frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} + \frac{\mu - \mu_0}{\sigma_0/\sqrt{n}}.
\]
So $Z$ is the sum of a random variable having an $N(0, 1)$ distribution and the constant $\sqrt{n}(\mu - \mu_0)/\sigma_0$, which implies that
\[
Z \sim N\left(\frac{\mu - \mu_0}{\sigma_0/\sqrt{n}}, 1\right).
\]
Note that
\[
\frac{\mu - \mu_0}{\sigma_0/\sqrt{n}} \leqslant 0
\]
if and only if $H_0$ is true.

This implies that, when the null hypothesis is false, we will tend to see values of $Z$ in the right tail of the $N(0, 1)$ distribution; when the null hypothesis is true, we will tend to see values of $Z$ that are reasonable for the $N(0, 1)$ distribution, or in the left tail of this distribution. Accordingly, to test $H_0$ we compute the P-value
\[
\prb\left(Z \geqslant \frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right) = 1 - \Phi\left(\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right)
\]
with $Z \sim N(0, 1)$ and conclude that we have evidence against $H_0$ when this is small.

Using the same reasoning, the P-value for the null hypothesis $H_0 : \mu \geqslant \mu_0$ equals
\[
\prb\left(Z \leqslant \frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right) = \Phi\left(\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}}\right).
\]
For more discussion of one-sided tests and confidence intervals, see Problems~\ref{exer:6.3.25} through~\ref{exer:6.3.32}.
\end{example}

\subsection{Inferences for the Variance}
\label{ssec:6.3.4}

In Sections~\ref{ssec:6.3.1}, \ref{ssec:6.3.2}, and~\ref{ssec:6.3.3}, we focused on inferences for the unknown mean of a distribution, e.g., when we are sampling from an $N(\mu, \sigma^2)$ distribution or a Bernoulli$(\theta)$ distribution and our interest is in $\mu$ or $\theta$, respectively. In general, location parameters tend to play a much more important role in a statistical analysis than other characteristics of a distribution. There are logical reasons for this, discussed in Chapter~\ref{ch:10}, when we consider regression models. Sometimes we refer to a parameter such as $\sigma^2$ as a \emph{nuisance parameter} because our interest is in $\mu$. Note that the variance of a Bernoulli$(\theta)$ distribution is $\theta(1 - \theta)$, so that inferences about $\theta$ are logically inferences about the variance too, i.e., there are no nuisance parameters.

But sometimes we are primarily interested in making inferences about $\sigma^2$ in the $N(\mu, \sigma^2)$ distribution when it is unknown. For example, suppose that previous experience with a system under study indicates that the true value of the variance is well-approximated by $\sigma_0^2$, i.e., the true value does not differ from $\sigma_0^2$ by an amount having any practical significance. Now based on the new sample, we may want to assess the hypothesis $H_0 : \sigma^2 = \sigma_0^2$, i.e., we wonder whether or not the basic variability in the process has changed.

The discussion in Section~\ref{ssec:6.3.1} led to consideration of the standard error $s/\sqrt{n}$ as an estimate of the standard deviation $\sigma/\sqrt{n}$ of $\bar{x}$. In many ways $s^2$ seems like a very natural estimator of $\sigma^2$, even when we aren't sampling from a normal distribution.

The following example develops confidence intervals and P-values for $\sigma^2$.

\begin{example}[Location-Scale Normal Model and Inferences for the Variance]
\label{ex:6.3.15}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma^2)$ distribution, where $\mu \in R^1$ and $\sigma > 0$ are unknown, and we want to make inferences about the population variance $\sigma^2$. The plug-in MLE is given by $(n - 1)s^2/n$, which is the average of the squared deviations of the data values from $\bar{x}$. Often $s^2$ is recommended as the estimate because it has the unbiasedness property, and we will use this here. An expression can be determined for the standard error of this estimate, but, as it is somewhat complicated, we will not pursue this further here.

We can form a $\gamma$-confidence interval for $\sigma^2$ using $(n - 1)S^2/\sigma^2 \sim \chi^2(n - 1)$ (Theorem~\ref{thm:4.6.6}). There are a number of possibilities for this interval, but one is to note that, letting $\chi^2_\alpha(\nu)$ denote the $\alpha$th quantile for the $\chi^2(\nu)$ distribution, then
\begin{align*}
\gamma &= \prb_{(\mu, \sigma^2)}\left(\chi^2_{(1-\gamma)/2}(n - 1) \leqslant \frac{(n - 1)S^2}{\sigma^2} \leqslant \chi^2_{(1+\gamma)/2}(n - 1)\right) \\
&= \prb_{(\mu, \sigma^2)}\left(\frac{(n - 1)S^2}{\chi^2_{(1+\gamma)/2}(n - 1)} \leqslant \sigma^2 \leqslant \frac{(n - 1)S^2}{\chi^2_{(1-\gamma)/2}(n - 1)}\right)
\end{align*}
for every $(\mu, \sigma^2) \in R^1 \times (0, \infty)$. So
\[
\left(\frac{(n - 1)s^2}{\chi^2_{(1+\gamma)/2}(n - 1)}, \frac{(n - 1)s^2}{\chi^2_{(1-\gamma)/2}(n - 1)}\right)
\]
is an exact $\gamma$-confidence interval for $\sigma^2$. To test a hypothesis such as $H_0 : \sigma = \sigma_0$ at the $1 - \gamma$ level, we need only see whether or not $\sigma_0^2$ is in the interval. The smallest value of $\gamma$ such that $\sigma_0^2$ is in the interval is the P-value for this hypothesis assessment procedure.

For the data in Example~\ref{ex:6.3.10}, let us pretend that we do not know that $\sigma^2 = 4$. Here, $n = 10$ and $s^2 = 4.8620$. From a statistics package (or Table D.3 in Appendix D) we obtain $\chi^2_{0.025}(9) = 2.700$, $\chi^2_{0.975}(9) = 19.023$. So a 0.95-confidence interval for $\sigma^2$ is given by
\[
\left(\frac{(n - 1)s^2}{\chi^2_{(1+\gamma)/2}(n - 1)}, \frac{(n - 1)s^2}{\chi^2_{(1-\gamma)/2}(n - 1)}\right) = \left(\frac{9 \times 4.8620}{19.023}, \frac{9 \times 4.8620}{2.700}\right) = [2.300\ldots, 16.207].
\]
The length of the interval indicates that there is a reasonable degree of uncertainty concerning the true value of $\sigma^2$. We see, however, that a test of $H_0 : \sigma^2 = 4$ would not reject this hypothesis at the 5\% level because the value 4 is in the 0.95-confidence interval.
\end{example}

\subsection{Sample-Size Calculations: Confidence Intervals}
\label{ssec:6.3.5}

Quite often a statistician is asked to determine the sample size $n$ to ensure that with a very high probability the results of a statistical analysis will yield definitive results. For example, suppose we are going to take a sample of size $n$ from a population and want to estimate the population mean so that the estimate is within 0.5 of the true mean with probability at least 0.95. This means that we want the half-length, or margin of error, of the 0.95-confidence interval for the mean to be guaranteed to be less than 0.5.

We consider such problems in the following examples. Note that in general, sample-size calculations are the domain of experimental design, which we will discuss more extensively in Chapter~\ref{ch:10}.

First, we consider the problem of selecting the sample size to ensure that a confidence interval is shorter than some prescribed value.

\begin{example}[The Length of a Confidence Interval for a Mean]
\label{ex:6.3.16}
Suppose we are in the situation described in Example~\ref{ex:6.3.6}, in which we have a sample $x_1, \ldots, x_n$ from the $N(\mu, \sigma_0^2)$ model, with $\mu \in R^1$ unknown and $\sigma_0^2 > 0$ known. Further suppose that the statistician is asked to determine $n$ so that the margin of error for a $\gamma$-confidence interval for the population mean is no greater than a prescribed value $\delta > 0$. This entails that $n$ be chosen so that
\[
z_{(1+\gamma)/2} \frac{\sigma_0}{\sqrt{n}} \leqslant \delta
\]
or, equivalently, so that
\[
n \geqslant \frac{\sigma_0^2 z_{(1+\gamma)/2}^2}{\delta^2}.
\]
For example, if $\sigma_0^2 = 10$, $\gamma = 0.95$, and $\delta = 0.5$, then the smallest possible value for $n$ is 154.

Now consider the situation described in Example~\ref{ex:6.3.8}, in which we have a sample $x_1, \ldots, x_n$ from the $N(\mu, \sigma^2)$ model with $\mu \in R^1$ and $\sigma^2 > 0$ both unknown. In this case, we want $n$ so that
\[
t_{(1+\gamma)/2}(n - 1) \frac{s}{\sqrt{n}} \leqslant \delta,
\]
which entails
\[
n \geqslant \frac{s^2 (t_{(1+\gamma)/2}(n - 1))^2}{\delta^2}.
\]
But note this also depends on the unobserved value of $s$, so we cannot determine an appropriate value of $n$.

Often, however, we can determine an upper bound on the population standard deviation, say, $\sigma \leqslant b$. For example, suppose we are measuring human heights in centimeters. Then we have a pretty good idea of upper and lower bounds on the possible heights we will actually obtain. Therefore, with the normality assumption, the interval given by the population mean, plus or minus three standard deviations, must be contained within the interval given by the upper and lower bounds. So dividing the length of this interval by 6 gives a plausible upper bound $b$ for the value of $\sigma$. In any case, when we have such an upper bound, we can expect that $s \leqslant b$ at least if we choose $b$ conservatively. Therefore, we take $n$ to satisfy
\[
n \geqslant \frac{b^2 (t_{(1+\gamma)/2}(n - 1))^2}{\delta^2}.
\]
Note that we need to evaluate $t_{(1+\gamma)/2}(n - 1)$ for each $n$ as well. It is wise to be fairly conservative in our choice of $n$ in this case, i.e., do not choose the smallest possible value.
\end{example}

\begin{example}[The Length of a Confidence Interval for a Proportion]
\label{ex:6.3.17}
Suppose we are in the situation described in Example~\ref{ex:6.3.2}, in which we have a sample $x_1, \ldots, x_n$ from the Bernoulli$(\theta)$ model and $\theta \in [0, 1]$ is unknown. The statistician is required to specify the sample size $n$ so that the margin of error of a $\gamma$-confidence interval for $\theta$ is no greater than a prescribed value $\delta$. So, from Example~\ref{ex:6.3.7}, we want $n$ to satisfy
\begin{equation}
\label{eq:6.3.9}
z_{(1+\gamma)/2} \sqrt{\frac{\bar{x}(1 - \bar{x})}{n}} \leqslant \delta
\end{equation}
and this entails
\[
n \geqslant \frac{\bar{x}(1 - \bar{x}) z_{(1+\gamma)/2}^2}{\delta^2}.
\]
Because this also depends on the unobserved $\bar{x}$, we cannot determine $n$. Note, however, that $0 \leqslant \bar{x}(1 - \bar{x}) \leqslant 1/4$ for every $\bar{x}$ (plot this function) and that this upper bound is achieved when $\bar{x} = 1/2$. Therefore, if we determine $n$ so that
\[
n \geqslant \frac{1}{4} \frac{z_{(1+\gamma)/2}^2}{\delta^2},
\]
then we know that \eqref{eq:6.3.9} is satisfied. For example, if $\gamma = 0.95$, $\delta = 0.1$, the smallest possible value of $n$ is 97; if $\gamma = 0.95$, $\delta = 0.01$, the smallest possible value of $n$ is 9604.
\end{example}

\subsection{Sample-Size Calculations: Power}
\label{ssec:6.3.6}

Suppose the purpose of a study is to assess a specific hypothesis $H_0 : \psi(\theta) = \psi_0$ and it is has been decided that the results will be declared statistically significant whenever the P-value is less than $\alpha$. Suppose that the statistician is asked to choose $n$ so that the P-value obtained is smaller than $\alpha$ with probability at least $\beta_0$ at some specific $\psi_1 = \psi(\theta_1)$ such that $\psi_1 \neq \psi_0$. The probability that the P-value is less than $\alpha$ for a specific value of $\theta$ is called the \emph{power} of the test at $\theta$. We will denote this by $\beta(\theta)$ and call $\beta$ the \emph{power function} of the test. The notation is not really complete, as it suppresses the dependence of $\beta$ on $\psi_0$, $\alpha$, $n$, and the test procedure, but we will assume that these are clear in a particular context. The problem the statistician is presented with can then be stated as: Find $n$ so that $\beta(\theta_1) \geqslant \beta_0$.

The power function of a test is a measure of the sensitivity of the test to detect departures from the null hypothesis. We choose $\alpha$ small ($\alpha = 0.05$, $0.01$, etc.) so that we do not erroneously declare that we have evidence against the null hypothesis when the null hypothesis is in fact true. When $\psi(\theta) \neq \psi_0$, then $\beta(\theta)$ is the probability that the test does the right thing and detects that $H_0$ is false.

For any test procedure, it is a good idea to examine its power function, perhaps for several choices of $\alpha$, to see how good the test is at detecting departures. For it can happen that we do not find any evidence against a null hypothesis when it is false because the sample size is too small. In such a case, the power will be small at values $\psi(\theta)$ that represent practically significant departures from $H_0$. To avoid this problem, we should always choose a value $\psi_1$ that represents a practically significant departure from $\psi_0$ and then determine $n$ so that we reject $H_0$ with high probability when $\psi(\theta) = \psi_1$.

We consider the computation and use of the power function in several examples.

\begin{example}[The Power Function in the Location Normal Model]
\label{ex:6.3.18}
For the two-sided $z$-test in Example~\ref{ex:6.3.9}, we have
\begin{align}
\beta(\mu) &= \prb_\mu\left(2\left(1 - \Phi\left(\left|\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}}\right|\right)\right) \leqslant \alpha\right) = \prb_\mu\left(\left|\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}}\right| \geqslant z_{1-\alpha/2}\right) \notag\\
&= \prb_\mu\left(\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} \leqslant -z_{1-\alpha/2}\right) + \prb_\mu\left(\frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} \geqslant z_{1-\alpha/2}\right) \notag \\
&= \prb_\mu\left(\frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} \leqslant \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - z_{1-\alpha/2}\right) + \prb_\mu\left(\frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} \geqslant \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha/2}\right) \notag\\
&= \Phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - z_{1-\alpha/2}\right) + 1 - \Phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha/2}\right). \label{eq:6.3.10}
\end{align}
Notice that
\[
\beta(\mu_0 - \delta) = \beta(\mu_0 + \delta)
\]
so $\beta$ is symmetric about $\mu_0$ (put $\mu = \mu_0 - \delta$ and $\mu = \mu_0 + \delta$ in the expression for $\beta$ and we get the same value).

Differentiating \eqref{eq:6.3.10} with respect to $n$, we obtain
\begin{equation}
\label{eq:6.3.11}
\phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - z_{1-\alpha/2}\right) + \phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha/2}\right) \cdot \frac{\mu - \mu_0}{2\sigma_0\sqrt{n}} \geqslant 0
\end{equation}
where $\phi$ is the density of the $N(0, 1)$ distribution. We can establish that \eqref{eq:6.3.11} is always nonnegative (see Challenge~\ref{exer:6.3.34}). This implies that $\beta(\mu)$ is increasing in $n$, so we need only solve $\beta(\mu_1) = \beta_0$ for $n$ (the solution may not be an integer) to determine a suitable sample size (all larger values of $n$ will give a larger power).

For example, when $\sigma_0 = 1$, $\alpha = 0.05$, $\mu_0 = 0$, $\beta_0 = 0.99$, and $\mu_1 - \mu_0 = 0.1$, we must find $n$ satisfying
\begin{equation}
\label{eq:6.3.12}
\Phi(-\sqrt{n} \cdot 0.1 - 1.96) + 1 - \Phi(-\sqrt{n} \cdot 0.1 + 1.96) = 0.99.
\end{equation}
(Note that the symmetry of $\beta$ about $\mu_0$ means we will get the same answer if we use $\mu_0 - 0.1$ here instead of $\mu_0 + 0.1$.) Tabulating \eqref{eq:6.3.12} as a function of $n$ using a statistical package determines that $n = 785$ is the smallest value achieving the required bound.

Also observe that the derivative of \eqref{eq:6.3.10} with respect to $\mu$ is given by
\begin{equation}
\label{eq:6.3.13}
\left[-\phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - z_{1-\alpha/2}\right) + \phi\left(\frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha/2}\right)\right] \cdot \frac{\sqrt{n}}{\sigma_0}.
\end{equation}
This is positive when $\mu > \mu_0$, negative when $\mu < \mu_0$, and takes the value 0 when $\mu = \mu_0$ (see Challenge~\ref{exer:6.3.35}). From \eqref{eq:6.3.10} we have that $\beta(\mu) \to 1$ as $|\mu - \mu_0| \to \infty$. These facts establish that $\beta$ takes its minimum value at $\mu_0$ and that it is increasing as we move away from $\mu_0$. Therefore, once we have determined $n$ so that the power is at least $\beta_0$ at some $\mu_1$, we know that the power is at least $\beta_0$ for all values of $\mu$ satisfying $|\mu - \mu_0| \geqslant |\mu_1 - \mu_0|$.

As an example of this, consider Figure~\ref{fig:6.3.5}, where we have plotted the power function when $n = 10$, $\mu_0 = 0$, $\sigma_0 = 1$, and $\alpha = 0.05$, so that
\[
\beta(\mu) = \Phi(-\sqrt{10}\mu - 1.96) + 1 - \Phi(-\sqrt{10}\mu + 1.96).
\]
Notice the symmetry about $\mu_0 = 0$ and the fact that $\beta(\mu)$ increases as $\mu$ moves away from 0. We obtain $\beta(1.2) = 0.967$, so that when $\mu = 1.2$ the probability that the P-value for testing $H_0 : \mu = 0$ will be less than 0.05 is 0.967. Of course, as we increase $n$ this graph will rise even more steeply to 1 as we move away from 0.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_3_5.pdf}
  \caption{Plot of the power function for Example~\ref{ex:6.3.18} when $\alpha = 0.05$, $\mu_0 = 0$, and $\sigma_0 = 1$ is assumed known.}
  \label{fig:6.3.5}
\end{figure}

Many statistical packages contain the power function as a built-in function for various tests. This is very convenient for examining the sensitivity of the test and determining sample sizes.

\begin{example}[The Power Function for $\theta$ in the Bernoulli Model]
\label{ex:6.3.19}
For the two-sided test in Example~\ref{ex:6.3.11}, we have that the power function is given by
\[
\beta(\theta) = \prb_\theta\left(2\left(1 - \Phi\left(\left|\frac{\sqrt{n}(\bar{X} - \theta_0)}{\sqrt{\theta_0(1 - \theta_0)}}\right|\right)\right) \leqslant \alpha\right).
\]
Under the assumption that we choose $n$ large enough so that $\bar{X}$ is approximately distributed $N(\theta, \theta(1 - \theta)/n)$, the approximate calculation of this power function can be approached as in Example~\ref{ex:6.3.18}, when we put $\sigma_0 = \sqrt{\theta_0(1 - \theta_0)}$. We do not pursue this calculation further here but note that many statistical packages will evaluate $\beta$ as a built-in function.
\end{example}

\begin{example}[The Power Function in the Location-Scale Normal Model]
\label{ex:6.3.20}
For the two-sided $t$-test in Example~\ref{ex:6.3.13}, we have
\[
\beta_n(\mu, \sigma^2) = \prb_{(\mu, \sigma^2)}\left(2\left(1 - G\left(\left|\frac{\bar{X} - \mu_0}{S/\sqrt{n}}\right| \mid n - 1\right)\right) \leqslant \alpha\right) = \prb_{(\mu, \sigma^2)}\left(\left|\frac{\bar{X} - \mu_0}{S/\sqrt{n}}\right| \geqslant t_{1-\alpha/2}(n - 1)\right),
\]
where $G(\cdot \mid n - 1)$ is the cumulative distribution function of the $t(n - 1)$ distribution. Notice that it is a function of both $\mu$ and $\sigma^2$. In particular, we have to specify both $\mu$ and $\sigma^2$ and then determine $n$ so that $\beta_n(\mu, \sigma^2) \geqslant \beta_0$. Many statistical packages will have the calculation of this power function built-in so that an appropriate $n$ can be determined using this. Alternatively, we can use Monte Carlo methods to approximate the distribution function of
\[
\frac{\bar{X} - \mu_0}{S/\sqrt{n}}
\]
when sampling from the $N(\mu, \sigma^2)$ for a variety of values of $n$ to determine an appropriate value.
\end{example}

\subsection*{Summary of Section~\ref{sec:6.3}}

\begin{itemize}
\item The MLE $\hat{\theta}(s)$ is the best-supported value of the parameter $\theta$ by the model and data. As such, it makes sense to base the derivation of inferences about some characteristic $\psi(\theta)$ on the MLE. These inferences include estimates and their standard errors, confidence intervals, and the assessment of hypotheses via P-values.
\item An important aspect of the design of a sampling study is to decide on the size $n$ of the sample to ensure that the results of the study produce sufficiently accurate results. Prescribing the half-lengths of confidence intervals (margins of error) or the power of a test are two techniques for doing this.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:6.3.1}
Suppose measurements (in centimeters) are taken using an instrument. There is error in the measuring process and a measurement is assumed to be distributed $N(\mu, \sigma_0^2)$, where $\mu$ is the exact measurement and $\sigma_0^2 = 0.5$. If the ($n = 10$) measurements 4.7, 5.5, 4.4, 3.3, 4.6, 5.3, 5.2, 4.8, 5.7, 5.3 were obtained, assess the hypothesis $H_0 : \mu = 5$ by computing the relevant P-value. Also compute a 0.95-confidence interval for the unknown $\mu$.
\end{exercise}

\begin{solution}
This is a two-sided $z$-test with the $z$ statistic equal to $-0.54$ and the $P$-value equal to 0.592, which is very high. So we conclude that we do not have any evidence against $H_0$. A 0.95-confidence interval for the unknown $\mu$ is $(4.442, 5.318)$. Note that the confidence interval contains the value 5, which confirms our conclusion using the above test.
\end{solution}

\begin{exercise}
\label{exer:6.3.2}
Suppose in Exercise~\ref{exer:6.3.1}, we drop the assumption that $\sigma_0^2 = 0.5$. Then assess the hypothesis $H_0 : \mu = 5$ and compute a 0.95-confidence interval for $\mu$.
\end{exercise}

\begin{solution}
This is a two-sided $t$-test with the $t$ statistic equal to $-0.55$ and the $P$-value equal to 0.599, which is very high. We conclude that we do not have enough evidence against $H_0$. A 0.95-confidence interval for the unknown $\mu$ is $(4.382, 5.378)$. Note that the confidence interval contains the value 5, which confirms our conclusion using the above test.
\end{solution}

\begin{exercise}
\label{exer:6.3.3}
Marks on an exam in a statistics course are assumed to be normally distributed with unknown mean but with variance equal to 5. A sample of four students is selected, and their marks are 52, 63, 64, 84. Assess the hypothesis $H_0 : \mu = 60$ by computing the relevant P-value and compute a 0.95-confidence interval for the unknown $\mu$.
\end{exercise}

\begin{solution}
This is a two-sided $z$-test with the $z$ statistic equal to 5.14 and the $P$-value equal to 0.000. So we conclude that we have enough evidence against $H_0$ being true. A 0.95-confidence interval for the unknown $\mu$ is $(63.56, 67.94)$. Note that the confidence interval does not contain the value 60, which confirms our conclusion using the above test.
\end{solution}

\begin{exercise}
\label{exer:6.3.4}
Suppose in Exercise~\ref{exer:6.3.3} that we drop the assumption that the population variance is 5. Assess the hypothesis $H_0 : \mu = 60$ by computing the relevant P-value and compute a 0.95-confidence interval for the unknown $\mu$.
\end{exercise}

\begin{solution}
This is a two-sided $t$-test with the $t$ statistic equal to 9.12 and (using the Student$(3)$ distribution) the $P$-value equals 0.452, which is not small and so we do not reject the null hypothesis. A 0.95-confidence interval for the unknown $\mu$ is $(44.55, 86.95)$. Note that the confidence interval contains the value 60.
\end{solution}

\begin{exercise}
\label{exer:6.3.5}
Suppose that in Exercise~\ref{exer:6.3.3} we had observed only one mark and that it was 52. Assess the hypothesis $H_0 : \mu = 60$ by computing the relevant P-value and compute a 0.95-confidence interval for the unknown $\mu$. Is it possible to compute a P-value and construct a 0.95-confidence interval for $\mu$ without the assumption that we know the population variance? Explain your answer and, if your answer is no, determine the minimum sample size $n$ for which inference is possible without the assumption that the population variance is known.
\end{exercise}

\begin{solution}
If we assume that the population variance is known then under $H_0$ we have $Z = (X - \mu_0)/\sigma_0 \sim N(0, 1)$ and the $P$-value then is given by
\[
    \prb\left( Z \geqslant \left| \frac{x_0 - \mu_0}{\sigma_0} \right| \right) = \prb\left( Z \geqslant \left| \frac{52 - 60}{\sqrt{5}} \right| \right) = 2\left( 1 - \Phi\left| \frac{52 - 60}{\sqrt{5}} \right| \right) = 2(1 - 0.99983) = 0.00034
\]
and a 0.95 confidence interval for $\mu$ is given by
\[
    [x_0 - z_{0.975}\sigma_0, x_0 + z_{0.975}\sigma_0] = \left[ 52 - 1.96\sqrt{5}, 52 + 1.96\sqrt{5} \right] = [47.617, 56.383].
\]
Note that both the $P$-value and the 0.95 confidence interval indicate that there is evidence against $H_0$ being true.

If we don't assume that the population variance is known, then, since we only have a single observation the sample variance is 0, and we do not have a sensible estimate of the population variance. So we cannot use the $t$ procedures to compute the $P$-value and construct a confidence interval. The minimum sample size $n$ for which inference is possible, without the assumption that the population variance is known, is 2.
\end{solution}

\begin{exercise}
\label{exer:6.3.6}
Assume that the speed of light data in Table~\ref{tab:6.3.1} is a sample from an $N(\mu, \sigma^2)$ distribution for some unknown values of $\mu$ and $\sigma^2$. Determine a 0.99-confidence interval for $\mu$. Assess the null hypothesis $H_0 : \mu = 24$.
\end{exercise}

\begin{solution}
A 0.99 confidence interval for $\mu$ is given by $(22.70, 29.72)$. The $P$-value for testing $H_0 : \mu = 24$ is 0.099, so we conclude that there is not much evidence against $H_0$ being true. Note also that the 0.99 confidence interval for $\mu$ contains the value 24.
\end{solution}

\begin{exercise}
\label{exer:6.3.7}
A manufacturer wants to assess whether or not rods are being constructed appropriately, where the diameter of the rods is supposed to be 1.0 cm and the variation in the diameters is known to be distributed $N(0, 0.1)$. The manufacturer is willing to tolerate a deviation of the population mean from this value of no more than 0.1 cm, i.e., if the population mean is within the interval $1.0 \pm 0.1$ cm, then the manufacturing process is performing correctly. A sample of $n = 500$ rods is taken, and the average diameter of these rods is found to be $\bar{x} = 1.05$ cm, with $s^2 = 0.083$ cm$^2$. Are these results statistically significant? Are the results practically significant? Justify your answers.
\end{exercise}

\begin{solution}
To detect if these results are statistically significant or not we need to perform a $z$-test for testing $H_0 : \mu = 1$. The $P$-value is given by
\[
    \prb\left( |Z| \geqslant \left| \frac{1.05 - 1}{\sqrt{0.1/100}} \right| \right) = 2[1 - \Phi(1.5811)] = 2(1 - 0.9431) = 0.1138.
\]
So these results are not statistically significant at the 5\% level, and so we have no evidence against $H_0 : \mu = 1$. Also, the observed difference of $1.05 - 1 = 0.05$ is well within the range that the manufacturer thinks is of practical significance. So the test has detected a small difference that is not practically significant.
\end{solution}

\begin{exercise}
\label{exer:6.3.8}
A polling firm conducts a poll to determine what proportion of voters in a given population will vote in an upcoming election. A random sample of $n = 250$ was taken from the population, and the proportion answering yes was 0.62. Assess the hypothesis $H_0 : \theta = 0.65$ and construct an approximate 0.90-confidence interval for $\theta$.
\end{exercise}

\begin{solution}
Based on a two-sided $z$-test, the $z$-statistic (using standard error $\sqrt{0.65 \cdot 0.35/250}$) equals $-0.994490$ and the $P$-value equals 0.32. So we conclude that there is no evidence against $H_0$ being true. A 0.90-confidence interval for $\theta$ is given by $(0.559832, 0.680168)$, which includes the value 0.65, and so agrees with the result of the above test.
\end{solution}

\begin{exercise}
\label{exer:6.3.9}
A coin was tossed $n = 1000$ times, and the proportion of heads observed was 0.51. Do we have evidence to conclude that the coin is unfair?
\end{exercise}

\begin{solution}
Based on a two-sided $z$-test to assess $H_0 : \theta = 0.5$, the $z$-statistic is equal to 0.63 and the $P$-value is equal to 0.527. So we conclude that there is no evidence against $H_0$ being true; in other words, there is not enough evidence to conclude that the coin is unfair.
\end{solution}

\begin{exercise}
\label{exer:6.3.10}
How many times must we toss a coin to ensure that a 0.95-confidence interval for the probability of heads on a single toss has length less than 0.1, 0.05, and 0.01, respectively?
\end{exercise}

\begin{solution}
Let $\theta$ be the probability of head on a single toss. The sample sizes required so that the margin of error (half of the length) of a $\gamma = 0.95$ confidence interval for $\theta$ is less than $0.05$, $0.025$, $0.005$ are given by
\[
    n \geqslant \frac{1}{4} \left( \frac{z_{(1+\gamma)/2}}{\delta} \right)^2.
\]
So for $\delta = 0.1$, $n > 384.15$; for $\delta = 0.05$, $n \geqslant 1536.6$; and for $\delta = 0.01$, $n \geqslant 38415$.
\end{solution}

\begin{exercise}
\label{exer:6.3.11}
Suppose a possibly biased die is rolled 30 times and that the face containing two pips comes up 10 times. Do we have evidence to conclude that the die is biased?
\end{exercise}

\begin{solution}
Based on a two-sided $z$-test to assess $H_0 : \theta = 1/6$, the $z$-statistic is equal to 2.45 and the $P$-value is equal to 0.014. So we can conclude that at the 5\% significance level, there is evidence to conclude that the die is biased.
\end{solution}

\begin{exercise}
\label{exer:6.3.12}
Suppose a measurement on a population is assumed to be distributed $N(\mu, 2)$ where $\mu \in R^1$ is unknown and that the size of the population is very large. A researcher wants to determine a 0.95-confidence interval for $\mu$ that is no longer than 1. What is the minimum sample size that will guarantee this?
\end{exercise}

\begin{solution}
The sample size that will guarantee that a 0.95-confidence interval for $\mu$ is no longer than 1 is given by
\[
    n \geqslant \sigma_0^2 \left( \frac{z_{(1+\gamma)/2}}{\delta} \right)^2 = 2 \left( \frac{1.96}{0.5} \right)^2 = 30.732.
\]
So the minimum sample size is 31.
\end{solution}

\begin{exercise}
\label{exer:6.3.13}
Suppose $x_1, \ldots, x_n$ is a sample from a Bernoulli$(\theta)$ with $\theta \in [0, 1]$ unknown.
\begin{enumerate}[(a)]
\item Show that $\sum_{i=1}^{n} (x_i - \bar{x})^2 = n\bar{x}(1 - \bar{x})$. (Hint: $x_i^2 = x_i$.)
\item If $X \sim \text{Bernoulli}(\theta)$ then $\sigma^2 = \var(X) = \theta(1 - \theta)$. Record the relationship between the plug-in estimate of $\sigma^2$ and that given by $s^2$ in \eqref{eq:5.5.5}.
\item Since $s^2$ is an unbiased estimator of $\sigma^2$ (see Problem~\ref{exer:6.3.23}), use the results in part (b) to determine the bias in the plug-in estimate. What happens to this bias as $n \to \infty$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Simple expansion is given by
    \begin{align*}
        \sum_{i=1}^{n} (x_i - \bar{x})^2 &= \sum_{i=1}^{n} (x_i^2 - 2\bar{x}x_i + \bar{x}^2) = \sum_{i=1}^{n} x_i^2 - 2\bar{x} \sum_{i=1}^{n} x_i + n\bar{x}^2 \\
        &= \sum_{i=1}^{n} x_i^2 - 2\bar{x} \sum_{i=1}^{n} x_i + n\bar{x}^2 = n\bar{x} - 2\bar{x} n\bar{x} + n\bar{x}^2 = n\bar{x}(1 - \bar{x}).
    \end{align*}
    
    \item The MLE of $\theta$ is $\hat{\theta} = \bar{x}$ as in Example~\ref{ex:6.3.2}. The plug-in estimator for $\sigma^2$ is $\hat{\sigma}^2 = \hat{\theta}(1 - \hat{\theta}) = \bar{x}(1 - \bar{x})$. Using (a), $s^2 = (n-1)^{-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 = (n-1)^{-1} n\bar{x}(1 - \bar{x})$. Thus, $\hat{\sigma}^2 = s^2(n-1)/n$ or
    \[
        \hat{\sigma}^2 - s^2 = -\frac{1}{n} \bar{x}(1 - \bar{x}) = -s^2/n.
    \]
    
    \item The bias of the plug-in estimator $\hat{\sigma}^2$ for $\sigma^2 = \theta(1 - \theta)$ is
    \[
        \text{bias}(\hat{\sigma}^2) = \expc_\theta(\hat{\sigma}^2) - \sigma^2 = \expc_\theta(\hat{\sigma}^2 - s^2) + \expc_\theta(s^2) - \sigma^2 = \expc_\theta(-s^2/n) = -\sigma^2/n \to 0 \text{ as } n \to \infty.
    \]
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:6.3.14}
Suppose you are told that, based on some data, a 0.95-confidence interval for a characteristic $\psi(\theta)$ is given by $(1.23, 2.45)$. You are then asked if there is any evidence against the hypothesis $H_0 : \psi(\theta) = 2$. State your conclusion and justify your reasoning.
\end{exercise}

\begin{solution}
Since the value $\psi(\theta) = 2$ is in the 0.95-confidence interval $(1.23, 2.45)$, we find no evidence against $H_0 : \psi(\theta) = 2$ at significance level $0.05 = 1 - 0.95$.
\end{solution}

\begin{exercise}
\label{exer:6.3.15}
Suppose that $x_1$ is a value from a Bernoulli$(\theta)$ with $\theta \in [0, 1]$ unknown.
\begin{enumerate}[(a)]
\item Is $x_1$ an unbiased estimator of $\theta$?
\item Is $x_1^2$ an unbiased estimator of $\theta^2$?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item To check unbiasedness, the expectation must be computed. $\expc_\theta(x_1) = 1 \cdot \theta + 0 \cdot (1 - \theta) = \theta$. Hence, $x_1$ is an unbiased estimator of $\theta$.
    
    \item Since the value of $x_1$ is only 0 or 1, the equation $x_1^2 = x_1$ always holds. Thus, $\expc_\theta(x_1^2) = \expc_\theta(x_1) = \theta$. Hence, $x_1^2$ is not an unbiased estimator of $\theta^2$. In this exercise, we showed an unbiased estimator is not transformation invariant.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:6.3.16}
Suppose a plug-in MLE of a characteristic $\psi(\theta)$ is given by 5.3. Also a P-value was computed to assess the hypothesis $H_0 : \psi(\theta) = 5$ and the value was $0.000132$. If you are told that differences among values of $\psi(\theta)$ less than $0.5$ are of no importance as far as the application is concerned, then what do you conclude from these results? Suppose instead you were told that differences among values of $\psi(\theta)$ less than $0.25$ are of no importance as far as the application is concerned, then what do you conclude from these results?
\end{exercise}

\begin{solution}
The $P$-value indicates that the true value of $\psi(\theta)$ is not equal to 5. The estimate $\widehat{\psi(\theta)} = 5.3$ suggests that the true difference from 5 is less than 0.5. This suggests that the statistically significant result is not practically significant. If instead we adopt the cutoff of 0.25 for a practical difference then the statistically significant result from the $P$-value suggests that a meaningful difference from 5 exists.
\end{solution}

\begin{exercise}
\label{exer:6.3.17}
A P-value was computed to assess the hypothesis $H_0 : \psi(\theta) = \psi_0$ and the value $0.22$ was obtained. The investigator says this is strong evidence that the hypothesis is correct. How do you respond?
\end{exercise}

\begin{solution}
Statistically, the $P$-value 0.22 shows no evidence against the null hypothesis. However, it does not imply that the null hypothesis is correct. It may be that we have just not taken a large enough sample size to detect a difference.
\end{solution}

\begin{exercise}
\label{exer:6.3.18}
A P-value was computed to assess the hypothesis $H_0 : \psi(\theta) = \psi_1$ and the value $0.55$ was obtained. You are told that differences in $\psi(\theta)$ greater than 0.5 are considered to be practically significant but not otherwise. The investigator wants to know if enough data were collected to reliably detect a difference of this size or greater. How would you respond?
\end{exercise}

\begin{solution}
We need to compute the power at $0.5 = 1 - 0.5$ and $1.5 = 1 + 0.5$. If these values are high, then we have a large probability of detecting a difference of magnitude 0.5 but not otherwise. If the power is low then more data needs to be collected to get a reliable result.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:6.3.19}
Suppose a measurement on a population can be assumed to follow the $N(\mu, \sigma^2)$ distribution, where $(\mu, \sigma^2) \in R^1 \times (0, \infty)$ is unknown and the size of the population is very large. A very conservative upper bound on $\sigma$ is given by 5. A researcher wants to determine a 0.95-confidence interval for $\mu$ that is no longer than 1. Determine a sample size that will guarantee this. (Hint: Start with a large sample approximation.)
\end{exercise}

\begin{solution}
The sample size that will guarantee that a 0.95-confidence interval for $\mu$ is no longer than 1 is given by
\[
    n \geqslant 25 \left( \frac{t_{0.975}(n-1)}{\delta} \right)^2.
\]
When $n$ is large, then $t_{0.975}(n-1) \approx z_{0.975} = 1.96$, and in that case
\[
    n \geqslant \frac{25}{0.5^2} (1.96)^2 = 384.16.
\]
So the minimum sample size is 385. Now when $n = 400$ we have that $t_{0.975}(400) = 1.9659$ and
\[
    400 \geqslant \frac{25}{0.5^2} (1.9659)^2 = 386.48
\]
so $n = 400$ suffices.
\end{solution}

\begin{exercise}
\label{exer:6.3.20}
Suppose a measurement on a population is assumed to be distributed $N(\mu, \sigma^2)$, where $\mu \in R^1$ is unknown and the size of the population is very large. A researcher wants to assess a null hypothesis $H_0 : \mu = \mu_0$ and ensure that the probability is at least 0.80 that the P-value is less than 0.05 when $|\mu - \mu_0| = 0.5$. What is the minimum sample size that will guarantee this? (Hint: Tabulate the power as a function of the sample size $n$.)
\end{exercise}

\begin{solution}
The power function is given by ($z_{0.975} = 1.96$)
\[
    1 - \Phi\left( \frac{0.5}{\sqrt{2}/\sqrt{n}} + 1.96 \right) + \Phi\left( \frac{0.5}{\sqrt{2}/\sqrt{n}} - 1.96 \right).
\]
A partial tabulation of the power function (as a function of $n$) is given below. We see that $n = 63$ is the appropriate sample size.

\begin{center}
\begin{tabular}{cc}
$n$ & Power \\
\hline
60 & 0.78190 \\
61 & 0.78853 \\
62 & 0.79500 \\
63 & 0.80129 \\
64 & 0.80742 \\
65 & 0.81339 \\
66 & 0.81919 \\
67 & 0.82484
\end{tabular}
\end{center}
\end{solution}

\begin{exercise}
\label{exer:6.3.21}
Generate $10^3$ samples of size $n = 5$ from the Bernoulli$(0.5)$ distribution. For each of these samples, calculate \eqref{eq:6.3.5} with $\gamma = 0.95$ and record the proportion of intervals that contain the true value. What do you notice? Repeat this simulation with $n = 20$. What do you notice?
\end{exercise}

\begin{solution}
We expect to observe approximately 950 confidence intervals containing the true value of $\theta$. In practice, we do not observe exactly this number. The number covering will be less for sample size $n = 5$ than for sample size $n = 20$.
\end{solution}

\begin{exercise}
\label{exer:6.3.22}
Generate $10^4$ samples of size $n = 5$ from the $N(0, 1)$ distribution. For each of these samples, calculate the interval $(\bar{x} - s/\sqrt{5}, \bar{x} + s/\sqrt{5})$ where $s$ is the sample standard deviation, and compute the proportion of times this interval contains $\mu$. Repeat this simulation with $n = 10$ and 100 and compare your results.
\end{exercise}

\begin{solution}
As $n$ increases, you should observe that the proportion of intervals that actually contains 0 increases as $s$ becomes a better estimate of $\sigma = 1$.
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:6.3.23}
Suppose that $x_1, \ldots, x_n$ is a sample from a distribution with mean $\mu$ and variance $\sigma^2$.
\begin{enumerate}[(a)]
\item Prove that $s^2$ given by \eqref{eq:5.5.5} is an unbiased estimator of $\sigma^2$.
\item If instead we estimate $\sigma^2$ by $(n - 1)s^2/n$, then determine the bias in this estimate and what happens to it as $n \to \infty$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First of all, $(n-1)s^2 = \sum_{i=1}^{n} (x_i - \bar{x})^2 = \sum_{i=1}^{n} [x_i^2 - 2\bar{x}x_i + \bar{x}^2] = \sum_{i=1}^{n} x_i^2 - n\bar{x}^2$. The expectation of the first summation term is
    \[
        \expc\left[ \sum_{i=1}^{n} x_i^2 \right] = n\expc[X_1^2] = n(\mu^2 + \sigma^2).
    \]
    Since $n\bar{x}^2 = n^{-1} \sum_{i=1}^{n} x_i \sum_{j=1}^{n} x_j$,
    \[
        \expc[n\bar{x}^2] = \frac{1}{n} \expc\left[ \sum_{i=1}^{n} \sum_{j=1}^{n} x_i x_j \right] = \frac{1}{n} \sum_{i=1}^{n} \sum_{\substack{j=1 \\ j \neq i}}^{n} \expc[x_i x_j] + \frac{1}{n} \sum_{i=1}^{n} \expc[x_i^2] = \frac{1}{n} \cdot n(n-1) \cdot \mu^2 + \frac{1}{n} \cdot n \cdot (\mu^2 + \sigma^2) = n\mu^2 + \sigma^2.
    \]
    Hence, $\expc[(n-1)s^2] = n(\mu^2 + \sigma^2) - (n\mu^2 + \sigma^2) = (n-1)\sigma^2$. Therefore $\expc[s^2] = \sigma^2$ and $s^2$ is an unbiased estimator of the variance $\sigma^2$.
    
    \item Let $\hat{\sigma}^2 = (n-1)s^2/n$. The bias of $\hat{\sigma}^2$ is
    \[
        \text{bias}(\hat{\sigma}^2) = \expc[\hat{\sigma}^2] - \sigma^2 = \frac{n-1}{n} \expc[s^2] - \sigma^2 = \frac{n-1}{n} \sigma^2 - \sigma^2 = -\frac{\sigma^2}{n}.
    \]
    Hence, the bias $-\sigma^2/n$ converges to 0 as $n \to \infty$.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:6.3.24}
Suppose we have two unbiased estimators $T_1$ and $T_2$ of $\psi(\theta) \in R^1$.
\begin{enumerate}[(a)]
\item Show that $\alpha T_1 + (1 - \alpha) T_2$ is also an unbiased estimator of $\psi(\theta)$ whenever $\alpha \in [0, 1]$.
\item If $T_1$ and $T_2$ are also independent, e.g., determined from independent samples, then calculate $\var(\alpha T_1 + (1 - \alpha) T_2)$ in terms of $\var(T_1)$ and $\var(T_2)$.
\item For the situation in part (b), determine the best choice of $\alpha$ in the sense that for this choice $\var(\alpha T_1 + (1 - \alpha) T_2)$ is smallest. What is the effect on this combined estimator of $T_1$ having a very large variance relative to $T_2$?
\item Repeat parts (b) and (c), but now do not assume that $T_1$ and $T_2$ are independent, so $\var(\alpha T_1 + (1 - \alpha) T_2)$ will also involve $\cov(T_1, T_2)$.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Since $T_1$ and $T_2$ are unbiased estimators of $\psi(\theta)$, $\expc[T_1] = \expc[T_2] = \psi(\theta)$. Hence, $\expc[\alpha T_1 + (1-\alpha)T_2] = \alpha \expc[T_1] + (1-\alpha)\expc[T_2] = \alpha\psi(\theta) + (1-\alpha)\psi(\theta) = \psi(\theta)$. Therefore, $\alpha T_1 + (1-\alpha)T_2$ is also an unbiased estimator of $\psi(\theta)$.
    
    \item From Theorems 3.3.4, 3.3.1(b) and 3.3.2, $\var_\theta(\alpha T_1 + (1-\alpha)T_2) = \var_\theta(\alpha T_1) + \var_\theta((1-\alpha)T_2) + 2\cov_\theta(\alpha T_1, (1-\alpha)T_2) = \alpha^2 \var_\theta(T_1) + (1-\alpha)^2 \var_\theta(T_2) + 2\alpha(1-\alpha)\cov_\theta(T_1, T_2)$. The independence between $T_1$ and $T_2$ implies $\cov_\theta(T_1, T_2) = 0$ and $\var_\theta(\alpha T_1 + (1-\alpha)T_2) = \alpha^2 \var_\theta(T_1) + (1-\alpha)^2 \var_\theta(T_2)$.
    
    \item The variance of $\alpha T_1 + (1-\alpha)T_2$ can be written as
    \[
        \alpha^2(\var_\theta(T_1) + \var_\theta(T_2)) - 2\alpha\var_\theta(T_2) + \var_\theta(T_2) = (\var_\theta(T_1) + \var_\theta(T_2))\left( \alpha - \frac{\var_\theta(T_2)}{\var_\theta(T_1) + \var_\theta(T_2)} \right)^2 + \frac{\var_\theta(T_1)\var_\theta(T_2)}{\var_\theta(T_1) + \var_\theta(T_2)}.
    \]
    Hence, it is minimized when $\alpha = \var_\theta(T_2)/(\var_\theta(T_1) + \var_\theta(T_2))$. If $\var_\theta(T_1)$ is very large relative to $\var_\theta(T_2)$, then $\alpha$ will be very small. Hence, the estimator $\alpha T_1 + (1-\alpha)T_2$ is almost similar to $T_2$.
    
    \item In part (b), the variance of $\alpha T_1 + (1-\alpha)T_2$ is given by $\alpha^2 \var_\theta(T_1) + (1-\alpha)^2 \var_\theta(T_2) + 2\alpha(1-\alpha)\cov_\theta(T_1, T_2)$. By rearranging terms, we get
    \[
        \alpha^2(\var_\theta(T_1) + \var_\theta(T_2) - 2\cov_\theta(T_1, T_2)) - 2\alpha(\var_\theta(T_2) + \cov_\theta(T_1, T_2)) + \var_\theta(T_2).
    \]
    If $T_1 = T_2$, then $\alpha T_1 + (1-\alpha)T_2 = T_1 = T_2$ and there is nothing to do. So $\prb(T_1 = T_2) < 1$ is assumed. Thus, $\var_\theta(T_1) + \var_\theta(T_2) - 2\cov_\theta(T_1, T_2) = \var_\theta(T_1 - T_2) > 0$. Therefore, the variance of $\alpha T_1 + (1-\alpha)T_2$ is maximized when $\alpha = (\var_\theta(T_2) + \cov_\theta(T_1, T_2))/\var_\theta(T_1 - T_2)$. If $\var_\theta(T_1)$ is very large relative to $\var_\theta(T_2)$, then $\alpha$ is very small again. Hence, the linear combination estimator $\alpha T_1 + (1-\alpha)T_2$ highly depends on $T_2$.
\end{enumerate}
\end{solution}

\begin{exercise}[One-sided confidence intervals for means]
\label{exer:6.3.25}
Suppose that $x_1, \ldots, x_n$ is a sample from an $N(\mu, \sigma_0^2)$ distribution, where $\mu \in R^1$ is unknown and $\sigma_0^2$ is known. Suppose we want to make inferences about the interval $(-\infty, \mu]$. Consider the problem of finding an interval $C(x_1, \ldots, x_n) = (-\infty, u(x_1, \ldots, x_n)]$ that covers the interval $(-\infty, \mu]$ with probability at least $\gamma$. So we want $u$ such that for every $\mu$,
\[
\prb_\mu(\mu \leqslant u(X_1, \ldots, X_n)) \geqslant \gamma.
\]
Note that $\mu \leqslant u(x_1, \ldots, x_n)$ if and only if $(-\infty, \mu] \subset (-\infty, u(x_1, \ldots, x_n)]$, so $C(x_1, \ldots, x_n)$ is called a \emph{left-sided $\gamma$-confidence interval} for $\mu$. Obtain an exact left-sided $\gamma$-confidence interval for $\mu$ using $u(x_1, \ldots, x_n) = \bar{x} + k\sigma_0/\sqrt{n}$, i.e., find the $k$ that gives this property.
\end{exercise}

\begin{solution}
Using $c(x_1, \ldots, x_n) = \bar{x} + k(\sigma_0/\sqrt{n})$, we have that $k$ satisfies
\[
    \prb\left( \mu \leqslant \bar{x} + k(\sigma_0/\sqrt{n}) \right) = \prb\left( \frac{\bar{x} - \mu}{\sigma_0/\sqrt{n}} \geqslant -k \right) = \prb(Z \geqslant -k) \geqslant \gamma.
\]
So $k = -z_{1-\gamma} = z_\gamma$, i.e., the $\gamma$-percentile of a $N(0, 1)$ distribution.
\end{solution}

\begin{exercise}[One-sided hypotheses for means]
\label{exer:6.3.26}
Suppose that $x_1, \ldots, x_n$ is a sample from a $N(\mu, \sigma_0^2)$ distribution, where $\mu$ is unknown and $\sigma_0^2$ is known. Suppose we want to assess the hypothesis $H_0 : \mu \leqslant \mu_0$. Under these circumstances, we say that the observed value $\bar{x}$ is surprising if $\bar{x}$ occurs in a region of low probability for every distribution in $H_0$. Therefore, a sensible P-value for this problem is $\max_{\mu \in H_0} \prb_\mu(\bar{X} \geqslant \bar{x})$. Show that this leads to the P-value $1 - \Phi((\bar{x} - \mu_0)/(\sigma_0/\sqrt{n}))$.
\end{exercise}

\begin{solution}
The $P$-value for testing $H_0 : \mu \leqslant \mu_0$ is given by
\[
    \max_{\mu \in H_0} \prb_\mu\left( \frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} > \frac{\bar{x}_o - \mu}{\sigma_0/\sqrt{n}} \right) = \max_{\mu \in H_0} \prb\left( Z > \frac{\bar{x}_o - \mu}{\sigma_0/\sqrt{n}} \right) = \max_{\mu \in H_0} \left( 1 - \Phi\left( \frac{\bar{x}_o - \mu}{\sigma_0/\sqrt{n}} \right) \right).
\]
Since $(1 - \Phi((\bar{x}_o - \mu)/(\sigma_0/\sqrt{n})))$ is an increasing function of $\mu$, its maximum is at $\mu = \mu_0$.
\end{solution}

\begin{exercise}
\label{exer:6.3.27}
Determine the form of the power function associated with the hypothesis assessment procedure of Problem~\ref{exer:6.3.26}, when we declare a test result as being statistically significant whenever the P-value is less than $\alpha$.
\end{exercise}

\begin{solution}
The form of the power function associated with the above hypothesis assessment procedure is given by
\begin{align*}
    \beta(\mu) &= \prb_\mu\left( 1 - \Phi\left( \frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} \right) < \alpha \right) = \prb_\mu\left( \Phi\left( \frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} \right) > 1 - \alpha \right) \\
    &= \prb_\mu\left( \frac{\bar{X} - \mu_0}{\sigma_0/\sqrt{n}} > z_{1-\alpha} \right) = \prb_\mu\left( \frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} > \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha} \right) \\
    &= 1 - \Phi\left( \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha} \right).
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:6.3.28}
Repeat Problems~\ref{exer:6.3.25} and~\ref{exer:6.3.26}, but this time obtain a right-sided $\gamma$-confidence interval for $\mu$ and assess the hypothesis $H_0 : \mu \geqslant \mu_0$.
\end{exercise}

\begin{solution}
Using $c(x_1, \ldots, x_n) = \bar{x} + k(\sigma_0/\sqrt{n})$ we have that $k$ satisfies
\[
    \prb\left( \mu \geqslant \bar{x} + k(\sigma_0/\sqrt{n}) \right) = \prb\left( \frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} \leqslant -k \right) = \prb(Z \leqslant -k) \geqslant \gamma.
\]
So $k = -z_\gamma = z_{1-\gamma}$, i.e., the $1 - \gamma$ percentile of a $N(0, 1)$ distribution.

The $P$-value for testing $H_0 : \mu \geqslant \mu_0$ is given by
\[
    \max_{\mu \in H_0} \prb_\mu\left( \frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} < \frac{\bar{x}_o - \mu}{\sigma_0/\sqrt{n}} \right) = \max_{\mu \in H_0} \prb\left( Z < \frac{\bar{x}_o - \mu}{\sigma_0/\sqrt{n}} \right) = \max_{\mu \in H_0} \Phi\left( \frac{\bar{x}_o - \mu}{\sigma_0/\sqrt{n}} \right).
\]
Since $\Phi((\bar{x}_o - \mu)/(\sigma_0/\sqrt{n}))$ is a decreasing function of $\mu$, its maximum is at $\mu = \mu_0$.
\end{solution}

\begin{exercise}
\label{exer:6.3.29}
Repeat Problems~\ref{exer:6.3.25} and~\ref{exer:6.3.26}, but this time do not assume the population variance is known. In particular, determine $k$ so that $u(x_1, \ldots, x_n) = \bar{x} + ks/\sqrt{n}$ gives an exact left-sided $\gamma$-confidence interval for $\mu$ and show that the P-value for testing $H_0 : \mu \leqslant \mu_0$ is given by
\[
1 - G\left(\frac{\bar{x} - \mu_0}{\sigma_0/\sqrt{n}} \mid n - 1\right).
\]
\end{exercise}

\begin{solution}
Using $c(x_1, \ldots, x_n) = \bar{x} + ks/\sqrt{n}$, we have that $k$ satisfies
\[
    \prb\left( \mu \leqslant \bar{X} + ks/\sqrt{n} \right) = \prb\left( \frac{\bar{X} - \mu}{s/\sqrt{n}} \geqslant -k \right) \geqslant \gamma.
\]
So $k = -t_{1-\gamma}(n-1) = t_\gamma(n-1)$, i.e., the $\gamma$ percentile of a $t(n-1)$ distribution.

The $P$-value for testing $H_0 : \mu \leqslant \mu_0$ is given by
\[
    \max_{\mu \in H_0} \prb_\mu\left( \frac{\bar{X} - \mu}{\sigma_0/\sqrt{n}} > \frac{\bar{x}_o - \mu}{\sigma_0/\sqrt{n}} \right) = \max_{\mu \in H_0} \left( 1 - G\left( \frac{\bar{x}_o - \mu}{\sigma_0/\sqrt{n}}; n-1 \right) \right).
\]
Since $(1 - G((\bar{x}_o - \mu)/(\sigma_0/\sqrt{n}); n-1))$ is an increasing function of $\mu$, its maximum is at $\mu = \mu_0$.
\end{solution}

\begin{exercise}[One-sided confidence intervals for variances]
\label{exer:6.3.30}
Suppose that $x_1, \ldots, x_n$ is a sample from the $N(\mu, \sigma^2)$ distribution, where $(\mu, \sigma^2) \in R^1 \times (0, \infty)$ is unknown, and we want a $\gamma$-confidence interval of the form
\[
C(x_1, \ldots, x_n) = (0, u(x_1, \ldots, x_n)]
\]
for $\sigma^2$. If $u(x_1, \ldots, x_n) = ks^2$, then determine $k$ so that this interval is an exact $\gamma$-confidence interval.
\end{exercise}

\begin{solution}
Using $c(x_1, \ldots, x_n) = ks^2$ we have that $k$ satisfies $\prb(\sigma^2 \leqslant kS^2) = \prb\left( \frac{(n-1)S^2}{\sigma^2} \geqslant \frac{n-1}{k} \right) \geqslant \gamma$. So $k = (n-1)/\chi^2_{1-\gamma}(n-1)$.
\end{solution}

\begin{exercise}[One-sided hypotheses for variances]
\label{exer:6.3.31}
Suppose that $x_1, \ldots, x_n$ is a sample from the $N(\mu, \sigma^2)$ distribution, where $(\mu, \sigma^2) \in R^1 \times (0, \infty)$ is unknown, and we want to assess the hypothesis $H_0 : \sigma^2 \leqslant \sigma_0^2$. Argue that the sample variance $s^2$ is surprising if $s^2$ is large and that, therefore, a sensible P-value for this problem is to compute $\max_{\sigma^2 \in H_0} \prb_{\sigma^2}(S^2 \geqslant s^2)$. Show that this leads to the P-value
\[
1 - H\left(\frac{(n - 1)s^2}{\sigma_0^2} \mid n - 1\right),
\]
where $H(\cdot \mid n - 1)$ is the distribution function of the $\chi^2(n - 1)$ distribution.
\end{exercise}

\begin{solution}
The $P$-value for testing $H_0 : \sigma^2 \leqslant \sigma_0^2$ is given by
\begin{align*}
    \max_{(\mu, \sigma^2) \in H_0} \prb_\mu(S^2 > s^2) &= \max_{(\mu, \sigma^2) \in H_0} \prb_\mu\left( \frac{(n-1)S^2}{\sigma^2} > \frac{(n-1)s^2}{\sigma^2} \right) \\
    &= \max_{(\mu, \sigma^2) \in H_0} \left( 1 - H\left( \frac{(n-1)s^2}{\sigma^2}; n-1 \right) \right) = \left( 1 - H\left( \frac{(n-1)s^2}{\sigma_0^2}; n-1 \right) \right)
\end{align*}
since $(1 - H((n-1)s_0^2/\sigma^2; n-1))$ is an increasing function of $\sigma^2$.
\end{solution}

\begin{exercise}
\label{exer:6.3.32}
Determine the form of the power function associated with the hypothesis assessment procedure of Problem~\ref{exer:6.3.31}, for computing the probability that the P-value is less than $\alpha$.
\end{exercise}

\begin{solution}
We have that
\begin{align*}
    \beta(\mu, \sigma^2) &= \prb_{(\mu, \sigma^2)}\left( 1 - H\left( \frac{(n-1)S^2}{\sigma_0^2}; n-1 \right) < \alpha \right) = \prb_{(\mu, \sigma^2)}\left( H\left( \frac{(n-1)S^2}{\sigma_0^2}; n-1 \right) > 1 - \alpha \right) \\
    &= \prb_{(\mu, \sigma^2)}\left( \frac{(n-1)S^2}{\sigma_0^2} > \chi^2_{1-\alpha}(n-1) \right) = \prb_{(\mu, \sigma^2)}\left( \frac{(n-1)S^2}{\sigma^2} > \frac{\sigma_0^2}{\sigma^2} \chi^2_{1-\alpha}(n-1) \right) \\
    &= 1 - H\left( \frac{\sigma_0^2}{\sigma^2} \chi^2_{1-\alpha}(n-1); n-1 \right).
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:6.3.33}
Repeat Exercise~\ref{exer:6.3.7}, but this time do not assume that the population variance is known. In this case, the manufacturer deems the process to be under control if the population standard deviation is less than or equal to 0.1 and the population mean is in the interval $1.0 \pm 0.1$ cm. Use Problem~\ref{exer:6.3.31} for the test concerning the population variance.
\end{exercise}

\begin{solution}
To detect if these results are statistically significant or not we need to perform a $t$-test for testing $H_0 : \mu = 1$. The $P$-value is given by
\[
    \prb\left( |T| \geqslant \left| \frac{1.05 - 1}{\sqrt{0.083/100}} \right| \right) = 2[1 - G(1.7355; 99)] = 2(1 - 0.95712) = 0.08576.
\]
Since the $P$-value is greater than 5\%, these results are not statistically significant at the 5\% level, so we have no evidence against $H_0 : \mu = 1$.

The $P$-value for testing $H_0 : \sigma^2 \leqslant \sigma_0^2$ is given by $(1 - H((n-1)s^2/\sigma_0^2; n-1))$. Using $\sigma_0^2 = 0.01$, $s^2 = 0.083$, $n = 100$ we obtain $P$-value equal to 0. So we have enough evidence against $H_0$, i.e., the result is statistically significant and we have evidence that the process is not under control.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:6.3.34}
Prove that \eqref{eq:6.3.11} is always nonnegative. (Hint: Use the facts that $\phi$ is symmetric about 0, increases to the left of 0, and decreases to the right of 0.)
\end{exercise}

\begin{solution}
Equation (6.3.11) is given by
\[
    \left[ \varphi\left( \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - z_{1-\alpha/2} \right) - \varphi\left( \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha/2} \right) \right] \frac{\mu_0 - \mu}{\sigma_0}.
\]
Put $x = \sqrt{n}(\mu_0 - \mu)/\sigma_0$. If $x < 0$, then $x - z_{1-\alpha/2} < x + z_{1-\alpha/2} < -x + z_{1-\alpha/2} = -(x - z_{1-\alpha/2})$. Since $\varphi(x - z_{1-\alpha/2}) = \varphi(-(x - z_{1-\alpha/2}))$ and $\varphi(z)$ increases to the left of 0 and decreases to the right, this implies that (6.3.11) is nonnegative. If $x > 0$, then $-(x + z_{1-\alpha/2}) < x - z_{1-\alpha/2} < x + z_{1-\alpha/2}$ and again (6.3.11) is nonnegative.
\end{solution}

\begin{exercise}
\label{exer:6.3.35}
Establish that \eqref{eq:6.3.13} is positive when $\mu > \mu_0$, negative when $\mu < \mu_0$, and takes the value 0 when $\mu = \mu_0$.
\end{exercise}

\begin{solution}
Equation (6.3.12) is given by
\[
    \left[ \varphi\left( \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} + z_{1-\alpha/2} \right) - \varphi\left( \frac{\mu_0 - \mu}{\sigma_0/\sqrt{n}} - z_{1-\alpha/2} \right) \right] \frac{\sqrt{n}}{\sigma_0}.
\]
Put $x = \sqrt{n}(\mu_0 - \mu)/\sigma_0$. Then if $x < 0$, we have that $x - z_{1-\alpha/2} < x + z_{1-\alpha/2} < -x + z_{1-\alpha/2} = -(x - z_{1-\alpha/2})$. Since $\varphi(x - z_{1-\alpha/2}) = \varphi(-(x - z_{1-\alpha/2}))$ and $\varphi(z)$ increases to the left of 0 and decreases to the right, this implies that (6.3.12) is positive when $\mu > \mu_0$. When $x = 0$, clearly (6.3.12) equals 0. When $x > 0$ then $-(x + z_{1-\alpha/2}) < x - z_{1-\alpha/2} < x + z_{1-\alpha/2}$, and this implies that (6.3.12) is less than 0 when $\mu < \mu_0$.
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:6.3.36}
Discuss the following statement: The accuracy of the results of a statistical analysis is so important that we should always take the largest possible sample size.
\end{exercise}

\begin{exercise}
\label{exer:6.3.37}
Suppose we have a sequence of estimators $T_1, T_2, \ldots$ for $\psi(\theta)$ and $T_n \xrightarrow{P} \psi(\theta) + \Delta$ as $n \to \infty$ for each $\theta \in \Omega$. Discuss under what circumstances you might consider $T_n$ a useful estimator of $\psi(\theta)$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distribution-Free Methods}
\label{sec:6.4}

The likelihood methods we have been discussing all depend on the assumption that the true distribution lies in $\{\prb_\theta : \theta \in \Omega\}$. There is typically nothing that guarantees that the assumption $\{\prb_\theta : \theta \in \Omega\}$ is correct. If the distribution we are sampling from is far different from any of the distributions in $\{\prb_\theta : \theta \in \Omega\}$, then methods of inference that depend on this assumption, such as likelihood methods, can be very misleading. So it is important in any application to check that our assumptions make sense. We will discuss the topic of model checking in Chapter~\ref{ch:9}.

Another approach to this problem is to take the model $\{\prb_\theta : \theta \in \Omega\}$ as large as possible, reflecting the fact that we may have very little information about what the true distribution is like. For example, inferences based on the Bernoulli$(\theta)$ model with $\theta \in [0, 1]$ really specify no information about the true distribution because this model includes all the possible distributions on the sample space $S = \{0, 1\}$. Inference methods that are suitable when $\{\prb_\theta : \theta \in \Omega\}$ is very large are sometimes called \emph{distribution-free}, to reflect the fact that very little information is specified in the model about the true distribution.

For finite sample spaces, it is straightforward to adopt the distribution-free approach, as with the just cited Bernoulli model, but when the sample space is infinite, things are more complicated. In fact, sometimes it is very difficult to determine inferences about characteristics of interest when the model is very big. Furthermore, if we have
\[
\{\prb_\theta : \theta \in \Omega_1\} \subset \{\prb_\theta : \theta \in \Omega\}
\]
then, when the smaller model contains the true distribution, methods based on the smaller model will make better use of the information in the data about the true value $\theta$ in $\Omega_1$ than will methods using the bigger model $\{\prb_\theta : \theta \in \Omega\}$. So there is a trade-off between taking too big a model and taking too precise a model. This is an issue that a statistician must always address.

We now consider some examples of distribution-free inferences. In some cases, the inferences have approximate sampling properties, while in other cases the inferences have exact sampling properties for very large models.

\subsection{Method of Moments}
\label{ssec:6.4.1}

Suppose we take $\{\prb_\theta : \theta \in \Omega\}$ to be the set of all distributions on $R^1$ that have their first $l$ moments, and we want to make inferences about the moments
\[
\mu_i = \expc_\theta(X^i)
\]
for $i = 1, \ldots, l$ based on a sample $x_1, \ldots, x_n$. The natural sample analog of the population moment $\mu_i$ is the $i$th \emph{sample moment}
\[
m_i = \frac{1}{n} \sum_{j=1}^{n} x_j^i,
\]
which would seem to be a sensible estimator.

In particular, we have that $\expc_\theta(M_i) = \mu_i$ for every $\theta$, so $m_i$ is unbiased, and the weak and strong laws of large numbers establish that $m_i$ converges to $\mu_i$ as $n$ increases. Furthermore, the central limit theorem establishes that
\[
\frac{M_i - \mu_i}{\sqrt{\var_\theta(M_i)}} \xrightarrow{D} N(0, 1)
\]
as $n \to \infty$ provided that $\var_\theta(M_i) < \infty$. Now, because $X_1, \ldots, X_n$ are i.i.d., we have that
\[
\var_\theta(M_i) = \frac{1}{n^2} \sum_{j=1}^{n} \var_\theta(X_j^i) = \frac{1}{n} \var_\theta(X_1^i) = \frac{1}{n} (\expc_\theta(X_1^{2i}) - \mu_i^2) = \frac{1}{n} (\mu_{2i} - \mu_i^2),
\]
so we have that $\var_\theta(M_i) < \infty$ provided that $i \leqslant l/2$. In this case, we can estimate $\mu_{2i} - \mu_i^2$ by
\[
s_i^2 = \frac{1}{n - 1} \sum_{j=1}^{n} (x_j^i - m_i)^2,
\]
as we can simply treat $x_1^i, \ldots, x_n^i$ as a sample from a distribution with mean $\mu_i$ and variance $\mu_{2i} - \mu_i^2$. Problem~\ref{exer:6.3.23} establishes that $s_i^2$ is an unbiased estimate of $\var_\theta(M_i)$. So, as with inferences for the population mean based on the $z$-statistic, we have that
\[
\left(m_i - z_{(1+\gamma)/2} \frac{s_i}{\sqrt{n}}\right)
\]
is an approximate $\gamma$-confidence interval for $\mu_i$ whenever $i \leqslant l/2$ and $n$ is large. Also, we can test hypothesis $H_0 : \mu_i = \mu_{i0}$ in exactly the same fashion, as we did this for the population mean using the $z$-statistic.

Notice that the model $\{\prb_\theta : \theta \in \Omega\}$ is very large (all distributions on $R^1$ having their first $l + 2$ moments finite), and these approximate inferences are appropriate for every distribution in the model. A cautionary note is that estimation of moments becomes more difficult as the order of the moments rises. Very large sample sizes are required for the accurate estimation of high-order moments.

The general method of moments principle allows us to make inference about characteristics that are functions of moments. This takes the following form:

\medskip
\noindent\textbf{Method of moments principle:} A function $\psi(\mu_1, \ldots, \mu_k)$ of the first $k \leqslant l$ moments is estimated by $\psi(m_1, \ldots, m_k)$.
\medskip

When $\psi$ is continuously differentiable and nonzero at $(\mu_1, \ldots, \mu_k)$ and $k \leqslant l/2$, then it can be proved that $\psi(M_1, \ldots, M_k)$ converges in distribution to a normal with mean given by $\psi(\mu_1, \ldots, \mu_k)$ and variance given by an expression involving the variances and covariances of $M_1, \ldots, M_k$ and the partial derivatives of $\psi$. We do not pursue this topic further here but note that, in the case $k = 1$ and $l \geqslant 2$, these conditions lead to the so-called \emph{delta theorem}, which says that
\begin{equation}
\label{eq:6.4.1}
\frac{\sqrt{n}(\psi(\bar{X}) - \psi(\mu_1))}{\psi'(\bar{X}) s} \xrightarrow{D} N(0, 1)
\end{equation}
as $n \to \infty$ provided that $\psi$ is continuously differentiable at $\mu_1$ and $\psi'(\mu_1) \neq 0$; see \emph{Approximation Theorems of Mathematical Statistics}, by R.\,J.\ Serfling (John Wiley \& Sons, New York, 1980), for a proof of this result. This result provides approximate confidence intervals and tests for $\psi(\mu_1)$.

\begin{example}[Inference about a Characteristic Using the Method of Moments]
\label{ex:6.4.1}
Suppose $x_1, \ldots, x_n$ is a sample from a distribution with unknown mean $\mu$ and variance $\sigma^2$, and we want to construct a $\gamma$-confidence interval for $\psi(\mu) = 1/\mu^2$. Then $\psi'(\mu) = -2/\mu^3$, so the delta theorem says that
\[
\frac{\sqrt{n}(1/\bar{X}^2 - 1/\mu^2)}{2s/\bar{X}^3} \xrightarrow{D} N(0, 1)
\]
as $n \to \infty$. Therefore,
\[
\left(\frac{1}{\bar{x}^2} - \frac{2s}{\sqrt{n}\bar{x}^3} z_{(1+\gamma)/2}\right)
\]
is an approximate $\gamma$-confidence interval for $1/\mu^2$.

Notice that if $\mu = 0$, then this confidence interval is not valid because $\psi$ is not continuously differentiable at 0. So if you think the population mean could be 0, or even close to 0, this would not be an appropriate choice of confidence interval for $\psi$.
\end{example}

\subsection{Bootstrapping}
\label{ssec:6.4.2}

Suppose that $\{\prb_\theta : \theta \in \Omega\}$ is the set of all distributions on $R^1$ and that $x_1, \ldots, x_n$ is a sample from some unknown distribution with cdf $F_\theta$. Then the \emph{empirical distribution function}
\[
\hat{F}(x) = \frac{1}{n} \sum_{i=1}^{n} \indc_{(-\infty, x]}(x_i),
\]
introduced in Section~\ref{ssec:5.4.1}, is a natural estimator of the cdf $F_\theta(x)$.

We have
\[
\expc_\theta(\hat{F}(x)) = \frac{1}{n} \sum_{i=1}^{n} \expc_\theta(\indc_{(-\infty, x]}(X_i)) = \frac{1}{n} \sum_{i=1}^{n} F_\theta(x) = F_\theta(x)
\]
for every $\theta$, so that $\hat{F}$ is unbiased for $F_\theta$. The weak and strong laws of large numbers then establish the consistency of $\hat{F}(x)$ for $F_\theta(x)$ as $n \to \infty$. Observing that the $\indc_{(-\infty, x]}(x_i)$ constitute a sample from the Bernoulli$(F_\theta(x))$ distribution, we have that the standard error of $\hat{F}(x)$ is given by
\[
\sqrt{\frac{\hat{F}(x)(1 - \hat{F}(x))}{n}}.
\]
These facts can be used to form approximate confidence intervals and test hypotheses for $F_\theta(x)$, just as in Examples~\ref{ex:6.3.7} and~\ref{ex:6.3.11}.

Observe that $\hat{F}(x)$ prescribes a distribution on the set $\{x_1, \ldots, x_n\}$, e.g., if the sample values are distinct, this probability distribution puts mass $1/n$ on each $x_i$. Note that it is easy to sample a value from $\hat{F}$, as we just select a value from $\{x_1, \ldots, x_n\}$ where each point has probability $1/n$ of occurring. When the $x_i$ are not distinct, then this is changed in an obvious way, namely, $x_i$ has probability $f_i/n$, where $f_i$ is the number of times $x_i$ occurs in $x_1, \ldots, x_n$.

Suppose we are interested in estimating $\psi(\theta) = T(F_\theta)$, where $T$ is a function of the distribution $F_\theta$. We use this notation to emphasize that $\psi(\theta)$ corresponds to some characteristic of the distribution rather than just being an arbitrary mathematical function of $\theta$. For example, $T(F_\theta)$ could be a moment of $F_\theta$, a quantile of $F_\theta$, etc.

Now suppose we have an estimator $\hat{\psi}(x_1, \ldots, x_n)$ that is being proposed for inferences about $\psi(\theta)$. Naturally, we are interested in the accuracy of $\hat{\psi}$, and we could choose to measure this by
\begin{equation}
\label{eq:6.4.2}
\text{MSE}_\theta(\hat{\psi}) = \expc_\theta[(\hat{\psi} - \psi(\theta))^2] = \var_\theta(\hat{\psi}) + (\expc_\theta(\hat{\psi}) - \psi(\theta))^2.
\end{equation}
Then, to assess the accuracy of our estimate $\hat{\psi}(x_1, \ldots, x_n)$, we need to estimate \eqref{eq:6.4.2}.

When $n$ is large, we expect $\hat{F}$ to be close to $F_\theta$, so a natural estimate of $\psi(\theta)$ is $T(\hat{F})$, i.e., simply compute the same characteristic of the empirical distribution. This is the approach adopted in Chapter~\ref{ch:5} when we discussed descriptive statistics. Then we estimate the square of the bias in $\hat{\psi}$ by
\begin{equation}
\label{eq:6.4.3}
(T(\hat{F}) - \hat{\psi})^2.
\end{equation}
To estimate the variance of $\hat{\psi}$, we use
\begin{equation}
\label{eq:6.4.4}
\widehat{\var}_{\hat{F}}(\hat{\psi}) = \expc_{\hat{F}}[(\hat{\psi} - \expc_{\hat{F}}(\hat{\psi}))^2] = \frac{1}{n^n} \sum_{i_1=1}^{n} \cdots \sum_{i_n=1}^{n} \hat{\psi}^2(x_{i_1}, \ldots, x_{i_n}) - \left(\frac{1}{n^n} \sum_{i_1=1}^{n} \cdots \sum_{i_n=1}^{n} \hat{\psi}(x_{i_1}, \ldots, x_{i_n})\right)^2,
\end{equation}
i.e., we treat $x_1, \ldots, x_n$ as i.i.d.\ random values with cdf given by $\hat{F}$. So to calculate an estimate of \eqref{eq:6.4.2}, we simply have to calculate $\widehat{\var}_{\hat{F}}(\hat{\psi})$. This is rarely feasible, however, because the sums in \eqref{eq:6.4.4} involve $n^n$ terms. For even very modest sample sizes, like $n = 10$, this cannot be carried out, even on a computer.

The solution to this problem is to approximate \eqref{eq:6.4.4} by drawing $m$ independent samples of size $n$ from $\hat{F}$, evaluating $\hat{\psi}$ for each of these samples to obtain $\hat{\psi}_1^*, \ldots, \hat{\psi}_m^*$, and then using the sample variance
\begin{equation}
\label{eq:6.4.5}
\widehat{\var}_{\hat{F}}(\hat{\psi}) = \frac{1}{m - 1} \sum_{i=1}^{m} \left(\hat{\psi}_i^* - \frac{1}{m} \sum_{i=1}^{m} \hat{\psi}_i^*\right)^2
\end{equation}
as the estimate. The $m$ samples from $\hat{F}$ are referred to as \emph{bootstrap samples} or \emph{resamples}, and this technique is referred to as \emph{bootstrapping} or \emph{resampling}. Combining \eqref{eq:6.4.3} and \eqref{eq:6.4.5} gives an estimate of $\text{MSE}_\theta(\hat{\psi})$. Furthermore, $m^{-1} \sum_{i=1}^{m} \hat{\psi}_i^*$ is called the \emph{bootstrap mean}, and
\[
\sqrt{\widehat{\var}_{\hat{F}}(\hat{\psi})}
\]
is the \emph{bootstrap standard error}. Note that the bootstrap standard error is a valid estimate of the error in $\hat{\psi}$ whenever $\hat{\psi}$ has little or no bias.

Consider the following example.

\begin{example}[The Sample Median as an Estimator of the Population Mean]
\label{ex:6.4.2}
Suppose we want to estimate the location of a unimodal, symmetric distribution. While the sample mean might seem like the obvious choice for this, it turns out that for some distributions there are better estimators. This is because the distribution we are sampling may have long tails, i.e., may produce extreme values that are far from the center of the distribution. This implies that the sample average itself could be highly influenced by a few extreme observations and would thus be a poor estimate of the true mean.

Not all estimators suffer from this defect. For example, if we are sampling from a symmetric distribution, then either the sample mean or the sample median could serve as an estimator of the population mean. But, as we have previously discussed, the sample median is not influenced by extreme values, i.e., it does not change as we move the smallest (or largest) values away from the rest of the data, and this is not the case for the sample mean.

A problem with working with the sample median $\hat{x}_{0.5}$ rather than the sample mean $\bar{x}$ is that the sampling distribution for $\hat{x}_{0.5}$ is typically more difficult to study than that of $\bar{x}$. In this situation, bootstrapping becomes useful. If we are estimating the population mean $T(F_\theta)$ by using the sample median (which is appropriate when we know the distribution we were sampling from is symmetric), then the estimate of the squared bias in the sample median is given by
\[
(T(\hat{F}) - \hat{\psi})^2 = (\hat{x}_{0.5} - \bar{x})^2
\]
because $\hat{x}_{0.5} = \hat{\psi}$ and $T(\hat{F}) = \bar{x}$ (the mean of the empirical distribution is $\bar{x}$). This should be close to 0, or else our assumption of a symmetric distribution would seem to be incorrect. To calculate \eqref{eq:6.4.5}, we have to generate $m$ samples of size $n$ from $\{x_1, \ldots, x_n\}$ (with replacement) and calculate $\hat{x}_{0.5}$ for each sample.

To illustrate, suppose we have a sample of size $n = 15$ given by the following table.
\begin{center}
\begin{tabular}{ccccc}
$-2.0$ & $-0.2$ & $-5.2$ & $-3.5$ & $-3.9$ \\
$-0.6$ & $-4.3$ & $-1.7$ & $9.5$ & $-1.6$ \\
$-2.9$ & $-0.9$ & $-1.0$ & $-2.0$ & $-3.0$
\end{tabular}
\end{center}
Then, using the definition of $\hat{x}_{0.5}$ given by \eqref{eq:5.5.4} (denoted $\hat{x}_{0.5}$ there), $\hat{\psi} = -2.000$ and $\bar{x} = -2.087$. The estimate of the squared bias \eqref{eq:6.4.3} equals $(-2.000 - (-2.087))^2 = 7.569 \times 10^{-3}$, which is appropriately small. Using a statistical package, we generated $m = 10^3$ samples of size $n = 15$ from the distribution that has probability $1/15$ at each of the sample points and obtained
\[
\widehat{\var}_{\hat{F}}(\hat{\psi}) = 0.770866.
\]
Based on $m = 10^4$ samples, we obtained
\[
\widehat{\var}_{\hat{F}}(\hat{\psi}) = 0.718612,
\]
and based on $m = 10^5$ samples we obtained
\[
\widehat{\var}_{\hat{F}}(\hat{\psi}) = 0.704928.
\]
Because these estimates appear to be stabilizing, we take this as our estimate. So in this case, the bootstrap estimate of the MSE of the sample median at the true value of $\theta$ is given by
\[
\widehat{\text{MSE}}(\hat{\psi}) = 0.007569 + 0.704928 = 0.71250.
\]
Note that the estimated MSE of the sample average is given by $s^2 = 0.62410$, so the sample mean and sample median appear to be providing similar accuracy in this problem. In Figure~\ref{fig:6.4.1}, we have plotted a density histogram of the sample medians obtained from the $m = 10^5$ bootstrap samples. Note that the histogram is very skewed. See Appendix B for more details on how these computations were carried out.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_4_1.pdf}
  \caption{A density histogram of $m = 10^5$ sample medians, each obtained from a bootstrap sample of size $n = 15$ from the data in Example~\ref{ex:6.4.2}.}
  \label{fig:6.4.1}
\end{figure}

Even with the very small sample size here, it was necessary to use the computer to carry out our calculations. To evaluate \eqref{eq:6.4.4} exactly would have required computing the median of $15^{15}$ (roughly $4.4 \times 10^{17}$) samples, which is clearly impossible even using a computer. So the bootstrap is a very useful device.

The validity of the bootstrapping technique depends on $\hat{\psi}$ having its first two moments. So the family $\{\prb_\theta : \theta \in \Omega\}$ must be appropriately restricted, but we can see that the technique is very general.

Broadly speaking, it is not clear how to choose $m$. Perhaps the most direct method is to implement bootstrapping for successively higher values of $m$ and stop when we see that the results stabilize for several values. This is what we did in Example~\ref{ex:6.4.2}, but it must be acknowledged that this approach is not foolproof, as we could have a sample $x_1, \ldots, x_n$ such that the estimate \eqref{eq:6.4.5} is very slowly convergent.

\subsubsection*{Bootstrap Confidence Intervals}

Bootstrap methods have also been devised to obtain approximate $\gamma$-confidence intervals for characteristics such as $\psi(\theta) = T(F_\theta)$. One very simple method is to simply form the \emph{bootstrap $t$ $\gamma$-confidence interval}
\[
\hat{\psi} \pm t_{(1+\gamma)/2}(n - 1) \sqrt{\widehat{\var}_{\hat{F}}(\hat{\psi})},
\]
where $t_{(1+\gamma)/2}(n - 1)$ is the $(1 + \gamma)/2$th quantile of the $t(n - 1)$ distribution. Another possibility is to compute a \emph{bootstrap percentile confidence interval} given by
\[
(\hat{\psi}^*_{(1-\gamma)/2}, \hat{\psi}^*_{(1+\gamma)/2}),
\]
where $\hat{\psi}^*_p$ denotes the $p$th empirical quantile of $\hat{\psi}^*$ in the bootstrap sample of $m$.

It should be noted that to be applicable, these intervals require some conditions to hold. In particular, $\hat{\psi}$ should be at least approximately unbiased for $\psi(\theta)$ and the bootstrap distribution should be approximately normal. Looking at the plot of the bootstrap distribution in Figure~\ref{fig:6.4.1} we can see that the median does not have an approximately normal bootstrap distribution, so these intervals are not applicable with the median.

Consider the following example.

\begin{example}[The 0.25-Trimmed Mean as an Estimator of the Population Mean]
\label{ex:6.4.3}
One of the virtues of the sample median as an estimator of the population mean is that it is not affected by extreme values in the sample. On the other hand, the sample median discards all but one or two of the data values and so seems to be discarding a lot of information. Estimators known as \emph{trimmed means} can be seen as an attempt at retaining the virtues of the median while at the same time not discarding too much information. Let $\lfloor x \rfloor$ denote the greatest integer less than or equal to $x \in R^1$.

\begin{definition}
\label{def:6.4.1}
For $\alpha \in [0, 1]$, a \emph{sample $\alpha$-trimmed mean} is given by
\[
\bar{x}_\alpha = \frac{1}{n - 2\lfloor n\alpha \rfloor} \sum_{i=\lfloor n\alpha \rfloor + 1}^{n - \lfloor n\alpha \rfloor} x_{(i)},
\]
where $x_{(i)}$ is the $i$th-order statistic.
\end{definition}

Thus for a sample $\alpha$-trimmed mean, we toss out (approximately) $n\alpha$ of the smallest data values and $n\alpha$ of the largest data values and calculate the average of the $n - 2\lfloor n\alpha \rfloor$ of the data values remaining. We need the greatest integer function because in general, $n\alpha$ will not be an integer. Note that the sample mean arises with $\alpha = 0$ and the sample median arises with $\alpha = 0.5$.

For the data in Example~\ref{ex:6.4.1} and $\alpha = 0.25$, we have $\lfloor 0.25 \times 15 \rfloor = 3.75$, so we discard the three smallest and three largest observations leaving the nine data values $-3.9$, $-3.5$, $-2.9$, $-2.0$, $-2.0$, $-1.7$, $-1.0$, $-0.6$, $-0.2$. The average of these nine values gives $\bar{x}_{0.25} = -1.97778$, which we note is close to both the sample median and the sample mean.

Now suppose we use a 0.25-trimmed mean as an estimator of a population mean $\mu$ where we believe the population distribution is symmetric. Consider the data in Example~\ref{ex:6.4.1} and suppose we generated $m = 10^4$ bootstrap samples. We have plotted a histogram of the $10^4$ values of $\hat{\psi}$ in Figure~\ref{fig:6.4.2}. Notice that it is very normal looking, so we feel justified in using the confidence intervals associated with the bootstrap. In this case, we obtained
\[
\sqrt{\widehat{\var}_{\hat{F}}(\hat{\psi})} = 0.7380,
\]
so the bootstrap $t$ 0.95-confidence interval for the mean $\mu$ is given by $-1.97778 \pm 2.14479 \times 0.7380 = (-3.6\ldots, 0.4\ldots)$. Sorting the bootstrap sample gives a bootstrap percentile 0.95-confidence interval as $(-3.36667, 0.488889) = (-3.4\ldots, 0.5\ldots)$, which shows that the two intervals are very similar.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_4_2.pdf}
  \caption{A density histogram of $m = 10^4$ sample 0.25-trimmed means, each obtained from a bootstrap sample of size $n = 15$ from the data in Example~\ref{ex:6.4.3}.}
  \label{fig:6.4.2}
\end{figure}

More details about the bootstrap can be found in \emph{An Introduction to the Bootstrap}, by B.\ Efron and R.\,J.\ Tibshirani (Chapman and Hall, New York, 1993).

\subsection{The Sign Statistic and Inferences about Quantiles}
\label{ssec:6.4.3}

Suppose that $\{\prb_\theta : \theta \in \Omega\}$ is the set of all distributions on $R^1$ such that the associated distribution functions are continuous. Suppose we want to make inferences about a $p$th quantile of $\prb_\theta$. We denote this quantile by $x_p$ so that, when the distribution function associated with $\prb_\theta$ is denoted by $F_\theta$, we have $p = F_\theta(x_p)$. Note that continuity implies there is always a solution in $x$ to $p = F_\theta(x)$ and that $x_p$ is the smallest solution.

Recall the definitions and discussion of estimation of these quantities in Example~\ref{ex:5.5.2} based on a sample $x_1, \ldots, x_n$. For simplicity, let us restrict attention to the cases where $p = i/n$ for some $i \in \{1, \ldots, n\}$. In this case, we have that $\hat{x}_p = x_{(i)}$ is the natural estimate of $x_p$.

Now consider assessing the evidence in the data concerning the hypothesis $H_0 : x_p = x_0$. For testing this hypothesis, we can use the \emph{sign test statistic}, given by $S = \sum_{i=1}^{n} \indc_{(-\infty, x_0]}(x_i)$. So $S$ is the number of sample values less than or equal to $x_0$.

Notice that when $H_0$ is true, $(\indc_{(-\infty, x_0]}(x_1), \ldots, \indc_{(-\infty, x_0]}(x_n))$ is a sample from the Bernoulli$(p)$ distribution. This implies that, when $H_0$ is true, $S \sim \text{Binomial}(n, p)$.

Therefore, we can test $H_0$ by computing the observed value of $S$, denoted $S_o$, and seeing whether this value lies in a region of low probability for the Binomial$(n, p)$ distribution. Because the binomial distribution is unimodal, the regions of low probability correspond to the left and right tails of this distribution. See, for example, Figure~\ref{fig:6.4.3}, where we have plotted the probability function of a Binomial$(20, 0.7)$ distribution.

The P-value is therefore obtained by computing the probability of the set
\begin{equation}
\label{eq:6.4.6}
\left\{i : \binom{n}{i} p^i (1 - p)^{n-i} \leqslant \binom{n}{S_o} p^{S_o} (1 - p)^{n - S_o}\right\}
\end{equation}
using the Binomial$(n, p)$ probability distribution. This is a measure of how far out in the tails the observed value $S_o$ is (see Figure~\ref{fig:6.4.3}). Notice that this P-value is completely independent of $\theta$ and is thus valid for the entire model. Tables of binomial probabilities (Table D.6 in Appendix D), or built-in functions available in most statistical packages, can be used to calculate this P-value.

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_4_3.pdf}
  \caption{Plot of the Binomial$(20, 0.7)$ probability function.}
  \label{fig:6.4.3}
\end{figure}

When $n$ is large, we have that, under $H_0$,
\[
Z = \frac{S - np}{\sqrt{np(1 - p)}} \xrightarrow{D} N(0, 1)
\]
as $n \to \infty$. Therefore, an approximate P-value is given by
\[
2\left(1 - \Phi\left(\frac{|S_o - 0.5 - np|}{\sqrt{np(1 - p)}}\right)\right)
\]
(as in Example~\ref{ex:6.3.11}), where we have replaced $S_o$ by $S_o - 0.5$ as a correction for continuity (see Example~\ref{ex:4.4.9} for discussion of the correction for continuity).

A special case arises when $p = 1/2$, i.e., when we are making inferences about an unknown population median $x_{0.5}$. In this case, the distribution of $S$ under $H_0$ is Binomial$(n, 1/2)$. Because the Binomial$(n, 1/2)$ is unimodal and symmetrical about $n/2$, \eqref{eq:6.4.6} becomes
\[
\{i : |S_o - n/2| \leqslant |i - n/2|\}.
\]
If we want a $\gamma$-confidence interval for $x_{0.5}$, then we can use the equivalence between tests, which we always reject when the P-value is less than or equal to $1 - \gamma$, and $\gamma$-confidence intervals (see Example~\ref{ex:6.3.12}). For this, let $j$ be the smallest integer greater than $n/2$ satisfying
\begin{equation}
\label{eq:6.4.7}
P(\{i : |i - n/2| \geqslant j - n/2\}) \leqslant 1 - \gamma
\end{equation}
where $P$ is the Binomial$(n, 1/2)$ distribution. If $S \in \{i : |i - n/2| \geqslant j - n/2\}$, we will reject $H_0 : x_{0.5} = x_0$ at the $1 - \gamma$ level and will not otherwise. This leads to the $\gamma$-confidence interval, namely, the set of all those values $x_{0.5}$ such that the null hypothesis $H_0 : x_{0.5} = x_{0.5}$ is not rejected at the $1 - \gamma$ level, equaling
\begin{equation}
\label{eq:6.4.8}
C(x_1, \ldots, x_n) = \left\{x_0 : \left|\sum_{i=1}^{n} \indc_{(-\infty, x_0]}(x_i) - n/2\right| < j - n/2\right\} = \left\{x_0 : n - j < \sum_{i=1}^{n} \indc_{(-\infty, x_0]}(x_i) < j\right\} = [x_{(n-j+1)}, x_{(j)}]
\end{equation}
because, for example, $n - j < \sum_{i=1}^{n} \indc_{(-\infty, x_0]}(x_i)$ if and only if $x_0 \geqslant x_{(n-j+1)}$.

\begin{example}[Application of the Sign Test]
\label{ex:6.4.4}
Suppose we have the following sample of size $n = 10$ from a continuous random variable $X$ and we wish to test the hypothesis $H_0 : x_{0.5} = 0$.
\begin{center}
\begin{tabular}{ccccc}
$-0.44$ & $0.06$ & $-0.43$ & $0.16$ & $2.13$ \\
$-1.15$ & $1.08$ & $5.67$ & $-4.97$ & $-0.11$
\end{tabular}
\end{center}
The boxplot in Figure~\ref{fig:6.4.4} indicates that it is very unlikely that this sample came from a normal distribution, as there are two extreme observations. So it is appropriate to measure the location of the distribution of $X$ by the median.
\end{example}

\begin{figure}[!htbp]
  \centering
  %\includegraphics[scale=0.5]{fig6_4_4.pdf}
  \caption{Boxplot of the data in Example~\ref{ex:6.4.4}.}
  \label{fig:6.4.4}
\end{figure}

In this case, the sample median (using \eqref{eq:5.5.4}) is given by $(-0.11 + 0.43)/2 = 0.27$. The sign statistic for the null is given by
\[
S = \sum_{i=1}^{10} \indc_{(-\infty, 0]}(x_i) = 4.
\]
The P-value is given by
\begin{align*}
P(\{i : |4 - 5| \leqslant |i - 5|\}) &= P(\{i : |i - 5| \geqslant 1\}) = 1 - P(\{i : |i - 5| < 1\}) \\
&= 1 - P(\{5\}) = 1 - \binom{10}{5} \left(\frac{1}{2}\right)^{10} = 1 - 0.24609 = 0.75391,
\end{align*}
and we have no reason to reject the null hypothesis.

Now suppose that we want a 0.95-confidence interval for the median. Using software (or Table D.6), we calculate
\begin{align*}
\binom{10}{5} \left(\frac{1}{2}\right)^{10} &= 0.24609, \quad \binom{10}{4} \left(\frac{1}{2}\right)^{10} = 0.20508, \\
\binom{10}{3} \left(\frac{1}{2}\right)^{10} &= 0.11719, \quad \binom{10}{2} \left(\frac{1}{2}\right)^{10} = 4.3945 \times 10^{-2}, \\
\binom{10}{1} \left(\frac{1}{2}\right)^{10} &= 9.7656 \times 10^{-3}, \quad \binom{10}{0} \left(\frac{1}{2}\right)^{10} = 9.7656 \times 10^{-4}.
\end{align*}
We will use these values to compute the value of $j$ in \eqref{eq:6.4.7}.

We can use the symmetry of the Binomial$(10, 1/2)$ distribution about $n/2$ to compute the values of $P(\{i : |i - n/2| \geqslant j - n/2\})$ as follows. For $j = 10$ we have that \eqref{eq:6.4.7} equals
\[
P(\{i : |i - 5| \geqslant 5\}) = P(\{0, 10\}) = 2 \binom{10}{0} \left(\frac{1}{2}\right)^{10} = 1.9531 \times 10^{-3}
\]
and note that $1.9531 \times 10^{-3} < 1 - 0.95 = 0.05$. For $j = 9$ we have that \eqref{eq:6.4.7} equals
\[
P(\{i : |i - 5| \geqslant 4\}) = P(\{0, 1, 9, 10\}) = 2 \binom{10}{0} \left(\frac{1}{2}\right)^{10} + 2 \binom{10}{1} \left(\frac{1}{2}\right)^{10} = 2.148\ldots4 \times 10^{-2}
\]
which is also less than 0.05. For $j = 8$ we have that \eqref{eq:6.4.7} equals
\begin{align*}
P(\{i : |i - 5| \geqslant 3\}) &= P(\{0, 1, 2, 8, 9, 10\}) \\
&= 2 \binom{10}{0} \left(\frac{1}{2}\right)^{10} + 2 \binom{10}{1} \left(\frac{1}{2}\right)^{10} + 2 \binom{10}{2} \left(\frac{1}{2}\right)^{10} = 0.10938
\end{align*}
and this is greater than 0.05. Therefore, the appropriate value is $j = 9$ and a 0.95-confidence interval for the median is given by $[x_{(2)}, x_{(9)}] = [-0.16, 1.15]$.

There are many other distribution-free methods for a variety of statistical situations. While some of these are discussed in the problems, we leave a thorough study of such methods to further courses in statistics.

\subsection*{Summary of Section~\ref{sec:6.4}}

\begin{itemize}
\item Distribution-free methods of statistical inference are appropriate methods when we feel we can make only very minimal assumptions about the distribution from which we are sampling.
\item The method of moments, bootstrapping, and methods of inference based on the sign statistic are three distribution-free methods that are applicable in different circumstances.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:6.4.1}
Suppose we obtained the following sample from a distribution that we know has its first six moments. Determine an approximate 0.95-confidence interval for $\mu_3$.
\begin{center}
\begin{tabular}{cccccccccc}
3.27 & 1.24 & 3.97 & 2.25 & 3.47 & 0.09 & 7.45 & 6.20 & 3.74 & 4.12 \\
1.42 & 2.75 & 1.48 & 4.97 & 8.00 & 3.26 & 0.15 & 3.64 & 4.88 & 4.55
\end{tabular}
\end{center}
\end{exercise}

\begin{solution}
An approximate 0.95-confidence interval for $\mu_3$ is given by
\[
    m_3 \pm z_{(1+\gamma)/2} \frac{s_3}{\sqrt{n}} = (26.027, 151.373)
\]
since $m_3 = 88.7$, $z_{0.975} = 1.96$, and $s_3 = 143.0$.
\end{solution}

\begin{exercise}
\label{exer:6.4.2}
Determine the method of moments estimator of the population variance. Is this estimator unbiased for the population variance? Justify your answer.
\end{exercise}

\begin{solution}
Recall that, the variance of a random variable can be expressed in terms of the moments as $\sigma_X^2 = \mu_2 - \mu_1^2$. Hence, the method of moments estimator of the population variance is given by $\hat{\sigma}_X^2 = m_2 - m_1^2$. To check if this estimator is unbiased we compute
\[
    \expc(m_2 - m_1^2) = \mu_2 - \left( \var(m_1) + \expc^2(m_1) \right) = \mu_2 - \left( \frac{1}{n}(\mu_2 - \mu_1^2) + \mu_1^2 \right) = \left( 1 - \frac{1}{n} \right) \sigma_X^2.
\]
Hence, this estimator is not unbiased.
\end{solution}

\begin{exercise}[Coefficient of variation]
\label{exer:6.4.3}
The coefficient of variation for a population measurement with nonzero mean is given by $\sigma/\mu$ where $\mu$ is the population mean and $\sigma$ is the population standard deviation. What is the method of moments estimate of the coefficient of variation? Prove that the coefficient of variation is invariant under rescalings of the distribution, i.e., under transformations of the form $T(x) = cx$ for constant $c > 0$. It is this invariance that leads to the coefficient of variation being an appropriate measure of sampling variability in certain problems, as it is independent of the units we use for the measurement.
\end{exercise}

\begin{solution}
The method of moments estimator of the coefficient of variation of a random variable $X$ is $\sqrt{m_2 - m_1^2}/m_1$. Now let $Y = cX$. Then $\expc(Y) = c\expc(X)$ and $\var(Y) = c^2 \var(X)$. Therefore, the coefficient of variation of $Y$ is
\[
    \frac{c \, \text{Sd}(X)}{c \expc(X)} = \frac{\text{Sd}(X)}{\expc(X)}
\]
which is the coefficient of variation of $X$.
\end{solution}

\begin{exercise}
\label{exer:6.4.4}
For the context described in Exercise~\ref{exer:6.4.1}, determine an approximate 0.95-confidence interval for $\exp(\mu_1)$.
\end{exercise}

\begin{solution}
Let $\psi(\mu) = \exp(\mu)$ then $\psi'(\mu) = \exp(\mu)$. By the delta theorem~\eqref{eq:6.4.1}, an approximate $\gamma$-confidence interval for $\psi(\mu)$ is given by
\[
    \exp \bar{x} \pm \frac{s \exp(\bar{x})}{\sqrt{n}} z_{(1+\gamma)/2} = \exp(2.9) \pm \frac{2.997 \exp(2.9)}{\sqrt{20}} \cdot 1.96 = (-5.6975, 42.046).
\]
\end{solution}

\begin{exercise}
\label{exer:6.4.5}
Verify that the third moment of an $N(\mu, \sigma^2)$ distribution is given by $\mu_3 = \mu^3 + 3\mu\sigma^2$. Because the normal distribution is specified by its first two moments, any characteristic of the normal distribution can be estimated by simply plugging in the MLE estimates of $\mu$ and $\sigma^2$. Compare the method of moments estimator of $\mu_3$ with this plug-in MLE estimator, i.e., determine whether they are the same or not.
\end{exercise}

\begin{solution}
Recall from Problem~\ref{exer:3.4.15} that the moment generating function of a $X \sim N(\mu, \sigma^2)$ is given by $m_X(s) = \exp(\mu s + \sigma^2 s^2/2)$. Then, by Theorem~\ref{thm:3.4.3} the third moment is given by
\[
    m_X'''(0) = \left. 3\sigma^2(\mu + \sigma^2 s) e^{\mu s + \frac{1}{2}\sigma^2 s^2} + (\mu + \sigma^2 s)^3 e^{\mu s + \frac{1}{2}\sigma^2 s^2} \right|_{s=0} = 3\sigma^2 \mu + \mu^3.
\]
The plug-in estimator of $\mu_3$ is given by $\hat{\mu}_3 = 3(m_2 - m_1^2)m_1 + m_1^3$, while the method of moments estimator of $\mu_3$ is $m_3 = \frac{1}{n} \sum x_i^3$. So these estimators are different.
\end{solution}

\begin{exercise}
\label{exer:6.4.6}
Suppose we have the sample data 1.48, 4.10, 2.02, 56.59, 2.98, 1.51, 76.49, 50.25, 43.52, 2.96. Consider this as a sample from a normal distribution with unknown mean and variance, and assess the hypothesis that the population median (which is the same as the mean in this case) is 3. Also carry out a sign test that the population median is 3 and compare the results. Plot a boxplot for these data. Does this support the assumption that we are sampling from a normal distribution? Which test do you think is more appropriate? Justify your answer.
\end{exercise}

\begin{solution}
The $t$-statistic for testing $H_0 : \mu = 3$ is 0.47 and the $P$-value (based on 9 df) is 0.650. Hence, we do not have evidence against $H_0$.

To test the hypothesis $H_0 : x_{0.5}(\theta) = 3$ using the sign test statistic, we have $S = \sum_{i=1}^{n} \indc_{(-\infty, 3]}(x_i) = 5$. The $P$-value is given by $\prb(\{i : |i - 5| \geqslant 0\}) = 1$. Therefore, we do not have evidence against $H_0$.

The following boxplot of the data indicates that the normal assumption is a problem, as it is strongly skewed to the right. Under these circumstances we prefer the sign test.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig_6_4_6.pdf}
    \caption{Boxplot for Exercise 6.4.6}
    %\label{fig:boxplot-6.4.6}
\end{figure}
\end{solution}

\begin{exercise}
\label{exer:6.4.7}
Determine the empirical distribution function based on the sample given below.
\begin{center}
\begin{tabular}{ccccc}
$-1.06$ & 1.28 & 0.40 & $-1.36$ & 0.35 \\
$-1.42$ & 0.44 & 0.58 & 0.24 & 1.34 \\
0.00 & 1.02 & 1.35 & 2.05 & 1.06 \\
0.98 & 0.38 & 2.13 & 0.03 & $-1.29$
\end{tabular}
\end{center}
Using the empirical cdf, determine the sample median, the first and third quartiles, and the interquartile range. What is your estimate of $F(2)$?
\end{exercise}

\begin{solution}
The empirical cdf is given by the following table. The sample median is estimated by $-0.03$ and the first quartile is $-1.28$, while the third quartile is $0.98$. The value $F(2)$ is estimated by $\hat{F}(2) = \hat{F}(1.36) = 0.90$.

\begin{center}
\begin{tabular}{ccc}
$i$ & $x_{(i)}$ & $\hat{F}(x_{(i)})$ \\
\hline
1 & $-1.42$ & 0.05 \\
2 & $-1.35$ & 0.10 \\
3 & $-1.34$ & 0.15 \\
4 & $-1.29$ & 0.20 \\
5 & $-1.28$ & 0.25 \\
6 & $-1.02$ & 0.30 \\
7 & $-0.58$ & 0.35 \\
8 & $-0.35$ & 0.40 \\
9 & $-0.24$ & 0.45 \\
10 & $-0.03$ & 0.50
\end{tabular}
\quad
\begin{tabular}{ccc}
$i$ & $x_{(i)}$ & $\hat{F}(x_{(i)})$ \\
\hline
11 & 0.00 & 0.55 \\
12 & 0.38 & 0.60 \\
13 & 0.40 & 0.65 \\
14 & 0.44 & 0.70 \\
15 & 0.98 & 0.75 \\
16 & 1.06 & 0.80 \\
17 & 1.06 & 0.85 \\
18 & 1.36 & 0.90 \\
19 & 2.05 & 0.95 \\
20 & 2.13 & 1.00
\end{tabular}
\end{center}
\end{solution}

\begin{exercise}
\label{exer:6.4.8}
Suppose you obtain the sample of $n = 3$ distinct values given by 1, 2, and 3.
\begin{enumerate}[(a)]
\item Write down all possible bootstrap samples.
\item If you are bootstrapping the sample median, what are the possible values for the sample median for a bootstrap sample?
\item If you are bootstrapping the sample mean, what are the possible values for the sample mean for a bootstrap sample?
\item What do you conclude about the bootstrap distribution of the sample median compared to the bootstrap distribution of the sample mean?
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item Bootstrap samples are resamples from $\{1, 2, 3\}$ with replacement. Hence, $\{1, 2, 3\}^3$ is all the possible bootstrap samples.
    
    \item Since the sample size $n = 3$ is an odd number, the sample median is a number in the resample. Hence, all the possible sample medians are 1, 2, and 3.
    
    \item Let $T$ be the sum of the resampled numbers. The smallest $T$ is 3 when $(1, 1, 1)$ is sampled and the maximum is obtained if $(3, 3, 3)$ is resampled. Besides, all integer values between 3 and 9 are obtainable (consider $(1, 1, 2)$, $(1, 1, 3)$, $(1, 2, 3)$, $(1, 3, 3)$ and $(2, 3, 3)$). Hence, the possible resample means are the values of $T/3$, i.e., $t/3$ for $t = 3, \ldots, 9$.
    
    \item The sample median has only 3 possible values and the sample mean has 7 possible values. Neither of them is large enough to have an asymptotic normality. Any estimate or confidence interval based on asymptotic normality of bootstrap samples is not acceptable for this problem.
\end{enumerate}
\end{solution}

\begin{exercise}
\label{exer:6.4.9}
Explain why the central limit theorem justifies saying that the bootstrap distribution of the sample mean is approximately normal when $n$ and $m$ are large. What result justifies the approximate normality of the bootstrap distribution of a function of the sample mean under certain conditions?
\end{exercise}

\begin{solution}
When $n$ is large then the distribution of the sample mean is approximately normal. When $n$ and $m$ are both large then the bootstrap procedure is sampling from a discrete distribution and by the CLT the distribution of the bootstrap mean is approximately normal.

The delta theorem justifies the approximate normality of functions of the sample or bootstrap mean.
\end{solution}

\begin{exercise}
\label{exer:6.4.10}
For the data in Exercise~\ref{exer:6.4.1}, determine an approximate 0.95-confidence interval for the population median when we assume the distribution we are sampling from is symmetric with finite first and second moments. (Hint: Use large sample results.)
\end{exercise}

\begin{solution}
If the distribution is symmetric, then the median is exactly the same as the mean, i.e., $\psi(\theta) = \text{median}(F_\theta) = \expc_\theta(X)$. By the central limit theorem, $\sqrt{n}(\bar{x} - \psi(\theta)) \xrightarrow{D} N(0, \sigma_\theta^2)$ as $n \to \infty$. Thus, an approximate $\gamma$-confidence interval is given by $(\bar{x} - z_{(1+\gamma)/2} s/\sqrt{n}, \bar{x} + z_{(1+\gamma)/2} s/\sqrt{n})$ where $s^2 = (n-1)^{-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$. From the data in Exercise~\ref{exer:6.4.1}, $\bar{x} = 2.9$ and $s^2 = 8.9839$. From Table D.2, $z_{0.975} = 1.96$. Hence, the approximate 0.95-confidence interval is $(1.5864, 4.2136)$.
\end{solution}

\begin{exercise}
\label{exer:6.4.11}
Suppose you have a sample of $n$ distinct values and are interested in the bootstrap distribution of the sample range given by $x_{(n)} - x_{(1)}$. What is the maximum number of values that this statistic can take over all bootstrap samples? What are the largest and smallest values that the sample range can take in a bootstrap sample? Do you think the bootstrap distribution of the sample range will be approximately normal? Justify your answer.
\end{exercise}

\begin{solution}
Let $y_1, \ldots, y_n$ be a random sample from $\text{Uniform}(\{x_1, \ldots, x_n\})$. The number of values that can arise from bootstrap samples is equal to the number of values $|x_i - x_j|$ for $1 \leqslant i, j \leqslant n$. Hence, the maximum number of possible values is $1 + \binom{n}{2} = 1 + n(n-1)/2$. Here, 0 is obtained when $i = j$. The sample range $y_{(n)} - y_{(1)}$ has the largest value $x_{(n)} - x_{(1)}$ when $x_{(1)}, x_{(n)}$ are sampled, in other words, $y_i = x_{(1)}$ and $y_j = x_{(n)}$ for some $i$ and $j$. The smallest sample range value of 0 is obtained when $y_i = x_{(k)}$ and $y_j = x_{(k)}$ for some $i$, $j$ and $k$.

If there are many repeated $x_i$ values in the bootstrap sample, then the value 0 will occur with high probability for $y_{(n)} - y_{(1)}$ and so the bootstrap distribution of the sample range will not be approximately normal.
\end{solution}

\begin{exercise}
\label{exer:6.4.12}
Suppose you obtain the data $-1.1$, $-1.0$, $-1.1$, $-3.1$, $-2.2$, and $-3.1$. How many distinct bootstrap samples are there?
\end{exercise}

\begin{solution}
Every bootstrap sample is a subset of $\{x_1, \ldots, x_n\}^n$. Hence, the number of distinct bootstrap samples is $|\{x_1, \ldots, x_n\}|^n$ in general. Thus,
\[
    |\{1.1, -1.0, 1.1, 3.1, 2.2, 3.1\}|^6 = 4^6 = 4096
\]
samples are possible.
\end{solution}

\subsection*{Computer Exercises}

\begin{exercise}
\label{exer:6.4.13}
For the data of Exercise~\ref{exer:6.4.7}, assess the hypothesis that the population median is 0. State a 0.95-confidence interval for the population median. What is the exact coverage probability of this interval?
\end{exercise}

\begin{solution}
To test the hypothesis $H_0 : x_{0.5}(\theta) = 0$ the sign test statistic is given by $S = \sum_{i=1}^{n} \indc_{(-\infty, 0]}(x_i) = 10$. The $P$-value is given by $\prb(\{i : |i - 10| \geqslant 0\}) = 1$. Hence, we do not have any evidence against $H_0$.

We have that
\begin{align*}
    \prb(\{i : |i - 10| \geqslant 10\}) &= \binom{20}{0} \left( \frac{1}{2} \right)^{20} = 1.9073 \times 10^{-6} \\
    \prb(\{i : |i - 10| \geqslant 9\}) &= 2\binom{20}{0} \left( \frac{1}{2} \right)^{20} + 2\binom{20}{1} \left( \frac{1}{2} \right)^{20} = 4.0054 \times 10^{-5} \\
    \prb(\{i : |i - 10| \geqslant 8\}) &= 2\binom{20}{0} \left( \frac{1}{2} \right)^{20} + 2\binom{20}{1} \left( \frac{1}{2} \right)^{20} + 2\binom{20}{2} \left( \frac{1}{2} \right)^{20} = 4.0245 \times 10^{-4} \\
    \prb(\{i : |i - 10| \geqslant 7\}) &= \sum_{j=0}^{3} 2\binom{20}{j} \left( \frac{1}{2} \right)^{20} = 2.5768 \times 10^{-3} \\
    \prb(\{i : |i - 10| \geqslant 6\}) &= \sum_{j=0}^{4} 2\binom{20}{j} \left( \frac{1}{2} \right)^{20} = 1.1818 \times 10^{-2} \\
    \prb(\{i : |i - 10| \geqslant 5\}) &= \sum_{j=0}^{5} 2\binom{20}{j} \left( \frac{1}{2} \right)^{20} = 4.1389 \times 10^{-2} \\
    \prb(\{i : |i - 10| \geqslant 4\}) &= \sum_{j=0}^{6} 2\binom{20}{j} \left( \frac{1}{2} \right)^{20} = 0.11532.
\end{align*}
Therefore, $j = 15$ and a 0.95-confidence interval is given by $[x_{(6)}, x_{(15)}) = [-1.02, 0.98)$. The exact coverage probability of this interval is $1 - 4.1389 \times 10^{-2} = 0.95861$.
\end{solution}

\begin{exercise}
\label{exer:6.4.14}
For the data of Exercise~\ref{exer:6.4.7}, assess the hypothesis that the first quartile of the distribution we are sampling from is $-1.0$.
\end{exercise}

\begin{solution}
To test the hypothesis $H_0 : x_{0.25}(\theta) = -1.0$ the sign test statistic is given by $S_0 = \sum_{i=1}^{n} \indc_{(-\infty, -1.0]}(x_i) = 6$. The $P$-value, using~\eqref{eq:6.4.6}, is given by $\prb(\{i : \binom{20}{i}(0.25)^i(0.75)^{20-i} \leqslant \binom{20}{6}(0.25)^6(0.75)^{14}\})$, and a tabulation of the $\text{Binomial}(20, 0.25)$ probability function reveals that this set is given by all the points except $\{5, 4\}$, so the $P$-value is given by $1 - \binom{20}{5}(0.25)^5(0.75)^{15} - \binom{20}{4}(0.25)^4(0.75)^{16} = 0.60798$ and we have no evidence against $H_0$.
\end{solution}

\begin{exercise}
\label{exer:6.4.15}
With a bootstrap sample size of $m = 1000$, use bootstrapping to estimate the MSE of the plug-in MLE estimator of $\mu_3$ for the normal distribution, using the sample data in Exercise~\ref{exer:6.4.1}. Determine whether $m = 1000$ is a large enough sample for accurate results.
\end{exercise}

\begin{solution}
The characteristic of the distribution we are interested in is $\psi(\theta) = T(F_\theta) = \mu_3$, which we estimate by $T(\hat{F}) = m_3 = 88.7442$. We want to estimate the MSE of the plug-in MLE of $\mu_3$, which is given by $\hat{\psi} = m_1^3 + 3m_1 s^2 = (2.9)^3 + 3(2.9)(2.997)^2 = 102.53$. First, the squared bias in this estimator is given by $(\hat{\psi} - T(\hat{F}))^2 = (m_1^3 + 3m_1 \hat{\sigma}^2 - m_3)^2 = (102.53 - 88.7442)^2 = 190.05$. Next, based on $10^3$ samples, we obtained $\widehat{\var}_{\hat{F}}(\hat{\psi}) = 956.598$. Hence, $\widehat{\text{MSE}}(\hat{\psi}) = 190.05 + 956.598 = 1146.65$. Note that, based on $10^4$ samples, we obtained $\widehat{\var}_{\hat{F}}(\hat{\psi}) = 981.057$ and, based on $10^5$ samples, we obtained $\widehat{\var}_{\hat{F}}(\hat{\psi}) = 973.434$. Hence, $m = 1000$ is a large enough sample for accurate results.

The R code for carrying out these simulations is given below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Bootstrap estimation of MSE for third moment estimator
set.seed(34256734)

# Original sample stored in x
x <- c(3.27, -1.24, 3.97, 2.25, 3.47, -0.09, 7.45, 6.20, 3.74, 4.12,
       1.42, 2.75, -1.48, 4.97, 8.00, 3.26, 0.15, -3.64, 4.88, 4.55)

n <- length(x)
m <- 1000  # Number of bootstrap samples

# Storage for third moments of bootstrap samples
third_moments <- numeric(m)

for (i in 1:m) {
  # Bootstrap sample with replacement
  boot_sample <- sample(x, n, replace = TRUE)
  # Calculate third moment (mean of cubed values)
  third_moments[i] <- mean(boot_sample^3)
}

# Variance estimate (equation 6.4.5)
var_estimate <- var(third_moments)
cat("Estimated variance:", var_estimate, "\n")
\end{minted}
\caption{R code for Exercise 6.4.15}
\label{lst:ex6.4.15}
\end{listing}
\end{solution}

\begin{exercise}
\label{exer:6.4.16}
For the data of Exercise~\ref{exer:6.4.1}, use the plug-in MLE to estimate the first quartile of an $N(\mu, \sigma^2)$ distribution. Use bootstrapping to estimate the MSE of this estimate for $m = 10^3$ and $m = 10^4$ (use \eqref{eq:5.5.3} to compute the first quartile of the empirical distribution).
\end{exercise}

\begin{solution}
The characteristic of the $N(\mu, \sigma^2)$ distribution that we are interested in is $\psi(\theta) = T(F_\theta) = x_{(0.25)}(\theta) = \mu + \sigma z_{0.25}$, which we estimate by $T(\hat{F}) = \hat{x}_{0.25} = 0.15$, i.e., the sample first quartile. We want to estimate the MSE of the plug-in MLE of $x_{(0.25)}(\theta)$, which is given by $\hat{\psi} = m_1 + s z_{0.25} = 2.9 + (2.997)(-0.6745) = 0.87852$. The squared bias in this estimator is given by $(\hat{\psi} - T(\hat{F}))^2 = (0.87852 - 0.15)^2 = 0.53074$.

Based on $10^3$ samples, the variance of this estimator is estimated as $\widehat{\var}_{\hat{F}}(\hat{\psi}) = 1.85568$. Hence, $\widehat{\text{MSE}}(\hat{\psi}) = 0.53074 + 1.85568 = 2.3864$. Based on $10^4$ samples, the variance of this estimator is estimated as $\widehat{\var}_{\hat{F}}(\hat{\psi}) = 1.89582$. Hence, $\widehat{\text{MSE}}(\hat{\psi}) = 0.53074 + 1.89582 = 2.4266$.

The R code for this simulation is given below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Bootstrap estimation of MSE for first quartile estimator
set.seed(34256734)

# Original sample
x <- c(3.27, -1.24, 3.97, 2.25, 3.47, -0.09, 7.45, 6.20, 3.74, 4.12,
       1.42, 2.75, -1.48, 4.97, 8.00, 3.26, 0.15, -3.64, 4.88, 4.55)

n <- length(x)
m <- 20000  # Number of bootstrap samples

# Storage for first quartiles of bootstrap samples
quartiles <- numeric(m)

for (i in 1:m) {
  # Bootstrap sample with replacement
  boot_sample <- sample(x, n, replace = TRUE)
  # Sort and get the 5th value (first quartile for n=20)
  sorted_sample <- sort(boot_sample)
  quartiles[i] <- sorted_sample[5]
}

# Variance estimate (equation 6.4.5)
var_estimate <- var(quartiles)
cat("Estimated variance:", var_estimate, "\n")
\end{minted}
\caption{R code for Exercise 6.4.16}
\label{lst:ex6.4.16}
\end{listing}
\end{solution}

\begin{exercise}
\label{exer:6.4.17}
For the data of Exercise~\ref{exer:6.4.1}, use the plug-in MLE to estimate $F(3)$ for an $N(\mu, \sigma^2)$ distribution. Use bootstrapping to estimate the MSE of this estimate for $m = 10^3$ and $m = 10^4$.
\end{exercise}

\begin{solution}
The characteristic of the $N(\mu, \sigma^2)$ distribution that we are interested in is $\psi(\mu, \sigma^2) = T(F_{(\mu, \sigma^2)}) = F_{(\mu, \sigma^2)}(3) = \Phi\left( \frac{3 - \mu}{\sigma} \right)$, where $\Phi$ is the cdf of the $N(0, 1)$ distribution. The plug-in estimator of $\psi(\theta)$ is
\[
    \hat{\psi}(x_1, \ldots, x_n) = \Phi\left( \frac{3 - 2.9}{2.997} \right) = \Phi(3.3367 \times 10^{-2}) = 0.5133.
\]
The bias squared in this estimator is given by $(\hat{\psi} - T(\hat{F}))^2 = (0.5133 - 0.4)^2 = 0.01284$. Based on $10^3$ samples the variance of this estimator is estimated as $\widehat{\var}_{\hat{F}}(\hat{\psi}) = 0.0117605$. Hence, $\widehat{\text{MSE}}(\hat{\psi}) = 0.01284 + 0.0117605 = 2.4601 \times 10^{-2}$. Based on $10^4$ samples, the variance of this estimator is estimated as $\widehat{\var}_{\hat{F}}(\hat{\psi}) = 0.0118861$. Hence, $\widehat{\text{MSE}}(\hat{\psi}) = 0.01284 + 0.0118861 = 2.4726 \times 10^{-2}$.

The R code for these simulations is given below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Bootstrap estimation of MSE for cdf at 3
set.seed(34256734)

# Original sample
x <- c(3.27, -1.24, 3.97, 2.25, 3.47, -0.09, 7.45, 6.20, 3.74, 4.12,
       1.42, 2.75, -1.48, 4.97, 8.00, 3.26, 0.15, -3.64, 4.88, 4.55)

n <- length(x)
m <- 10000  # Number of bootstrap samples

# Storage for ecdf values at 3
ecdf_values <- numeric(m)

for (i in 1:m) {
  # Bootstrap sample with replacement
  boot_sample <- sample(x, n, replace = TRUE)
  # Calculate empirical cdf at 3
  ecdf_values[i] <- mean(boot_sample <= 3)
}

# Variance estimate (equation 6.4.5)
var_estimate <- var(ecdf_values)
cat("Estimated variance:", var_estimate, "\n")
\end{minted}
\caption{R code for Exercise 6.4.17}
\label{lst:ex6.4.17}
\end{listing}
\end{solution}

\begin{exercise}
\label{exer:6.4.18}
For the data of Exercise~\ref{exer:6.4.1}, form a 0.95-confidence interval for $\mu$ assuming that this is a sample from an $N(\mu, \sigma^2)$ distribution. Also compute a 0.95-confidence interval for $\mu$ based on the sign statistic, a bootstrap $t$ 0.95-confidence interval, and a bootstrap percentile 0.95-confidence interval using $m = 10^3$ for the bootstrapping. Compare the four intervals.
\end{exercise}

\begin{solution}
The sampling model $X_i \sim N(\mu, \sigma^2)$ is assumed. The characteristic $\psi(\theta) = \mu$ is of interest. It is known that $\sqrt{n}(\bar{x} - \mu)/s \sim t(n-1)$. Thus, an exact $\gamma$-confidence interval is $(\bar{x} - t_{(1+\gamma)/2}(n-1)s, \bar{x} + t_{(1+\gamma)/2}(n-1)s)$. For the confidence interval based on the sign statistic, the median of $F_{(\mu, \sigma^2)}$ is exactly the same as the mean of $F_{(\mu, \sigma^2)}$, because a normal distribution is symmetric, so a sign confidence interval for the median is also a confidence interval for the mean. The other intervals are described in the text very clearly. The four confidence intervals are given in the following table.

\begin{center}
\begin{tabular}{lcc}
Method & Lower bound & Upper bound \\
\hline
$t$ Confidence interval & 1.49721 & 4.30279 \\
Bootstrap $t$ & 1.58097 & 4.21903 \\
Sign statistic & 1.42000 & 4.55000 \\
Bootstrap quantile & 1.68400 & 4.07550
\end{tabular}
\end{center}

The R code for this simulation is given below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Exercise 6.4.18: Four confidence interval methods
set.seed(34256734)

x <- c(3.27, -1.24, 3.97, 2.25, 3.47, -0.09, 7.45, 6.20, 3.74, 4.12,
       1.42, 2.75, -1.48, 4.97, 8.00, 3.26, 0.15, -3.64, 4.88, 4.55)

n <- length(x)
gamma <- 0.95
M <- 1000  # Bootstrap samples

# 1. t Confidence interval
xbar <- mean(x)
s <- sd(x)
t_crit <- qt((1 + gamma)/2, df = n - 1)
t_lower <- xbar - t_crit * s / sqrt(n)
t_upper <- xbar + t_crit * s / sqrt(n)
cat("t CI:", t_lower, t_upper, "\n")

# 2. Bootstrap t confidence interval
boot_means <- numeric(M)
for (i in 1:M) {
  boot_sample <- sample(x, n, replace = TRUE)
  boot_means[i] <- mean(boot_sample)
}
boot_se <- sd(boot_means)
boot_t_lower <- xbar - t_crit * boot_se
boot_t_upper <- xbar + t_crit * boot_se
cat("Bootstrap t CI:", boot_t_lower, boot_t_upper, "\n")

# 3. Sign statistic confidence interval
sorted_x <- sort(x)
j <- 0
while (j <= n/2) {
  if (pbinom(j, n, 0.5) >= (1 - gamma)/2) break
  j <- j + 1
}
if (j == 0) {
  sign_lower <- sorted_x[1]
  sign_upper <- sorted_x[n]
} else {
  sign_lower <- sorted_x[j]
  sign_upper <- sorted_x[n + 1 - j]
}
cat("Sign CI:", sign_lower, sign_upper, "\n")

# 4. Bootstrap percentile confidence interval
sorted_boots <- sort(boot_means)
lower_idx <- floor((1 - gamma)/2 * M)
upper_idx <- floor((1 + gamma)/2 * M)
boot_perc_lower <- sorted_boots[max(1, lower_idx)]
boot_perc_upper <- sorted_boots[min(M, upper_idx)]
cat("Bootstrap percentile CI:", boot_perc_lower, boot_perc_upper, "\n")
\end{minted}
\caption{R code for Exercise 6.4.18}
\label{lst:ex6.4.18}
\end{listing}
\end{solution}

\begin{exercise}
\label{exer:6.4.19}
For the data of Exercise~\ref{exer:6.4.1}, use the plug-in MLE to estimate the first quintile, i.e., $x_{0.2}$ of an $N(\mu, \sigma^2)$ distribution. Plot a density histogram estimate of the bootstrap distribution of this estimator for $m = 10^3$ and compute a bootstrap $t$ 0.95-confidence interval for $x_{0.2}$, if you think it is appropriate.
\end{exercise}

\begin{solution}
The characteristic $\psi(\theta)$ of interest, i.e., the first quintile of $N(\mu, \sigma^2)$ is given by $\psi(\theta) = \mu + \sigma z_{0.2}$ where $z_{0.2}$ is the 0.2-quantile of a standard normal. The maximum likelihood estimator is given by $\hat{\mu} = \bar{x}$ and $\hat{\sigma}^2 = n^{-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 = (n-1)s^2/n$. The plug-in estimate of the quantile is $\hat{x}_{0.2} = \bar{x} + ((n-1)s^2/n)^{1/2} \cdot z_{0.2} = 2.9 + (19 \cdot 8.9839/20)^{1/2} \cdot (-0.841621) = 0.44127$. According to the graph, it seems there exist a few clusters. Thus, the bootstrap $t$ confidence interval is not applicable for this problem.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig_6_4_19.pdf}
    \caption{Histogram of bootstrap first quintile estimates for Exercise 6.4.19}
    %\label{fig:bootstrap-quintile}
\end{figure}

The R code for this simulation is given below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Exercise 6.4.19: Bootstrap for first quintile
set.seed(34256734)

x <- c(3.27, -1.24, 3.97, 2.25, 3.47, -0.09, 7.45, 6.20, 3.74, 4.12,
       1.42, 2.75, -1.48, 4.97, 8.00, 3.26, 0.15, -3.64, 4.88, 4.55)

n <- length(x)
M <- 1000

# Plug-in estimate of first quintile
xbar <- mean(x)
sigma_hat <- sd(x) * sqrt((n - 1)/n)
z_02 <- qnorm(0.2)
plugin_quintile <- xbar + sigma_hat * z_02
cat("Plug-in first quintile estimate:", plugin_quintile, "\n")

# Bootstrap resampling for first quintile
quintiles <- numeric(M)
k <- floor(0.2 * n)

for (i in 1:M) {
  boot_sample <- sample(x, n, replace = TRUE)
  sorted_boot <- sort(boot_sample)
  # Linear interpolation for quintile
  if (k < n) {
    quintiles[i] <- sorted_boot[k] + 
      (sorted_boot[k + 1] - sorted_boot[k]) * (0.2 * n - k)
  } else {
    quintiles[i] <- sorted_boot[n]
  }
}

# Draw histogram
hist(quintiles, freq = FALSE, main = "", xlab = "quintile x_0.2",
     col = "lightblue", border = "darkblue")
\end{minted}
\caption{R code for Exercise 6.4.19}
\label{lst:ex6.4.19}
\end{listing}
\end{solution}

\begin{exercise}
\label{exer:6.4.20}
For the data of Exercise~\ref{exer:6.4.1}, use the plug-in MLE to estimate $\mu^3$ of an $N(\mu, \sigma^2)$ distribution. Plot a density histogram estimate of the bootstrap distribution of this estimator for $m = 10^3$ and compute a bootstrap percentile 0.95-confidence interval for $\mu^3$ if you think it is appropriate.
\end{exercise}

\begin{solution}
The characteristic of interest is $\psi(\theta) = \mu_3 = \expc_\theta(X^3) = \mu^3 + 3\mu\sigma^2$. The maximum likelihood estimator is given by $\hat{\mu} = \bar{x}$ and $\hat{\sigma}^2 = n^{-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 = (n-1)s^2/n$. The plug-in estimate of $\mu_3$ is $\hat{\mu}_3 = \bar{x}^3 + 3\bar{x}(n-1)s^2/n = 2.9^3 + 3 \cdot 2.9 \cdot (19 \cdot 8.9839/20) = 98.6410$.

\begin{figure}[!htbp]
    \centering
    %\includegraphics[scale=0.5]{fig_6_4_20.pdf}
    \caption{Histogram of bootstrap $\mu_3$ estimates for Exercise 6.4.20}
    %\label{fig:bootstrap-mu3}
\end{figure}

The graph indicates that bootstrap inference is applicable for this problem. The bootstrap percentile 0.95-confidence interval is given by $(34.539, 158.801)$. The R code for this simulation is given below.

\begin{listing}[!htbp]
\begin{minted}{R}
# Exercise 6.4.20: Bootstrap for third moment
set.seed(34256734)

x <- c(3.27, -1.24, 3.97, 2.25, 3.47, -0.09, 7.45, 6.20, 3.74, 4.12,
       1.42, 2.75, -1.48, 4.97, 8.00, 3.26, 0.15, -3.64, 4.88, 4.55)

n <- length(x)
gamma <- 0.95
M <- 1000

# Plug-in estimator of mu_3
xbar <- mean(x)
sigma2_hat <- var(x) * (n - 1) / n
plugin_mu3 <- xbar^3 + 3 * xbar * sigma2_hat
cat("Plug-in estimator of mu_3:", plugin_mu3, "\n")

# Bootstrap resampling
mu3_boots <- numeric(M)

for (i in 1:M) {
  boot_sample <- sample(x, n, replace = TRUE)
  mu3_boots[i] <- mean(boot_sample^3)
}

# Draw histogram
hist(mu3_boots, freq = FALSE, main = "", xlab = "mu_3",
     col = "lightblue", border = "darkblue")

# Bootstrap percentile confidence interval
sorted_mu3 <- sort(mu3_boots)
lower_idx <- floor((1 - gamma)/2 * M)
upper_idx <- floor((1 + gamma)/2 * M)

if (lower_idx < 1) {
  lower_bound <- sorted_mu3[1]
} else {
  lower_bound <- sorted_mu3[lower_idx] + 
    (sorted_mu3[lower_idx + 1] - sorted_mu3[lower_idx]) * 
    (M * (1 - gamma)/2 - lower_idx)
}

if (upper_idx >= M) {
  upper_bound <- sorted_mu3[M]
} else {
  upper_bound <- sorted_mu3[upper_idx] + 
    (sorted_mu3[upper_idx + 1] - sorted_mu3[upper_idx]) * 
    (M * (1 + gamma)/2 - upper_idx)
}

cat("Bootstrap percentile CI:", lower_bound, upper_bound, "\n")
\end{minted}
\caption{R code for Exercise 6.4.20}
\label{lst:ex6.4.20}
\end{listing}
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:6.4.21}
Prove that when $x_1, \ldots, x_n$ is a sample of distinct values from a distribution on $R^1$, then the $i$th moment of the empirical distribution on $R^1$ (i.e., the distribution with cdf given by $\hat{F}$) is $m_i$.
\end{exercise}

\begin{solution}
For a random variable with this distribution, we have that $\expc_{\hat{F}}(X_i) = \sum_{j=1}^{n} x_{(j)} (\hat{F}(x_{(j)}) - \hat{F}(x_{(j-1)}))$, where we take $x_{(0)} = -\infty$. Now $\hat{F}(x_{(j)}) - \hat{F}(x_{(j-1)}) = 1/n$ since all the $x_{(j)}$ are distinct. This implies the result.
\end{solution}

\begin{exercise}
\label{exer:6.4.22}
Suppose that $x_1, \ldots, x_n$ is a sample from a distribution on $R^1$. Determine the general form of the $i$th moment of $\hat{F}$, i.e., in contrast to Problem~\ref{exer:6.4.21}, we are now allowing for several of the data values to be equal.
\end{exercise}

\begin{solution}
We have that $\expc_{\hat{F}}(X_i) = \sum_{j=1}^{n^*} (x_{(j)}^*)^i (\hat{F}(x_{(j)}^*) - \hat{F}(x_{(j)}^*))$, where $n^*$ is the number of distinct values, $x_1^*, \ldots, x_{n^*}^*$ are the distinct values in the sample, $x_{(1)}^*, \ldots, x_{(n^*)}^*$ are the ordered distinct values in the sample, and $\hat{F}(x_{(j)}^*) - \hat{F}(x_{(j)}^*)$ equals the relative frequency of $x_{(j)}^*$ in the original sample.
\end{solution}

\begin{exercise}[Variance stabilizing transformations]
\label{exer:6.4.23}
From the delta theorem, we have that $\psi(M_1)$ is asymptotically normal with mean $\psi(\mu_1)$ and variance $(\psi'(\mu_1))^2 \sigma^2/n$ when $\psi$ is continuously differentiable, $\psi'(\mu_1) \neq 0$, and $M_1$ is asymptotically normal with mean $\mu_1$ and variance $\sigma^2/n$. In some applications, it is important to choose the transformation $\psi$ so that the asymptotic variance does not depend on the mean $\mu_1$, i.e., $(\psi'(\mu_1))^2 \sigma^2$ is constant as $\mu_1$ varies (note that $\sigma^2$ may change as $\mu_1$ changes). Such transformations are known as \emph{variance stabilizing transformations}.
\begin{enumerate}[(a)]
\item If we are sampling from a Poisson$(\lambda)$ distribution, then show that $\psi(x) = \sqrt{x}$ is variance stabilizing.
\item If we are sampling from a Bernoulli$(\theta)$ distribution, show that $\psi(x) = \arcsin(\sqrt{x})$ is variance stabilizing.
\item If we are sampling from a distribution on $(0, \infty)$ whose variance is proportional to the square of its mean (like the Gamma$(\alpha, \lambda)$ distribution), then show that $\psi(x) = \ln(x)$ is variance stabilizing.
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item First, note that for the Poisson distribution we have $\mu_1 = \lambda = \sigma^2$, i.e., the mean and the variance are the same. Now using $\psi(x) = \sqrt{x}$ as a transformation, by the delta theorem, we have $\psi(M_1) = \sqrt{M_1}$ is asymptotically normal with mean $\psi(\mu_1) = \sqrt{\mu_1}$ and variance given by $(\psi'(\mu_1))^2 \frac{\sigma^2}{n} = \frac{1}{4\lambda} \cdot \frac{\lambda}{n} = \frac{1}{4n}$, which is free of $\mu_1$, and hence this transformation is variance stabilizing.
    
    \item Using $\psi(x) = \arcsin \sqrt{x}$ as a transformation, by the delta theorem, we have $\psi(M_1) = \arcsin \sqrt{M_1}$ is asymptotically normal with mean $\psi(\mu_1) = \arcsin \sqrt{\mu_1}$ and variance given by
    \[
        (\psi'(\mu_1))^2 \frac{\sigma^2}{n} = \left( \frac{1}{2\sqrt{(1 - \theta)\theta}} \right)^2 \frac{\theta(1 - \theta)}{n} = \frac{1}{4n},
    \]
    which is free of $\theta$, and hence this transformation is variance stabilizing.
    
    \item First, we have $\sigma^2 = a\mu_1^2$. Next, the mean of $\psi(M_1) = \ln(M_1)$ is approximately $\psi(\mu_1) = \ln(\mu_1)$ and the variance is approximately
    \[
        (\psi'(\mu_1))^2 \frac{\sigma^2}{n} = \frac{1}{\mu_1^2} \cdot \frac{a\mu_1^2}{n} = \frac{a}{n},
    \]
    which is free of $\mu_1$, and hence this transformation is variance stabilizing.
\end{enumerate}
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:6.4.24}
Suppose that $X$ has an absolutely continuous distribution on $R^1$ with density $f$ that is symmetrical about its median. Assuming that the median is 0, prove that $|X|$ and
\[
\text{sgn}(X) = \begin{cases}
1 & x > 0 \\
0 & x = 0 \\
-1 & x < 0
\end{cases}
\]
are independent, with $|X|$ having density $2f$ and $\text{sgn}(X)$ uniformly distributed on $\{-1, 1\}$.
\end{exercise}

\begin{solution}
Let $Y = |X|$, then $Y$ has a distribution on $\mathbb{R}^+ = (0, \infty)$ given by
\[
    F_Y(y) = \prb(Y \leqslant y) = \prb(|X| \leqslant y) = \prb(-y \leqslant X \leqslant y) = F_X(y) - F_X(-y) = 2F_X(y) - 1
\]
where the last equality follows by symmetry of the distribution of $X$. Therefore, the density of $Y$ is $2f$, where $f$ is the density of $X$.

Next, let $Z = \text{sgn}(X)$, then $\prb(Z = -1) = \prb(X < 0) = 0.5$, $\prb(Z = 1) = \prb(X > 0) = 0.5$, and $\prb(Z = 0) = \prb(X = 0) = 0$. Therefore, $Z$ is uniform on $\{-1, 1\}$.

To show that $Y$ and $Z$ are independent we proceed as follows.
\[
    \prb(Y \leqslant y, Z = 1) = \prb(-y \leqslant X \leqslant y, X > 0) = \prb(0 \leqslant X \leqslant y) = F_X(y) - \frac{1}{2}
\]
which is the same as $\prb(Y \leqslant y)\prb(Z = 1) = (2F_X(y) - 1)/2 = F_X(y) - 1/2$. Hence, we have established that $Y$ and $Z$ are independent.
\end{solution}

\begin{exercise}[Fisher signed deviation statistic]
\label{exer:6.4.25}
Suppose that $x_1, \ldots, x_n$ is a sample from an absolutely continuous distribution on $R^1$ with density that is symmetrical about its median. Suppose we want to assess the hypothesis $H_0 : x_{0.5} = x_0$.

One possibility for this is to use the \emph{Fisher signed deviation test} based on the statistic $S_*$. The observed value of $S_*$ is given by $S_{*o} = \sum_{i=1}^{n} |x_i - x_0| \text{sgn}(x_i - x_0)$. We then assess $H_0$ by comparing $S_{*o}$ with the conditional distribution of $S_*$ given the absolute deviations $|x_1 - x_0|, \ldots, |x_n - x_0|$. If a value $S_{*o}$ occurs near the smallest or largest possible value for $S_*$ under this conditional distribution, then we assert that we have evidence against $H_0$. We measure this by computing the P-value given by the conditional probability of obtaining a value as far, or farther, from the center of the conditional distribution of $S_*$, using the conditional mean as the center. This is an example of a \emph{randomization test}, as the distribution for the test statistic is determined by randomly modifying the observed data (in this case, by randomly changing the signs of the deviations of the $x_i$ from $x_0$).
\begin{enumerate}[(a)]
\item Prove that $S_{*o} = n(\bar{x} - x_0)$.
\item Prove that the P-value described above does not depend on which distribution we are sampling from in the model. Prove that the conditional mean of $S_*$ is 0 and the conditional distribution of $S_*$ is symmetric about this value.
\item Use the Fisher signed deviation test statistic to assess the hypothesis $H_0 : x_{0.5} = 2$ when the data are 2.2, 1.5, 3.4, 0.4, 5.3, 4.3, 2.1, with the results declared to be statistically significant if the P-value is less than or equal to 0.05. (Hint: Based on the results obtained in part (b), you need only compute probabilities for the extreme values of $S_*$.)
\item Show that using the Fisher signed deviation test statistic to assess the hypothesis $H_0 : x_{0.5} = x_0$ is equivalent to the following randomized $t$-test statistic hypothesis assessment procedure. For this, we compute the conditional distribution of
\[
T = \frac{\bar{X} - x_0}{S/\sqrt{n}}
\]
when the $|X_i - x_0| = |x_i - x_0|$ are fixed and the $\text{sgn}(X_i - x_0)$ are i.i.d.\ uniform on $\{-1, 1\}$. Compare the observed value of the $t$-statistic with this distribution, as we did for the Fisher signed deviation test statistic. (Hint: Show that $\sum_{i=1}^{n} (x_i - \bar{x})^2 = \sum_{i=1}^{n} (x_i - x_0)^2 - n(\bar{x} - x_0)^2$ and that large absolute values of $T$ correspond to large absolute values of $S_*$.)
\end{enumerate}
\end{exercise}

\begin{solution}
\begin{enumerate}[(a)]
    \item We have that $|x_i - x_0| \text{sgn}(x_i - x_0) = x_i - x_0$, so $S_o^+ = n(\bar{x} - x_0)$.
    
    \item Note that, under $H_0$, $Y = X - x_0$ is distributed from an absolutely continuous distribution that is symmetric about 0. Therefore, by Challenge~\ref{exer:6.4.24} we have that $|Y|$ and $\text{sgn}(Y) = \text{sgn}(X - x_0)$ are independent and $\text{sgn}(Y)$ is uniform on $\{-1, 1\}$. The conditional distribution of $S^+$, given the values $|Y_1| = |x_1 - x_0|, \ldots, |Y_n| = |x_n - x_0|$, is therefore determined by $(\text{sgn}(Y_1), \ldots, \text{sgn}(Y_n))$ and, because of independence, this is uniform on $\{-1, 1\}^n$. This implies that the conditional distribution of $S^+$ is the same no matter which absolutely continuous distribution, symmetric about its median, that we are sampling from. The conditional mean of $S^+$ is then
    \[
        \expc(S^+ \mid |x_1 - x_0|, \ldots, |x_n - x_0|) = \sum_{i=1}^{n} |x_i - x_0| \expc(\text{sgn}(X_i - x_0)) = 0
    \]
    since $\expc(\text{sgn}(X_i - x_0)) = 0$ for each $i$. Further, it is clear that this conditional distribution is symmetric about 0 since the distribution of each $\text{sgn}(X_i - x_0)$ is symmetric about 0.
    
    \item We have that
    \[
        S_o^+ = \sum_{i=1}^{n} |x_i - x_0| \text{sgn}(x_i - x_0) = 0.2 - 0.5 + 1.4 - 1.6 + 3.3 + 2.3 + 0.1 = 5.2.
    \]
    Now each possible value of $(\text{sgn}(X_1 - x_0), \ldots, \text{sgn}(X_n - x_0))$ occurs with probability $(1/2)^6 = 1.5625 \times 10^{-2}$ and $4(1.5625 \times 10^{-2}) = 0.0625$, while $2(1.5625 \times 10^{-2}) = 0.03125$. So to determine if 5.2 yields a $P$-value less than 0.05, we need to evaluate the 4 extreme points (2 on each tail) of the conditional distribution of $S^+$. Starting from the most extreme values and moving towards the center 0 we have that $S^+$ takes the values
    \begin{align*}
        0.2 + 0.5 + 1.4 + 1.6 + 3.3 + 2.3 + 0.1 &= 9.4 \\
        0.2 + 0.5 + 1.4 + 1.6 + 3.3 + 2.3 - 0.1 &= 9.2 \\
        &\vdots \\
        -0.2 - 0.5 - 1.4 - 1.6 - 3.3 - 2.3 + 0.1 &= -9.2 \\
        -0.2 - 0.5 - 1.4 - 1.6 - 3.3 - 2.3 - 0.1 &= -9.4
    \end{align*}
    so the $P$-value is greater than 0.05 and we have no evidence against $H_0$.
    
    \item We have that $t = n(\bar{x} - x_0)/s$ and
    \begin{align*}
        (n-1)s^2 &= \sum_{i=1}^{n} (x_i - \bar{x})^2 = \sum_{i=1}^{n} (x_i - x_0 + x_0 - \bar{x})^2 \\
        &= \sum_{i=1}^{n} |x_i - \bar{x}|^2 + 2\sum_{i=1}^{n} (x_i - x_0)(x_0 - \bar{x}) + n(\bar{x} - \bar{x})^2 \\
        &= \sum_{i=1}^{n} |x_i - \bar{x}|^2 - 2n(\bar{x} - x_0)^2 + n(\bar{x} - \bar{x})^2 \\
        &= \sum_{i=1}^{n} |x_i - \bar{x}|^2 - n(\bar{x} - \bar{x})^2
    \end{align*}
    and note that $\sum_{i=1}^{n} |x_i - \bar{x}|^2$ is fixed under the conditional distribution. Therefore,
    \[
        t = \frac{n(\bar{x} - x_0)}{s} = \frac{n(\bar{x} - x_0)}{\sqrt{n-1}} \cdot \frac{1}{\sqrt{\sum_{i=1}^{n} |x_i - \bar{x}|^2 - n(\bar{x} - \bar{x})^2}}.
    \]
    Then we see that $t$ is an increasing function of $n(\bar{x} - x_0)$ for $-\sum_{i=1}^{n} |x_i - \bar{x}|^2 \leqslant n(\bar{x} - x_0) \leqslant \sum_{i=1}^{n} |x_i - \bar{x}|^2$ so that $t$ is large whenever $S_o^+$ is large and conversely.
\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotics for the MLE (Advanced)}
\label{sec:6.5}

As we saw in Examples~\ref{ex:6.3.7} and~\ref{ex:6.3.11}, implementing exact sampling procedures based on the MLE can be difficult. In those examples, because the MLE was the sample average and we could use the central limit theorem, large sample theory allowed us to work out approximate procedures. In fact, there is some general large sample theory available for the MLE that allows us to obtain approximate sampling inferences. This is the content of this section. The results we develop are all for the case when $\theta$ is one-dimensional. Similar results exist for the higher-dimensional problems, but we leave those to a later course.

In Section~\ref{sec:6.3}, the basic issue was the need to measure the accuracy of the MLE. One approach is to plot the likelihood and examine how concentrated it is about its peak, with a more highly concentrated likelihood implying greater accuracy for the MLE. There are several problems with this. In particular, the appearance of the likelihood will depend greatly on how we choose the scales for the axes. With appropriate choices, we can make a likelihood look as concentrated or as diffuse as we want. Also, when $\theta$ is more than two-dimensional, we cannot even plot the likelihood. One solution, when the likelihood is a smooth function of $\theta$, is to compute a numerical measure of how concentrated the log-likelihood is at its peak. The quantity typically used for this is called the \emph{observed Fisher information}.

\begin{definition}
\label{def:6.5.1}
The \emph{observed Fisher information} is given by
\begin{equation}
\label{eq:6.5.1}
\hat{I}(s) = -\left.\frac{\partial^2 l(\theta \mid s)}{\partial \theta^2}\right|_{\hat{\theta}(s)}
\end{equation}
where $\hat{\theta}(s)$ is the MLE.
\end{definition}

The larger the observed Fisher information is, the more peaked the likelihood function is at its maximum value. We will show that the observed Fisher information is estimating a quantity of considerable importance in statistical inference.

Suppose that response $X$ is real-valued, $\theta$ is real-valued, and the model $\{f_\theta : \theta \in \Omega\}$ satisfies the following \emph{regularity conditions}:
\begin{align}
&\frac{\partial^2 \ln f_\theta(x)}{\partial \theta^2} \text{ exists for each } x \label{eq:6.5.2} \\
&\expc_\theta(S_\theta(X)) = \int \frac{\partial \ln f_\theta(x)}{\partial \theta} f_\theta(x) \, \mathrm{d}x = 0 \label{eq:6.5.3} \\
&\frac{\partial}{\partial \theta} \int \ln f_\theta(x) f_\theta(x) \, \mathrm{d}x = \int \frac{\partial}{\partial \theta} (\ln f_\theta(x) f_\theta(x)) \, \mathrm{d}x \label{eq:6.5.4}
\end{align}
and
\begin{equation}
\left|\int \frac{\partial^2 \ln f_\theta(x)}{\partial \theta^2} f_\theta(x) \, \mathrm{d}x\right| < \infty. \label{eq:6.5.5}
\end{equation}

Note that we have
\[
\frac{\partial f_\theta(x)}{\partial \theta} = \frac{\partial \ln f_\theta(x)}{\partial \theta} f_\theta(x),
\]
so we can write \eqref{eq:6.5.3} equivalently as
\[
\int \frac{\partial f_\theta(x)}{\partial \theta} \, \mathrm{d}x = 0.
\]
Also note that \eqref{eq:6.5.4} can be written as
\begin{align*}
0 &= \frac{\partial}{\partial \theta} \int l_\theta(x) f_\theta(x) \, \mathrm{d}x = \int \left(\frac{\partial^2 l_\theta(x)}{\partial \theta^2} + \left(\frac{\partial l_\theta(x)}{\partial \theta}\right)^2\right) f_\theta(x) \, \mathrm{d}x \\
&= \expc_\theta\left[\frac{\partial^2 l_\theta(X)}{\partial \theta^2}\right] + \expc_\theta[S_\theta^2(X)].
\end{align*}
This together with \eqref{eq:6.5.3} and \eqref{eq:6.5.5}, implies that we can write \eqref{eq:6.5.4} equivalently as
\[
\var_\theta(S_\theta(X)) = \expc_\theta[S_\theta^2(X)] = -\expc_\theta\left[\frac{\partial^2}{\partial \theta^2} l_\theta(X)\right].
\]
We give a name to the quantity on the left.

\begin{definition}
\label{def:6.5.2}
The function $I(\theta) = \var_\theta(S_\theta(X))$ is called the \emph{Fisher information} of the model.
\end{definition}

Our developments above have proven the following result.

\begin{theorem}
\label{thm:6.5.1}
If \eqref{eq:6.5.2} and \eqref{eq:6.5.3} are satisfied, then $\expc_\theta(S_\theta(X)) = 0$. If, in addition, \eqref{eq:6.5.4} and \eqref{eq:6.5.5} are satisfied, then
\[
I(\theta) = \var_\theta(S_\theta(X)) = -\expc_\theta\left[\frac{\partial^2 l_\theta(X)}{\partial \theta^2}\right].
\]
\end{theorem}

Now we see why $\hat{I}$ is called the observed Fisher information, as it is a natural estimate of the Fisher information at the true value $\theta$. We note that there is another natural estimate of the Fisher information at the true value, given by $I(\hat{\theta})$. We call this the \emph{plug-in Fisher information}.

When we have a sample $x_1, \ldots, x_n$ from $f_\theta$, then
\[
S_\theta(x_1, \ldots, x_n) = \frac{\partial}{\partial \theta} \ln \prod_{i=1}^{n} f_\theta(x_i) = \sum_{i=1}^{n} \frac{\partial}{\partial \theta} \ln f_\theta(x_i) = \sum_{i=1}^{n} S_\theta(x_i).
\]
So, if \eqref{eq:6.5.3} holds for the basic model, then $\expc_\theta(S_\theta(X_1, \ldots, X_n)) = 0$ and \eqref{eq:6.5.3} also holds for the sampling model. Furthermore, if \eqref{eq:6.5.4} holds for the basic model, then
\[
0 = -\sum_{i=1}^{n} \expc_\theta\left[\frac{\partial^2}{\partial \theta^2} \ln f_\theta(X_i)\right] + \sum_{i=1}^{n} \expc_\theta[S_\theta^2(X_i)] = -\expc_\theta\left[\frac{\partial^2}{\partial \theta^2} l_\theta(X_1, \ldots, X_n)\right] + \var_\theta(S_\theta(X_1, \ldots, X_n)),
\]
which implies
\[
\var_\theta(S_\theta(X_1, \ldots, X_n)) = -\expc_\theta\left[\frac{\partial^2}{\partial \theta^2} l_\theta(X_1, \ldots, X_n)\right] = nI(\theta)
\]
because $l_\theta(x_1, \ldots, x_n) = \sum_{i=1}^{n} \ln f_\theta(x_i)$. Therefore, \eqref{eq:6.5.4} holds for the sampling model as well, and the Fisher information for the sampling model is given by the sample size times the Fisher information for the basic model. We have established the following result.

\begin{corollary}
\label{cor:6.5.1}
Under i.i.d.\ sampling from a model with Fisher information $I(\theta)$. the Fisher information for a sample of size $n$ is given by $nI(\theta)$.
\end{corollary}

The conditions necessary for Theorem~\ref{thm:6.5.1} to apply do not hold in general and have to be checked in each example. There are, however, many models where these conditions do hold.

\begin{example}[Nonexistence of the Fisher Information]
\label{ex:6.5.1}
If $X \sim U[0, \theta]$, then $f_\theta(x) = \theta^{-1} \indc_{[0, \theta]}(x)$, which is not differentiable at $x = \theta$ for any $x$. Indeed, if we ignored the lack of differentiability at $x = \theta$ and wrote
\[
\frac{\partial}{\partial \theta} f_\theta(x) = -\theta^{-2} \indc_{[0, \theta]}(x),
\]
then
\[
\int \frac{\partial}{\partial \theta} f_\theta(x) \, \mathrm{d}x = \int -\theta^{-2} \indc_{[0, \theta]}(x) \, \mathrm{d}x = -\theta^{-1} \neq 0.
\]
So we cannot define the Fisher information for this model.
\end{example}

\begin{example}[Location Normal]
\label{ex:6.5.2}
Suppose we have a sample $x_1, \ldots, x_n$ from an $N(\theta, \sigma_0^2)$ distribution where $\theta \in R^1$ is unknown and $\sigma_0^2$ is known. We saw in Example~\ref{ex:6.2.2} that
\[
S_\theta(x_1, \ldots, x_n) = \frac{n}{\sigma_0^2}(\bar{x} - \theta)
\]
and therefore
\[
-\frac{\partial^2}{\partial \theta^2} l_\theta(x_1, \ldots, x_n) = \frac{n}{\sigma_0^2} = nI(\theta) = -\expc_\theta\left[\frac{\partial^2}{\partial \theta^2} l_\theta(X_1, \ldots, X_n)\right] = \frac{n}{\sigma_0^2}.
\]
We also determined in Example~\ref{ex:6.2.2} that the MLE is given by $\hat{\theta}(x_1, \ldots, x_n) = \bar{x}$. Then the plug-in Fisher information is
\[
nI(\bar{x}) = \frac{n}{\sigma_0^2}
\]
while the observed Fisher information is
\[
\hat{I}(x_1, \ldots, x_n) = -\left.\frac{\partial^2 l_\theta(x_1, \ldots, x_n)}{\partial \theta^2}\right|_{\bar{x}} = \frac{n}{\sigma_0^2}.
\]
In this case, there is no need to estimate the Fisher information, but it is comforting that both of our estimates give the exact value.
\end{example}

We now state, without proof, some theorems about the large sample behavior of the MLE under repeated sampling from the model. First, we have a result concerning the consistency of the MLE as an estimator of the true value of $\theta$.

\begin{theorem}
\label{thm:6.5.2}
Under regularity conditions (like those specified above) for the model $\{f_\theta : \theta \in \Omega\}$, the MLE exists a.s.\ and $\hat{\theta} \xrightarrow{a.s.} \theta$ as $n \to \infty$.
\end{theorem}

\begin{proof}
See \emph{Approximation Theorems of Mathematical Statistics}, by R.\,J.\ Serfling (John Wiley \& Sons, New York, 1980), for the proof of this result.
\end{proof}

We see that Theorem~\ref{thm:6.5.2} serves as a kind of strong law for the MLE. It also turns out that when the sample size is large, the sampling distribution of the MLE is approximately normal.

\begin{theorem}
\label{thm:6.5.3}
Under regularity conditions (like those specified above) for the model $\{f_\theta : \theta \in \Omega\}$, then $(nI(\theta))^{1/2}(\hat{\theta} - \theta) \xrightarrow{D} N(0, 1)$ as $n \to \infty$.
\end{theorem}

\begin{proof}
See \emph{Approximation Theorems of Mathematical Statistics}, by R.\,J.\ Serfling (John Wiley \& Sons, New York, 1980), for the proof of this result.
\end{proof}

We see that Theorem~\ref{thm:6.5.3} serves as a kind of central limit theorem for the MLE. To make this result fully useful to us for inference, we need the following corollary to this theorem.

\begin{corollary}
\label{cor:6.5.2}
When $I(\theta)$ is a continuous function of $\theta$, then
\[
(nI(\hat{\theta}))^{1/2}(\hat{\theta} - \theta) \xrightarrow{D} N(0, 1).
\]
\end{corollary}

In Corollary~\ref{cor:6.5.2}, we have estimated the Fisher information $I(\theta)$ by the plug-in Fisher estimation $I(\hat{\theta})$. Often it is very difficult to evaluate the function $I(\theta)$. In such a case, we instead estimate $nI(\theta)$ by the observed Fisher information $\hat{I}(x_1, \ldots, x_n)$. A result such as Corollary~\ref{cor:6.5.2} again holds in this case.

From Corollary~\ref{cor:6.5.2}, we can devise large sample approximate inference methods based on the MLE. For example, the approximate standard error of the MLE is
\[
(nI(\hat{\theta}))^{-1/2}.
\]
An approximate $\gamma$-confidence interval is given by
\[
\hat{\theta} \pm (nI(\hat{\theta}))^{-1/2} z_{(1+\gamma)/2}.
\]
Finally, if we want to assess the hypothesis $H_0 : \theta = \theta_0$, we can do this by computing the approximate P-value
\[
2\left(1 - \Phi\left((nI(\theta_0))^{1/2}|\hat{\theta} - \theta_0|\right)\right).
\]
Notice that we are using Theorem~\ref{thm:6.5.3} for the P-value, rather than Corollary~\ref{cor:6.5.2}, as, when $H_0$ is true, we know the asymptotic variance of the MLE is $(nI(\theta_0))^{-1}$. So we do not have to estimate this quantity.

When evaluating $I(\theta)$ is difficult, we can replace $nI(\theta)$ by $\hat{I}(x_1, \ldots, x_n)$ in the above expressions for the confidence interval and P-value. We now see very clearly the significance of the observed information. Of course, as we move from using $nI(\theta)$ to $nI(\hat{\theta})$ to $\hat{I}(x_1, \ldots, x_n)$, we expect that larger sample sizes $n$ are needed to make the normality approximation accurate.

We consider some examples.

\begin{example}[Location Normal Model]
\label{ex:6.5.3}
Using the Fisher information derived in Example~\ref{ex:6.5.2}, the approximate $\gamma$-confidence interval based on the MLE is
\[
\hat{\theta} \pm (nI(\hat{\theta}))^{-1/2} z_{(1+\gamma)/2} = \bar{x} \pm \frac{\sigma_0}{\sqrt{n}} z_{(1+\gamma)/2}.
\]
This is just the $z$-confidence interval derived in Example~\ref{ex:6.3.6}. Rather than being an approximate $\gamma$-confidence interval, the coverage is exact in this case. Similarly, the approximate P-value corresponds to the $z$-test and the P-value is exact.
\end{example}

\begin{example}[Bernoulli Model]
\label{ex:6.5.4}
Suppose that $x_1, \ldots, x_n$ is a sample from a Bernoulli$(\theta)$ distribution, where $\theta \in [0, 1]$ is unknown. The likelihood function is given by
\[
L(\theta \mid x_1, \ldots, x_n) = \theta^{n\bar{x}} (1 - \theta)^{n(1 - \bar{x})},
\]
and the MLE of $\theta$ is $\bar{x}$. The log-likelihood is
\[
l_\theta(x_1, \ldots, x_n) = n\bar{x} \ln \theta + n(1 - \bar{x}) \ln(1 - \theta),
\]
the score function is given by
\[
S_\theta(x_1, \ldots, x_n) = \frac{n\bar{x}}{\theta} - \frac{n(1 - \bar{x})}{1 - \theta},
\]
and
\[
\frac{\partial S_\theta(x_1, \ldots, x_n)}{\partial \theta} = -\frac{n\bar{x}}{\theta^2} - \frac{n(1 - \bar{x})}{(1 - \theta)^2}.
\]
Therefore, the Fisher information for the sample is
\[
nI(\theta) = -\expc_\theta\left[\frac{\partial S_\theta(X_1, \ldots, X_n)}{\partial \theta}\right] = \expc_\theta\left[\frac{n\bar{X}}{\theta^2} + \frac{n(1 - \bar{X})}{(1 - \theta)^2}\right] = \frac{n}{\theta(1 - \theta)},
\]
and the plug-in Fisher information is
\[
nI(\bar{x}) = \frac{n}{\bar{x}(1 - \bar{x})}.
\]
Note that the plug-in Fisher information is the same as the observed Fisher information in this case.

So an approximate $\gamma$-confidence interval is given by
\[
\hat{\theta} \pm (nI(\hat{\theta}))^{-1/2} z_{(1+\gamma)/2} = \bar{x} \pm z_{(1+\gamma)/2} \sqrt{\bar{x}(1 - \bar{x})/n},
\]
which is precisely the interval obtained in Example~\ref{ex:6.3.7} using large sample considerations based on the central limit theorem. Similarly, we obtain the same P-value as in Example~\ref{ex:6.3.11} when testing $H_0 : \theta = \theta_0$.
\end{example}

\begin{example}[Poisson Model]
\label{ex:6.5.5}
Suppose that $x_1, \ldots, x_n$ is a sample from a Poisson$(\lambda)$ distribution, where $\lambda > 0$ is unknown. The likelihood function is given by
\[
L(\lambda \mid x_1, \ldots, x_n) = \lambda^{n\bar{x}} e^{-n\lambda}.
\]
The log-likelihood is
\[
l_\lambda(x_1, \ldots, x_n) = n\bar{x} \ln \lambda - n\lambda,
\]
the score function is given by
\[
S_\lambda(x_1, \ldots, x_n) = \frac{n\bar{x}}{\lambda} - n,
\]
and
\[
\frac{\partial S_\lambda(x_1, \ldots, x_n)}{\partial \lambda} = -\frac{n\bar{x}}{\lambda^2}.
\]
From this we deduce that the MLE of $\lambda$ is $\bar{x}$.

Therefore, the Fisher information for the sample is
\[
nI(\lambda) = -\expc_\lambda\left[\frac{\partial S_\lambda(X_1, \ldots, X_n)}{\partial \lambda}\right] = \expc_\lambda\left[\frac{n\bar{X}}{\lambda^2}\right] = \frac{n}{\lambda},
\]
and the plug-in Fisher information is
\[
nI(\bar{x}) = \frac{n}{\bar{x}}.
\]
Note that the plug-in Fisher information is the same as the observed Fisher information in this case.

So an approximate $\gamma$-confidence interval is given by
\[
\hat{\lambda} \pm (nI(\hat{\lambda}))^{-1/2} z_{(1+\gamma)/2} = \bar{x} \pm z_{(1+\gamma)/2} \sqrt{\bar{x}/n}.
\]
Similarly, the approximate P-value for testing $H_0 : \lambda = \lambda_0$ is given by
\[
2\left(1 - \Phi\left((nI(\lambda_0))^{1/2}|\hat{\lambda} - \lambda_0|\right)\right) = 2\left(1 - \Phi\left((n/\lambda_0)^{1/2}|\bar{x} - \lambda_0|\right)\right).
\]
Note that we have used the Fisher information evaluated at $\lambda_0$ for this test.
\end{example}

\subsection*{Summary of Section~\ref{sec:6.5}}

\begin{itemize}
\item Under regularity conditions on the statistical model with parameter $\theta$, we can define the Fisher information $I(\theta)$ for the model.
\item Under regularity conditions on the statistical model, it can be proved that, when $\theta$ is the true value of the parameter, the MLE is consistent for $\theta$ and the MLE is approximately normally distributed with mean given by $\theta$ and with variance given by $(nI(\theta))^{-1}$.
\item The Fisher information $I(\theta)$ can be estimated by plugging in the MLE or by using the observed Fisher information. These estimates lead to practically useful inferences for $\theta$ in many problems.
\end{itemize}

\subsection*{Exercises}

\begin{exercise}
\label{exer:6.5.1}
If $x_1, \ldots, x_n$ is a sample from an $N(\mu_0, \sigma^2)$ distribution, where $\mu_0$ is known and $\sigma^2 > 0$ is unknown, determine the Fisher information.
\end{exercise}

\begin{solution}
The score function for the $N(\mu_0, \sigma^2)$ family is given by $S(\sigma^2 \mid x_1, \ldots, x_n) = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^{n} (x_i - \mu_0)^2$. The Fisher information is then given by
\begin{align*}
    nI(\sigma^2) &= -\expc_{\sigma^2}\left( \frac{\partial}{\partial \sigma^2} S(\sigma^2 \mid X_1, \ldots, X_n) \right) = -\expc_{\sigma^2}\left( \frac{n}{2\sigma^4} - \frac{1}{\sigma^6} \sum_{i=1}^{n} (X_i - \mu_0)^2 \right) \\
    &= -\frac{n}{2\sigma^4} + \frac{1}{\sigma^6} \expc_{\sigma^2}\left( \sum_{i=1}^{n} (X_i - \mu_0)^2 \right) = -\frac{n}{2\sigma^4} + \frac{n\sigma^2}{\sigma^6} = \frac{n}{2\sigma^4}.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:6.5.2}
If $x_1, \ldots, x_n$ is a sample from a Gamma$(\alpha_0, \lambda)$ distribution, where $\alpha_0$ is known and $\lambda > 0$ is unknown, determine the Fisher information.
\end{exercise}

\begin{solution}
The score function for $\text{Gamma}(\alpha_0, \theta)$, where $\alpha_0$ is known, is given by $S(\theta \mid x_1, \ldots, x_n) = n\alpha_0/\theta - n\bar{x}$. The Fisher information is then given by $nI(\theta) = -\expc_\theta\left( \frac{\partial}{\partial \theta} S(\theta \mid X_1, \ldots, X_n) \right) = -\expc_\theta\left( -\frac{n\alpha_0}{\theta^2} \right) = \frac{n\alpha_0}{\theta^2}$.
\end{solution}

\begin{exercise}
\label{exer:6.5.3}
If $x_1, \ldots, x_n$ is a sample from a Pareto$(\beta)$ distribution (see Exercise~\ref{exer:6.2.9}), where $\beta > 0$ is unknown, determine the Fisher information.
\end{exercise}

\begin{solution}
The score function for $\text{Pareto}(\alpha)$ is given by $S(\alpha \mid x_1, \ldots, x_n) = n/\alpha - \sum_{i=1}^{n} \ln(1 + x_i)$. The Fisher information is then given by $nI(\alpha) = -\expc_\alpha\left( \frac{\partial}{\partial \alpha} S(\alpha \mid X_1, \ldots, X_n) \right) = -\expc_\alpha\left( -\frac{n}{\alpha^2} \right) = \frac{n}{\alpha^2}$.
\end{solution}

\begin{exercise}
\label{exer:6.5.4}
Suppose the number of calls arriving at an answering service during a given hour of the day is Poisson$(\lambda)$, where $\lambda > 0$ is unknown. The number of calls actually received during this hour was recorded for 20 days and the following data were obtained.
\begin{center}
\begin{tabular}{cccccccccc}
9 & 10 & 8 & 12 & 11 & 12 & 5 & 13 & 9 & 9 \\
7 & 5 & 16 & 13 & 9 & 5 & 13 & 8 & 9 & 10
\end{tabular}
\end{center}
Construct an approximate 0.95-confidence interval for $\lambda$. Assess the hypothesis that this is a sample from a Poisson$(11)$ distribution. If you are going to decide that the hypothesis is false when the P-value is less than 0.05, then compute an approximate power for this procedure when $\lambda = 10$.
\end{exercise}

\begin{solution}
An approximate 0.95-confidence interval for $\lambda$ in a Poisson model is given by (Example~\ref{ex:6.5.5}) $\bar{x} \pm (\bar{x}/n)^{1/2} z_{(1+\gamma)/2}$. The average number of calls per day is $\bar{x} = 9.650$. Therefore, the confidence interval is given by $(8.2885, 11.011)$. This contains the value $\lambda_0 = 11$, and therefore we don't have enough evidence against $H_0 : \lambda_0 = 11$ at the 5\% level.

An approximate power for this procedure when $\lambda = 10$ is given by
\begin{align*}
    &\prb_{10}\left( 2\left\{ 1 - \Phi\left( \sqrt{\frac{n}{\lambda_0}} |\bar{X} - \lambda_0| \right) \right\} < 0.05 \right) \\
    &= \prb_{10}\left( \Phi\left( \sqrt{\frac{20}{11}} |\bar{X} - 11| \right) > 0.975 \right) = \prb_{10}\left( |\bar{X} - 11| > \sqrt{\frac{11}{20}} z_{0.975} \right) \\
    &= \prb_{10}\left( (\bar{X} - 11) < -\sqrt{\frac{11}{20}} z_{0.975} \text{ or } (\bar{X} - 11) > \sqrt{\frac{11}{20}} z_{0.975} \right) \\
    &= \prb_{10}\left( (\bar{X} - 10)\sqrt{\frac{20}{10}} < \sqrt{\frac{20}{10}} - \sqrt{\frac{11}{10}} z_{0.975} \right) + \prb\left( (\bar{X} - 10)\sqrt{\frac{20}{10}} > \sqrt{\frac{20}{10}} + \sqrt{\frac{11}{10}} z_{0.975} \right) \\
    &\approx \prb(Z < -0.64145) + \prb(Z > 3.4699) = 0.26062 + 0.00026 = 0.26088.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:6.5.5}
Suppose the lifelengths in hours of lightbulbs from a manufacturing process are known to be distributed Gamma$(2, \lambda)$, where $\lambda > 0$ is unknown. A random sample of 27 bulbs was taken and their lifelengths measured with the following data obtained.
\begin{center}
\begin{tabular}{ccccccc}
336.87 & 2750.71 & 2199.44 & 292.99 & 1835.55 & 1385.36 & 2690.52 \\
710.64 & 2162.01 & 1856.47 & 2225.68 & 3524.23 & 2618.51 & 361.68 \\
979.54 & 2159.18 & 1908.94 & 1397.96 & 914.41 & 1548.48 & 1801.84 \\
1016.16 & 1666.71 & 1196.42 & 1225.68 & 2422.53 & 753.24 &
\end{tabular}
\end{center}
Determine an approximate 0.90-confidence interval for $\lambda$.
\end{exercise}

\begin{solution}
The score function for $\text{Gamma}(2, \theta)$ is given by $S(\theta \mid x_1, \ldots, x_n) = 2n/\theta - n\bar{x}$, so the MLE is $\hat{\theta} = 2/\bar{x} = 2/1627 = 1.2293 \times 10^{-3}$. The Fisher information is then given by
\[
    nI(\theta) = -\expc_\theta\left( \frac{\partial}{\partial \theta} S(\theta \mid X_1, \ldots, X_n) \right) = -\expc_\theta\left( -\frac{2n}{\theta^2} \right) = \frac{2n}{\theta^2}.
\]
By Corollary~\ref{cor:6.5.2} we have that
\[
    \sqrt{\frac{2n}{\hat{\theta}^2}} (\hat{\theta} - \theta) \xrightarrow{D} N(0, 1).
\]
Hence, an approximate 0.90-confidence interval is given by
\begin{align*}
    \frac{2}{\bar{x}} \pm \frac{1}{\sqrt{2n}} \left( \frac{2}{\bar{x}} \right) z_{0.95} &= (1.2293 \times 10^{-3}) \pm \frac{1}{\sqrt{54}} (1.2293 \times 10^{-3})(1.6449) \\
    &= (9.5413 \times 10^{-4}, 1.5045 \times 10^{-3}).
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:6.5.6}
Repeat the analysis of Exercise~\ref{exer:6.5.5}, but this time assume that the lifelengths are distributed Gamma$(1, \lambda)$. Comment on the differences in the two analyses.
\end{exercise}

\begin{solution}
The score function for $\text{Gamma}(1, \theta)$ is given by $S(\theta \mid x_1, \ldots, x_n) = n/\theta - n\bar{x}$, so the MLE is $\hat{\theta} = 1/\bar{x} = 1/1627 = 6.1463 \times 10^{-4}$. The Fisher information is then given by
\[
    nI(\theta) = -\expc_\theta\left( \frac{\partial}{\partial \theta} S(\theta \mid X_1, \ldots, X_n) \right) = -\expc_\theta\left( -\frac{n}{\theta^2} \right) = \frac{n}{\theta^2}.
\]
By Corollary~\ref{cor:6.5.2} we have that
\[
    \sqrt{\frac{n}{\hat{\theta}^2}} (\hat{\theta} - \theta) \xrightarrow{D} N(0, 1).
\]
Hence, an approximate 0.90-confidence interval is given by
\begin{align*}
    \frac{1}{\bar{x}} \pm \frac{1}{\sqrt{n}} \left( \frac{1}{\bar{x}} \right) z_{0.95} &= (6.1463 \times 10^{-4}) \pm \frac{1}{\sqrt{27}} (6.1463 \times 10^{-4})(1.6449) \\
    &= (4.2006 \times 10^{-4}, 8.0920 \times 10^{-4}).
\end{align*}
Note that this interval is shorter than the one in Exercise~\ref{exer:6.5.5} and is shifted to the left.
\end{solution}

\begin{exercise}
\label{exer:6.5.7}
Suppose that incomes (measured in thousands of dollars) above \$20K can be assumed to be Pareto$(\beta)$, where $\beta > 0$ is unknown, for a particular population. A sample of 20 is taken from the population and the following data obtained.
\begin{center}
\begin{tabular}{cccccccc}
21.265 & 20.857 & 21.090 & 20.047 & 20.019 & 32.509 & 21.622 & 20.693 \\
20.109 & 23.182 & 21.199 & 20.035 & 20.084 & 20.038 & 22.054 & 20.190 \\
20.488 & 20.456 & 20.066 & 20.302 & & & &
\end{tabular}
\end{center}
Construct an approximate 0.95-confidence interval for $\beta$. Assess the hypothesis that the mean income in this population is \$25K.
\end{exercise}

\begin{solution}
The score function for $\text{Pareto}(\alpha)$ is given by $S(\alpha \mid x_1, \ldots, x_n) = n/\alpha - \sum_{i=1}^{n} \ln(1 + x_i)$, so the MLE for $\alpha$ is
\[
    \hat{\alpha} = \frac{n}{\sum_{i=1}^{n} \ln(1 + x_i)}.
\]
Using the result of Exercise~\ref{exer:6.5.3} the Fisher information is $n/\alpha^2$. Note this is a continuous function of $\alpha \in (0, \infty)$. Hence, by Corollary~\ref{cor:6.5.2} an approximate 0.95-confidence interval is given by $\hat{\alpha} \pm (\hat{\alpha}/\sqrt{n}) z_{(1+\gamma)/2}$. Substituting $\hat{\alpha} = 0.322631$, $z_{0.975} = 1.96$, we obtain $(0.18123, 0.46403)$ as a 0.95-confidence interval.

The mean of the $\text{Pareto}(\alpha)$ distribution is $1/(\alpha - 1)$. Hence, assessing that the mean income in this population is \$25K is equivalent to assessing $\alpha = 1 + 1/25 = 1.04$. Since the 0.95-confidence interval does not contain this value, we have enough evidence against $H_0$ at the 5\% level to conclude that the mean income of this population is not \$25K.
\end{solution}

\begin{exercise}
\label{exer:6.5.8}
Suppose that $x_1, \ldots, x_n$ is a sample from an Exponential$(\lambda)$ distribution. Construct an approximate left-sided $\gamma$-confidence interval for $\lambda$. (See Problem~\ref{exer:6.3.25}.)
\end{exercise}

\begin{solution}
The score function for a sample from $\text{Exponential}(\theta)$ is given by $S(\theta \mid x_1, \ldots, x_n) = n/\theta - n\bar{x}$, so the MLE is $\hat{\theta} = 1/\bar{x}$. The Fisher information is given by $nI(\theta) = -\expc_\theta\left( -\frac{n}{\theta^2} \right) = \frac{n}{\theta^2}$. By Corollary~\ref{cor:6.5.2} we have that
\[
    \sqrt{\frac{n}{\theta^2}} (\hat{\theta} - \theta) \xrightarrow{D} N(0, 1).
\]
A left-sided $\gamma$-confidence interval for $\theta$ should satisfy $\prb_\theta(\theta \leqslant c(x_1, \ldots, x_n)) \geqslant \gamma$ for every $\theta > 0$. Using the same method as Problem~\ref{exer:6.3.25} we obtain the interval
\[
    \left( -\infty, \hat{\theta} + (nI(\hat{\theta}))^{-1/2} z_\gamma \right) = \left( -\infty, \frac{1}{\bar{x}} + \frac{1}{\bar{x}} \frac{z_\gamma}{\sqrt{n}} \right).
\]
\end{solution}

\begin{exercise}
\label{exer:6.5.9}
Suppose that $x_1, \ldots, x_n$ is a sample from a Geometric$(\theta)$ distribution. Construct an approximate left-sided $\gamma$-confidence interval for $\theta$. (See Problem~\ref{exer:6.3.25}.)
\end{exercise}

\begin{solution}
The likelihood function for the $\text{Geometric}(\theta)$ is $L(\theta \mid x) = \theta(1 - \theta)^x$. The score function is then given by $S(\theta \mid x) = \frac{1}{\theta} - \frac{x}{1 - \theta}$. The Fisher information is then
\[
    I(\theta) = -\expc_\theta\left( \frac{\partial}{\partial \theta} S(\theta \mid X) \right) = -\expc_\theta\left( -\frac{1}{\theta^2} - \frac{X}{(1 - \theta)^2} \right) = \frac{1}{\theta^2(1 - \theta)}
\]
since $\expc_\theta(X) = (1 - \theta)/\theta$.

The score function for a sample is then
\[
    S(\theta \mid x_1, \ldots, x_n) = \frac{n}{\theta} - \frac{\sum_{i=1}^{n} x_i}{1 - \theta} = \frac{n}{\theta} - \frac{n\bar{x}}{1 - \theta},
\]
so the MLE for $\theta$ in this model is $\hat{\theta} = 1/(1 + \bar{x})$ and the Fisher information is given by $n/(\theta^2(1 - \theta))$. A left-sided $\gamma$-confidence interval for $\theta$ should satisfy $\prb_\theta(\theta < c(X_1, \ldots, X_n)) \geqslant \gamma$ for every $\theta \in [0, 1]$. Using the same method as in Exercise~\ref{exer:6.3.17} we obtain the interval
\[
    \left[ 0, \min\left( \hat{\theta} + (nI(\hat{\theta}))^{-1/2} z_\gamma, 1 \right) \right] = \left[ 0, \min\left( \frac{1}{1 + \bar{x}} + \frac{1}{\sqrt{n}} \frac{1}{1 + \bar{x}} \sqrt{\frac{\bar{x}}{1 + \bar{x}}} z_\gamma, 1 \right) \right].
\]
\end{solution}

\begin{exercise}
\label{exer:6.5.10}
Suppose that $x_1, \ldots, x_n$ is a sample from a Negative-Binomial$(r, \theta)$ distribution. Construct an approximate left-sided $\gamma$-confidence interval for $\theta$. (See Problem~\ref{exer:6.3.25}.)
\end{exercise}

\begin{solution}
The likelihood function for the $\text{Negative-Binomial}(r, \theta)$ family is given by (from Example~\ref{ex:2.3.5}) $L(\theta \mid x) = \binom{r - 1 + x}{x} \theta^r (1 - \theta)^x$. The score function is given by $S(\theta \mid x) = \frac{r}{\theta} - \frac{x}{1 - \theta}$ and the Fisher information is given by
\[
    I(\theta) = -\expc_\theta\left( \frac{\partial}{\partial \theta} S(\theta \mid X) \right) = -\expc_\theta\left( -\frac{r}{\theta^2} - \frac{X}{(1 - \theta)^2} \right) = \frac{r}{\theta^2(1 - \theta)}
\]
since $\expc_\theta(X) = r(1 - \theta)/\theta$. The score function for a sample is given by
\[
    S(\theta \mid x_1, \ldots, x_n) = \frac{rn}{\theta} - \frac{n\bar{x}}{1 - \theta},
\]
so the MLE for $\theta$ in this model is $\hat{\theta} = r/(r + \bar{x})$.

A left-sided $\gamma$-confidence interval for $\theta$ should satisfy $\prb_\theta(\theta < c(X_1, \ldots, X_n)) \geqslant \gamma$ for every $\theta$. Using the same method as in Problem~\ref{exer:6.3.25} we obtain the following interval
\[
    \left( -\infty, \hat{\theta} + (nI(\hat{\theta}))^{-1/2} z_\gamma \right) = \left( -\infty, \frac{r}{r + \bar{x}} + \frac{1}{\sqrt{nr}} \frac{r}{r + \bar{x}} \sqrt{\frac{\bar{x}}{r + \bar{x}}} z_\gamma \right).
\]
\end{solution}

\subsection*{Problems}

\begin{exercise}
\label{exer:6.5.11}
In Exercise~\ref{exer:6.5.1}, verify that \eqref{eq:6.5.2}, \eqref{eq:6.5.3}, \eqref{eq:6.5.4}, and \eqref{eq:6.5.5} are satisfied.
\end{exercise}

\begin{solution}
Conditions~\eqref{eq:6.5.2}, \eqref{eq:6.5.3}, \eqref{eq:6.5.4}, and~\eqref{eq:6.5.5} require that
\[
    \frac{\partial^2 \ln f_\theta(x)}{\partial \theta^2} \text{ exists for each } x, \quad \expc_\theta(S(\theta \mid s)) = 0, \quad \expc_\theta\left( \frac{\partial^2 \ln f_\theta(X)}{\partial \theta^2} + S^2(\theta \mid X) \right) = 0, \quad \expc_\theta\left( \left| \frac{\partial^2 \ln f_\theta(X)}{\partial \theta^2} \right| \right) < \infty.
\]

We have
\[
    \frac{\partial^2 \ln f_{\sigma^2}(x)}{\partial \sigma^2} = \frac{1}{2\sigma^4} - \frac{(x - \mu_0)^2}{\sigma^6}
\]
and this exists for each $x$. Also,
\[
    \expc_{\sigma^2}(S(\theta \mid X)) = \expc_{\sigma^2}\left( -\frac{1}{2\sigma^2} + \frac{(X - \mu_0)^2}{2\sigma^4} \right) = -\frac{1}{2\sigma^2} + \frac{\sigma^2}{2\sigma^4} = 0
\]
and
\begin{align*}
    &\expc_{\sigma^2}\left( \frac{\partial^2 \ln f_{\sigma^2}(X)}{\partial \sigma^2} + S^2(\sigma^2 \mid X) \right) \\
    &= \expc_{\sigma^2}\left( \frac{1}{2\sigma^4} - \frac{(X - \mu_0)^2}{\sigma^6} \right) + \expc_{\sigma^2}\left( \left( -\frac{1}{2\sigma^2} + \frac{(X - \mu_0)^2}{2\sigma^4} \right)^2 \right) \\
    &= \frac{1}{2\sigma^4} - \frac{1}{\sigma^4} + \frac{1}{4\sigma^4} - \frac{2}{2\sigma^2} \expc_{\sigma^2}\left( \frac{(X - \mu_0)^2}{2\sigma^4} \right) + \frac{1}{4\sigma^4} \expc_{\sigma^2}\left( \left( \frac{X - \mu_0}{\sigma} \right)^4 \right) \\
    &= -\frac{3}{4\sigma^4} + \frac{1}{4\sigma^4} \expc(Z^4) = -\frac{3}{4\sigma^4} + \frac{3}{4\sigma^4} = 0
\end{align*}
since $\expc(Z^4) = 3$ for $Z \sim N(0, 1)$. Finally, by the triangular inequality and monotonicity of expected values we have
\begin{align*}
    \expc_{\sigma^2}\left( \left| \frac{\partial^2 \ln f_{\sigma^2}(X)}{\partial \sigma^2} \right| \right) &= \expc_\theta\left( \left| \frac{1}{2\sigma^4} - \frac{(X - \mu_0)^2}{\sigma^6} \right| \right) \\
    &\leqslant \expc_\theta\left( \left| \frac{1}{2\sigma^4} \right| + \left| \frac{(X - \mu_0)^2}{\sigma^6} \right| \right) = \frac{1}{2(\sigma^2)^2} + \frac{\sigma^2}{(\sigma^2)^3} = \frac{3}{2\sigma^4} < \infty.
\end{align*}
\end{solution}

\begin{exercise}
\label{exer:6.5.12}
In Exercise~\ref{exer:6.5.2}, verify that \eqref{eq:6.5.2}, \eqref{eq:6.5.3}, \eqref{eq:6.5.4}, and \eqref{eq:6.5.5} are satisfied.
\end{exercise}

\begin{solution}
In Exercise~\ref{exer:6.5.2} we have $\frac{\partial^2 \ln f_\theta(x)}{\partial \theta^2} = -\frac{\alpha_0}{\theta^2}$ and this exists for each $x$. Also, $\expc_\theta(S(\theta \mid X)) = \expc_\theta\left( \frac{\alpha_0}{\theta} - X \right) = \frac{\alpha_0}{\theta} - \frac{\alpha_0}{\theta} = 0$ and
\begin{align*}
    \expc_\theta\left( \frac{\partial^2 \ln f_\theta(X)}{\partial \theta^2} + S^2(\theta \mid X) \right) &= \expc_\theta\left( -\frac{\alpha_0}{\theta^2} \right) + \expc_\theta\left( \left( \frac{\alpha_0}{\theta} - X \right)^2 \right) \\
    &= -\frac{\alpha_0}{\theta^2} + \frac{\alpha_0^2}{\theta^2} + \expc_\theta(X^2) = -\frac{\alpha_0}{\theta^2} + \frac{\alpha_0^2}{\theta^2} + \frac{\alpha_0(\alpha_0 + 1)}{\theta^2} = 0.
\end{align*}
Finally we have that
\[
    \expc_\theta\left( \left| \frac{\partial^2 \ln f_\theta(s)}{\partial \theta^2} \right| \right) = \expc_\theta\left( \left| -\frac{\alpha_0}{\theta^2} \right| \right) = \frac{\alpha_0}{\theta^2}.
\]
\end{solution}

\begin{exercise}
\label{exer:6.5.13}
In Exercise~\ref{exer:6.5.3}, verify that \eqref{eq:6.5.2}, \eqref{eq:6.5.3}, \eqref{eq:6.5.4}, and \eqref{eq:6.5.5} are satisfied.
\end{exercise}

\begin{solution}
In Exercise~\ref{exer:6.5.3} we have $\frac{\partial^2 \ln f_\alpha(x)}{\partial \alpha^2} = -\frac{1}{\alpha^2}$ and this exists for each $x$. Also, since $\ln(1 + X) \sim \text{Exponential}(\alpha)$ we have
\[
    \expc_\alpha(S(\theta \mid X)) = \expc_\theta\left( \frac{1}{\alpha} - \ln(1 + X) \right) = \frac{1}{\alpha} - \frac{1}{\alpha} = 0,
\]
and
\begin{align*}
    \expc_\alpha\left( \frac{\partial^2 \ln f_\alpha(X)}{\partial \alpha^2} + S^2(\alpha \mid X) \right) &= \expc_\alpha\left( -\frac{1}{\alpha^2} + \left( \frac{1}{\alpha} - \ln(1 + X) \right)^2 \right) \\
    &= -\frac{2}{\alpha^2} + \expc_\alpha\left( (\ln(1 + X))^2 \right) = -\frac{2}{\alpha^2} + \frac{2}{\alpha^2} = 0.
\end{align*}
Finally, we have
\[
    \expc_\alpha\left( \left| \frac{\partial^2 \ln f_\theta(s)}{\partial \theta^2} \right| \right) = \expc_\alpha\left( \left| -\frac{1}{\alpha^2} \right| \right) = \frac{1}{\alpha^2} < \infty.
\]
\end{solution}

\begin{exercise}
\label{exer:6.5.14}
Suppose that sampling from the model $\{f_\theta : \theta \in \Omega\}$ satisfies \eqref{eq:6.5.2}, \eqref{eq:6.5.3}, \eqref{eq:6.5.4}, and \eqref{eq:6.5.5}. Prove that $n^{-1}\hat{I} \xrightarrow{a.s.} I(\theta)$ as $n \to \infty$.
\end{exercise}

\begin{solution}
Under i.i.d.\ sampling from $f_\theta$, where the model $\{f_\theta : \theta \in \Omega\}$ satisfies the appropriate conditions, we have
\[
    \hat{I}(x_1, \ldots, x_n)|_{\theta = \hat{\theta}} = \left. -\frac{\partial^2}{\partial \theta^2} \sum_{i=1}^{n} \ln f_\theta(x_i) \right|_{\theta = \hat{\theta}} = \sum_{i=1}^{n} \left. \left( -\frac{\partial^2 \ln f_\theta(x_i)}{\partial \theta^2} \right) \right|_{\theta = \hat{\theta}}.
\]
By the strong law of large numbers (Theorem~\ref{thm:4.3.2}) we have
\[
    \frac{\hat{I}(x_1, \ldots, x_n)}{n} \xrightarrow{\text{a.s.}} \expc_\theta\left( -\frac{\partial^2 \ln f_\theta(x_i)}{\partial \theta^2} \right) = I(\theta).
\]
\end{solution}

\begin{exercise}[(MV)]
\label{exer:6.5.15}
When $\theta = (\theta_1, \theta_2)$, then, under appropriate regularity conditions for the model $\{f_\theta : \theta \in \Omega\}$, the Fisher information matrix is defined by
\[
I(\theta) = \begin{pmatrix}
-\expc_\theta\left[\dfrac{\partial^2}{\partial \theta_1^2} l_\theta(X)\right] & -\expc_\theta\left[\dfrac{\partial^2}{\partial \theta_1 \partial \theta_2} l_\theta(X)\right] \\[2ex]
-\expc_\theta\left[\dfrac{\partial^2}{\partial \theta_1 \partial \theta_2} l_\theta(X)\right] & -\expc_\theta\left[\dfrac{\partial^2}{\partial \theta_2^2} l_\theta(X)\right]
\end{pmatrix}.
\]
If $(X_1, X_2, X_3) \sim \text{Multinomial}(1, (\theta_1, \theta_2, \theta_3))$ (Example~\ref{ex:6.1.5}), then determine the Fisher information for this model. Recall that $\theta_3 = 1 - \theta_1 - \theta_2$ and so $\theta$ is determined from $(\theta_1, \theta_2)$.
\end{exercise}

\begin{solution}
Recall that the likelihood function is given by $L(\theta_1, \theta_2 \mid x_1, x_2, x_3) = \theta_1^{x_1} \theta_2^{x_2} (1 - \theta_1 - \theta_2)^{x_3}$. The log-likelihood function is then given by
\[
    l(\theta_1, \theta_2 \mid x_1, x_2, x_3) = x_1 \ln \theta_1 + x_2 \ln \theta_2 + x_3 \ln(1 - \theta_1 - \theta_2).
\]
Using the methods discussed in Section \ref{ssec:6.2.1} we obtain the score function as
\[
    S(\theta_1, \theta_2 \mid x) = \begin{pmatrix} \frac{x_1}{\theta_1} - \frac{x_3}{1 - \theta_1 - \theta_2} \\ \frac{x_2}{\theta_2} - \frac{x_3}{1 - \theta_1 - \theta_2} \end{pmatrix}.
\]
The Fisher information is then given by
\[
    I(\theta) = -\expc_\theta \begin{pmatrix} -\frac{X_1}{\theta_1^2} - \frac{X_3}{(1 - \theta_1 - \theta_2)^2} & -\frac{X_3}{(1 - \theta_1 - \theta_2)^2} \\ -\frac{X_3}{(1 - \theta_1 - \theta_2)^2} & -\frac{X_2}{\theta_2^2} - \frac{X_3}{(1 - \theta_1 - \theta_2)^2} \end{pmatrix}.
\]
Now $X_i \sim \text{Binomial}(n, \theta_i)$ and so $\expc_{(\theta_1, \theta_2)}(X_i) = n\theta_i$. Therefore,
\[
    I(\theta) = n \begin{pmatrix} \frac{\theta_1}{\theta_1^2} + \frac{\theta_3}{(1 - \theta_1 - \theta_2)^2} & \frac{\theta_3}{(1 - \theta_1 - \theta_2)^2} \\ \frac{\theta_3}{(1 - \theta_1 - \theta_2)^2} & \frac{\theta_2}{\theta_2^2} + \frac{\theta_3}{(1 - \theta_1 - \theta_2)^2} \end{pmatrix} = n \begin{pmatrix} \frac{1}{\theta_1} + \frac{1}{\theta_3} & \frac{1}{\theta_3} \\ \frac{1}{\theta_3} & \frac{1}{\theta_2} + \frac{1}{\theta_3} \end{pmatrix}.
\]
\end{solution}

\begin{exercise}[(MV)]
\label{exer:6.5.16}
Generalize Problem~\ref{exer:6.5.15} to the case where
\[
(X_1, \ldots, X_k) \sim \text{Multinomial}(1, (\theta_1, \ldots, \theta_k)).
\]
\end{exercise}

\begin{solution}
The likelihood function is given by $L(\theta_1, \ldots, \theta_{k-1} \mid x_1, \ldots, x_k) = \theta_1^{x_1} \theta_2^{x_2} \cdots (1 - \theta_1 - \cdots - \theta_{k-1})^{x_k}$. The log-likelihood function is then given by $l(\theta_1, \ldots, \theta_{k-1} \mid x_1, \ldots, x_k) = x_1 \ln \theta_1 + x_2 \ln \theta_2 + \cdots + x_k \ln(1 - \theta_1 - \cdots - \theta_{k-1})$. Using the methods discussed in Section \ref{ssec:6.2.1} we obtain the score function as
\[
    S(\theta_1, \ldots, \theta_{k-1} \mid x_1, \ldots, x_k) = \begin{pmatrix} \frac{x_1}{\theta_1} - \frac{x_k}{1 - \theta_1 - \cdots - \theta_{k-1}} \\ \vdots \\ \frac{x_{k-1}}{\theta_{k-1}} - \frac{x_k}{1 - \theta_1 - \cdots - \theta_{k-1}} \end{pmatrix}.
\]
The Fisher information is then given by
\begin{align*}
    \indc_{ij}(\theta) &= \frac{\expc_{(\theta_1, \ldots, \theta_{k-1})}(X_k)}{(1 - \theta_1 - \cdots - \theta_{k-1})^2} \text{ when } i \neq j, \\
    \indc_{ii}(\theta) &= \frac{\expc_{(\theta_1, \ldots, \theta_{k-1})}(X_i)}{\theta_1^2} + \frac{\expc_{(\theta_1, \ldots, \theta_{k-1})}(X_k)}{(1 - \theta_1 - \cdots - \theta_{k-1})^2}.
\end{align*}
Now $X_i \sim \text{Binomial}(n, \theta_i)$ and so $\expc_{(\theta_1, \ldots, \theta_{k-1})}(X_i) = n\theta_i$. Therefore
\[
    \indc_{ij}(\theta) = \frac{n}{\theta_k} \text{ when } i \neq j, \quad \indc_{ii}(\theta) = \frac{n}{\theta_i} + \frac{n}{\theta_k}.
\]
\end{solution}

\begin{exercise}[(MV)]
\label{exer:6.5.17}
Using the definition of the Fisher information matrix in Exercise~\ref{exer:6.5.15}, determine the Fisher information for the Bivariate Normal$(\mu_1, \mu_2, 1, 1, 0)$ model, where $(\mu_1, \mu_2) \in R^1 \times R^1$ are unknown.
\end{exercise}

\begin{solution}
The likelihood function is given by (see Example~\ref{ex:2.7.8}) $L(\mu_1, \mu_2 \mid x_1, x_2) = \frac{1}{2\pi} \exp\left\{ -\frac{1}{2}\left( (x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 \right) \right\}$. The log-likelihood function is then given by $l(\mu_1, \mu_2 \mid x_1, x_2) = -\ln(2\pi) - \frac{1}{2}\left\{ (x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 \right\}$. Using the methods discussed in Section \ref{ssec:6.2.1} we obtain the score function as
\[
    S(\mu_1, \mu_2 \mid x_1, x_2) = \begin{pmatrix} \frac{\partial l(\mu_1, \mu_2 \mid x_1, x_2)}{\partial \mu_1} \\ \frac{\partial l(\mu_1, \mu_2 \mid x_1, x_2)}{\partial \mu_2} \end{pmatrix} = \begin{pmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{pmatrix}.
\]
The Fisher information matrix is then given by
\[
    I(\theta) = \begin{pmatrix} \expc_{(\mu_1, \mu_2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2 \mid x_1, x_2)}{\partial \mu_1^2} \right) & \expc_{(\mu_1, \mu_2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2 \mid x_1, x_2)}{\partial \mu_1 \partial \mu_2} \right) \\ \expc_{(\mu_1, \mu_2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2 \mid x_1, x_2)}{\partial \mu_1 \partial \mu_2} \right) & \expc_{(\mu_1, \mu_2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2 \mid x_1, x_2)}{\partial \mu_2^2} \right) \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = I.
\]
\end{solution}

\begin{exercise}[(MV)]
\label{exer:6.5.18}
Extending the definition in Exercise~\ref{exer:6.5.15} to the three-dimensional case, determine the Fisher information for the Bivariate Normal$(\mu_1, \mu_2, \sigma^2, \sigma^2, 0)$ model where $(\mu_1, \mu_2) \in R^1 \times R^1$ and $\sigma^2 > 0$ are unknown.
\end{exercise}

\begin{solution}
The likelihood function is given by (see Example~\ref{ex:2.7.8}) $L(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2) = \frac{1}{2\pi\sigma^2} \exp\left\{ -\frac{1}{2\sigma^2}\left( (x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 \right) \right\}$. The log-likelihood function is then given by $l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2) = -\ln(2\pi) - \ln(\sigma^2) - \frac{1}{2\sigma^2}\left\{ (x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 \right\}$. Using the methods discussed in Section \ref{ssec:6.2.1} we obtain the score function as
\[
    S(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2) = \begin{pmatrix} \frac{\partial l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial \mu_1} \\ \frac{\partial l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial \mu_2} \\ \frac{\partial l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial \sigma^2} \end{pmatrix} = \begin{pmatrix} \frac{x_1 - \mu_1}{\sigma^2} \\ \frac{x_2 - \mu_2}{\sigma^2} \\ -\frac{1}{\sigma^2} + \frac{1}{2\sigma^4}\left\{ (x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 \right\} \end{pmatrix}.
\]
The Fisher information matrix is then given by
\begin{align*}
    (I(\theta))_{11} &= \expc_{(\mu_1, \mu_2, \sigma^2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial \mu_1^2} \right) = \expc_{(\mu_1, \mu_2, \sigma^2)}\left( \frac{1}{\sigma^2} \right) = \frac{1}{\sigma^2} \\
    (I(\theta))_{12} &= \expc_{(\mu_1, \mu_2, \sigma^2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial \mu_1 \partial \mu_2} \right) = 0 \\
    (I(\theta))_{13} &= \expc_{(\mu_1, \mu_2, \sigma^2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial \mu_1 \partial \sigma^2} \right) = \expc_{(\mu_1, \mu_2, \sigma^2)}\left( \frac{X_1 - \mu_1}{\sigma^4} \right) = 0 \\
    (I(\theta))_{22} &= \expc_{(\mu_1, \mu_2, \sigma^2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial \mu_2^2} \right) = \expc_{(\mu_1, \mu_2, \sigma^2)}\left( \frac{1}{\sigma^2} \right) = \frac{1}{\sigma^2} \\
    (I(\theta))_{23} &= \expc_{(\mu_1, \mu_2, \sigma^2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial \mu_2 \partial \sigma^2} \right) = \expc_{(\mu_1, \mu_2, \sigma^2)}\left( \frac{X_2 - \mu_2}{\sigma^4} \right) = 0 \\
    (I(\theta))_{33} &= \expc_{(\mu_1, \mu_2, \sigma^2)}\left( -\frac{\partial^2 l(\mu_1, \mu_2, \sigma^2 \mid x_1, x_2)}{\partial(\sigma^2)^2} \right) \\
    &= \expc_{(\mu_1, \mu_2, \sigma^2)}\left( -\frac{1}{\sigma^4} + \frac{1}{\sigma^6}\left\{ (X_1 - \mu_1)^2 + (X_2 - \mu_2)^2 \right\} \right) = \frac{1}{\sigma^4}
\end{align*}
and the remaining elements follow by symmetry.
\end{solution}

\subsection*{Challenges}

\begin{exercise}
\label{exer:6.5.19}
Suppose that model $\{f_\theta : \theta \in \Omega\}$ satisfies \eqref{eq:6.5.2}, \eqref{eq:6.5.3}, \eqref{eq:6.5.4}, \eqref{eq:6.5.5}, and has Fisher information $I(\theta)$. If $\psi : \Omega \to R^1$ is 1--1, and $\psi$ and $\psi^{-1}$ are continuously differentiable, then, putting $\Psi = \{\psi(\theta) : \theta \in \Omega\}$, prove that the model given by $\{g_\phi : \phi \in \Psi\}$ satisfies the regularity conditions and that its Fisher information at $\phi = \psi(\theta)$ is given by $I(\theta)((\psi^{-1})'(\phi))^2$.
\end{exercise}

\begin{solution}
Since $\Psi$ is a 1--1 function of $\theta$, for each $\psi \in \Psi$ there is a unique $\theta \in \Omega$ such that $\Psi(\theta) = \psi$. Therefore, we can write the model as $\{g_\psi : \psi \in \Psi\}$, where $g_\psi = f_{\Psi^{-1}(\psi)}$.

Now, using the chain rule, we have that
\begin{align*}
    \frac{\partial \ln g_\psi(X)}{\partial \psi} &= \frac{\partial \ln f_{\Psi^{-1}(\psi)}(X)}{\partial \psi} = \frac{\partial \ln f_{\Psi^{-1}(\psi)}(X)}{\partial \theta} \frac{\partial \Psi^{-1}(\psi)}{\partial \psi} \\
    \frac{\partial^2 \ln g_\psi(X)}{\partial \psi^2} &= \frac{\partial^2 \ln f_{\Psi^{-1}(\psi)}(X)}{\partial \theta^2} \left( \frac{\partial \Psi^{-1}(\psi)}{\partial \psi} \right)^2 + \frac{\partial \ln f_{\Psi^{-1}(\psi)}(X)}{\partial \theta} \frac{\partial^2 \Psi^{-1}(\psi)}{\partial \psi^2}.
\end{align*}
Therefore, the Fisher information in the new parameterization is given by
\begin{align*}
    I^*(\psi) &= \expc_\psi\left( -\frac{\partial^2 \ln g_\psi(X)}{\partial \psi^2} \right) \\
    &= \expc_{\Psi^{-1}(\psi)}\left( -\frac{\partial^2 \ln f_{\Psi^{-1}(\psi)}(X)}{\partial \theta^2} \left( \frac{\partial \Psi^{-1}(\psi)}{\partial \psi} \right)^2 - \frac{\partial \ln f_{\Psi^{-1}(\psi)}(X)}{\partial \theta} \frac{\partial^2 \Psi^{-1}(\psi)}{\partial \psi^2} \right) \\
    &= \expc_{\Psi^{-1}(\psi)}\left( -\frac{\partial^2 \ln f_{\Psi^{-1}(\psi)}(X)}{\partial \theta^2} \right) \left( \frac{\partial \Psi^{-1}(\psi)}{\partial \psi} \right)^2 - \expc_{\Psi^{-1}(\psi)}\left( \frac{\partial \ln f_{\Psi^{-1}(\psi)}(X)}{\partial \theta} \right) \frac{\partial^2 \Psi^{-1}(\psi)}{\partial \psi^2} \\
    &= \expc_\theta\left( -\frac{\partial^2 \ln f_\theta(X)}{\partial \theta^2} \right) \left( \frac{\partial \Psi^{-1}(\psi)}{\partial \psi} \right)^2 - \expc_\theta\left( \frac{\partial \ln f_\theta(X)}{\partial \theta} \right) \frac{\partial^2 \Psi^{-1}(\psi)}{\partial \psi^2} \\
    &= I(\theta) \left( \frac{\partial \Psi^{-1}(\psi)}{\partial \psi} \right)^2 = I(\Psi^{-1}(\psi)) \left( \frac{\partial \Psi^{-1}(\psi)}{\partial \psi} \right)^2
\end{align*}
since $\expc_\theta\left( \frac{\partial \ln f_\theta(X)}{\partial \theta} \right) = 0$.
\end{solution}

\subsection*{Discussion Topics}

\begin{exercise}
\label{exer:6.5.20}
The method of moments inference methods discussed in Section~\ref{ssec:6.4.1} are essentially large sample methods based on the central limit theorem. The large sample methods in Section~\ref{sec:6.5} are based on the form of the likelihood function. Which methods do you think are more likely to be correct when we know very little about the form of the distribution from which we are sampling? In what sense will your choice be ``more correct''?
\end{exercise}

